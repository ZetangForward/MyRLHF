nohup: 忽略输入
2025-01-22 02:56:22.284 | INFO     | test_jbb_retain:<module>:7 - ['/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis', '/data/anaconda3/envs/zecheng/lib/python310.zip', '/data/anaconda3/envs/zecheng/lib/python3.10', '/data/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/root/.local/lib/python3.10/site-packages', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages', '__editable__.lm_eval-0.4.3.finder.__path_hook__', '__editable__.trl-0.8.7.dev0.finder.__path_hook__', '/data/zecheng/modelzipper/src', '/tmp/tmpn5ue93yf', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 02:56:22.943 | INFO     | __main__:<module>:11 - ['/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/faiss_attn', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis', '/data/anaconda3/envs/zecheng/lib/python310.zip', '/data/anaconda3/envs/zecheng/lib/python3.10', '/data/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/root/.local/lib/python3.10/site-packages', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages', '__editable__.lm_eval-0.4.3.finder.__path_hook__', '__editable__.trl-0.8.7.dev0.finder.__path_hook__', '/data/zecheng/modelzipper/src', '/tmp/tmpn5ue93yf', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 02:56:23.288 | INFO     | __main__:<module>:72 - Selected idx: 0
2025-01-22 02:56:23.288 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 02:56:23.288 | INFO     | __main__:<module>:74 - Answer: office
2025-01-22 02:56:23.288 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 02:56:23.289 | INFO     | __main__:<module>:76 - Needle: [' John went back to the bedroom.', ' John took the milk.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 02:56:23.289 | INFO     | __main__:<module>:77 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 02:56:23.289 | INFO     | __main__:<module>:78 - =============================================
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-01-22 06:56:17
Pid: 46839
begin to read data from /data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/haystack_for_detect/reasoning_needle_jbb_200.jsonl | file size: 247.16 KB | file type: jsonl
  0%|          | 0/100 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.37it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.39it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]
Processing depth (3, 5, 8, 9):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-22 02:56:33.751 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:56:33.763 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3850, 3856) --> . Mary journeyed to the
2025-01-22 02:56:33.763 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:56:33.774 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3850, 3856) --> . Mary journeyed to the
2025-01-22 02:56:33.774 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:56:33.800 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9643, 9647) -->  dropped the football.
2025-01-22 02:56:33.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 02:56:33.831 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10770, 10776) --> . Daniel went back to the
2025-01-22 02:56:33.831 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:56:33.832 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (67, 73) --> . John went back to the
2025-01-22 02:56:33.832 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John took the milk.
2025-01-22 02:56:33.838 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2272, 2276) -->  took the milk.
2025-01-22 02:56:33.838 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 02:56:33.841 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (939, 945) --> . Sandra journeyed to the
2025-01-22 02:56:33.841 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:56:33.852 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3851, 3857) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-22 02:56:37.822 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 02:56:37.822 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12196])
your chose emoji: ['🤸🏻\u200d♂', '👩🏾\u200d❤️\u200d👩🏾', '🇸🇬', '🏄🏾\u200d♀', '🥎', '🃏', '♻', '🥬', '🤽🏾\u200d♂️', '🔂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 198546.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  6.12it/s][A
 25%|██▌       | 2/8 [00:00<00:00,  6.16it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.17it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.17it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.20it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.22it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.24it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.24it/s][A100%|██████████| 8/8 [00:01<00:00,  6.21it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  5.87it/s][A
 25%|██▌       | 2/8 [00:00<00:00,  6.03it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.09it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.12it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.13it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.13it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.15it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.17it/s][A100%|██████████| 8/8 [00:01<00:00,  6.12it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  5.61it/s][A
 25%|██▌       | 2/8 [00:00<00:01,  5.93it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.01it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.06it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.11it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.14it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.17it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.19it/s][A100%|██████████| 8/8 [00:01<00:00,  6.10it/s]
2025-01-22 02:56:50.080 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3878999189897017, 0.7271037004017052, 0.5709797252308239, 0.7280002949915376, 0.7316414969308036], 'topk_tokens': ['irie', ' Minneapolis', ' the', ' from', ',', ' the', ' glance', ' whistle', ' the', ' the', 'from', ' the', ' cont', ' Minnesota', ' marched', ' Minnesota', ' the', 'Minnesota', ' Min', ' Minnesota'], 'evidence_proportions': [0.3926595052083333, 0.3926595052083333, 0.350250244140625, 0.40348052978515625]}, 'weight': {'score': [0.08384545011953874, 0.0025609384346915224, 0.010570929809050127, 0.0023993061270312847, 0.0019763518893529497], 'topk_tokens': [' the', '.', ' the', ' \n', ' football', ' kitchen', '.', 'Answer', '.', '<|eot_id|>', ' bathroom', '<|eot_id|>', '<|start_header_id|>', 'assistant', ' bathroom', '<|end_header_id|>', ':', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.022333979606628418, 0.022333979606628418, 0.36887359619140625, 0.016849627097447712]}, 'saliency': {'score': [0.01072337952527133, 8.577140633601364e-05, 0.0020680075342004948, 6.292817995543147e-05, 0.00010106771711319212], 'topk_tokens': ['Question', '.', ' bathroom', ' the', '.', '<|eot_id|>', ':', ' random', ' dropped', ' Mary', '<|end_header_id|>', '.', '.', '<|eot_id|>', ' bathroom', ' office', ' kitchen', ' office', ' football', 'office'], 'evidence_proportions': [0.0020123918851216636, 0.0020123918851216636, 0.05191326141357422, 0.000685433546702067]}}, 25: {'grad': {'score': [0.6856724132191051, 0.5285024472505738, 0.706998651677912, 0.5278948566837885, 0.5155000232514881], 'topk_tokens': [' location', ' office', ' Mary', '186', ' office', 'op', '186', ' location', ' Robert', ' location', ' Ot', 'ation', ' news', ' S', ' news', '186', ' news', ' actions', ' Key', ' Wood'], 'evidence_proportions': [0.7333984375, 0.7333984375, 0.904510498046875, 0.44432830810546875]}, 'weight': {'score': [0.0245350951498205, 0.0025480578113411425, 0.0027517080307006836, 0.002507890366362333, 0.0020373633929661344], 'topk_tokens': [' random', ' the', ' directly', '.', ' Miss', ' Mary', '.\n\n', ' \n', '.', 'Answer', '<|start_header_id|>', ' football', '<|eot_id|>', '<|eot_id|>', 'assistant', '.', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.008061279853185018, 0.008061279853185018, 0.0974273681640625, 0.008887877066930136]}, 'saliency': {'score': [0.001145314086567272, 5.557537469771636e-05, 0.00014120069417086515, 5.344784395149787e-05, 0.00017860768333313956], 'topk_tokens': [' People', ' return', ' Miss', ' dropped', ' \n', ' Min', ' Az', '.', '�', 'assistant', ' random', ' directly', 'Answer', ' Miss', ':', '<|eot_id|>', '<|eot_id|>', '.', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0004419982433319092, 0.0004419982433319092, 0.004427671432495117, 0.0003637075424194336]}}, 26: {'grad': {'score': [0.7063820578835227, 0.6728341095876373, 1.0755115855823865, 0.6720444963463674, 0.6196705651661706], 'topk_tokens': [' dispatch', 'str', ' broad', ' Col', ' black', ' generally', ' Col', ' wh', ' worth', ' sm', ' Gen', ' Guards', ' Capt', ' generally', ' Merch', 'burn', ' STR', 'ol', 'BO', ' str'], 'evidence_proportions': [0.8681844075520834, 0.8681844075520834, 0.183685302734375, 0.7312418619791666]}, 'weight': {'score': [0.019835071130232376, 0.0025260166606662273, 0.0017684427174654875, 0.002496056765028574, 0.0013721348747374519], 'topk_tokens': [' the', '.', ' before', '?', ' football', '.\n\n', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' football', ' \n', ' kitchen', ' the', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.005336453517278035, 0.005336453517278035, 0.07402515411376953, 0.012705584367116293]}, 'saliency': {'score': [0.00046204707839272237, 9.763620000996538e-05, 8.387186310508035e-05, 9.700149341851292e-05, 9.233705581180633e-05], 'topk_tokens': [' Seventh', ' Mary', 'Answer', '.', ' Bridge', 'assistant', ' North', ' Bridge', ' bathroom', 'Bridge', ' \n', ' Col', ' bathroom', '<|start_header_id|>', ' kitchen', ':', ' Az', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00014147162437438965, 0.00014147162437438965, 0.0010652542114257812, 0.000701059897740682]}}, 27: {'grad': {'score': [1.2825650301846592, 0.7209618764346615, 0.8162675337357954, 0.7197728033852024, 0.43985058012462797], 'topk_tokens': [',', '\n', '.', '.', ' duration', 'bread', ' down', ' per', ' as', ' accepted', 'ile', ' for', 'ab', 'arch', '.', 'isc', 'sur', ' later', 'roduced', 'sur'], 'evidence_proportions': [1.57470703125, 1.57470703125, 0.8221435546875, 1.0052286783854167]}, 'weight': {'score': [0.02020265297456221, 0.002546195339660094, 0.0022372549230402165, 0.0025147945350852867, 0.0016331663207402305], 'topk_tokens': [' Miss', '.', ' \n', ' THE', ' before', '<|eot_id|>', ' Mary', '<|eot_id|>', ' bathroom', 'Answer', '.', ' football', ' football', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.006032804648081462, 0.006032804648081462, 0.08122062683105469, 0.00786370038986206]}, 'saliency': {'score': [0.0022875531153245406, 0.00013219882160117342, 0.0006100318648598411, 0.00012743247953982695, 0.00018625032334100631], 'topk_tokens': ['Answer', ' the', ' Third', 'THE', ' THE', ' Miss', '<|eot_id|>', 'assistant', ' bathroom', ' kitchen', ' THE', ' football', '.', ' football', ' Mary', ':', '.\n\n', '.', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0009394288063049316, 0.0009394288063049316, 0.008134692907333374, 0.0010857085386912029]}}, 28: {'grad': {'score': [0.5324041193181818, 0.553528162275578, 0.7135259454900568, 0.5532767864087311, 0.7229197668650794], 'topk_tokens': [',', ' did', 'half', ' ', ' July', ' about', 'did', ' ende', ' about', 'half', ' dry', ' ende', ' adj', '      ', ' executed', ' firm', ' endeavor', 'na', ' reached', ' ins'], 'evidence_proportions': [0.4957275390625, 0.4957275390625, 0.725250244140625, 0.4771931966145833]}, 'weight': {'score': [0.032457492568276146, 0.0024605663629179413, 0.007551764900034124, 0.002397053219563125, 0.002003423751346649], 'topk_tokens': ['Question', ' to', ' the', ' the', ' football', ' the', '?', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' \n', ' before', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.02568278710047404, 0.02568278710047404, 0.02178955078125, 0.05311886469523112]}, 'saliency': {'score': [0.0016940479928796942, 4.408974826560683e-05, 0.00033983588218688965, 4.056782163011414e-05, 8.14350824507456e-05], 'topk_tokens': ['.', ' the', ' the', ' Mary', ' football', ' kitchen', ' the', ' nearly', '<|eot_id|>', 'Answer', '<|eot_id|>', ' before', ' during', ' Bridge', ' Bridge', '<|end_header_id|>', ' Far', ':', 'assistant', '<|start_header_id|>'], 'evidence_proportions': [0.0011934141318003337, 0.0011934141318003337, 0.0024354010820388794, 0.002201080322265625]}}, 29: {'grad': {'score': [0.80267333984375, 0.8824829762051156, 0.8792891068892046, 0.8826332211553295, 0.7713133796812996], 'topk_tokens': ['boat', '.', ' Newspaper', '"The', ' The', '.', '.', 'ION', '.', '\n', ' printed', ' young', ' The', ' The', ' NEW', ' were', ' The', ' The', '.', '\n'], 'evidence_proportions': [0.9100748697916666, 0.9100748697916666, 0.642242431640625, 0.69482421875]}, 'weight': {'score': [0.0066766197031194515, 0.002515499191922152, 0.0008713142438368363, 0.002510943277623282, 0.0011442303657531738], 'topk_tokens': ['Question', ' the', '?', ' Where', ' bathroom', ' football', ' Does', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' \n', ' the', ' before', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0024269421895345054, 0.0024269421895345054, 0.011432647705078125, 0.012005289395650228]}, 'saliency': {'score': [0.00027003613385287196, 4.131837660493489e-05, 4.8705122687599875e-05, 4.0891002977053275e-05, 8.080970673334031e-05], 'topk_tokens': [' bathroom', '.\n\n', ' the', '      ', ' Where', ':', ' part', 'Does', ' the', ' the', ' Do', ' before', '<|end_header_id|>', 'Answer', ' \n', ' the', '<|start_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00012762347857157388, 0.00012762347857157388, 0.00044211745262145996, 0.0004401405652364095]}}, 30: {'grad': {'score': [0.6977705522017046, 0.7847071177139695, 0.7355207963423296, 0.784953514251031, 0.9730570959666419], 'topk_tokens': [' an', ' secured', ' Paul', ' Press', ' Was', ' States', ' anx', ' Loan', 'Emp', ' o', 'ire', ' LINE', 'office', ' Bridge', 'ob', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.63714599609375, 0.63714599609375, 1.103363037109375, 0.5486246744791666]}, 'weight': {'score': [0.018565286289561878, 0.0024513647114180956, 0.003765637224370783, 0.002419817873339757, 0.004517279920123872], 'topk_tokens': [' football', ' Where', '.', ':', '.', 'Question', '?', ' before', '<|eot_id|>', '<|eot_id|>', ' bathroom', '.\n\n', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.010394235452016195, 0.010394235452016195, 0.038204193115234375, 0.021814783414204914]}, 'saliency': {'score': [0.0015553615309975364, 0.00012338990206639481, 0.0004644773223183372, 0.00012018048137509746, 0.00036720434824625653], 'topk_tokens': [' \n', ' the', ' Wide', ':', ' Third', ' Min', '.', ' football', '.\n\n', '?', 'Question', ' bathroom', ' Mary', '.', 'assistant', ':', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0011439025402069092, 0.0011439025402069092, 0.0033147335052490234, 0.0012053648630777996]}}, 31: {'grad': {'score': [0.8121282404119318, 0.7965540425428349, 0.9783991033380682, 0.796196693218282, 0.47979966420975945], 'topk_tokens': ['editary', ' Mr', ' an', ' an', 'IR', ' were', ' B', ' Mr', ' an', ' an', ' W', ' M', ' C', ' C', ' an', ' H', ' an', 'Mr', ' most', ' Mr'], 'evidence_proportions': [0.923095703125, 0.923095703125, 0.857513427734375, 0.5599365234375]}, 'weight': {'score': [0.0034753246740861373, 0.0023193565777704975, 0.0014889646660197866, 0.002318767251124091, 0.000604969168466235], 'topk_tokens': [' was', 'Question', ':', ' the', '?', '.\n\n', ' football', ' before', '<|eot_id|>', ' Where', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0032345553239186606, 0.0032345553239186606, 0.0033888816833496094, 0.004014492034912109]}, 'saliency': {'score': [0.0005034614693034779, 0.00012608634035867597, 0.0005444830114191228, 0.00012464591090334312, 2.8939474196661086e-05], 'topk_tokens': ['?', ' the', 'Answer', 'Question', ' the', ' Mary', ' office', ' Where', ' football', ' the', ' the', ' \n', '<|eot_id|>', ' the', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0006217062473297119, 0.0006217062473297119, 0.00041684508323669434, 0.00032471617062886554]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 02:56:50.081 | INFO     | modelzipper.tutils:auto_save_data:296 - /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label not exist! --> Create data dir /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label
2025-01-22 02:56:50.081 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:56:50.081 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-0_3-5-8-9.pkl | len: 10 |  size: 9.03 KB
Processing depth (3, 5, 8, 9):   1%|          | 1/100 [00:26<44:02, 26.69s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
Processing depth (1, 2, 3, 4):   1%|          | 1/100 [00:34<44:02, 26.69s/it]2025-01-22 02:56:57.634 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:56:57.638 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1466, 1472) --> . Mary journeyed to the
2025-01-22 02:56:57.638 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:56:57.643 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1466, 1472) --> . Mary journeyed to the
2025-01-22 02:56:57.643 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:56:57.653 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3692, 3696) -->  Mary dropped the football
2025-01-22 02:56:57.653 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 02:56:57.667 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4849, 4855) --> . Daniel went back to the
2025-01-22 02:56:57.667 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:56:57.668 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (67, 73) --> . John went back to the
2025-01-22 02:56:57.668 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John took the milk.
2025-01-22 02:56:57.674 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2203, 2207) -->  John took the milk
2025-01-22 02:56:57.674 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 02:56:57.676 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (885, 891) --> . Sandra journeyed to the
2025-01-22 02:56:57.676 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:56:57.681 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1467, 1473) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:56:59.725 | INFO     | test_jbb_retain:begin_test:632 - The office.<|eot_id|>
2025-01-22 02:56:59.725 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12200])
your chose emoji: ['🚴🏿\u200d♀️', '💂🏾', '💇🏻\u200d♀', '🏃🏾', '🤸🏼\u200d♀', '📟', '🧑🏼', '\U0001fabc', '🦨', '👩🏻\u200d🤝\u200d👨🏼']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 117323.19it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.05s/it][A
 25%|██▌       | 2/8 [00:05<00:13,  2.17s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.25s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.22it/s][A
 62%|██████▎   | 5/8 [00:05<00:01,  1.71it/s][A
 75%|███████▌  | 6/8 [00:05<00:00,  2.28it/s][A
 88%|████████▊ | 7/8 [00:06<00:00,  2.88it/s][A
100%|██████████| 8/8 [00:06<00:00,  3.48it/s][A100%|██████████| 8/8 [00:06<00:00,  1.30it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  5.91it/s][A
 25%|██▌       | 2/8 [00:00<00:00,  6.09it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.16it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.16it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.21it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.21it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.21it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.21it/s][A100%|██████████| 8/8 [00:01<00:00,  6.18it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  5.57it/s][A
 25%|██▌       | 2/8 [00:00<00:01,  5.95it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.05it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.09it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.15it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.18it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.19it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.20it/s][A100%|██████████| 8/8 [00:01<00:00,  6.12it/s]
2025-01-22 02:57:11.228 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6290421919389204, 0.8519005593345353, 0.7599972811612216, 0.8524701230952292, 0.8586116050606343], 'topk_tokens': [' The', ' notified', ' the', ' Min', ' THE', ' Minnesota', ' the', ' scramble', ' the', ' the', ' the', ',', 'Minnesota', ' marched', ' glance', ' the', ' Minnesota', ' Ramsey', ' Min', ' Minnesota'], 'evidence_proportions': [0.6509145100911459, 0.6509145100911459, 0.577911376953125, 0.619384765625]}, 'weight': {'score': [0.060163294727152046, 0.0025722953535028997, 0.016404406590895218, 0.002443054653269069, 0.003063571097245857], 'topk_tokens': ['.', ' the', '.', 'Answer', ' upper', ' office', '<|start_header_id|>', '<|eot_id|>', '.', ' bathroom', 'assistant', '<|eot_id|>', ' office', ':', ' bathroom', '<|end_header_id|>', 'Bridge', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.01572696367899577, 0.01572696367899577, 0.27056121826171875, 0.008770674467086792]}, 'saliency': {'score': [0.005882623520764438, 6.93783508404574e-05, 0.0019893646240234375, 5.538500557410166e-05, 0.00019421239397419032], 'topk_tokens': [' dropped', ' Dul', ' Mary', '<|end_header_id|>', '�', ' the', '<|eot_id|>', ':', ' Married', ' kitchen', '<|eot_id|>', ' bedroom', 'Question', '.', '<|begin_of_text|>', ' office', ' office', 'office', ' football', 'Bridge'], 'evidence_proportions': [0.001428773005803426, 0.001428773005803426, 0.027530193328857422, 0.0003586113452911377]}}, 25: {'grad': {'score': [0.7313176935369318, 0.5518905872244304, 0.7322748357599432, 0.5512395056479659, 0.4214172932639051], 'topk_tokens': [' Mary', ' Louis', '186', ' news', ' news', 'ush', ' Chat', ' Paul', ' S', ' location', ' location', ' speech', 'bed', ' heads', ' balance', ' population', ' location', ' location', ' actions', ' Wood'], 'evidence_proportions': [0.7067769368489584, 0.7067769368489584, 1.11602783203125, 0.52392578125]}, 'weight': {'score': [0.01887306029146368, 0.002543177571067691, 0.0037216679616407914, 0.002511496027356446, 0.0013829327341335924], 'topk_tokens': [' the', ' the', ' the', 'Bridge', '.', ' Mary', '.\n\n', ' \n', 'Answer', '<|start_header_id|>', ' football', '.', '<|eot_id|>', '.', '<|eot_id|>', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006736069917678833, 0.006736069917678833, 0.07436370849609375, 0.006153275569279988]}, 'saliency': {'score': [0.0009689141403545033, 5.9773229884429165e-05, 6.815520199862394e-05, 5.811296475719986e-05, 4.282282359564482e-05], 'topk_tokens': [' cartoon', 'Mer', 'office', ' THE', ' Min', ' double', ' Dan', 'through', ' random', 'Answer', ' directly', '.', ' football', ' Married', ':', '.', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0001554290453592936, 0.0001554290453592936, 0.004605591297149658, 0.000171432892481486]}}, 26: {'grad': {'score': [1.0918135209517046, 0.9693952259363219, 1.4331637295809658, 0.9683345161509526, 0.5841088366152635], 'topk_tokens': [' Merch', ' worth', ' generally', 'BO', ' Press', ' some', ' gold', ' Gen', ' STR', ' Capt', 'str', ' black', ' kids', ' sm', ' Col', ' Col', ' wh', ' Guards', 'ol', ' str'], 'evidence_proportions': [1.007110595703125, 1.007110595703125, 0.92333984375, 1.37353515625]}, 'weight': {'score': [0.006398666988719593, 0.0025192583375163906, 0.0019468719309026544, 0.0025132742538364295, 0.00106739642015144], 'topk_tokens': [' the', ' football', '.\n\n', ' before', '?', ' kitchen', '<|eot_id|>', ' bathroom', '<|eot_id|>', ' the', ' the', 'Answer', ' \n', 'assistant', 'Bridge', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00285529096921285, 0.00285529096921285, 0.02285289764404297, 0.0025159319241841636]}, 'saliency': {'score': [0.0006686205213720149, 8.273495941587285e-05, 0.0003199225122278387, 8.124560191259119e-05, 5.407297789160885e-05], 'topk_tokens': [' the', '.\n\n', ' upper', ' bedroom', ' office', ' Bridge', ' Bridge', ' bathroom', ' football', ' kitchen', '<|start_header_id|>', 'Answer', ' bathroom', '<|begin_of_text|>', 'assistant', ' the', '<|end_header_id|>', ':', 'Bridge', 'office'], 'evidence_proportions': [0.00032480557759602863, 0.00032480557759602863, 0.0025137215852737427, 0.00012618303298950195]}}, 27: {'grad': {'score': [0.4685724431818182, 0.4760877843411941, 0.35341644287109375, 0.4763233582034945, 0.3737279408013643], 'topk_tokens': [' medicine', 'na', 'sur', ' Date', ' carn', 'fire', ' noble', '.', ' accepted', ' later', 'roduced', '.', '185', ' N', 'use', 'ly', ' earnest', 'Mal', 'bread', 'ile'], 'evidence_proportions': [0.487701416015625, 0.487701416015625, 0.513916015625, 0.40008544921875]}, 'weight': {'score': [0.01828780770301819, 0.0025449833153232434, 0.004652429710734974, 0.0025126831049902655, 0.002294125841624701], 'topk_tokens': ['Question', '<|eot_id|>', '<|eot_id|>', ' before', ' \n', '.', ' upper', ' bathroom', ' football', '.', 'Answer', 'Bridge', 'assistant', '<|start_header_id|>', ' football', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.007176101207733154, 0.007176101207733154, 0.06891775131225586, 0.0067579249540964765]}, 'saliency': {'score': [0.0038086744872006502, 0.0001404601967621975, 0.001177381385456432, 0.0001319462157997926, 0.0002542671872608697], 'topk_tokens': [' office', ' the', ' Mary', ' THE', ' Mary', 'Question', '�', '.', '<|begin_of_text|>', ' football', ' kitchen', ' Mary', ' bathroom', '.', 'Bridge', ' upper', ':', '.\n\n', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00253214438756307, 0.00253214438756307, 0.011247217655181885, 0.0014027059078216553]}}, 28: {'grad': {'score': [0.5648692737926136, 0.45668486723487955, 0.5620255903764204, 0.4562984917739991, 0.5211060936771222], 'topk_tokens': [' ', ' July', 'avored', ' the', ' become', 't', ' July', ' half', 'na', 'half', ' on', '600', ' adj', ' firm', 'half', 'hom', ' take', ' reached', ' endeavor', ' ins'], 'evidence_proportions': [0.457916259765625, 0.457916259765625, 0.90545654296875, 0.5517171223958334]}, 'weight': {'score': [0.006780553947795521, 0.0024477541788623597, 0.0018016723069277677, 0.0024410830328075836, 0.00142470135617612], 'topk_tokens': [' the', ' football', 'Question', '.\n\n', ' the', ' bathroom', ' the', '<|eot_id|>', ' the', '?', ' before', '<|eot_id|>', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003801455100377401, 0.003801455100377401, 0.014825820922851562, 0.007375240325927734]}, 'saliency': {'score': [0.0005142851309342818, 4.067139691201376e-05, 7.841532880609685e-05, 3.974609105166175e-05, 9.250551906984244e-05], 'topk_tokens': [' the', '<|eot_id|>', ' the', ' the', ' during', 'Bridge', ' kitchen', '.\n\n', ' nearly', ' before', ' Bridge', ' Bridge', ' the', 'assistant', '<|start_header_id|>', '<|eot_id|>', ' Far', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0001312394936879476, 0.0001312394936879476, 0.0019342899322509766, 0.000333706537882487]}}, 29: {'grad': {'score': [0.5618480335582386, 0.8067558725004098, 0.7176055908203125, 0.8073603533898398, 0.5722669914587221], 'topk_tokens': ['measure', '.', ' mailing', ' and', ' routes', '\n', 'd', ' and', 'paper', ' to', '\n', 'mail', ' mail', ' of', ' press', ' press', 'boat', '\n', ' were', '\n'], 'evidence_proportions': [0.7094268798828125, 0.7094268798828125, 0.43426513671875, 0.35174560546875]}, 'weight': {'score': [0.0064300027760592375, 0.00252089276116044, 0.001179871234026822, 0.0025162457841304323, 0.0012741391338519197], 'topk_tokens': ['Question', ' in', ' Where', ' bathroom', '<|eot_id|>', ' Does', '?', '<|eot_id|>', '.\n\n', ' the', ' before', ' the', ' \n', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0025159716606140137, 0.0025159716606140137, 0.010814666748046875, 0.01133495569229126]}, 'saliency': {'score': [0.0005443990230560303, 4.601213330617597e-05, 0.00010317022150213069, 4.5006870144898805e-05, 6.511051263382186e-05], 'topk_tokens': ['.', ' the', 'assistant', ' the', ':', ' to', '      ', ' Where', ' the', ' Do', ' the', ' from', ' Does', ' the', 'Answer', ' \n', ':', '<|start_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00024448831876118976, 0.00024448831876118976, 0.0006041824817657471, 0.0011043647925059001]}}, 30: {'grad': {'score': [0.81201171875, 0.7658460511084249, 0.8377616188742898, 0.7656323821514448, 0.6934882776061101], 'topk_tokens': [' Date', ' head', ' Fourth', ' Fourth', ' Loan', ' Press', 'ch', ' Bridge', ' Project', ' United', 'con', ' Crew', 'ire', ' States', ' o', ' Published', ' o', ' Out', ' Collection', ' Was'], 'evidence_proportions': [0.7445068359375, 0.7445068359375, 0.776763916015625, 0.97052001953125]}, 'weight': {'score': [0.015797688202424484, 0.0024538795159813067, 0.005053127353841608, 0.002425030425379672, 0.004206523076811833], 'topk_tokens': ['.', ':', '<|eot_id|>', ' the', ' the', '<|eot_id|>', ' kitchen', '?', ' the', ' bathroom', 'Question', '.\n\n', 'assistant', 'Answer', '<|end_header_id|>', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.009362419446309408, 0.009362419446309408, 0.03281211853027344, 0.01732527216275533]}, 'saliency': {'score': [0.0022328766909512606, 9.94568124323121e-05, 0.0007262880151922053, 9.44621156246024e-05, 0.00032366567583226445], 'topk_tokens': [' double', 'Answer', ':', ' Mess', ' \n', ' football', ' the', ' football', '?', '.\n\n', '.', ' office', 'assistant', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', 'Question', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012638668219248454, 0.0012638668219248454, 0.005824685096740723, 0.0017763574918111165]}}, 31: {'grad': {'score': [0.6839322176846591, 0.6435867807275447, 0.8665771484375, 0.6431102724447123, 0.2981645954189016], 'topk_tokens': [' W', ' Mr', ' C', ' DAYS', ' an', ' DAY', ' M', ' J', ' M', ' an', ' H', ' the', ' an', 'IR', ' the', 'Mr', ' an', ' most', ' an', ' Mr'], 'evidence_proportions': [0.7119954427083334, 0.7119954427083334, 0.8568191528320312, 0.5125478108723959]}, 'weight': {'score': [0.0020310282707214355, 0.0023015851838493755, 0.0012505758892406118, 0.0023039765686635066, 0.0007229047035103414], 'topk_tokens': [' bathroom', ' the', ':', ' football', 'Question', ' before', '?', '.\n\n', '<|eot_id|>', ' Where', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0013840595881144206, 0.0013840595881144206, 0.005094289779663086, 0.001282791296641032]}, 'saliency': {'score': [0.00043432549996809524, 8.373191328913125e-05, 0.00034980611367659134, 8.261604790704034e-05, 3.419260480510655e-05], 'topk_tokens': [' bathroom', '<|eot_id|>', ' office', 'Question', ' office', ' football', ' the', ' the', ' Where', ' the', ' the', '<|eot_id|>', ' the', ' the', ' \n', ':', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.000356823205947876, 0.000356823205947876, 0.0011687278747558594, 9.972850481669109e-05]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 100}
2025-01-22 02:57:11.229 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:57:11.229 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-1_1-2-3-4.pkl | len: 10 |  size: 9.14 KB
Processing depth (1, 2, 3, 4):   2%|▏         | 2/100 [00:47<38:16, 23.43s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
Processing depth (2, 4, 6, 8):   2%|▏         | 2/100 [00:55<38:16, 23.43s/it]2025-01-22 02:57:18.727 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:57:18.735 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2511, 2517) --> . Mary journeyed to the
2025-01-22 02:57:18.735 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:57:18.743 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2511, 2517) --> . Mary journeyed to the
2025-01-22 02:57:18.743 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:57:18.762 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7216, 7220) -->  Mary dropped the football
2025-01-22 02:57:18.762 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 02:57:18.791 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9698, 9704) --> . Daniel went back to the
2025-01-22 02:57:18.791 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:57:18.791 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (67, 73) --> . John went back to the
2025-01-22 02:57:18.791 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John took the milk.
2025-01-22 02:57:18.797 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2264, 2268) -->  took the milk.
2025-01-22 02:57:18.797 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 02:57:18.800 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (939, 945) --> . Sandra journeyed to the
2025-01-22 02:57:18.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:57:18.808 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2512, 2518) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:57:20.942 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the football.<|eot_id|>
2025-01-22 02:57:20.942 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12198])
your chose emoji: ['🚴🏿', '🧏🏼', '🇵🇲', '🧗🏽\u200d♀', '⚰', '\U0001faaf', '🧑🏾\u200d🦼\u200d➡', '🏍️', '🏂🏽', '🧘🏾\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 217885.92it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.81s/it][A
 25%|██▌       | 2/8 [00:04<00:12,  2.08s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.21s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.25it/s][A
 62%|██████▎   | 5/8 [00:05<00:01,  1.75it/s][A
 75%|███████▌  | 6/8 [00:05<00:00,  2.30it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.88it/s][A
100%|██████████| 8/8 [00:05<00:00,  3.45it/s][A100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  6.05it/s][A
 25%|██▌       | 2/8 [00:00<00:00,  6.21it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.23it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.27it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.12it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.05it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.04it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.09it/s][A100%|██████████| 8/8 [00:01<00:00,  6.12it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:01,  5.59it/s][A
 25%|██▌       | 2/8 [00:00<00:01,  5.95it/s][A
 38%|███▊      | 3/8 [00:00<00:00,  6.05it/s][A
 50%|█████     | 4/8 [00:00<00:00,  6.07it/s][A
 62%|██████▎   | 5/8 [00:00<00:00,  6.10it/s][A
 75%|███████▌  | 6/8 [00:00<00:00,  6.16it/s][A
 88%|████████▊ | 7/8 [00:01<00:00,  6.17it/s][A
100%|██████████| 8/8 [00:01<00:00,  6.21it/s][A100%|██████████| 8/8 [00:01<00:00,  6.12it/s]
2025-01-22 02:57:32.248 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7058493874289773, 0.7617166688012295, 0.739959716796875, 0.7618571535934544, 0.7249947474553035], 'topk_tokens': [' the', 'from', 'Minnesota', ' THE', 'irie', ' Minnesota', ' from', ' the', ',', 'Minnesota', ' marched', ' Minnesota', ' Minnesota', ' Ramsey', ' of', 'Minnesota', ' Phil', ' Min', ' Minnesota', ' Minnesota'], 'evidence_proportions': [0.709197998046875, 0.709197998046875, 0.78363037109375, 0.6472981770833334]}, 'weight': {'score': [0.07472445477138866, 0.0025688838958740235, 0.01783774657682939, 0.002410662643962018, 0.0008481942690335788], 'topk_tokens': [' upper', ' \n', ' the', ' football', 'Answer', ' the', ' office', '<|eot_id|>', ' bathroom', '<|start_header_id|>', ' bathroom', '<|eot_id|>', 'assistant', ':', '.', '<|end_header_id|>', 'Bridge', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.025689353545506794, 0.025689353545506794, 0.32169342041015625, 0.008148680130640665]}, 'saliency': {'score': [0.008193888447501442, 6.531188722516669e-05, 0.0031099454923109574, 4.5090546024281084e-05, 3.0504740201509916e-05], 'topk_tokens': [' office', ' Sandra', ' Dul', ' bedroom', ' bathroom', ' upper', ' office', ' bathroom', '<|eot_id|>', '.', ' Mary', 'Question', '<|begin_of_text|>', '<|end_header_id|>', ' Mary', '<|eot_id|>', '.', ' Mary', ' football', 'office'], 'evidence_proportions': [0.007691611846288045, 0.007691611846288045, 0.02183873951435089, 0.00010187427202860515]}}, 25: {'grad': {'score': [0.8069000244140625, 0.5518812435963115, 0.7325716885653409, 0.5510926944874509, 0.5728494497445914], 'topk_tokens': [' news', ' Ot', ' location', '186', ' head', '186', ' location', ' news', ' heading', ' location', ' population', ' news', ' Key', ' news', '186', ' location', ' news', ' news', ' actions', ' Wood'], 'evidence_proportions': [0.7978922526041666, 0.7978922526041666, 1.2454833984375, 0.5325266520182291]}, 'weight': {'score': [0.027428570118817417, 0.0025494878800188907, 0.006795804608951916, 0.0024967765623740987, 0.0005733453310452975], 'topk_tokens': [' dropped', ' Mary', '.\n\n', ' the', '.', ' the', ' Mary', ' \n', ' football', 'Answer', '.', '.', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.01916537682215373, 0.01916537682215373, 0.0768585205078125, 0.011001656452814737]}, 'saliency': {'score': [0.0016944354230707343, 5.618777919988163e-05, 0.00041860342025756836, 5.2566967068553246e-05, 3.977005298321064e-05], 'topk_tokens': ['Mer', ' dropped', ' Mary', ' Mary', 'assistant', ' Dan', 'Answer', ' Market', ' football', ' random', ' directly', ' Minnesota', '.', 'office', '.', ':', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0012162824471791585, 0.0012162824471791585, 0.004505634307861328, 0.0007766087849934896]}}, 26: {'grad': {'score': [0.6484606482765891, 0.6886632300204918, 0.9156438654119318, 0.688325198005006, 0.3982926001915565], 'topk_tokens': [' Knowledge', ' generally', ' wh', '.', ' Press', 'BO', ' sm', ' Field', 'Cut', ' black', ' Merch', ' Gen', ' generally', ' generally', ' Guards', 'ol', 'str', ' STR', ' worth', ' str'], 'evidence_proportions': [0.7289632161458334, 0.7289632161458334, 0.34862279891967773, 0.687347412109375]}, 'weight': {'score': [0.015346345576373014, 0.00251366099373239, 0.003483680161562833, 0.002488680779639731, 0.0006469240555396447], 'topk_tokens': ['.\n\n', ' before', '?', ' football', ' football', ' kitchen', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' the', ' \n', 'Bridge', 'Answer', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.008843262990315756, 0.008843262990315756, 0.04524993896484375, 0.008416781822840372]}, 'saliency': {'score': [0.00027009302919561213, 0.000106208685968743, 0.0002587085420435125, 0.00010563609199172457, 1.8821312830998348e-05], 'topk_tokens': ['.', ' Merch', ' Seventh', ' North', '<|eot_id|>', ' bathroom', ' Bridge', ' the', ' Bridge', ' kitchen', 'Answer', ' bathroom', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.0003001987934112549, 0.0003001987934112549, 0.0004900246858596802, 6.326039632161458e-05]}}, 27: {'grad': {'score': [0.9958274147727273, 0.4948032866931352, 0.5537747469815341, 0.4937898033973064, 0.4187736511230469], 'topk_tokens': ['.', ' product', ' plainly', '.', ' noble', ' N', ' medicine', ' Temper', ' later', 'sur', 'n', 'fire', '.', ' carn', 'Mal', 'na', 'use', ' N', 'ile', 'bread'], 'evidence_proportions': [1.1343994140625, 1.1343994140625, 0.802001953125, 0.847900390625]}, 'weight': {'score': [0.035322118889201774, 0.00254565848678839, 0.009139960462396795, 0.002474405050434616, 0.0010150560965904822], 'topk_tokens': ['.', ' Mary', '<|eot_id|>', ' Bridge', ' before', '<|eot_id|>', ' \n', '.', ' bathroom', 'Answer', 'Bridge', ' football', ' football', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.026625831921895344, 0.026625831921895344, 0.10310745239257812, 0.007524470488230388]}, 'saliency': {'score': [0.005196270617571744, 0.00014221097602218878, 0.0022767755118283358, 0.00012920096188087375, 5.397888330312876e-05], 'topk_tokens': [' Jackson', ' kitchen', ':', ' John', '.', ' Mary', ' Minnesota', ' the', ' Mary', ' upper', '<|begin_of_text|>', ' the', ' bathroom', '.', ' Mary', 'Bridge', '.', '.\n\n', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.005492548147837321, 0.005492548147837321, 0.010653853416442871, 0.0009653270244598389]}}, 28: {'grad': {'score': [0.4754416725852273, 0.4740741867315574, 0.5633517178622159, 0.47391013684889405, 0.4852174025315505], 'topk_tokens': [' popularity', ' endeavor', ' about', 'been', ' been', ' exact', ' executed', ' be', ' escaping', ' have', ' become', '600', ' kept', ' office', 'hum', 'na', ' reached', ' take', 'hom', ' ins'], 'evidence_proportions': [0.3706868489583333, 0.3706868489583333, 0.62774658203125, 0.5834147135416666]}, 'weight': {'score': [0.021182453090494328, 0.002460247102330943, 0.004159287972883744, 0.002423288610154918, 0.0009573716383713942], 'topk_tokens': ['Question', '.\n\n', ' the', ' football', ' the', '?', '<|eot_id|>', ' bathroom', ' the', ' the', ' \n', ' before', '<|eot_id|>', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.011207511027654013, 0.011207511027654013, 0.03300285339355469, 0.033252070347468056]}, 'saliency': {'score': [0.0015466050668196244, 4.9138616343013576e-05, 0.0001910355958071622, 4.617168680544392e-05, 3.05780997643104e-05], 'topk_tokens': ['Question', ' before', ' Nearly', 'office', ' nearly', ' the', '<|eot_id|>', 'Bridge', ' Dul', ' football', ' the', ' Bridge', ' the', ':', ' Bridge', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ' Far', '<|begin_of_text|>'], 'evidence_proportions': [0.0006343225638071696, 0.0006343225638071696, 0.004426479339599609, 0.001451253890991211]}}, 29: {'grad': {'score': [0.5329159823330966, 0.8398628009733606, 0.6700210571289062, 0.8407256957063867, 0.42001348642202524], 'topk_tokens': ['\n', ' The', '\n', ' printed', ' papers', ' The', 'boat', 'APER', ' paper', '\n', ' NEW', ' The', ' the', '\n', ' and', 'paper', 'ION', ' were', '\n', 'ION'], 'evidence_proportions': [0.6637674967447916, 0.6637674967447916, 0.44138336181640625, 0.3322347005208333]}, 'weight': {'score': [0.01216032017361034, 0.002524695474593366, 0.004352187568491156, 0.002503949458679897, 0.0005615967970628005], 'topk_tokens': [' the', ' bathroom', ' football', '?', ' Where', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' the', ' Does', ' \n', ' before', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.013764907916386923, 0.013764907916386923, 0.011485099792480469, 0.009401291608810425]}, 'saliency': {'score': [0.0010893480344252152, 6.87479386564161e-05, 0.0004175359552556818, 6.62696120298862e-05, 3.061202856210562e-05], 'topk_tokens': ['.', ' to', ' Does', ' the', 't', '.\n\n', ' the', ' Do', 'ot', ' before', ' the', ' the', 'Answer', ' \n', '<|end_header_id|>', ' the', ':', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0013391176859537761, 0.0013391176859537761, 0.000634610652923584, 0.0008929669857025146]}}, 30: {'grad': {'score': [0.7786227139559659, 0.7101610527663934, 0.8610673384232954, 0.7097640393713109, 0.8749896709735577], 'topk_tokens': [' o', 'con', 'tele', ' United', 'ch', ' dre', ' Bridge', ',', ' Loan', 'Country', ' Project', 'ire', ' Collection', ' Out', ' Crew', ' States', ' Was', ' Published', ' o', ' o'], 'evidence_proportions': [0.8272705078125, 0.8272705078125, 0.6786041259765625, 0.7480061848958334]}, 'weight': {'score': [0.02540411732413552, 0.0024650534645455783, 0.007247439839623191, 0.0024148830215410797, 0.003360855579376221], 'topk_tokens': [' Where', ' the', ' before', '<|eot_id|>', ' the', ' the', ' the', '<|eot_id|>', '?', 'Question', ' bathroom', '.\n\n', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.018988152345021565, 0.018988152345021565, 0.043331146240234375, 0.02628469467163086]}, 'saliency': {'score': [0.004053327170285312, 8.504037974310704e-05, 0.0014472332867709074, 7.539324636481317e-05, 0.00026285281548133263], 'topk_tokens': [' Mess', ' the', ' football', ' Mary', ' Mary', ' Miles', '?', ' kitchen', '.\n\n', ' Mary', ' the', 'office', ' football', ' bathroom', '.', 'Question', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.003669440746307373, 0.003669440746307373, 0.009762167930603027, 0.0010152061780293782]}}, 31: {'grad': {'score': [0.8300933837890625, 0.5825691998591188, 0.799603895707564, 0.5817284384774863, 0.48511329064002406], 'topk_tokens': [' an', ' Mr', ' DAY', ' membership', 'k', ' very', ' an', 'Mexico', ' more', 'IR', ' M', ' M', ' Mary', ' most', ' marriage', ' Mr', ' Mr', 'Mr', ' Mary', ' Mary'], 'evidence_proportions': [0.8992919921875, 0.8992919921875, 0.9899826049804688, 0.5851033528645834]}, 'weight': {'score': [0.0038095414638519287, 0.002312011406069896, 0.0016529424624009566, 0.0023104939542345484, 0.0007988205322852502], 'topk_tokens': [' bathroom', ' the', ':', 'Question', ' football', ' before', '.\n\n', '?', '<|eot_id|>', ' Where', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003549764553705851, 0.003549764553705851, 0.006591796875, 0.0024742583433787027]}, 'saliency': {'score': [0.0017404014414007013, 8.673986450570529e-05, 0.0010044899853793058, 8.208610855383714e-05, 3.980856675368089e-05], 'topk_tokens': [' office', ' Where', ' before', ' office', '.', ' Mary', 'ot', ' was', ' football', ' Mary', '<|end_header_id|>', ' the', ' \n', ' the', '<|eot_id|>', ':', '<|begin_of_text|>', 'office', 'assistant', '<|start_header_id|>'], 'evidence_proportions': [0.002365797758102417, 0.002365797758102417, 0.0020523667335510254, 0.0002816319465637207]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 02:57:32.250 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:57:32.250 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-2_2-4-6-8.pkl | len: 10 |  size: 9.25 KB
Processing depth (2, 4, 6, 8):   3%|▎         | 3/100 [01:08<36:05, 22.33s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (0, 1, 4, 6):   3%|▎         | 3/100 [01:16<36:05, 22.33s/it]2025-01-22 02:57:39.618 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:57:39.618 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 02:57:39.618 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:57:39.623 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1515, 1521) -->  tragedy. Mary journeyed to
2025-01-22 02:57:39.623 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:57:39.636 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4910, 4914) -->  Mary dropped the football
2025-01-22 02:57:39.636 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 02:57:39.657 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7215, 7221) -->  Daniel went back to the kitchen
2025-01-22 02:57:39.657 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:57:39.657 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (74, 80) --> . John went back to the
2025-01-22 02:57:39.657 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John took the milk.
2025-01-22 02:57:39.663 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2276, 2280) -->  took the milk.
2025-01-22 02:57:39.664 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 02:57:39.666 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (916, 922) --> . Sandra journeyed to the
2025-01-22 02:57:39.666 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:57:39.667 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:57:41.772 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the football.<|eot_id|>
2025-01-22 02:57:41.773 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12199])
your chose emoji: ['🪑', '🦶🏾', '🇰🇲', '🇦🇷', '🦸🏽', '💁🏼\u200d♂️', '🧑🏽\u200d🎓', '👷🏿', '🧚🏽\u200d♀', '👴🏻']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 250406.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.92s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.33s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.86it/s][A100%|██████████| 8/8 [00:05<00:00,  1.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.09it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.46it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.71it/s][A100%|██████████| 8/8 [00:00<00:00, 21.76it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.08it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.10it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.57it/s][A100%|██████████| 8/8 [00:00<00:00, 21.47it/s]
2025-01-22 02:57:51.614 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.5799851851029829, 0.6942911153542742, 0.5479209206321023, 0.6947628497253704, 1.0362842444217566], 'topk_tokens': ['irie', ' Minneapolis', ' the', ' the', 'Minnesota', ' effect', ' Minnesota', ' of', 'Minnesota', 'from', ' marched', 'active', ' Minnesota', 'super', ' Min', ' the', ' Minnesota', 'Minnesota', ' Minnesota', ' Minnesota'], 'evidence_proportions': [0.5429890950520834, 0.8125813802083334, 0.59942626953125, 0.3714243570963542]}, 'weight': {'score': [0.043987981297753075, 0.002559072667873743, 0.004281962459737604, 0.0024809827143261287, 0.000802707491499005], 'topk_tokens': [' \n', ' the', '\n\n', ' the', ' Mary', 'Answer', '<|start_header_id|>', '<|eot_id|>', ' bathroom', '.', '<|eot_id|>', 'assistant', ' Bridge', ' bathroom', ':', 'Bridge', ' football', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005147695541381836, 0.024131784836451214, 0.14582061767578125, 0.03479603926340739]}, 'saliency': {'score': [0.011058574373071844, 0.00010046183344829513, 0.00030580433932217687, 8.025980901784819e-05, 4.1800918001117125e-05], 'topk_tokens': ['.', '.', ' \n', '<|eot_id|>', ' top', ' bathroom', 'Question', ' bedroom', '<|end_header_id|>', ' office', ' Bridge', '<|eot_id|>', 'Mary', ' office', 'Bridge', ' football', ' office', ' Mary', ' Mary', 'office'], 'evidence_proportions': [0.0009159545103708903, 0.017359753449757893, 0.030497074127197266, 0.001941015323003133]}}, 25: {'grad': {'score': [1.0262867320667615, 0.987935639240636, 1.1346823952414773, 0.98760067562509, 0.8407222863399622], 'topk_tokens': [' the', ' THE', ' Gree', ' storm', 'ylinder', ' private', 'ised', ' for', ' very', 'don', 'ervative', 'ex', 'ian', 'ting', 'irie', ' be', ' private', 'ation', 'ation', 'erc'], 'evidence_proportions': [1.2030436197916667, 0.9939778645833334, 0.9573974609375, 0.927764892578125]}, 'weight': {'score': [0.021440533074465664, 0.0025479690568015845, 0.0010439943183552134, 0.0025165016747054434, 0.0006143532016060569], 'topk_tokens': [' bathroom', 'Mary', ' Mary', ' football', '.', '.\n\n', '<|start_header_id|>', ' \n', ' Dan', 'Answer', '.', ' the', ' Mary', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006110668182373047, 0.026689231395721436, 0.063995361328125, 0.008651415506998697]}, 'saliency': {'score': [0.002237869934602217, 7.572468455995989e-05, 6.225434216586027e-05, 7.183631999894487e-05, 4.61195454452977e-05], 'topk_tokens': ['Print', ' Press', ' directly', ' Moore', ' Dan', ' the', ' Min', ' Mary', ' football', ' Mary', 'Answer', 'Mary', 'assistant', 'office', '.', '<|eot_id|>', '<|eot_id|>', ':', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [4.774332046508789e-05, 0.002643833557764689, 0.00774739682674408, 0.0003490149974822998]}}, 26: {'grad': {'score': [0.6776296442205255, 0.5979668834777887, 0.9546231356534091, 0.5971772957271756, 0.437733505711411], 'topk_tokens': [' generally', ' Field', ' Col', 'io', ' some', ' Col', 'BO', ' Col', ' gold', ' gold', ' sm', ' Guards', ' Gen', ' generally', 'str', ' Merch', ' generally', ' STR', ' worth', ' str'], 'evidence_proportions': [0.9088160196940104, 0.669189453125, 0.3326416015625, 0.68487548828125]}, 'weight': {'score': [0.012067475102164528, 0.0025105330604244945, 0.0005937462503259832, 0.002496707000122108, 0.0004450868476520885], 'topk_tokens': [' before', ' bathroom', ' football', '.\n\n', '?', ' kitchen', 'Bridge', '<|eot_id|>', ' Bridge', '<|eot_id|>', ' bathroom', ' \n', 'Answer', '<|start_header_id|>', 'assistant', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00039772192637125653, 0.005298515160878499, 0.017412185668945312, 0.026943047841389973]}, 'saliency': {'score': [0.0006475502794439143, 8.796961637180699e-05, 5.484711040150035e-05, 8.701690768905305e-05, 2.7865171432495117e-05], 'topk_tokens': [' Mary', '.', '.\n\n', ' \n', ':', ' bedroom', ' kitchen', ' Father', ' Bridge', ' Dan', 'assistant', 'Answer', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom', ' Bridge', 'Bridge', '<|start_header_id|>', 'office'], 'evidence_proportions': [3.62396240234375e-05, 0.0006438990434010824, 0.0004309117794036865, 0.001406937837600708]}}, 27: {'grad': {'score': [0.4728885997425426, 0.5759432820567987, 0.49920654296875, 0.5762686428588757, 0.430930166533499], 'topk_tokens': [' remember', ' N', ' earnest', ' product', '185', ' talk', ' newspaper', 'APER', ' newspaper', '\n', ' B', 'ile', ' tell', ' Newspaper', 'na', 'ly', 'sur', 'bread', 'remember', ' remember'], 'evidence_proportions': [0.4344278971354167, 0.263763427734375, 0.7074737548828125, 0.5640843709309896]}, 'weight': {'score': [0.020624396475878628, 0.0025504138584791975, 0.0014829987829381769, 0.0025196378046895384, 0.0006373181487574722], 'topk_tokens': ['Bridge', ' Mary', ' football', '<|eot_id|>', ' \n', '<|eot_id|>', ' the', '.', '.', ' bathroom', ' Bridge', 'Answer', ' football', '<|start_header_id|>', 'assistant', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0017855167388916016, 0.019855330387751263, 0.05389881134033203, 0.01804939905802409]}, 'saliency': {'score': [0.004059913483532992, 0.00013884647235881304, 0.00032103874466635966, 0.00013142098052393617, 5.6803226470947266e-05], 'topk_tokens': [' St', ' the', '<|eot_id|>', '.', 'Question', 'assistant', ' Dan', ' Mary', ' bathroom', 'Mary', ' ST', ' kitchen', ' Bridge', ' Mary', '<|begin_of_text|>', '.', ':', '.\n\n', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0004412929217020671, 0.00410538911819458, 0.007939636707305908, 0.005046576261520386]}}, 28: {'grad': {'score': [0.6081685152920809, 0.6362408738909721, 0.6183747378262606, 0.6363240068088465, 0.7671426715272845], 'topk_tokens': [' exact', 'being', ' executed', ' on', 'hum', ' ', ' returns', ' escaping', ' become', ' acted', ' reached', 'been', ' have', ' ', 'hom', '600', ' kept', ' be', 'na', ' ins'], 'evidence_proportions': [0.4423230489095052, 0.5271097819010416, 0.710357666015625, 0.7869466145833334]}, 'weight': {'score': [0.014747467907992277, 0.0024377503812083473, 0.0005268508737737483, 0.002418932087515357, 0.0005076672091628566], 'topk_tokens': ['Question', ' football', ' Bridge', '?', ' the', ' Bridge', '.\n\n', '<|eot_id|>', ' before', ' \n', ' bathroom', 'Answer', '<|eot_id|>', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003847976525624593, 0.003910551468531291, 0.013792037963867188, 0.04058400789896647]}, 'saliency': {'score': [0.0010466602715578947, 7.160782139864664e-05, 4.6199018304998226e-05, 6.988929213686798e-05, 2.4088404395363547e-05], 'topk_tokens': ['Question', ' \n', ' the', ' to', '<|eot_id|>', 'Bridge', ' before', ' the', ' soon', 'Answer', ' the', ' Far', ':', ' Bridge', 'assistant', '<|start_header_id|>', ' Bridge', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [5.598862965901693e-05, 0.0006803572177886963, 0.00287628173828125, 0.0011838873227437337]}}, 29: {'grad': {'score': [0.9247733029452238, 0.9174402488781658, 1.1526336669921875, 0.9170013591530705, 0.6811115380489465], 'topk_tokens': [' THE', ' were', '\n', ' The', ' o', ' paper', 'ants', 'APER', 'ATING', ' NEW', ' routes', ' Paul', '\n', ' printed', '\n', 'paper', '\n', ' the', 'ION', 'AILY'], 'evidence_proportions': [1.5862630208333333, 0.8926258087158203, 0.488128662109375, 0.5865275065104166]}, 'weight': {'score': [0.009178752248937433, 0.0025239149846656237, 0.0007719831033186479, 0.0025150424076791677, 0.00043586438352411443], 'topk_tokens': ['?', ' was', ' bathroom', ' to', ':', '<|eot_id|>', '<|eot_id|>', ' Where', '.\n\n', ' \n', ' Does', 'Answer', ' before', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0017025470733642578, 0.014741361141204834, 0.010318756103515625, 0.010332345962524414]}, 'saliency': {'score': [0.0007204819809306751, 7.944572226902117e-05, 0.00010494752363725142, 7.823951701109098e-05, 3.990892207983768e-05], 'topk_tokens': ['ot', ' the', ' of', ' before', ' in', '.\n\n', ' Where', 'Does', ' Do', 'assistant', 'Answer', ':', ' Does', '<|end_header_id|>', ' \n', ' the', '<|start_header_id|>', '<|begin_of_text|>', 'office', ':'], 'evidence_proportions': [0.00027372439702351886, 0.00085259477297465, 0.0005259811878204346, 0.0011647939682006836]}}, 30: {'grad': {'score': [0.7290760387073864, 0.8399221887550201, 0.8757234053178267, 0.8400579943432956, 1.0632477962609492], 'topk_tokens': [' Bull', ' office', ' Paul', 'G', ' one', ' Capt', 'op', ' Bridge', ' anx', 'Emp', 'ob', ' o', ' o', 'office', ' o', ' o', ' o', ' o', ' O', ' o'], 'evidence_proportions': [0.7609888712565104, 0.9774169921875, 0.7214469909667969, 0.4539082845052083]}, 'weight': {'score': [0.018358252265236595, 0.002441291505807189, 0.0012600719928741455, 0.002414624868690884, 0.002022740515795621], 'topk_tokens': ['.', ' before', ' the', '<|eot_id|>', ':', '?', ' Where', '<|eot_id|>', ' the', ' bathroom', 'Question', '.\n\n', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0014002323150634766, 0.010447025299072266, 0.031459808349609375, 0.03449312845865885]}, 'saliency': {'score': [0.004930864680897106, 0.00017262790918487006, 0.0001476190306923606, 0.00016406239025332166, 0.0001499652862548828], 'topk_tokens': ['.', ' football', ' Mary', ' Mary', 'Mary', ' the', ' Where', ' \n', '?', ' the', ' office', '<|begin_of_text|>', ' football', 'assistant', ' bathroom', '<|end_header_id|>', 'Question', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.000252912441889445, 0.004348983367284139, 0.015440225601196289, 0.003184457619984945]}}, 31: {'grad': {'score': [0.9311540777033026, 0.7822829903798869, 0.6804115988991477, 0.7821979370518833, 0.5856520334879557], 'topk_tokens': [' U', 'G', ' may', ' DAY', ' very', 'IR', ' an', 'editary', 'issippi', ' membership', ' Mary', ' Mary', 'Mary', ' marriage', ' most', 'Mr', 'Mexico', ' M', ' M', ' Mr'], 'evidence_proportions': [0.5069376627604166, 1.2762680053710938, 1.1463432312011719, 0.866797129313151]}, 'weight': {'score': [0.002707318826155229, 0.0023243102702183095, 0.000493466854095459, 0.0023269303546901436, 0.0007571614149845008], 'topk_tokens': [' bathroom', ' was', 'Question', ':', ' before', '?', ' football', '.\n\n', '<|eot_id|>', ' Where', ' the', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007175604502360026, 0.001633604367574056, 0.004729747772216797, 0.004422505696614583]}, 'saliency': {'score': [0.0010425258766521108, 0.00012066295699754335, 0.00016519156369295987, 0.00011891411981898781, 2.8855872876716383e-05], 'topk_tokens': [' before', 'Question', ' office', ' Mary', 'ot', 'Mary', ' was', ' Mary', ' Where', ' football', '<|eot_id|>', ' the', ' the', '<|end_header_id|>', ' \n', ':', '<|begin_of_text|>', '<|start_header_id|>', 'assistant', 'office'], 'evidence_proportions': [0.0005153516928354899, 0.0013532042503356934, 0.00264526903629303, 0.00019052624702453613]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 02:57:51.615 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:57:51.615 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-3_0-1-4-6.pkl | len: 10 |  size: 9.19 KB
Processing depth (0, 1, 4, 6):   4%|▍         | 4/100 [01:28<33:51, 21.16s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 2, 7, 8):   4%|▍         | 4/100 [01:35<33:51, 21.16s/it]2025-01-22 02:57:59.046 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:57:59.050 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1508, 1514) -->  tragedy. Mary journeyed to
2025-01-22 02:57:59.051 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:57:59.055 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1508, 1514) -->  tragedy. Mary journeyed to
2025-01-22 02:57:59.055 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:57:59.078 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8389, 8393) -->  Mary dropped the football
2025-01-22 02:57:59.078 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 02:57:59.105 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9638, 9644) -->  Daniel went back to the kitchen
2025-01-22 02:57:59.105 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:57:59.106 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (67, 73) --> . John went back to the
2025-01-22 02:57:59.106 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John took the milk.
2025-01-22 02:57:59.112 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2269, 2273) -->  took the milk.
2025-01-22 02:57:59.112 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 02:57:59.114 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (909, 915) --> . Sandra journeyed to the
2025-01-22 02:57:59.115 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:57:59.119 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1510, 1516) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:58:01.268 | INFO     | test_jbb_retain:begin_test:632 - kitchen<|eot_id|>
2025-01-22 02:58:01.268 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12190])
your chose emoji: ['🥷', '👨🏻\u200d❤\u200d👨🏾', '🎭', '😓', '🤸🏾\u200d♂', '🏮', '\U0001faf4🏻', '🧘🏼', '\U0001fae2', '🧑\u200d🚀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 215092.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.60s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.10it/s][A
 88%|████████▊ | 7/8 [00:04<00:00,  2.25it/s][A100%|██████████| 8/8 [00:04<00:00,  1.63it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.09it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.99it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.78it/s][A100%|██████████| 8/8 [00:00<00:00, 22.14it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.80it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.14it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.27it/s][A100%|██████████| 8/8 [00:00<00:00, 21.37it/s]
2025-01-22 02:58:09.520 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.5238848599520597, 0.5506442012436433, 0.5175614790482954, 0.5507525750826878, 1.2783963387472588], 'topk_tokens': [' super', ' rep', ' rept', 'sur', '�', 'enter', ' the', ' res', ' the', ' sure', ' S', ' Minnesota', 'cont', ' the', ' up', ' cons', 'Spring', 'con', ' cont', ' conn'], 'evidence_proportions': [0.7006632486979166, 0.7006632486979166, 0.20399856567382812, 0.3835856119791667]}, 'weight': {'score': [0.055747807025909424, 0.0025661670629746644, 0.008891563523899425, 0.0024583999571691894, 0.00640487461759333], 'topk_tokens': [' the', 'Bridge', '.', 'Answer', ' bathroom', ' the', '<|eot_id|>', '<|eot_id|>', ' office', '�', 'assistant', '<|start_header_id|>', ' bathroom', ':', '.', ' kitchen', ' football', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.006369243065516154, 0.006369243065516154, 0.14035797119140625, 0.09809815883636475]}, 'saliency': {'score': [0.010964445092461327, 0.00010323274166878127, 0.0016798079013824463, 8.070793715518754e-05, 0.0008572224985089218], 'topk_tokens': [' random', '�', ' bathroom', ' Bench', '<|eot_id|>', 'Question', ' dropped', '<|eot_id|>', ' hall', ' top', ' bathroom', '.', '.', ' office', '<|begin_of_text|>', '�', ' kitchen', ' office', ' football', 'office'], 'evidence_proportions': [0.00019981463750203451, 0.00019981463750203451, 0.03648996353149414, 0.015476693709691366]}}, 25: {'grad': {'score': [0.7021949074485085, 0.5785129977649278, 0.6718555797230114, 0.5781199669107858, 0.5586245352761787], 'topk_tokens': [' Europe', '\n', ' of', ' consisted', ' printed', ' the', ' Robert', 'age', 'le', '186', 'ing', ' down', 'THE', ' for', 'THE', ' be', 'ing', 'ation', 'ation', ' Wood'], 'evidence_proportions': [0.6955388387044271, 0.6955388387044271, 0.8962745666503906, 0.58612060546875]}, 'weight': {'score': [0.020158670165322044, 0.0025480378644047136, 0.0036668452349576082, 0.0025141188923292817, 0.002672720373722545], 'topk_tokens': ['.', ' top', ' the', '�', ' the', ' football', ' \n', ' Mary', '.\n\n', 'Answer', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '.', 'assistant', '.', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007755100727081299, 0.007755100727081299, 0.058200836181640625, 0.01960436503092448]}, 'saliency': {'score': [0.0016791793433102694, 4.543914530533818e-05, 0.00032200867479497737, 4.197957879193017e-05, 0.00012406027107908014], 'topk_tokens': [' People', ' Min', ' Merch', ' Mary', ' Market', 'assistant', '<|start_header_id|>', ' Anthony', '.', ' football', 'office', '<|begin_of_text|>', ' directly', ' Mary', 'Answer', ':', '<|eot_id|>', '<|eot_id|>', '.', '<|end_header_id|>'], 'evidence_proportions': [0.001061876614888509, 0.001061876614888509, 0.004946529865264893, 0.000735551118850708]}}, 26: {'grad': {'score': [0.7682245427911932, 0.8203895548822999, 1.38604736328125, 0.819459620611739, 0.4662391261050576], 'topk_tokens': [' Gal', ' black', ' marshal', ' marched', ' clear', ' journey', ' school', 'burn', 'BO', ' l', ' much', ' march', ' Col', 'k', ' journey', ' Gal', ' wh', ' Guards', ' kids', ' Merch'], 'evidence_proportions': [0.8867390950520834, 0.8867390950520834, 0.1643218994140625, 0.9337972005208334]}, 'weight': {'score': [0.044742334972728386, 0.0024985168862530567, 0.0012717788869684393, 0.0024242350487556747, 0.0024291297845673143], 'topk_tokens': [' before', '�', ' football', ' the', '?', ' bathroom', '.\n\n', ' the', '<|eot_id|>', '<|eot_id|>', ' \n', 'Answer', ' the', 'assistant', '<|start_header_id|>', ' kitchen', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015344818433125813, 0.0015344818433125813, 0.027153968811035156, 0.1428836186726888]}, 'saliency': {'score': [0.010023444890975952, 0.00012109389533520996, 0.00013496659018776634, 0.00010313563708769164, 0.00039599025458620307], 'topk_tokens': [' bathroom', ' \n', ' the', '<|eot_id|>', ' Anthony', ' Az', '<|end_header_id|>', ' football', ' Bridge', ' bathroom', 'Answer', '<|start_header_id|>', ' the', 'Bridge', '�', ' the', '<|begin_of_text|>', ' kitchen', ':', 'office'], 'evidence_proportions': [0.00015311439832051596, 0.00015311439832051596, 0.0028839111328125, 0.03452379504839579]}}, 27: {'grad': {'score': [0.42235287753018463, 0.45050189000727936, 0.36480539495294745, 0.4507080639590156, 0.38458867658648577], 'topk_tokens': [' earnest', 'b', '186', 're', '186', 'AT', 'ole', '186', '-per', ' plainly', ' Bottle', 'sur', ' very', '186', 'ab', 'ile', ' B', ' per', ' B', 'sur'], 'evidence_proportions': [0.4008585611979167, 0.4008585611979167, 0.4405250549316406, 0.4532267252604167]}, 'weight': {'score': [0.032505994493311104, 0.0025495950944154594, 0.0031046975742686877, 0.002494338834839194, 0.0028095579983895285], 'topk_tokens': ['Question', ' before', '<|eot_id|>', '<|eot_id|>', 'THE', ' \n', ' football', ' football', ' bathroom', '.', 'Answer', '.', 'assistant', ' kitchen', '<|start_header_id|>', '.\n\n', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005327512820561727, 0.005327512820561727, 0.05654621124267578, 0.07083614667256673]}, 'saliency': {'score': [0.007744862274690108, 0.0001290991530800116, 0.0006167292594909668, 0.00011442392662162642, 0.0002109544319018983], 'topk_tokens': [' Mary', ' office', 'Bridge', 'Question', ' football', ' Jackson', ' Mary', ' the', '.', 'THE', 'assistant', 'THE', '<|start_header_id|>', '<|begin_of_text|>', '.', '.\n\n', '<|end_header_id|>', ' kitchen', ':', 'office'], 'evidence_proportions': [0.001045544942220052, 0.001045544942220052, 0.006640344858169556, 0.021879841883977253]}}, 28: {'grad': {'score': [0.5440271550958807, 0.6818859671044537, 0.7331903631036932, 0.6820427169523469, 0.574041533888432], 'topk_tokens': [' ', ' ', 'hom', '600', ' ', ' about', ' on', ' ', ' about', ' ', ' ', ' ', ' ', ' ', ' reached', ' ins', ' become', ' ', ' ', 'na'], 'evidence_proportions': [0.50164794921875, 0.50164794921875, 0.45654296875, 0.6871083577473959]}, 'weight': {'score': [0.024387094107541172, 0.002407197094964856, 0.008972457864067772, 0.002355501880016145, 0.0017663710995724326], 'topk_tokens': [' kitchen', ' bathroom', '?', ' the', ' the', ' the', '<|eot_id|>', ' the', ' the', ' before', '.\n\n', 'Answer', '<|eot_id|>', ' \n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0021256705125172934, 0.0021256705125172934, 0.015119552612304688, 0.07508830229441325]}, 'saliency': {'score': [0.0030695443803613835, 6.245134344564023e-05, 0.0010523172942074862, 5.521285993157149e-05, 0.00010072871258384304], 'topk_tokens': [' nearly', ' the', '<|start_header_id|>', 'Bridge', ' to', ' Floral', ' Far', '.\n\n', ' the', '<|eot_id|>', ' the', ':', ' the', ' Bridge', ' Bridge', '<|begin_of_text|>', 'assistant', ' kitchen', 'office', '<|end_header_id|>'], 'evidence_proportions': [8.830428123474121e-05, 8.830428123474121e-05, 0.0019112229347229004, 0.009804238875706991]}}, 29: {'grad': {'score': [0.6655495383522727, 0.9684874366900427, 0.8812796852805398, 0.9691939895624858, 0.8918681897615132], 'topk_tokens': [' o', ' printing', ' were', '\n', ' printed', ' regular', ' The', ' The', '.', ' Press', 'paper', '\n', 'APER', ' Travel', '\n', 'AILY', '\n', ' printed', '\n', ' Paul'], 'evidence_proportions': [0.7913411458333334, 0.7913411458333334, 0.418182373046875, 0.5788777669270834]}, 'weight': {'score': [0.01036083698272705, 0.0024685459187024535, 0.0017169253392653031, 0.002455614180913441, 0.0012804531214530008], 'topk_tokens': [' bathroom', ' Where', ' to', ' the', ' the', '?', ' Does', '<|eot_id|>', '<|eot_id|>', ' before', ' the', '.\n\n', ' \n', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0020241141319274902, 0.0020241141319274902, 0.008800506591796875, 0.02807450294494629]}, 'saliency': {'score': [0.0008199242028323087, 5.375136305966715e-05, 0.00019313801418651235, 5.211139690887783e-05, 0.00021262231626008687], 'topk_tokens': ['.\n\n', ' the', ' bathroom', '<|end_header_id|>', '?', ' the', '<|eot_id|>', 'IVE', ' the', ' to', ' office', 'assistant', '�', 'Answer', '<|eot_id|>', ' \n', '<|start_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.00030549367268880207, 0.00030549367268880207, 0.0005339384078979492, 0.002039442459742228]}}, 30: {'grad': {'score': [0.6050969904119318, 0.9813970931245899, 0.6728099476207386, 0.9826374223525092, 1.1439540929961622], 'topk_tokens': [' in', ' hall', ' men', ' in', 'op', ' months', ' rapid', ' Bridge', 'ob', ' it', ' an', ' I', ' an', ' its', ' the', ' O', 'deal', ' account', ' to', '3'], 'evidence_proportions': [0.5241495768229166, 0.5241495768229166, 0.6541748046875, 0.7342732747395834]}, 'weight': {'score': [0.022892328825863926, 0.0024214914151689825, 0.0034606971523978495, 0.002382536776606723, 0.007423125860983865], 'topk_tokens': [' before', ' the', ' the', '.', ':', '<|eot_id|>', ' the', '?', ' bathroom', '<|eot_id|>', 'Question', 'Answer', 'assistant', '.\n\n', ' \n', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00522427757581075, 0.00522427757581075, 0.03510475158691406, 0.05008681615193685]}, 'saliency': {'score': [0.0017080523750998757, 0.00014726496900473366, 0.0003247965465892445, 0.00014411687733195195, 0.000315689204031961], 'topk_tokens': [' the', ' the', '<|eot_id|>', 'Question', '<|eot_id|>', '.', ' Wild', ' football', ':', '.', '?', ' \n', '.\n\n', 'assistant', ' office', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.00045100847880045575, 0.00045100847880045575, 0.004198431968688965, 0.0025618871053059897]}}, 31: {'grad': {'score': [1.0483176491477273, 1.153950848917323, 1.0003995028409092, 1.1544202315324539, 0.707096300627056], 'topk_tokens': [' S', ' an', ' M', ' an', ' F', 'C', ' S', ' U', ' Aw', ' w', ' L', ' Aw', 'c', ' H', 'CO', ' W', ' C', ' B', ' W', ' C'], 'evidence_proportions': [0.9797770182291666, 0.9797770182291666, 1.16375732421875, 1.1084391276041667]}, 'weight': {'score': [0.0034280798651955342, 0.002283800618229263, 0.0008786916732788086, 0.002284272980211124, 0.0009445548057556152], 'topk_tokens': [' was', ' football', ' the', ':', '?', 'Question', ' before', ' the', '<|eot_id|>', ' Where', '.\n\n', 'Answer', '<|start_header_id|>', ' \n', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019875168800354004, 0.0019875168800354004, 0.007892847061157227, 0.0033326943715413413]}, 'saliency': {'score': [0.0002083832567388361, 0.0001235028381735634, 0.00022175908088684082, 0.00012317117810210077, 4.183095798157809e-05], 'topk_tokens': [' office', ' the', ' football', ' before', ' office', ' Where', 'Question', '.\n\n', ' the', ' the', 'Answer', '<|start_header_id|>', ' the', ' \n', '<|eot_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [8.544325828552246e-05, 8.544325828552246e-05, 0.0004140138626098633, 0.00031717618306477863]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 02:58:09.522 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:58:09.522 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-4_1-2-7-8.pkl | len: 10 |  size: 9.02 KB
Processing depth (1, 2, 7, 8):   5%|▌         | 5/100 [01:46<31:38, 19.99s/it]Processing depth (1, 2, 7, 8):   5%|▌         | 5/100 [01:46<33:43, 21.29s/it]
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:72 - Selected idx: 1
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:74 - Answer: office
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:76 - Needle: [' Mary journeyed to the office.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary got the apple.', ' John took the milk.', ' Mary journeyed to the bathroom.', ' Sandra journeyed to the bedroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:77 - Real Needle: [' Mary journeyed to the office.', ' Mary got the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 02:58:09.868 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (2, 4, 5, 6, 7):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 02:58:16.865 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:58:16.872 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2478, 2484) --> . Mary journeyed to the
2025-01-22 02:58:16.873 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the apple.
2025-01-22 02:58:16.886 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4917, 4921) -->  Mary got the apple
2025-01-22 02:58:16.886 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 02:58:16.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (2478, 2484) --> . Mary journeyed to the
2025-01-22 02:58:16.894 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 02:58:16.913 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7320, 7324) -->  Mary dropped the apple
2025-01-22 02:58:16.913 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 02:58:16.937 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (8497, 8503) --> . Daniel went back to the
2025-01-22 02:58:16.938 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:58:16.943 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1825, 1831) --> . John went back to the
2025-01-22 02:58:16.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the office.
2025-01-22 02:58:16.950 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2479, 2485) -->  Mary journeyed to the office
2025-01-22 02:58:16.950 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John took the milk.
2025-01-22 02:58:16.951 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (147, 151) -->  John took the milk
2025-01-22 02:58:16.951 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 02:58:16.970 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6723, 6729) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:58:19.113 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 02:58:19.114 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12235])
your chose emoji: ['🚶\u200d♂️\u200d➡️', '🈂️', '🇱🇰', '👮🏾\u200d♀️', '👷🏽\u200d♂️', '🧑🏻\u200d❤\u200d💋\u200d🧑🏼', '🙌🏾', '💆🏻', '🧏🏻\u200d♀️', '🧑🏾\u200d🔬']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 211034.16it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.55s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.48s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.96it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.20it/s][A
100%|██████████| 8/8 [00:00<00:00, 20.71it/s][A100%|██████████| 8/8 [00:00<00:00, 20.28it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 14.91it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.26it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.94it/s][A100%|██████████| 8/8 [00:00<00:00, 20.72it/s]
2025-01-22 02:58:28.420 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.923431396484375, 0.6899966705738744, 1.0988637750799006, 0.6887607710601483, 1.4523870294744319], 'topk_tokens': [' offices', ' office', ' Ot', ' office', 'office', ' office', ' Or', ' sure', '800', ' office', ' offices', 'active', 'Official', ' office', ' office', ' or', ' office', 'office', ' office', ' office'], 'evidence_proportions': [1.1615397135416667, 0.8691864013671875, 1.1615397135416667, 1.071533203125, 0.3846435546875]}, 'weight': {'score': [0.0239674082169166, 0.0025519373916260625, 0.02236191521991383, 0.00247050152702028, 0.002586351199583574], 'topk_tokens': [' Bridge', '.', ' Mary', '?\n', ' the', '.', 'Answer', '<|eot_id|>', ' Mary', ' Sandra', 'assistant', ':', '<|eot_id|>', 'Bridge', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', ' bedroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00900368889172872, 0.030162334442138672, 0.00900368889172872, 0.0882110595703125, 0.006935795148213704]}, 'saliency': {'score': [0.0030526862694666935, 0.00013686077865771417, 0.00155302340334112, 0.00012808507593361305, 0.00039374760606072164], 'topk_tokens': [' Bridge', ' Mary', ' Mary', ' John', '.', ' Mary', ' Market', ' office', '<|eot_id|>', '.', '<|end_header_id|>', ' Mary', '<|start_header_id|>', '<|eot_id|>', ' bedroom', ' bathroom', ' office', '<|begin_of_text|>', 'Bridge', 'office'], 'evidence_proportions': [0.0015221238136291504, 0.004465579986572266, 0.0015221238136291504, 0.010418891906738281, 0.0002610782782236735]}}, 25: {'grad': {'score': [1.0046245868389423, 0.6708350730622293, 0.9937522194602273, 0.6695402412812013, 0.6056269732388583], 'topk_tokens': ['regular', ' marched', ' at', ' regular', ' Aw', ' to', ' thirst', ' United', ' Tribune', ' mentioned', 'ised', 'itable', ' principles', ' subscribers', ' circulated', ' at', 'op', 'op', ' at', ' Wood'], 'evidence_proportions': [1.0487467447916667, 1.0836181640625, 1.0487467447916667, 0.832977294921875, 0.9781494140625]}, 'weight': {'score': [0.02070084672707778, 0.002530787752336504, 0.008229128339073875, 0.0024817447606020303, 0.0025002726099707866], 'topk_tokens': ['.\n\n', '.', ' Geo', '?\n', ' Sandra', ' bedroom', 'Answer', ' Mary', ' Judge', ' Mary', '.', '<|eot_id|>', ':', '<|start_header_id|>', ' Mary', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004430413246154785, 0.041230201721191406, 0.004430413246154785, 0.06774711608886719, 0.00819129745165507]}, 'saliency': {'score': [0.008418172597885132, 7.108424997138775e-05, 0.0014045970006422563, 5.0872454289993336e-05, 0.0002791868014769121], 'topk_tokens': ['�', ' Ramsey', ' apple', 'assistant', ' Sandra', '<|end_header_id|>', '<|begin_of_text|>', ' apple', ' Judge', '.', ' Paul', 'Right', ' Mary', ' bedroom', '.', '<|eot_id|>', ' Mary', '<|eot_id|>', ' Mary', ' Mary'], 'evidence_proportions': [0.0029266774654388428, 0.01654428243637085, 0.0029266774654388428, 0.028290510177612305, 0.0007355312506357828]}}, 26: {'grad': {'score': [0.7142620086669922, 0.567154213719151, 0.824981689453125, 0.5663750680029486, 0.5412297682328657], 'topk_tokens': [' Seventh', 'leading', 'istributed', ' Marshall', ' effect', ' existence', ' date', 'ers', ' week', ' office', 'str', ' office', ' office', ' Press', ' str', ' Roman', ' there', ' Jul', ' clear', ' part'], 'evidence_proportions': [0.76904296875, 0.5220060348510742, 0.76904296875, 1.1470947265625, 0.4443155924479167]}, 'weight': {'score': [0.007869465993000911, 0.0024729137415978276, 0.006221204996109009, 0.002454637199950791, 0.0021178478544408626], 'topk_tokens': [' discarded', ' the', ' Sandra', ' Bridge', ' Mary', '.\n\n', ' bathroom', '<|eot_id|>', '<|eot_id|>', '?\n', 'Bridge', ' the', 'Answer', 'assistant', ' bedroom', '<|start_header_id|>', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0017900168895721436, 0.010646820068359375, 0.0017900168895721436, 0.02861499786376953, 0.004346440235773723]}, 'saliency': {'score': [0.0004451916768000676, 6.49493504575901e-05, 0.00045680999755859375, 6.343099499601602e-05, 0.00011836195533925837], 'topk_tokens': ['?\n', ' Sandra', ' Mary', '.', '<|eot_id|>', 'assistant', '<|eot_id|>', ' apple', ' bathroom', ' Bridge', ' Ramsey', ' Seventh', ':', ' the', '<|end_header_id|>', '<|start_header_id|>', ' the', 'Bridge', 'office', ' bedroom'], 'evidence_proportions': [0.00014052788416544595, 0.00043688714504241943, 0.00014052788416544595, 0.0013534575700759888, 0.0004545450210571289]}}, 27: {'grad': {'score': [0.7498098520132211, 0.8709062902212552, 0.5638427734375, 0.8717188183008886, 0.5957931605252352], 'topk_tokens': [' excursion', ' intended', 'aha', 'ile', ' exchanged', ' insisted', ' designated', ' opposition', ' intended', 'mission', ' decision', ' anticipated', ' ', ' N', 'roduced', 'bread', ' excessive', ' accepted', 'itated', ' interruption'], 'evidence_proportions': [0.7021280924479166, 0.7947540283203125, 0.7021280924479166, 0.76788330078125, 0.80316162109375]}, 'weight': {'score': [0.017697091285999004, 0.0025291350909363475, 0.006498832594264637, 0.0024896158353661735, 0.003024430437521501], 'topk_tokens': [' Mary', ' Mary', '<|eot_id|>', ' Bridge', ' THE', '?\n', '<|eot_id|>', 'Bridge', ' Mary', ' bathroom', 'Answer', '.', 'assistant', '.\n\n', ' bedroom', '<|start_header_id|>', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.008114596207936605, 0.03390836715698242, 0.008114596207936605, 0.04612112045288086, 0.00710521141688029]}, 'saliency': {'score': [0.004640086339070247, 0.0001221966040117869, 0.0015376345677809281, 0.00011000489277095985, 0.00042204829779538244], 'topk_tokens': [' prior', 'Christmas', ' the', ' Mary', '<|start_header_id|>', ' the', 'Answer', 'THE', ' Sandra', 'Bridge', ' Bridge', ' Mary', ' Mary', ' bedroom', ' Mary', '<|begin_of_text|>', '.', ':', '.\n\n', 'office'], 'evidence_proportions': [0.0018492639064788818, 0.00873596966266632, 0.0018492639064788818, 0.012811616063117981, 0.0020434558391571045]}}, 28: {'grad': {'score': [0.9815855759840745, 0.8388297593875133, 0.897796630859375, 0.8384188214595544, 0.5152265375310724], 'topk_tokens': ['600', ' after', ' PA', ' acted', ' ', ' have', ' been', ' been', ' had', ' first', ' ', ' next', ' first', 'being', 'hom', ' taken', ' ins', 'na', ' be', 'been'], 'evidence_proportions': [0.7316385904947916, 0.97265625, 0.7316385904947916, 1.27398681640625, 1.292498270670573]}, 'weight': {'score': [0.005850945527736957, 0.0024276927297407773, 0.007876935330304232, 0.002410555318143411, 0.0013997961174358022], 'topk_tokens': [' the', ' the', ' bedroom', ' discarded', ' to', 'Question', ' the', '.\n\n', ' Bridge', '<|eot_id|>', ' the', 'Answer', '<|eot_id|>', '?\n', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006603201230367025, 0.007786750793457031, 0.0006603201230367025, 0.01801013946533203, 0.006835530201594035]}, 'saliency': {'score': [0.0006699424523573655, 8.194427275137446e-05, 0.0013085143132643266, 7.847618730047278e-05, 5.9406865726817735e-05], 'topk_tokens': [' to', ' Mary', ' apple', ' Bridge', ' to', ' bathroom', ' Far', ' the', 'Bridge', 'Answer', 'assistant', '?\n', 'office', ' the', ' bedroom', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.0001293818155924479, 0.000841975212097168, 0.0001293818155924479, 0.0015120506286621094, 0.00107496976852417]}}, 29: {'grad': {'score': [0.9796870304987981, 0.9501288357031952, 1.2945168235085227, 0.9494441980137701, 0.6204088384454901], 'topk_tokens': ['\n', ' newspaper', ' Printing', 'ATING', ' the', ' regular', ' press', '\n', 'nes', ' printing', ' the', ' business', '\n', 'aper', '\n', ' press', 'paper', 'APER', '\n', ' Press'], 'evidence_proportions': [1.1513671875, 0.76739501953125, 1.1513671875, 0.8221282958984375, 0.8828938802083334]}, 'weight': {'score': [0.0027652222376603345, 0.0024994389336902333, 0.0019634366035461426, 0.0024998394329403726, 0.0013369626619599082], 'topk_tokens': [':', 'Does', 'am', ' was', ' Does', ' discarded', 'Question', ' Where', ' the', '.\n\n', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00038965543111165363, 0.003960609436035156, 0.00038965543111165363, 0.007065296173095703, 0.003852715094884237]}, 'saliency': {'score': [0.00026467442512512207, 7.237931759887207e-05, 0.00033767114986072886, 7.149031168325608e-05, 0.000118015164678747], 'topk_tokens': ['�', ' discarded', ' bathroom', '.\n\n', ' the', ' to', '<|eot_id|>', ' in', ' to', '<|eot_id|>', 'Answer', 'Does', ' the', ' Does', '?\n', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.6994616190592446e-05, 0.00022664666175842285, 3.6994616190592446e-05, 0.0008872151374816895, 0.00033035874366760254]}}, 30: {'grad': {'score': [1.0495840219350963, 1.1835880041063986, 1.4033660888671875, 1.1834771652903937, 1.1022817438299006], 'topk_tokens': [' office', ' head', ' office', ' offices', ' office', ' office', 'op', ' office', ' offices', ' o', 'ob', 'office', ' o', ' office', ' o', ' o', 'op', ' o', 'office', ' O'], 'evidence_proportions': [0.9394276936848959, 1.4312744140625, 0.9394276936848959, 0.7516326904296875, 1.2140706380208333]}, 'weight': {'score': [0.01182621717453003, 0.0024516425257835005, 0.01158474792133678, 0.0024151614970223567, 0.005143918774344705], 'topk_tokens': ['.', ':', ' Where', ',', ' Mary', ' Miles', ' the', '<|eot_id|>', '<|eot_id|>', 'Question', ' Sandra', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0036512911319732666, 0.01596212387084961, 0.0036512911319732666, 0.03505134582519531, 0.009935379028320312]}, 'saliency': {'score': [0.00229567289352417, 0.00022113698267455466, 0.0012775605375116522, 0.00021480510541731403, 0.000324503941969438], 'topk_tokens': [' Nearly', ' nearly', ':', '.\n\n', ' the', ' Geo', ' Mary', ' the', ' office', ' to', 'Question', '<|begin_of_text|>', ' Mary', '.', ' bedroom', '?\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0010995964209238689, 0.003058522939682007, 0.0010995964209238689, 0.007051944732666016, 0.0010084112485249837]}}, 31: {'grad': {'score': [0.9952392578125, 1.883194761532647, 0.8834214643998579, 1.8868933303761652, 1.220922426743941], 'topk_tokens': [' w', ' two', ' its', ' OCC', ' and', ' C', ' Aw', ' their', ' S', ' U', ' were', ' S', ' Aw', ' B', ' L', ' was', 'editary', ' C', ' S', ' W'], 'evidence_proportions': [1.0314127604166667, 0.8653564453125, 1.0314127604166667, 1.4953765869140625, 0.676055908203125]}, 'weight': {'score': [0.002784577699807974, 0.0022971348171385323, 0.0018960183317011054, 0.002296819044534563, 0.00101056153124029], 'topk_tokens': [' to', ' where', ' was', ' the', ',', ':', 'Question', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019831260045369468, 0.004329502582550049, 0.0019831260045369468, 0.004927635192871094, 0.0019288261731465657]}, 'saliency': {'score': [0.000742107629776001, 0.00013403261449199258, 0.00036160512404008347, 0.00013232480042952294, 7.09749080918052e-05], 'topk_tokens': [' the', ' Mary', ' Mary', ' Mary', ' Judge', ' Miles', ' Where', ' the', ' Emily', ':', 'Question', ' Market', '?\n', '<|start_header_id|>', '<|eot_id|>', 'Answer', '<|end_header_id|>', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0006732443968454996, 0.0011110752820968628, 0.0006732443968454996, 0.0012309849262237549, 0.0003079374631245931]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 02:58:28.421 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:58:28.421 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-0_2-4-5-6-7.pkl | len: 10 |  size: 9.7 KB
Processing depth (2, 4, 5, 6, 7):   1%|          | 1/100 [00:18<30:29, 18.48s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (0, 3, 6, 8, 9):   1%|          | 1/100 [00:25<30:29, 18.48s/it]2025-01-22 02:58:35.799 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:58:35.800 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 02:58:35.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the apple.
2025-01-22 02:58:35.810 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3818, 3822) -->  Mary got the apple
2025-01-22 02:58:35.810 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 02:58:35.831 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7211, 7217) --> . Mary journeyed to the
2025-01-22 02:58:35.831 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 02:58:35.856 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9662, 9666) -->  dropped the apple.
2025-01-22 02:58:35.857 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 02:58:35.887 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10768, 10774) --> . Daniel went back to the
2025-01-22 02:58:35.887 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:58:35.892 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1875, 1881) --> . John went back to the
2025-01-22 02:58:35.892 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the office.
2025-01-22 02:58:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 02:58:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John took the milk.
2025-01-22 02:58:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (151, 155) -->  John took the milk
2025-01-22 02:58:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 02:58:35.912 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6618, 6624) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:58:38.014 | INFO     | test_jbb_retain:begin_test:632 - Mary's hand<|eot_id|>
2025-01-22 02:58:38.015 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12208])
your chose emoji: ['👩🏻\u200d❤\u200d👨🏾', '🧨', '🇹🇷', '🤾🏽\u200d♂', '🐈', '🛌', '⭕', '🏌🏼\u200d♀', '☎', '🙇🏿\u200d♂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 215092.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.73s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.07it/s][A
 88%|████████▊ | 7/8 [00:04<00:00,  2.20it/s][A100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.89it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.75it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.79it/s][A100%|██████████| 8/8 [00:00<00:00, 18.29it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.73it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.71it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.10it/s][A100%|██████████| 8/8 [00:00<00:00, 17.81it/s]
2025-01-22 02:58:46.942 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6724021618182843, 0.7536802249692874, 0.6828960071910511, 0.7539820242155502, 1.333341004418545], 'topk_tokens': [' sure', ' editors', ' evil', ' editor', ' office', ' Or', ' office', ' office', 'ors', 'ent', ' election', ' election', 'editor', ' office', ' or', 'office', 'active', 'enter', ' office', ' effect'], 'evidence_proportions': [0.868560791015625, 0.9215087890625, 0.5081787109375, 0.95166015625, 0.2882239023844401]}, 'weight': {'score': [0.01950168151121873, 0.0025669116645831735, 0.002663796598261053, 0.0025305331508063736, 0.002574523941415255], 'topk_tokens': ['\n\n', ' office', 'Mary', ' the', ' the', '?\n', '<|eot_id|>', 'Answer', '.', ' Bridge', '<|eot_id|>', ':', '<|start_header_id|>', 'Bridge', 'assistant', ' bedroom', ' bathroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0029665629069010415, 0.02277350425720215, 0.030779143174489338, 0.023403167724609375, 0.019977132479349773]}, 'saliency': {'score': [0.004612633815178504, 0.0001743137787640046, 0.0006400698965246028, 0.00016398299800939926, 0.00022586838143770813], 'topk_tokens': [' random', ' office', ' Third', ' bathroom', ' Mary', '<|eot_id|>', ' top', '<|start_header_id|>', '<|end_header_id|>', 'Bridge', ' Market', '<|eot_id|>', ' Mary', '.', ' Mary', ' bedroom', 'Mary', '<|begin_of_text|>', ' office', 'office'], 'evidence_proportions': [0.002036402622858683, 0.01365804672241211, 0.007246673107147217, 0.001617431640625, 0.0005213518937428793]}}, 25: {'grad': {'score': [1.4137244591346154, 1.141404490427928, 1.4606489701704546, 1.1402448375960985, 0.7977077296522798], 'topk_tokens': ['ENT', ' subscribers', ' exc', ' at', 'ished', ' from', ' grand', 'ised', ' thirst', ' circulated', ' rest', ' be', 'THE', ' at', 'ation', ' THE', ' principles', 'ation', 'itable', 'erc'], 'evidence_proportions': [1.4842122395833333, 1.3465576171875, 1.3548990885416667, 1.0328369140625, 1.7007649739583333]}, 'weight': {'score': [0.021609308627935555, 0.0025417065444683852, 0.0007897100665352561, 0.0025041129141727416, 0.001870552047354276], 'topk_tokens': [' prior', ' to', '?\n', '.', '.', 'Mary', ' Mary', '.\n\n', ' Mary', 'Answer', '<|eot_id|>', ' Mary', '<|start_header_id|>', '<|eot_id|>', ' the', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004673302173614502, 0.03630661964416504, 0.023069967826207478, 0.026266098022460938, 0.02838789423306783]}, 'saliency': {'score': [0.002742629784804124, 7.974117335289058e-05, 3.927133300087669e-05, 7.412163977206606e-05, 0.00019649794844330335], 'topk_tokens': [' the', ' Paul', ':', ' Bench', '<|end_header_id|>', ' Mary', 'Answer', ' directly', 'office', '.', '<|start_header_id|>', ' top', 'assistant', ' Paul', ' Mary', '<|eot_id|>', '<|eot_id|>', ' Mary', 'Mary', '<|begin_of_text|>'], 'evidence_proportions': [5.389253298441569e-05, 0.007515117526054382, 0.0035569965839385986, 0.002444028854370117, 0.0016344090302785237]}}, 26: {'grad': {'score': [0.9549161470853366, 0.6927501631603195, 1.2241973876953125, 0.6912283612756113, 0.5238044301017386], 'topk_tokens': ['BO', ' STR', ' proprietor', 'little', ' esp', ' searched', ' Press', ' some', ' July', 'istributed', 'prob', ' Col', 'ols', ' Merch', 'str', ' wh', ' Jul', ' part', ' clear', ' str'], 'evidence_proportions': [1.1344095865885417, 1.08367919921875, 1.14739990234375, 0.69891357421875, 0.6677652994791666]}, 'weight': {'score': [0.013632143919284526, 0.0024795569717444717, 0.0007989596236835827, 0.0024587549557126757, 0.0015807591500829477], 'topk_tokens': [' Anthony', '.', ' bedroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'Bridge', ' Bridge', ' the', ' kitchen', ' bathroom', '?\n', 'Answer', 'assistant', '<|start_header_id|>', ' the', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002640982468922933, 0.010239183902740479, 0.011447668075561523, 0.007953643798828125, 0.0352323055267334]}, 'saliency': {'score': [0.0021327298421126148, 9.885548959493052e-05, 0.00011562488295815207, 9.447712585381468e-05, 0.0001052285804123175], 'topk_tokens': [' the', 'Answer', ' apple', ' bedroom', 'assistant', ' Third', ' Bridge', ' Az', ' Seventh', '<|end_header_id|>', ' bedroom', ' the', '<|start_header_id|>', ' the', ' Bridge', 'Bridge', ':', ' kitchen', '<|begin_of_text|>', 'office'], 'evidence_proportions': [3.095467885335287e-05, 0.0008513778448104858, 0.0011678636074066162, 0.0004231780767440796, 0.007193307081858317]}}, 27: {'grad': {'score': [0.6216407189002404, 0.6657818018657863, 0.7142764004794034, 0.6657884444400014, 0.6029402936091188], 'topk_tokens': [' tell', ' product', 'na', ' news', '186', ' Moore', ' express', 'APER', 'paper', '185', ' N', 'ly', ' earnest', ' Newspaper', ' told', ' paper', 'bread', ' talk', 'ile', ' newspaper'], 'evidence_proportions': [0.5444132486979166, 0.72381591796875, 0.4818318684895833, 0.6580810546875, 0.7462666829427084]}, 'weight': {'score': [0.01769225184734051, 0.002539509578770443, 0.0016079192811792548, 0.002508801116968452, 0.002408350100282763], 'topk_tokens': [' the', ' Geo', '?\n', 'Mary', 'THE', ' Mary', '.', '.', ' the', ' bedroom', 'Answer', ' Bridge', 'assistant', ' bathroom', '.\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0010162989298502605, 0.03740561008453369, 0.024008760849634807, 0.007801532745361328, 0.0215032696723938]}, 'saliency': {'score': [0.004856219658484826, 0.0001464854099045612, 0.0004668371243910356, 0.00013583742205866504, 0.0004425341965722256], 'topk_tokens': [' the', ' kitchen', ' Mary', ' prior', '.', 'THE', '<|start_header_id|>', ' the', '.', '.', '<|begin_of_text|>', ' Mary', '<|end_header_id|>', ' bathroom', 'THE', ' Mary', 'Mary', ':', '.\n\n', 'office'], 'evidence_proportions': [0.0004234711329142253, 0.015240117907524109, 0.009543379147847494, 0.0008633285760879517, 0.00034113725026448566]}}, 28: {'grad': {'score': [1.060570056621845, 1.099781493140868, 1.1119162819602273, 1.099843368818838, 0.750873471869797], 'topk_tokens': [' child', 'messages', ' returns', 'hum', ' ', 'being', ' become', ' acted', ' taken', ' balance', 'been', ' executed', 'hom', ' kept', 'CH', '600', ' be', 'na', ' acted', ' ins'], 'evidence_proportions': [1.0553690592447917, 1.1422119140625, 0.6425666809082031, 1.284423828125, 1.2801106770833333]}, 'weight': {'score': [0.03843141977603619, 0.0024410697693320986, 0.001257059249010953, 0.0023662711450328677, 0.0008735930333372022], 'topk_tokens': [' ', ' Bridge', ' the', ' the', '.\n\n', ' to', ' Mary', '<|eot_id|>', ' Bridge', '<|eot_id|>', 'Answer', '?\n', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', ' the', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00012609362602233887, 0.010340690612792969, 0.02809859315554301, 0.006785392761230469, 0.12689407666524252]}, 'saliency': {'score': [0.002128066924902109, 0.000107342038947378, 0.00021628087217157537, 0.00010282505971980396, 4.996237207631596e-05], 'topk_tokens': ['Question', ' bedroom', ' Far', ' Floral', ' the', ' Third', ' Mary', 'Answer', ' bathroom', 'office', 'Bridge', ' Bridge', 'assistant', ' the', ' the', '<|end_header_id|>', '<|start_header_id|>', ' Bridge', '<|begin_of_text|>', ':'], 'evidence_proportions': [2.3672978083292644e-05, 0.0008163303136825562, 0.0026291310787200928, 0.0004464834928512573, 0.005726943413416545]}}, 29: {'grad': {'score': [1.457933866060697, 1.1403815392096641, 1.4874350807883523, 1.1390748841847622, 0.7871551513671875], 'topk_tokens': ['nes', 'The', ' printed', ' should', '\n', ' Newspaper', 'LY', '\n', 'ION', ' THE', 'aper', ' regular', 'AILY', ' Press', 'paper', ' the', 'ATING', '\n', '\n', 'APER'], 'evidence_proportions': [1.8935139973958333, 1.2252883911132812, 1.7856038411458333, 1.0157470703125, 1.1445719401041667]}, 'weight': {'score': [0.011497832261599027, 0.002514383658144339, 0.0010572319680994208, 0.00249781464593345, 0.0012082877706308835], 'topk_tokens': ['Does', ' the', ' Does', ' Where', '.\n\n', '<|eot_id|>', ' where', ' ', '<|eot_id|>', '?\n', ' was', 'Answer', ' the', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00028249621391296387, 0.0050868988037109375, 0.0067729651927948, 0.0029649734497070312, 0.03740056355794271]}, 'saliency': {'score': [0.00043964844483595627, 9.530246697128258e-05, 0.00014249032193964177, 9.448096325201064e-05, 0.0001485445460335153], 'topk_tokens': [',', ' soon', '<|eot_id|>', '.', '.\n\n', 'assistant', ' the', ' where', '?\n', ' was', ' the', ' ', 'Does', ' Does', '<|end_header_id|>', ' the', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [1.9749005635579426e-05, 2.6091933250427246e-05, 0.0009104907512664795, 0.0003803372383117676, 0.0007039507230122884]}}, 30: {'grad': {'score': [1.0116888192983775, 1.1515135519041768, 1.4156827059659092, 1.151334611076878, 1.6485488141169313], 'topk_tokens': [' office', ' o', 'op', ' office', ' an', 'office', ' offices', ' office', ' one', 'ob', ' office', ' o', ' office', ' o', ' o', 'op', ' o', 'office', ' o', ' O'], 'evidence_proportions': [1.4081624348958333, 1.4765625, 0.8852335611979166, 1.05010986328125, 0.40614064534505206]}, 'weight': {'score': [0.020056428817602303, 0.002445851418934915, 0.002883036028255116, 0.0024074125870182568, 0.0060311458149894335], 'topk_tokens': [' to', '<|eot_id|>', ' the', 'Question', ':', '<|eot_id|>', ' Where', '.', ' bathroom', 'Answer', ' the', '.\n\n', ' the', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009823242823282878, 0.012423038482666016, 0.019520193338394165, 0.008391380310058594, 0.05253239472707113]}, 'saliency': {'score': [0.005168878115140474, 0.00024787256895373043, 0.0005100938406857578, 0.00023687807691467766, 0.0005221708876187684], 'topk_tokens': [' ', '.', ' Geo', ':', '.\n\n', ' Mary', ' kitchen', ' bedroom', 'assistant', ' Third', '?\n', ' bathroom', ' the', '<|start_header_id|>', ' the', '<|begin_of_text|>', ' office', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [0.0009304185708363851, 0.003437519073486328, 0.004842023054758708, 0.0006203204393386841, 0.013920803864796957]}}, 31: {'grad': {'score': [1.3264981783353365, 1.8867301072379197, 0.8131464177911932, 1.889869794075553, 0.9189062900230532], 'topk_tokens': [' miles', ' the', 'UX', ' S', '186', ' and', ' was', ' B', ' C', '7', ' M', ' its', ' U', 'G', ' S', ' most', ' W', ' L', ' S', 'editary'], 'evidence_proportions': [1.0406087239583333, 1.349273681640625, 1.4751485188802083, 2.0810546875, 0.9455159505208334]}, 'weight': {'score': [0.005008500355940599, 0.00231959056307506, 0.00128024545582858, 0.002315722279712531, 0.0009766377386499624], 'topk_tokens': [' to', ' the', ' was', ',', ' where', 'Question', ':', '.\n\n', ' the', '<|eot_id|>', ' Where', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0006691813468933105, 0.007944345474243164, 0.005881547927856445, 0.001987457275390625, 0.008531570434570312]}, 'saliency': {'score': [0.0015555207545940692, 0.00019092630109857284, 0.00030866807157343084, 0.00018779607788353174, 7.177083218683962e-05], 'topk_tokens': [' the', ' soon', ' Mary', ' the', ' Miles', 'Question', ' ', ' Where', ' Mary', ' office', ' Mary', '?\n', '<|eot_id|>', 'Answer', ':', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0007753074169158936, 0.0031868815422058105, 0.0021043618520100913, 0.0002886056900024414, 0.0015439291795094807]}}, 'pred_res': "Mary's hand<|eot_id|>", 'score': 0}
2025-01-22 02:58:46.943 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:58:46.943 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-1_0-3-6-8-9.pkl | len: 10 |  size: 9.56 KB
Processing depth (0, 3, 6, 8, 9):   2%|▏         | 2/100 [00:36<30:12, 18.50s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
Processing depth (2, 4, 5, 8, 9):   2%|▏         | 2/100 [00:44<30:12, 18.50s/it]2025-01-22 02:58:54.791 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:58:54.798 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2512, 2518) --> . Mary journeyed to the
2025-01-22 02:58:54.799 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the apple.
2025-01-22 02:58:54.812 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4877, 4881) -->  Mary got the apple
2025-01-22 02:58:54.812 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 02:58:54.819 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (2512, 2518) --> . Mary journeyed to the
2025-01-22 02:58:54.820 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 02:58:54.845 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9678, 9682) -->  dropped the apple.
2025-01-22 02:58:54.846 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 02:58:54.877 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10781, 10787) --> . Daniel went back to the
2025-01-22 02:58:54.877 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:58:54.883 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1881, 1887) --> . John went back to the
2025-01-22 02:58:54.883 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the office.
2025-01-22 02:58:54.890 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2513, 2519) -->  Mary journeyed to the office
2025-01-22 02:58:54.890 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John took the milk.
2025-01-22 02:58:54.891 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (164, 168) -->  John took the milk
2025-01-22 02:58:54.891 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 02:58:54.910 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6635, 6641) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:58:56.879 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 02:58:56.880 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12219])
your chose emoji: ['🧔\u200d♀️', '⛷', '🧑🏻\u200d⚖', '🇩🇯', '👩🏼\u200d🦱', '☸️', '\U0001faf6🏻', '🤷🏾\u200d♂', '👩🏾\u200d❤️\u200d👩🏻', '🏢']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 236298.82it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.49s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.92it/s][A100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.61it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.69it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.85it/s][A100%|██████████| 8/8 [00:00<00:00, 20.96it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.68it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.35it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.64it/s][A100%|██████████| 8/8 [00:00<00:00, 20.64it/s]
2025-01-22 02:59:05.923 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.8209662804236779, 0.6972452483174454, 0.741609746759588, 0.6969008167228927, 1.2785733805762396], 'topk_tokens': ['sur', ' turtle', ' upper', ' earlier', ' Or', ' sure', 'ire', ' over', 'con', ' election', 'editor', 'ent', ' election', ' utter', ' up', ' or', 'enter', 'active', ' rept', ' conn'], 'evidence_proportions': [0.9203008015950521, 1.0379638671875, 0.9203008015950521, 0.9962158203125, 0.3607991536458333]}, 'weight': {'score': [0.015527055813716007, 0.0025559968404455413, 0.018423137339678677, 0.002499615946311954, 0.0018570439683066474], 'topk_tokens': [' the', ' the', '?\n', ' office', '.', 'Bridge', ' Sandra', '.', 'Answer', '<|eot_id|>', ' bathroom', '<|eot_id|>', ':', 'assistant', '.', '<|start_header_id|>', '<|end_header_id|>', ' bedroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.015973567962646484, 0.012694835662841797, 0.015973567962646484, 0.0319366455078125, 0.005582451820373535]}, 'saliency': {'score': [0.002277908416894766, 0.00015293954876243517, 0.002341194586320357, 0.0001444460959243743, 0.00013252844413121542], 'topk_tokens': [' bedroom', 'assistant', 'Question', ' apple', ' Sandra', ' Mary', '<|eot_id|>', '<|end_header_id|>', ' Mary', ' random', ' office', '.', 'Bridge', '<|start_header_id|>', '<|eot_id|>', '.', '<|begin_of_text|>', ' bedroom', ' office', 'office'], 'evidence_proportions': [0.0029044349988301596, 0.0016136020421981812, 0.0029044349988301596, 0.004268050193786621, 0.00014096498489379883]}}, 25: {'grad': {'score': [1.1206524188701923, 1.2834198370121104, 1.1714008938182483, 1.2839699372028566, 0.7043441666497124], 'topk_tokens': ['ervative', ' be', ' the', ' granted', ' extraordinary', 'THE', 'ed', 'ised', 'ated', ' at', 'ation', 'age', 'ished', ' subscribers', 'ation', ' circulated', 'erc', ' subscribers', ' thirst', 'ing'], 'evidence_proportions': [1.2171223958333333, 1.011474609375, 1.2171223958333333, 0.75531005859375, 1.2440592447916667]}, 'weight': {'score': [0.014089169410558848, 0.00253598645747698, 0.008518473668531939, 0.0025004982889545667, 0.0015990030434396532], 'topk_tokens': ['.', ' Geo', ' Mary', ' Mary', '.', '.\n\n', '?\n', ' bedroom', ' Mary', ' Mary', 'Answer', '.', '<|eot_id|>', ':', '<|eot_id|>', '<|start_header_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.013055523236592611, 0.023302078247070312, 0.013055523236592611, 0.023054122924804688, 0.00403788685798645]}, 'saliency': {'score': [0.0038774563716008114, 6.532749053977553e-05, 0.0018131434917449951, 5.402647165091478e-05, 0.00015502009126875136], 'topk_tokens': [' Ramsey', ' Sandra', ' Miss', 'Answer', '.', ':', ' bedroom', '.', ' directly', ' apple', 'assistant', ' top', ' Mary', '<|eot_id|>', '.', ' Mary', ' Mary', ' Mary', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.005070924758911133, 0.006287559866905212, 0.005070924758911133, 0.0033032596111297607, 0.00026658177375793457]}}, 26: {'grad': {'score': [0.5890643780048077, 0.6079701276235578, 0.9010786576704546, 0.6074807792155282, 0.5306135813395182], 'topk_tokens': [' printers', ' worth', ' Merch', ' went', ' some', ' B', ' Seventh', 'ers', ' Col', ' week', 'little', 'BO', ' Jul', 'str', ' wh', ' STR', ' part', ' clear', ' str', ' Press'], 'evidence_proportions': [0.611114501953125, 0.651153564453125, 0.611114501953125, 0.42864990234375, 0.6105143229166666]}, 'weight': {'score': [0.006070054494417631, 0.0024849779525293616, 0.005490487272089178, 0.0024718888869646356, 0.0013264094789822896], 'topk_tokens': [' Bridge', ' the', ' the', '.\n\n', '.', 'Bridge', ' bathroom', ' kitchen', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', ' the', 'assistant', ' bedroom', '<|start_header_id|>', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0057888031005859375, 0.005876898765563965, 0.0057888031005859375, 0.007866382598876953, 0.005563775698343913]}, 'saliency': {'score': [0.00016464636876032903, 8.594345006622023e-05, 0.00043160536072470927, 8.515064320509038e-05, 9.217361609141032e-05], 'topk_tokens': [' the', ' apple', ' Seventh', '?\n', ' Bridge', '<|start_header_id|>', ' bedroom', ' bathroom', 'assistant', '<|end_header_id|>', '.', ' Az', ' kitchen', 'Bridge', ' the', ':', ' the', '<|begin_of_text|>', ' bedroom', 'office'], 'evidence_proportions': [8.821487426757812e-05, 0.00022237002849578857, 8.821487426757812e-05, 0.00026866793632507324, 0.00020967920621236166]}}, 27: {'grad': {'score': [0.48393014761117786, 0.4639828551392071, 0.39724800803444604, 0.4640608586742464, 0.42745335896809894], 'topk_tokens': [' interruption', ' ar', '186', ' talk', ' newspaper', '185', 'paper', '186', ' remin', ' const', '186', ' excessive', ' told', ' plainly', ' N', ' EX', '186', 'ile', 'bread', ' N'], 'evidence_proportions': [0.3337198893229167, 0.76202392578125, 0.3337198893229167, 0.49915313720703125, 0.58880615234375]}, 'weight': {'score': [0.013171828710115872, 0.0025291458478234187, 0.008859512480822477, 0.0024949736782394556, 0.0018886766499943202], 'topk_tokens': [' Geo', ' Mary', ' Mary', '<|eot_id|>', ' bathroom', '<|eot_id|>', 'THE', '?\n', ' THE', '.', '.', 'Answer', 'assistant', ' bedroom', '.\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.01705743869145711, 0.020795702934265137, 0.01705743869145711, 0.005534172058105469, 0.005409797032674153]}, 'saliency': {'score': [0.003165153356698843, 0.00011696490472023055, 0.0020481916991147127, 0.00010696409232984834, 0.00029402971267700195], 'topk_tokens': [' bathroom', ' prior', ' the', ' Mary', ' office', '<|start_header_id|>', ' Bridge', '.', ' Mary', ' Mary', '.', ' the', '<|end_header_id|>', 'THE', ' Mary', '.', ':', '.\n\n', ' bedroom', 'office'], 'evidence_proportions': [0.004052877426147461, 0.006646901369094849, 0.004052877426147461, 0.0004996657371520996, 0.0008455316225687662]}}, 28: {'grad': {'score': [0.7832606388972356, 0.7678542889800344, 0.8257057883522727, 0.7677168291850753, 0.5395165019565158], 'topk_tokens': [' become', ' ', ' first', ' made', ' returns', ' been', ' taken', ' child', ' been', ' had', ' executed', ' balance', 'being', ' have', ' acted', 'na', 'hom', 'been', ' be', ' ins'], 'evidence_proportions': [0.65838623046875, 0.6953125, 0.65838623046875, 0.9686050415039062, 0.96807861328125]}, 'weight': {'score': [0.00727315590931819, 0.002425250362505178, 0.004076413132927634, 0.0024119117339693667, 0.0008390827311409845], 'topk_tokens': [' the', ' bedroom', ' the', ' the', ' to', 'Question', ' the', ' Bridge', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'Answer', ' the', '?\n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0017882883548736572, 0.004194021224975586, 0.0017882883548736572, 0.006564617156982422, 0.020768006642659504]}, 'saliency': {'score': [0.0006436545115250808, 5.983485985129996e-05, 0.00040821595625443894, 5.795827272697667e-05, 4.560500383377075e-05], 'topk_tokens': [' the', ' during', 'Question', ' bathroom', ' the', ' Third', 'Bridge', ' the', 'Answer', ' Bridge', ' the', ' Far', '?\n', 'assistant', ':', ' bedroom', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00025602181752522785, 0.0003555119037628174, 0.00025602181752522785, 0.0003918558359146118, 0.0017788807551066081]}}, 29: {'grad': {'score': [0.8776057316706731, 0.7434243246767859, 0.8863123113458807, 0.742879491662035, 0.7000362343258328], 'topk_tokens': ['\n', ' Newspaper', ' Printing', 'ATING', 'AILY', ' o', ' press', ' printing', ' regular', ' ox', '\n', '\n', 'nes', 'paper', '\n', 'aper', '\n', '\n', ' Press', 'APER'], 'evidence_proportions': [0.9742024739583334, 0.562042236328125, 0.9742024739583334, 0.7623291015625, 0.9716389973958334]}, 'weight': {'score': [0.0027901392716627857, 0.002502922886532483, 0.0023711757226423783, 0.002502547532190266, 0.0008342622054947747], 'topk_tokens': [' Does', ' the', ' ', ' to', ' the', 'Question', ' to', ' Where', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' the', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009885927041371663, 0.0034322738647460938, 0.0009885927041371663, 0.0028700828552246094, 0.005911846955617269]}, 'saliency': {'score': [0.00026330122580895055, 5.1406974751901904e-05, 0.0003245798024264249, 5.0460695877645534e-05, 8.402268091837566e-05], 'topk_tokens': [' office', 'THE', 'IVE', '.\n\n', '<|eot_id|>', 'Answer', ' the', ' to', ' to', ' the', ' Does', 'Does', ' to', '?\n', ' the', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00014509757359822592, 0.0002117455005645752, 0.00014509757359822592, 0.0003184974193572998, 0.000497281551361084]}}, 30: {'grad': {'score': [0.8489896334134616, 1.1736336275673023, 1.2884753834117542, 1.1741194753631965, 1.3408416112263997], 'topk_tokens': [' an', 'deal', 'op', ' o', 'G', ' head', ' an', ' its', ' o', ' office', 'office', ' o', 'ob', ' o', ' office', ' o', 'op', ' o', 'office', ' O'], 'evidence_proportions': [0.5392049153645834, 1.33837890625, 0.5392049153645834, 1.2825927734375, 0.8532307942708334]}, 'weight': {'score': [0.006994495025047889, 0.002450850598892675, 0.009680030020800505, 0.0024280808048926746, 0.003679700195789337], 'topk_tokens': [' bedroom', ',', ' Where', ' to', ' the', ' to', ':', '<|eot_id|>', 'Question', '<|eot_id|>', ' the', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0042537252108256025, 0.007386684417724609, 0.0042537252108256025, 0.0073528289794921875, 0.011975685755411783]}, 'saliency': {'score': [0.0008987876085134653, 0.00019802979679293392, 0.0010955279523676093, 0.00019491103703548902, 0.0002858315904935201], 'topk_tokens': ['<|eot_id|>', ',', ' the', ' the', ' Geo', '.\n\n', ' to', '.', ':', '<|begin_of_text|>', 'Question', ' to', ' the', '?\n', ' bedroom', ' office', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0009497404098510742, 0.0011490583419799805, 0.0009497404098510742, 0.00036835670471191406, 0.0009836554527282715]}}, 31: {'grad': {'score': [0.9438007061298077, 1.3957075039890352, 0.6032298694957386, 1.398104947897948, 0.6658312479654948], 'topk_tokens': ['7', ' H', ' OCC', ' w', ' S', ' most', ' Aw', ' were', ' U', ' their', ' and', ' Aw', ' was', ' L', ' C', 'editary', ' S', ' B', ' C', ' W'], 'evidence_proportions': [1.0936686197916667, 0.7544708251953125, 1.0936686197916667, 1.1938323974609375, 0.6035970052083334]}, 'weight': {'score': [0.002764399235065167, 0.0023097881132662272, 0.002073748545213179, 0.002309243710188136, 0.0006252038809988233], 'topk_tokens': [' to', ' was', '.', ' the', ',', ':', 'Question', '.\n\n', ' the', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0025719006856282554, 0.002859771251678467, 0.0025719006856282554, 0.0015056133270263672, 0.003925005594889323]}, 'saliency': {'score': [0.0005609851617079515, 0.00012685731510592797, 0.0004548322070728649, 0.00012533733057993397, 2.6312139299180773e-05], 'topk_tokens': [' Anthony', ' Mary', ' the', ' Market', ' Mary', ' Emily', ' the', '?\n', ' Where', ' the', 'Question', ' office', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', 'Answer', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0008070170879364014, 0.0006216317415237427, 0.0008070170879364014, 0.00014582276344299316, 0.0003052651882171631]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 02:59:05.924 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:59:05.924 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-2_2-4-5-8-9.pkl | len: 10 |  size: 9.48 KB
Processing depth (2, 4, 5, 8, 9):   3%|▎         | 3/100 [00:55<30:16, 18.73s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (1, 2, 3, 8, 9):   3%|▎         | 3/100 [01:03<30:16, 18.73s/it]2025-01-22 02:59:13.290 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:59:13.295 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1535, 1541) --> . Mary journeyed to the
2025-01-22 02:59:13.295 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the apple.
2025-01-22 02:59:13.302 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2528, 2532) -->  Mary got the apple
2025-01-22 02:59:13.302 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 02:59:13.307 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1535, 1541) --> . Mary journeyed to the
2025-01-22 02:59:13.307 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 02:59:13.332 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9660, 9664) -->  dropped the apple.
2025-01-22 02:59:13.332 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 02:59:13.362 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10769, 10775) --> . Daniel went back to the
2025-01-22 02:59:13.362 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:59:13.370 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1894, 1900) --> . John went back to the
2025-01-22 02:59:13.370 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the office.
2025-01-22 02:59:13.374 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1536, 1542) -->  Mary journeyed to the office
2025-01-22 02:59:13.375 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John took the milk.
2025-01-22 02:59:13.375 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (164, 168) -->  John took the milk
2025-01-22 02:59:13.375 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 02:59:13.393 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6619, 6625) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:59:15.448 | INFO     | test_jbb_retain:begin_test:632 - the bedroom<|eot_id|>
2025-01-22 02:59:15.448 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12207])
your chose emoji: ['🧑🏽\u200d🎤', '🥌', '🧙🏽\u200d♀️', '🙍🏼', '🏋️', '\U0001fa77', '👟', '⛹🏽\u200d♂️', '🚣🏿\u200d♂', '⛲']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 155344.59it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.38s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.43s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.72it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.41it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.62it/s][A100%|██████████| 8/8 [00:00<00:00, 20.77it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.54it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.18it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.45it/s][A100%|██████████| 8/8 [00:00<00:00, 20.46it/s]
2025-01-22 02:59:25.122 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.8420224556556115, 0.731909797587845, 0.6358046098188921, 0.7318482389184227, 1.4258758544921875], 'topk_tokens': [' utter', 'oub', ' election', 'cont', ' editor', ' over', 'sur', 'con', ' sure', ' Or', ' upper', ' election', ' effect', 'enter', 'editor', ' up', ' or', 'active', ' rept', ' conn'], 'evidence_proportions': [0.8982747395833334, 0.9852294921875, 0.8982747395833334, 1.10601806640625, 0.4580494562784831]}, 'weight': {'score': [0.016948867302674513, 0.002567393430787483, 0.02288698066364635, 0.0024998867093178703, 0.0032564640045166016], 'topk_tokens': ['\n\n', ' bedroom', ' random', ' the', '<|eot_id|>', 'Answer', ' office', '<|start_header_id|>', '<|eot_id|>', '.', ' the', 'assistant', ':', ' bathroom', ' the', 'Bridge', '<|end_header_id|>', ' bedroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.015252470970153809, 0.0037050247192382812, 0.015252470970153809, 0.036769866943359375, 0.01595688859621684]}, 'saliency': {'score': [0.0028963891359475944, 0.00012084652549736779, 0.002191725102337924, 0.00011116611791886288, 0.000196760892868042], 'topk_tokens': ['\n\n', 'Question', ' top', '.', ' bathroom', ' apple', 'assistant', '<|eot_id|>', ' the', '<|start_header_id|>', 'Answer', '.', '<|eot_id|>', ' Mary', ' random', ' bedroom', ' bedroom', '<|begin_of_text|>', 'office', ' office'], 'evidence_proportions': [0.004381020863850911, 0.0003518909215927124, 0.004381020863850911, 0.004599928855895996, 0.0004877646764119466]}}, 25: {'grad': {'score': [0.9926440899188702, 1.0755486955063887, 0.8599288463592529, 1.0761160136896395, 0.6865837097167968], 'topk_tokens': [' circulated', ' from', 'ated', ' thirst', 'erc', ' the', 'was', 'ished', ' THE', ' be', ' subscribers', ' "', 'ervative', ' the', ' extraordinary', ' most', 'ing', 'THE', 'ation', 'ation'], 'evidence_proportions': [1.0928548177083333, 0.86627197265625, 1.0928548177083333, 0.89959716796875, 0.9385019938151041]}, 'weight': {'score': [0.02076738843551049, 0.002543487598820062, 0.014810816808180376, 0.002482332869409662, 0.0028286178906758628], 'topk_tokens': [' bedroom', ' apple', ' Mary', ' Mary', ' Geo', ' random', ' the', '.', 'Answer', '<|start_header_id|>', ' the', '<|eot_id|>', ' Mary', '<|eot_id|>', ':', '.', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.027951667706171673, 0.0029835104942321777, 0.027951667706171673, 0.03305816650390625, 0.010060896476109823]}, 'saliency': {'score': [0.002028015943673941, 6.01486585993411e-05, 0.0008305365389043635, 5.4547714369536796e-05, 0.0002360751231511434], 'topk_tokens': [' top', '<|start_header_id|>', ' Miss', ' Paul', ' Senator', ' Merch', ' Bench', ' George', ':', ' Geo', 'Answer', ' Anthony', ' apple', 'assistant', ' directly', ' random', ' Mary', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0027592281500498452, 0.0001923590898513794, 0.0027592281500498452, 0.003512948751449585, 0.0007994075616200765]}}, 26: {'grad': {'score': [0.7526145348182092, 0.8645614435303874, 1.3034529252485796, 0.8640068022203568, 0.703282356262207], 'topk_tokens': [' Press', ' Col', ' press', 'burn', ' steam', ' steam', ' Marshall', ' mar', ' steam', ' Merch', ' went', 'str', ' steam', 'BO', ' clear', ' steam', ' str', ' STR', ' wh', ' Press'], 'evidence_proportions': [0.8285624186197916, 0.9759521484375, 0.8285624186197916, 0.22387313842773438, 0.8043212890625]}, 'weight': {'score': [0.008847456711989183, 0.002493923248808432, 0.007608749649741433, 0.002471086471334275, 0.002189570665359497], 'topk_tokens': [' the', ' the', ' the', ' Bridge', ' bedroom', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' kitchen', 'Bridge', ' bedroom', '?\n', 'assistant', 'Answer', '<|start_header_id|>', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.008898595968882242, 0.0011002570390701294, 0.008898595968882242, 0.014431476593017578, 0.010187298059463501]}, 'saliency': {'score': [0.0005649603330172026, 0.00011335876827910195, 0.0005608255212957209, 0.00011158375723975023, 0.00017970303694407144], 'topk_tokens': ['.', ' Bench', ' office', ' Bridge', 'assistant', '<|start_header_id|>', ' bathroom', '?\n', 'Answer', ' the', '.', ' Az', ' Bridge', ' bedroom', ' kitchen', ' bedroom', 'Bridge', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.0005853772163391113, 0.00020413100719451904, 0.0005853772163391113, 0.0007446706295013428, 0.0006448725859324137]}}, 27: {'grad': {'score': [0.8544546274038461, 0.6976664403564993, 0.6292474920099432, 0.6974550041259585, 0.45986760457356773], 'topk_tokens': [' possessed', ' very', ' print', ' producing', ' talk', ' remember', '185', 'bread', ' told', ' print', 'ATION', '186', 'ly', ' print', ' tell', ' N', ' earnest', 'ile', ' plainly', ' N'], 'evidence_proportions': [1.0296223958333333, 0.813323974609375, 1.0296223958333333, 0.79827880859375, 0.5689900716145834]}, 'weight': {'score': [0.014536128594325138, 0.0025431776488088003, 0.011105483228510077, 0.002502047195117751, 0.0036913355191548667], 'topk_tokens': ['<|eot_id|>', 'Bridge', ' Joseph', ' Mary', ' Geo', '?\n', '.', ' the', ' bedroom', ' Bridge', '.', 'Answer', ' bedroom', 'assistant', '.\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.022780825694402058, 0.0021552443504333496, 0.022780825694402058, 0.00797891616821289, 0.01067213217417399]}, 'saliency': {'score': [0.0034389083202068624, 0.00013349897709747372, 0.0027386844158172607, 0.00012171912983387044, 0.00047015448411305745], 'topk_tokens': [' the', ' the', ' the', 'RE', '<|start_header_id|>', '.\n\n', '.', '�', ' Mary', ' Bridge', '.', ' bedroom', '.', 'THE', ' Mary', ' bedroom', ':', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.006183822949727376, 0.0005790889263153076, 0.006183822949727376, 0.0006726086139678955, 0.001699825127919515]}}, 28: {'grad': {'score': [0.6736684945913461, 0.7187578387357687, 0.6598039106889204, 0.718960890241795, 0.6202962239583333], 'topk_tokens': ['being', ' ', ' execution', ' half', 'half', 'been', ' taken', ' have', ' executed', ' returns', ' ', ' acted', '600', 'hom', ' become', 'PA', ' be', ' balance', 'na', ' ins'], 'evidence_proportions': [0.60552978515625, 0.360595703125, 0.60552978515625, 0.959320068359375, 0.8282267252604166]}, 'weight': {'score': [0.0063185554284315845, 0.002429178222136334, 0.007617164741862904, 0.0024114774153936596, 0.0017167985439300536], 'topk_tokens': [' ', ' Mary', ' the', 'Question', ' to', '.\n\n', ' Bridge', ' the', ' the', '<|eot_id|>', '<|eot_id|>', 'Answer', '?\n', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008365412553151449, 0.0004755258560180664, 0.0008365412553151449, 0.0076122283935546875, 0.02031548817952474]}, 'saliency': {'score': [0.00044446954360375035, 6.500438425696954e-05, 0.00020344961773265493, 6.394263857162447e-05, 6.9504976272583e-05], 'topk_tokens': [' during', ' bedroom', ' apple', ' the', ' Floral', ' Third', '<|eot_id|>', ' Far', ' Bridge', '?\n', 'Bridge', 'Answer', 'assistant', '<|start_header_id|>', 'office', ':', ' the', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [7.121761639912923e-05, 2.7939677238464355e-05, 7.121761639912923e-05, 0.00033155083656311035, 0.0015439391136169434]}}, 29: {'grad': {'score': [0.5826040414663461, 0.9182834192501433, 0.7894932140003551, 0.9192340851771127, 0.6271990458170573], 'topk_tokens': ['\n', ' The', ' THE', 'nes', ' were', ' NEW', 'mail', '\n', ' mail', '\n', 'ION', 'AILY', 'aper', ' regular', 'ATING', '\n', 'paper', ' Press', '\n', 'APER'], 'evidence_proportions': [0.6305338541666666, 0.35931396484375, 0.6305338541666666, 0.6522216796875, 0.5891927083333334]}, 'weight': {'score': [0.0019696538264934835, 0.002503563100343177, 0.0034596269780939274, 0.0025029750102033505, 0.0019281307856241863], 'topk_tokens': ['Does', ' the', 'Question', ' to', ' ', ' Where', '.\n\n', ' part', '<|eot_id|>', '<|eot_id|>', ' Does', ' the', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00021750728289286295, 0.0008672475814819336, 0.00021750728289286295, 0.00295257568359375, 0.005553603172302246]}, 'saliency': {'score': [0.00016245016684898964, 5.754140331622316e-05, 0.00016661123795942828, 5.711979619394683e-05, 0.00012780725955963135], 'topk_tokens': [' office', ' the', '<|eot_id|>', 'IVE', 'DW', ':', '.\n\n', ' the', ' Where', '<|eot_id|>', 'Does', 'NEW', '?\n', ' part', ' Does', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.1864970525105793e-05, 1.8462538719177246e-05, 2.1864970525105793e-05, 0.0003808736801147461, 0.0003939966360727946]}}, 30: {'grad': {'score': [0.9892625075120193, 1.101887567573102, 1.3809925426136365, 1.1016234397966604, 1.0983784993489583], 'topk_tokens': [' o', '2', ' account', ' the', ' o', ' office', ' to', 'ob', ' head', ' anx', ' it', 'deal', ' o', '3', ' an', ' o', 'office', ' o', 'op', ' O'], 'evidence_proportions': [1.0365193684895833, 0.7523193359375, 1.0365193684895833, 1.3402099609375, 0.8187459309895834]}, 'weight': {'score': [0.00870951322408823, 0.0024612860125418972, 0.01382175629789179, 0.002427375622460662, 0.005486206213633219], 'topk_tokens': [' Anthony', ',', ' the', ' part', ' Where', ':', 'Question', '<|eot_id|>', '<|eot_id|>', ' the', ' the', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.005452394485473633, 0.0013189911842346191, 0.005452394485473633, 0.010225296020507812, 0.019140243530273438]}, 'saliency': {'score': [0.001425094329393827, 0.00016982128309846187, 0.0019869533452120695, 0.00016385022771073235, 0.00033918321132659913], 'topk_tokens': [' George', ' to', ' Mary', '<|eot_id|>', ',', ' Wild', ' Geo', ' the', ':', 'Question', '.\n\n', 'assistant', ' the', '?\n', ' office', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0018528103828430176, 0.0001125037670135498, 0.0018528103828430176, 0.0007919520139694214, 0.0018668174743652344]}}, 31: {'grad': {'score': [0.95721435546875, 1.2800657110021296, 0.6422028975053267, 1.2819098946334755, 0.5488477071126302], 'topk_tokens': [' Aw', ' M', ' and', ' their', ' S', ' were', 'user', ' L', ' the', '7', ' S', ' an', ' H', ' was', ' C', ' most', ' W', ' C', 'editary', ' B'], 'evidence_proportions': [0.8537190755208334, 1.26177978515625, 0.8537190755208334, 1.24609375, 0.7685750325520834]}, 'weight': {'score': [0.00266651465342595, 0.002286539195708516, 0.0016597509384155273, 0.002286860713656036, 0.0011699169874191285], 'topk_tokens': [' where', ' to', ' was', ' the', ',', ':', 'Question', ' the', '.\n\n', '<|eot_id|>', ' Where', 'Answer', '<|eot_id|>', '?\n', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0021597047646840415, 0.0005053877830505371, 0.0021597047646840415, 0.0019478797912597656, 0.0055999755859375]}, 'saliency': {'score': [0.0005147594671982985, 0.00013302053863502894, 0.00027593157508156514, 0.00013194585275849997, 3.3801794052124025e-05], 'topk_tokens': [' Ramsey', 'Bridge', ' the', ' Mary', ' office', ' Miles', ' open', ' Mary', ' the', ' Where', 'Question', ' Peter', '<|start_header_id|>', '<|eot_id|>', 'Answer', '<|end_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0006714959939320883, 9.185075759887695e-05, 0.0006714959939320883, 0.0002648085355758667, 0.0006498595078786215]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 02:59:25.123 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:59:25.123 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-3_1-2-3-8-9.pkl | len: 10 |  size: 9.55 KB
Processing depth (1, 2, 3, 8, 9):   4%|▍         | 4/100 [01:15<30:14, 18.91s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (2, 3, 4, 5, 7):   4%|▍         | 4/100 [01:22<30:14, 18.91s/it]2025-01-22 02:59:32.437 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:59:32.444 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2471, 2477) --> . Mary journeyed to the
2025-01-22 02:59:32.444 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the apple.
2025-01-22 02:59:32.458 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3808, 3812) -->  Mary got the apple
2025-01-22 02:59:32.458 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 02:59:32.465 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (2471, 2477) --> . Mary journeyed to the
2025-01-22 02:59:32.466 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 02:59:32.482 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5950, 5954) -->  Mary dropped the apple
2025-01-22 02:59:32.482 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 02:59:32.506 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (8432, 8438) --> . Daniel went back to the
2025-01-22 02:59:32.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 02:59:32.512 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1872, 1878) --> . John went back to the
2025-01-22 02:59:32.512 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the office.
2025-01-22 02:59:32.519 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2472, 2478) -->  Mary journeyed to the office
2025-01-22 02:59:32.519 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John took the milk.
2025-01-22 02:59:32.520 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (164, 168) -->  John took the milk
2025-01-22 02:59:32.520 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 02:59:32.539 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6633, 6639) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:59:34.704 | INFO     | test_jbb_retain:begin_test:632 - Mary's bedroom.<|eot_id|>
2025-01-22 02:59:34.704 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12238])
your chose emoji: ['🙋\u200d♂️', '🔯', '🚣🏿\u200d♀', '👰🏼\u200d♀', '👷\u200d♂️', '🧑🏻\u200d🎄', '👩🏾\u200d❤️\u200d💋\u200d👩🏿', '⛄', '👨🏿\u200d🤝\u200d👨🏽', '🧑🏾\u200d🦯']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.71s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.07it/s][A
 88%|████████▊ | 7/8 [00:04<00:00,  2.21it/s][A100%|██████████| 8/8 [00:05<00:00,  1.60it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.17it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.97it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.84it/s][A100%|██████████| 8/8 [00:00<00:00, 22.19it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.89it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.57it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.48it/s][A100%|██████████| 8/8 [00:00<00:00, 21.72it/s]
2025-01-22 02:59:43.411 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.8688389704777644, 0.7253160264756945, 0.8702073530717329, 0.7247485063207431, 1.2706785097226991], 'topk_tokens': [' effect', ' office', ' election', 'ent', ' conn', ' offices', 'Official', ' office', ' Or', '800', ' rept', ' sure', 'enter', ' office', ' office', 'office', ' office', 'active', ' or', ' office'], 'evidence_proportions': [1.0925699869791667, 0.87799072265625, 1.0925699869791667, 0.9305419921875, 0.3741404215494792]}, 'weight': {'score': [0.021185056521342352, 0.0025548419142081068, 0.0168976214799014, 0.0024892311259673964, 0.0019403420961820162], 'topk_tokens': [' Bridge', '?\n', ' THE', '.', ' Sandra', ' bathroom', '188', ' the', 'Answer', '<|eot_id|>', '<|eot_id|>', ':', ' Mary', 'assistant', 'Bridge', '<|start_header_id|>', '<|end_header_id|>', ' bedroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0038191775480906167, 0.027497291564941406, 0.0038191775480906167, 0.08774948120117188, 0.007332374652226766]}, 'saliency': {'score': [0.00473133188027602, 0.0001636322612076803, 0.003246372396295721, 0.00014832874471590587, 0.0001228392779172122], 'topk_tokens': ['assistant', ' Mary', '.', ' apple', ' East', ' THE', ' office', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', ' top', 'Bridge', '188', '<|eot_id|>', ' office', ' Mary', '<|begin_of_text|>', ' office', ' bedroom', 'office'], 'evidence_proportions': [0.0005645354588826498, 0.004358142614364624, 0.0005645354588826498, 0.024365901947021484, 0.00022400418917338052]}}, 25: {'grad': {'score': [1.0037724421574519, 0.885812557444853, 1.0354891690340908, 0.8852909168233396, 0.6581976859124152], 'topk_tokens': [' subscribers', ' to', ' trib', ' near', ' rest', 'ation', 'graph', ' at', ' fifteen', ' subscribers', 'ing', ' circulated', ' marched', 'erc', ' thirst', 'ised', ' Tribune', 'itable', ' Wood', ' at'], 'evidence_proportions': [1.0374348958333333, 0.93646240234375, 1.0374348958333333, 0.85650634765625, 1.079498291015625]}, 'weight': {'score': [0.022665163645377524, 0.0025311004102619647, 0.006947969848459417, 0.0024801935228150034, 0.0008903840085962316], 'topk_tokens': [' Emily', ' the', ' top', ' the', '188', ' bedroom', ' Mary', 'Answer', ' Miss', ' THE', '<|start_header_id|>', '<|eot_id|>', ':', '189', '<|eot_id|>', ' Mary', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003820518652598063, 0.03701066970825195, 0.003820518652598063, 0.08333206176757812, 0.010346184174219767]}, 'saliency': {'score': [0.0078965975688054, 6.808392167870515e-05, 0.0009136633439497514, 4.986344086216504e-05, 7.040684039776142e-05], 'topk_tokens': [' directly', '.', 'assistant', ' Dan', ':', '189', ' apple', '.', ' Mary', ' apple', '<|start_header_id|>', ' bedroom', '.', ' Miss', ' top', '<|eot_id|>', '<|eot_id|>', ' Mary', '<|begin_of_text|>', ' Mary'], 'evidence_proportions': [0.0019735793272654214, 0.014281749725341797, 0.0019735793272654214, 0.03028428554534912, 0.0005607406298319498]}}, 26: {'grad': {'score': [0.6817580003004807, 0.7089206271701389, 0.9390009099786932, 0.7085633816055739, 0.46598966829069366], 'topk_tokens': ['istributed', 'prob', ' hand', 'leading', ' B', 'press', ' worth', 'BO', ' STR', 'ers', ' Jul', ' some', 'ers', 'str', ' wh', ' week', ' str', ' Press', ' clear', ' part'], 'evidence_proportions': [0.7014567057291666, 0.8546142578125, 0.7014567057291666, 0.45123291015625, 0.6808064778645834]}, 'weight': {'score': [0.008869767189025879, 0.0024822380028518974, 0.005365276878530329, 0.0024634139695427237, 0.001573805625622089], 'topk_tokens': [' discarded', ' kitchen', '.\n\n', ' Bridge', '188', '�', ' Mary', '<|eot_id|>', '<|eot_id|>', '?\n', 'Bridge', 'Answer', ' bedroom', 'assistant', '<|start_header_id|>', ' the', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0013657212257385254, 0.010830819606781006, 0.0013657212257385254, 0.035219669342041016, 0.005003889401753743]}, 'saliency': {'score': [0.000341724890929002, 9.023329011755052e-05, 0.0006585663015192205, 8.867143743776587e-05, 0.00016627730904044685], 'topk_tokens': [' the', 'Answer', ' kitchen', ' Dan', '<|end_header_id|>', '.', ' the', '<|start_header_id|>', ' apple', '188', ' Seventh', '�', ' Bridge', 'assistant', '<|begin_of_text|>', ':', 'Bridge', ' bedroom', 'office', ' the'], 'evidence_proportions': [5.240241686503092e-05, 0.00041612982749938965, 5.240241686503092e-05, 0.0010155141353607178, 0.0004215737183888753]}}, 27: {'grad': {'score': [0.42986121544471156, 0.5177791420930351, 0.4206792658025568, 0.5181418441412017, 0.4115814376663376], 'topk_tokens': ['APER', '185', ' interruption', ' party', 'AP', ' EX', ' talk', ' Gutenberg', ' earnest', ' remin', ' excessive', '186', ' told', 'SP', 'sp', ' newspaper', ' Newspaper', 'ile', 'bread', ' N'], 'evidence_proportions': [0.22141520182291666, 0.70977783203125, 0.22141520182291666, 0.664794921875, 0.5035196940104166]}, 'weight': {'score': [0.02293755687200106, 0.0025250279046351615, 0.006409875371239402, 0.0024744871896239403, 0.00174080932533348], 'topk_tokens': ['188', ' Mary', ' the', '.', '<|eot_id|>', '?\n', '<|eot_id|>', ' Bridge', 'Bridge', 'Answer', ' Mary', 'assistant', ' bedroom', '.\n\n', ' THE', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.007023324569066365, 0.03420603275299072, 0.007023324569066365, 0.08383798599243164, 0.006653418143590291]}, 'saliency': {'score': [0.0037033305718348576, 0.00012821773298425613, 0.0009370852600444447, 0.00011913406997527976, 0.00033486806429349457], 'topk_tokens': ['��', ' Dan', ' the', '.', ' top', '<|end_header_id|>', 'Bridge', '189', '<|begin_of_text|>', 'Answer', ' Mary', ' Bridge', ' bedroom', '.', ':', ' THE', ' Mary', '<|start_header_id|>', '.\n\n', 'office'], 'evidence_proportions': [0.0004126926263173421, 0.004567950963973999, 0.0004126926263173421, 0.016715675592422485, 0.0010332961877187092]}}, 28: {'grad': {'score': [0.7800058218149039, 0.6521987815308415, 0.6719526811079546, 0.6518905819870355, 0.5725010463169643], 'topk_tokens': [' looked', ' acted', ' first', ' have', '600', ' had', ' been', ' child', ' balance', ' been', ' came', ' been', ' acted', 'being', 'na', 'hom', ' be', ' taken', 'been', ' ins'], 'evidence_proportions': [0.6233113606770834, 0.685150146484375, 0.6233113606770834, 0.878692626953125, 1.0908406575520833]}, 'weight': {'score': [0.008917297308261577, 0.0024344607895495844, 0.005298289385708896, 0.0024154681731944324, 0.0006183130400521415], 'topk_tokens': ['.', ' the', ' bedroom', ' the', ' discarded', 'Question', '.\n\n', ' Mary', ' Bridge', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012255509694417317, 0.007337331771850586, 0.0012255509694417317, 0.03162574768066406, 0.010215133428573608]}, 'saliency': {'score': [0.0007604360580444336, 5.4725220585181044e-05, 0.0004854879596016624, 5.244296484106169e-05, 2.473592758178711e-05], 'topk_tokens': [' Bridge', 'Question', ' the', ' Third', ' Far', ' Mary', ' apple', ' the', ' the', '?\n', 'Answer', 'Bridge', ' the', '<|start_header_id|>', ' bedroom', 'office', 'assistant', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00015304485956827799, 0.000657692551612854, 0.00015304485956827799, 0.0017682164907455444, 0.0013718605041503906]}}, 29: {'grad': {'score': [0.7383305476262019, 0.7919448752808415, 0.9224007346413352, 0.7918238076638049, 0.69500246152773], 'topk_tokens': [' the', ' press', ' the', ' business', ' o', '\n', ' Printing', '\n', '\n', ' printing', ' regular', ' ox', '\n', '\n', 'aper', 'paper', ' press', 'APER', '\n', ' Press'], 'evidence_proportions': [0.8097330729166666, 0.576416015625, 0.8097330729166666, 0.6334991455078125, 0.7733561197916666]}, 'weight': {'score': [0.004594534635543823, 0.002505221086389878, 0.004118970849297263, 0.002497853579248969, 0.0006564626326927772], 'topk_tokens': [' discarded', ' bedroom', ' to', ' Does', 'Question', ' Where', ' to', '.\n\n', ' in', '<|eot_id|>', '<|eot_id|>', '?\n', ' the', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0007619261741638184, 0.0030057430267333984, 0.0007619261741638184, 0.01229095458984375, 0.008187999327977499]}, 'saliency': {'score': [0.0005449354648590088, 6.465540310136633e-05, 0.0008445978164672852, 6.222380740748929e-05, 6.980358899294675e-05], 'topk_tokens': ['<|eot_id|>', ' Mary', 'NEW', 'IVE', 'THE', ' the', 'Answer', ' the', 'Does', 'THE', ' to', '?\n', ' Does', ' to', 'office', ' the', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.88993231455485e-05, 0.00013802945613861084, 5.88993231455485e-05, 0.0017857551574707031, 0.0009610652923583984]}}, 30: {'grad': {'score': [0.9014704777644231, 1.2009548611111112, 1.3017855557528408, 1.20141158016335, 1.3502722100897149], 'topk_tokens': [' office', ' Paul', ' o', ' offices', '2', 'op', ' an', 'G', ' office', 'ob', 'office', ' o', 'op', ' office', ' o', ' o', ' o', ' o', 'office', ' O'], 'evidence_proportions': [0.7875773111979166, 1.33154296875, 0.7875773111979166, 0.68218994140625, 0.9887288411458334]}, 'weight': {'score': [0.010041984227987437, 0.002431035509296492, 0.01299265297976407, 0.002395746774795487, 0.0025589544694502274], 'topk_tokens': [' Broadway', ' Where', ' Mary', ' the', '.', ' Miles', ' the', '<|eot_id|>', 'Question', '<|eot_id|>', '.\n\n', ' the', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002099384864171346, 0.00864553451538086, 0.002099384864171346, 0.027631759643554688, 0.015131632486979166]}, 'saliency': {'score': [0.0012869399327498216, 0.00021704392495498157, 0.0024493607607754793, 0.00021073419180285587, 0.00011882284185388586], 'topk_tokens': [' was', ' Miles', ' lounge', ',', ':', '<|eot_id|>', 'assistant', ' the', ' to', ' Mary', 'Question', ' the', ' office', '?\n', ' bedroom', ' the', '<|start_header_id|>', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [0.00034041206041971844, 0.0014463216066360474, 0.00034041206041971844, 0.004151135683059692, 0.0011642773946126301]}}, 31: {'grad': {'score': [0.8218149038461539, 1.499005406198938, 0.5452825372869318, 1.502170502670168, 0.7679512107765282], 'topk_tokens': [' its', ' their', ' OCC', ' an', ' Aw', ' S', ' w', ' S', ' U', ' were', ' Aw', ' C', ' was', ' B', ' and', ' L', 'editary', ' S', ' C', ' W'], 'evidence_proportions': [0.746337890625, 0.72760009765625, 0.746337890625, 1.3792724609375, 0.6639404296875]}, 'weight': {'score': [0.0022823214530944824, 0.0022878210529003267, 0.0012967071749947288, 0.0022896212083226425, 0.0006706354382273915], 'topk_tokens': [' where', ' was', ' the', ' the', ',', ':', 'Question', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0011786023775736492, 0.0036922097206115723, 0.0011786023775736492, 0.004915833473205566, 0.0017941594123840332]}, 'saliency': {'score': [0.000581894929592426, 0.0001161770298590068, 0.0003103830597617409, 0.00011483342765666681, 4.470872355031443e-05], 'topk_tokens': [' the', ' the', ' Mary', ' Where', ' the', '<|eot_id|>', ' Miles', 'Question', ' open', ' Market', '?\n', ' Emily', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', 'Answer', '<|begin_of_text|>', 'office', 'assistant'], 'evidence_proportions': [0.00037228067715962726, 0.0010554194450378418, 0.00037228067715962726, 0.0012637525796890259, 0.00023086865743001303]}}, 'pred_res': "Mary's bedroom.<|eot_id|>", 'score': 0}
2025-01-22 02:59:43.412 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 02:59:43.413 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-4_2-3-4-5-7.pkl | len: 10 |  size: 9.6 KB
Processing depth (2, 3, 4, 5, 7):   5%|▌         | 5/100 [01:33<29:34, 18.68s/it]Processing depth (2, 3, 4, 5, 7):   5%|▌         | 5/100 [01:33<29:40, 18.74s/it]
2025-01-22 02:59:43.643 | INFO     | __main__:<module>:72 - Selected idx: 2
2025-01-22 02:59:43.643 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 02:59:43.643 | INFO     | __main__:<module>:74 - Answer: office
2025-01-22 02:59:43.643 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 02:59:43.644 | INFO     | __main__:<module>:76 - Needle: [' Daniel went back to the kitchen.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' John moved to the bedroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 02:59:43.644 | INFO     | __main__:<module>:77 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 02:59:43.644 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 6, 8, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 02:59:50.658 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 02:59:50.662 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1502, 1508) -->  tragedy. Mary journeyed to
2025-01-22 02:59:50.663 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 02:59:50.667 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1502, 1508) -->  tragedy. Mary journeyed to
2025-01-22 02:59:50.667 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 02:59:50.693 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9663, 9667) -->  dropped the football.
2025-01-22 02:59:50.693 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 02:59:50.710 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6202, 6208) --> . Daniel went back to the
2025-01-22 02:59:50.710 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 02:59:50.728 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (6202, 6208) --> . Daniel went back to the
2025-01-22 02:59:50.728 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 02:59:50.760 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (11296, 11302) --> . Sandra journeyed to the
2025-01-22 02:59:50.760 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 02:59:50.773 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4479, 4485) -->  business. John went back to
2025-01-22 02:59:50.773 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 02:59:50.778 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1504, 1510) -->  Mary journeyed to the office
2025-01-22 02:59:50.778 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John took the milk.
2025-01-22 02:59:50.779 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (328, 332) -->  John took the milk
2025-01-22 02:59:50.779 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the bedroom.
2025-01-22 02:59:50.790 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4057, 4062) -->  John moved to the bedroom
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 02:59:52.869 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 02:59:52.869 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12213])
your chose emoji: ['✍🏿', '👩🏼\u200d⚕', '💇🏻\u200d♂️', '🇵🇭', '🇲🇳', '💊', '🧑\u200d🏭', '🇧🇭', '🇭🇺', '🚶🏽\u200d➡️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 264208.13it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.32s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.42s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.74it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.31it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.41it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.55it/s][A100%|██████████| 8/8 [00:00<00:00, 20.67it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.40it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.12it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.35it/s][A100%|██████████| 8/8 [00:00<00:00, 20.36it/s]
2025-01-22 03:00:02.092 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7045510031960227, 0.596441922712853, 0.7151128595525568, 0.5959242795643054, 1.068974509168027], 'topk_tokens': [' whistle', 'enter', 'Spring', ' cont', ' up', ' Aw', ' cons', 'super', ' over', 'active', ' res', 'sur', 'cont', 'ire', ' Minnesota', 'con', 'cont', ' rept', ' cont', ' conn'], 'evidence_proportions': [0.83184814453125, 0.83184814453125, 0.3603515625, 0.6794230143229166]}, 'weight': {'score': [0.06234547224911777, 0.0025562317606566435, 0.007967040394291733, 0.002433376499500714, 0.0021057297934347122], 'topk_tokens': [' kitchen', ' bedroom', ' lounge', ' \n', ' bedroom', ' football', ' bathroom', 'Answer', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', ' bathroom', ':', '<|end_header_id|>', ' bedroom', ' hallway', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0006546477476755778, 0.0006546477476755778, 0.330078125, 0.0072386860847473145]}, 'saliency': {'score': [0.016226614063436336, 0.00014693300791368395, 0.0010955026655486136, 0.00011526723637392646, 9.901132156599813e-05], 'topk_tokens': ['Question', 'assistant', ':', ' bathroom', ' football', ' kitchen', '<|eot_id|>', ' random', ' top', '<|eot_id|>', ' dropped', '<|end_header_id|>', ' bathroom', ' bedroom', '<|begin_of_text|>', ' bedroom', ' hallway', ' bedroom', ' football', 'office'], 'evidence_proportions': [5.0455331802368164e-05, 5.0455331802368164e-05, 0.08879327774047852, 0.0002011557420094808]}}, 25: {'grad': {'score': [0.5755642977627841, 0.8919067232910356, 0.5886043201793324, 0.8933021601877714, 0.5371282207432078], 'topk_tokens': [' be', ' print', ' circulated', ' thirst', ' resort', ' of', 'ated', ' the', 'ing', 'le', 'bling', ' actions', 'aud', ' the', 'erc', 'ation', '\n', '\n', 'ing', 'ing'], 'evidence_proportions': [0.6624247233072916, 0.6624247233072916, 0.75286865234375, 0.2836405436197917]}, 'weight': {'score': [0.017467748035084118, 0.0025384767886024004, 0.004314102909781716, 0.0025066478717091834, 0.0017173566035370328], 'topk_tokens': [' Mary', '.', ' top', ' hallway', ' Daniel', ' random', '.\n\n', ' \n', '.', 'Answer', '<|start_header_id|>', '.', '<|eot_id|>', '<|eot_id|>', ' football', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0014682312806447346, 0.0014682312806447346, 0.08326339721679688, 0.005603015422821045]}, 'saliency': {'score': [0.002013366330753673, 5.447883075053605e-05, 0.00020938569849187678, 5.051440216208759e-05, 0.0001384820511092001], 'topk_tokens': [' Do', ' Mary', ' Anthony', ' Merch', ' Miss', '<|start_header_id|>', ' Mary', ' directly', ' Miss', ' Daniel', 'Answer', 'office', ' random', '.', ':', '<|eot_id|>', '<|eot_id|>', ' football', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00031189123789469403, 0.00031189123789469403, 0.009652853012084961, 0.00032332539558410645]}}, 26: {'grad': {'score': [0.664563612504439, 0.817027844223291, 0.9672435413707386, 0.8168960263854579, 0.5281301469945195], 'topk_tokens': [' wh', ' press', ' steam', 'just', ' some', ' mar', ' generally', 'oth', ' Press', ' generally', ' flash', ' broad', ' steam', 'BO', ' Merch', ' str', ' went', ' Guards', ' Press', 'burn'], 'evidence_proportions': [0.7997639973958334, 0.7997639973958334, 0.12165260314941406, 0.756103515625]}, 'weight': {'score': [0.013256376439874823, 0.0025159991182755895, 0.005549058769688461, 0.002488336349396329, 0.0015477093298043778], 'topk_tokens': [' the', ' kitchen', '?', ' football', '.\n\n', ' bathroom', ' the', '<|eot_id|>', '<|eot_id|>', ' football', ' bedroom', ' \n', 'Answer', ' hallway', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00011259317398071289, 0.00011259317398071289, 0.06791114807128906, 0.003107428550720215]}, 'saliency': {'score': [0.00016947767951271751, 0.00010600275452684487, 0.00046685789570663917, 0.00010490862064455684, 6.929440284842876e-05], 'topk_tokens': [' Anthony', ' bathroom', ' bedroom', ' Bridge', ' kitchen', '.', ' \n', ' sample', ' bedroom', ' Az', ' Bridge', 'assistant', 'Bridge', 'Answer', '<|start_header_id|>', ' bedroom', ':', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [6.4174334208170576e-06, 6.4174334208170576e-06, 0.0008197426795959473, 6.20881716410319e-05]}}, 27: {'grad': {'score': [0.5160910866477273, 0.4820643483933688, 0.4017038056344697, 0.48222087057013263, 0.5339881555357976], 'topk_tokens': [' accepted', ' Pills', 'conciliation', 'use', '�', 'roduced', ' im', 'ex', ' plainly', 'ly', ' exchanged', ' N', 'sur', ' Bottle', ' EX', 'ab', 'sur', ' N', 'ole', 'ile'], 'evidence_proportions': [0.40875244140625, 0.40875244140625, 0.6429443359375, 0.6461995442708334]}, 'weight': {'score': [0.011170357465744019, 0.002545009555262325, 0.0047005559458877105, 0.002523554729199723, 0.0023640812332950422], 'topk_tokens': ['?', ' Bridge', ' before', ' \n', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' football', ' hallway', 'Answer', '<|start_header_id|>', 'assistant', '.', ' football', ' bedroom', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00047569473584493, 0.00047569473584493, 0.05332374572753906, 0.0044574240843455]}, 'saliency': {'score': [0.0012731335379860618, 0.00010578685054708786, 0.0005100983561891499, 0.00010257764768443609, 0.00018298892832514067], 'topk_tokens': [' Bridge', ' the', ' the', ' Daniel', 'assistant', ' football', ' bathroom', '<|start_header_id|>', ' Bridge', '.', 'THE', 'THE', '<|begin_of_text|>', ' football', ' bedroom', '.\n\n', ' hallway', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [8.25027624766032e-05, 8.25027624766032e-05, 0.0058879852294921875, 0.0005778272946675619]}}, 28: {'grad': {'score': [0.5676796653053977, 0.5328624673812935, 0.577829071969697, 0.5326774446587813, 0.5445975687966418], 'topk_tokens': [' execution', ' ', ' cord', ' returns', ' locality', ' become', ' genu', ' reached', ' half', ' balance', ' executed', ' next', 'half', ' endeavor', 'oggle', 'half', 'na', ' half', ' ins', ' reached'], 'evidence_proportions': [0.54644775390625, 0.54644775390625, 0.5794677734375, 0.6022847493489584]}, 'weight': {'score': [0.007104098796844482, 0.002444720024268158, 0.008174092480630585, 0.0024207417657108685, 0.0017653187709068185], 'topk_tokens': [' hallway', 'Question', ' the', ' football', ' bathroom', ' the', '?', '.\n\n', ' bedroom', '<|eot_id|>', ' before', '<|eot_id|>', 'Answer', ' \n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [4.397829373677572e-05, 4.397829373677572e-05, 0.019273757934570312, 0.013111233711242676]}, 'saliency': {'score': [0.0005405599420720881, 4.850793850992822e-05, 0.00047602436759255147, 4.645751201008496e-05, 6.668603242333255e-05], 'topk_tokens': ['Answer', '<|eot_id|>', ' the', ' football', ' during', ' Floral', ' nearly', '<|eot_id|>', ' Far', ' hallway', ' before', '<|begin_of_text|>', ' bathroom', ' the', 'assistant', ' Bridge', ' Bridge', '<|end_header_id|>', 'office', ':'], 'evidence_proportions': [1.0927518208821614e-06, 1.0927518208821614e-06, 0.00238153338432312, 0.0003921786944071452]}}, 29: {'grad': {'score': [0.7503675981001421, 0.9132057792672943, 0.7892506917317709, 0.9138367790924875, 0.872170063986707], 'topk_tokens': ['\n', ' were', ' The', ' Hotel', '.', ' of', ' The', ' The', '.', 'ants', '\n', ' to', ' The', 'nes', ' Paul', 'paper', '\n', ' The', '.', '\n'], 'evidence_proportions': [0.8664957682291666, 0.8664957682291666, 0.529815673828125, 0.6651458740234375]}, 'weight': {'score': [0.0030304220589724455, 0.0025014871854153607, 0.004117246830102169, 0.0024961453568386406, 0.0011873156277101432], 'topk_tokens': [' the', 'Question', ' Where', ' football', '?', ' bathroom', ' the', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' Does', ' before', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [4.151463508605957e-05, 4.151463508605957e-05, 0.008353710174560547, 0.005459378163019816]}, 'saliency': {'score': [0.00013852932236411354, 3.973536446501474e-05, 0.00020499482299342299, 3.9108141668533023e-05, 8.549797001169689e-05], 'topk_tokens': ['<|eot_id|>', ' Sandra', ' part', ' to', ' Do', '<|end_header_id|>', 'DW', 'assistant', 'Does', ' the', ' the', '<|eot_id|>', ' \n', ' Does', ' before', 'Answer', ':', '<|start_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [5.374352137247722e-06, 5.374352137247722e-06, 0.0003754943609237671, 0.00024686257044474286]}}, 30: {'grad': {'score': [0.7700389515269886, 0.978977403550962, 0.7583460952296401, 0.9799541699258905, 1.0899266484958023], 'topk_tokens': [' in', ' in', ' LINE', '2', ' anx', 'Emp', 'office', ' anything', ' it', ' o', ' an', ' o', ' the', ' to', ' an', 'deal', 'ob', 'op', '3', ' O'], 'evidence_proportions': [0.5374247233072916, 0.5374247233072916, 1.41357421875, 0.806243896484375]}, 'weight': {'score': [0.008578506383028898, 0.0024413085013103445, 0.010062275510845762, 0.002409523117699121, 0.005312981890208686], 'topk_tokens': [' bedroom', ':', '.', '.', ' the', ' before', 'Question', '<|eot_id|>', '?', ' bathroom', '<|eot_id|>', '.\n\n', 'assistant', 'Answer', '<|start_header_id|>', '<|end_header_id|>', ' \n', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0010791321595509846, 0.0010791321595509846, 0.027557373046875, 0.01092467705408732]}, 'saliency': {'score': [0.000941777771169489, 0.00011404982561758683, 0.0005619887149695194, 0.00011133666787492601, 0.0004585162917179848], 'topk_tokens': [' the', ' Daniel', ' Miles', ' bathroom', 'assistant', '<|eot_id|>', ':', ' the', ' football', 'Question', ' bathroom', '<|eot_id|>', ' kitchen', '.', '?', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'office', ':'], 'evidence_proportions': [7.259845733642578e-05, 7.259845733642578e-05, 0.003979146480560303, 0.0006552239259084066]}}, 31: {'grad': {'score': [0.840972900390625, 0.8960736031518625, 0.6624922318892046, 0.896807188736765, 0.6032015672370569], 'topk_tokens': [' J', ' an', ' be', ' was', ' most', ' the', ' w', ' an', ' the', ' an', ' Bre', ' B', ' an', ' C', ' W', ' C', ' W', ' Aw', ' H', ' an'], 'evidence_proportions': [0.8567708333333334, 0.8567708333333334, 0.8960723876953125, 0.77264404296875]}, 'weight': {'score': [0.00135825438932939, 0.0023110805453699823, 0.001654229380867698, 0.0023145869815428006, 0.0009299178621662197], 'topk_tokens': [' bathroom', 'Question', ':', ' the', '?', ' football', '.\n\n', ' before', ' Where', '<|eot_id|>', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0005951424439748129, 0.0005951424439748129, 0.00311434268951416, 0.0017137527465820312]}, 'saliency': {'score': [0.00013175877657803622, 9.91470931838196e-05, 0.00016002582781242603, 9.892287811166362e-05, 4.724780125404472e-05], 'topk_tokens': [' the', ' before', ' football', ' the', '?', 'Question', ' Where', 'Answer', ' the', ' the', ' hallway', '<|start_header_id|>', ' the', '<|eot_id|>', ' \n', ':', '<|end_header_id|>', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [3.4908453623453774e-05, 3.4908453623453774e-05, 0.0003561079502105713, 0.00017589330673217773]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:00:02.094 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:00:02.094 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-0_1-6-8-9.pkl | len: 10 |  size: 9.25 KB
Processing depth (1, 6, 8, 9):   1%|          | 1/100 [00:18<30:19, 18.38s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (1, 4, 5, 8):   1%|          | 1/100 [00:26<30:19, 18.38s/it]2025-01-22 03:00:10.141 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:00:10.146 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1496, 1502) -->  tragedy. Mary journeyed to
2025-01-22 03:00:10.146 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:00:10.151 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1496, 1502) -->  tragedy. Mary journeyed to
2025-01-22 03:00:10.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:00:10.167 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5940, 5944) -->  Mary dropped the football
2025-01-22 03:00:10.167 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:00:10.185 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6194, 6200) --> . Daniel went back to the
2025-01-22 03:00:10.185 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:00:10.203 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (6194, 6200) --> . Daniel went back to the
2025-01-22 03:00:10.203 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:00:10.235 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (11298, 11304) --> . Sandra journeyed to the
2025-01-22 03:00:10.235 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:00:10.248 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4485, 4491) --> . John went back to the
2025-01-22 03:00:10.248 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:00:10.252 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1498, 1504) -->  Mary journeyed to the office
2025-01-22 03:00:10.252 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John took the milk.
2025-01-22 03:00:10.253 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (328, 332) -->  John took the milk
2025-01-22 03:00:10.253 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:00:10.265 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4046, 4051) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:00:12.306 | INFO     | test_jbb_retain:begin_test:632 - the bedroom<|eot_id|>
2025-01-22 03:00:12.306 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12215])
your chose emoji: ['🎛', '🙋🏻', '👨🏾\u200d🎓', '🏍', '👍', '🤸🏻\u200d♂️', '👩🏿\u200d🦽', '🚶🏾\u200d♀\u200d➡', '🤯', '🧘🏻\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 233016.89it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.27s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.99it/s][A100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.83it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.06it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 19.21it/s][A100%|██████████| 8/8 [00:00<00:00, 18.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.63it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.14it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.10it/s][A100%|██████████| 8/8 [00:00<00:00, 17.08it/s]
2025-01-22 03:00:21.982 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.5561356977982954, 0.6011303728922812, 0.4837258078835227, 0.601530326312554, 1.3108025150022644], 'topk_tokens': ['cont', 'sur', ' the', 'enter', ' super', 'ir', 'spir', '�', ' rept', 'active', 'fire', ' Minnesota', 'ire', 'cont', ' cont', ' conn', ' over', ' cons', 'con', 'super'], 'evidence_proportions': [0.65234375, 0.65234375, 0.40936279296875, 0.4615681966145833]}, 'weight': {'score': [0.026800892569802025, 0.002559029256847688, 0.005982942653424812, 0.002505887492748688, 0.0022640228271484375], 'topk_tokens': [' top', ' bedroom', ' Mary', ' football', ' bathroom', 'Answer', 'Bridge', ' bedroom', ' bathroom', '<|start_header_id|>', '<|eot_id|>', ' hallway', '.', '<|eot_id|>', 'assistant', ':', ' football', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0021700660387674966, 0.0021700660387674966, 0.12850189208984375, 0.008261879285176596]}, 'saliency': {'score': [0.005250448530370539, 0.00010472623418061874, 0.0003262162208557129, 9.481708600791224e-05, 0.00017811854680379233], 'topk_tokens': [' football', ':', '.', 'Question', ' hands', '<|end_header_id|>', 'Bridge', ' kitchen', '<|eot_id|>', ' bedroom', '<|eot_id|>', ' office', ' bedroom', ' Mary', ' top', '<|begin_of_text|>', ' bedroom', ' hallway', ' football', 'office'], 'evidence_proportions': [0.00020597378412882486, 0.00020597378412882486, 0.027722835540771484, 0.0003578066825866699]}}, 25: {'grad': {'score': [0.6140497381036932, 0.7990934283426783, 0.573467139041785, 0.8000403637753527, 0.47473741614300274], 'topk_tokens': ['THE', ' the', 'ated', '\n', 'bel', ' extraordinary', '"', 'ised', 'ervative', 'ation', ' thirst', '\n', ' stere', ' safe', 'aud', 'ing', 'ing', ' be', 'ation', 'erc'], 'evidence_proportions': [0.6381429036458334, 0.6381429036458334, 0.77044677734375, 0.4615987141927083]}, 'weight': {'score': [0.0150000507181341, 0.0025486227552131827, 0.006228670929417465, 0.0025161138746891735, 0.0018912994343301525], 'topk_tokens': [' top', ' \n', ' Dan', ' the', '.\n\n', ' the', '.', ' Mary', '<|start_header_id|>', '.', ' Miss', 'Answer', '.', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004543304443359375, 0.004543304443359375, 0.056392669677734375, 0.008318463961283365]}, 'saliency': {'score': [0.0019397843967784536, 5.855827760099242e-05, 0.0006229678789774576, 5.3623851399107253e-05, 9.292968805285467e-05], 'topk_tokens': [' Min', ' Press', ' Daniel', ' THE', 'neh', ' Mary', ' Dan', 'count', 'office', ' Min', 'assistant', ' Miss', ' Mary', ':', 'Answer', '.', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011782745520273845, 0.0011782745520273845, 0.005364775657653809, 0.0011794765790303547]}}, 26: {'grad': {'score': [0.7452184503728693, 0.9376027161179095, 1.0636282256155303, 0.9376087687435442, 0.5201966658882473], 'topk_tokens': [' ne', 'ol', ' generally', ' black', 'oth', ' wh', ' gold', 'burn', ' generally', ' clear', ' Press', ' some', ' went', ' Merch', ' Guards', ' worth', 'BO', 'str', ' STR', ' str'], 'evidence_proportions': [0.8080647786458334, 0.8080647786458334, 0.20868682861328125, 0.9772135416666666]}, 'weight': {'score': [0.005752579732374711, 0.0025126978741580475, 0.008686076511036266, 0.0024900865523443862, 0.0015084277028622835], 'topk_tokens': [' before', '.\n\n', ' bathroom', ' Bridge', '?', ' hallway', ' football', 'Bridge', '<|eot_id|>', '<|eot_id|>', ' the', ' \n', 'Answer', 'assistant', '<|start_header_id|>', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006295740604400635, 0.0006295740604400635, 0.019094467163085938, 0.0071039994557698565]}, 'saliency': {'score': [0.0003235150467265736, 0.00010631146305680929, 0.0004601279894510905, 0.00010495852569356754, 9.950347568677819e-05], 'topk_tokens': [' colon', ' Dan', ' Az', ' football', ' bathroom', ' the', ' bedroom', '.', ' Seventh', 'Answer', ' bedroom', 'assistant', ' Bridge', '<|begin_of_text|>', '<|start_header_id|>', ' the', 'Bridge', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [2.263983090718587e-05, 2.263983090718587e-05, 0.0008058547973632812, 0.0006037056446075439]}}, 27: {'grad': {'score': [0.40373160622336646, 0.4754609195946222, 0.38299491188742896, 0.4758415661287865, 0.33786900838216144], 'topk_tokens': ['\n', 'nes', ' const', ' N', ' very', '186', ' plainly', ' earnest', ' B', ' vigorous', ' per', '185', ' product', 'sur', 'remember', 'bread', ' N', 'sur', 'ly', 'ile'], 'evidence_proportions': [0.3500404357910156, 0.3500404357910156, 0.5399665832519531, 0.42029062906901044]}, 'weight': {'score': [0.013388988646593962, 0.002545255192799352, 0.005597889423370361, 0.0025173568976511592, 0.0019980012506678486], 'topk_tokens': ['?', 'THE', '<|eot_id|>', ' \n', ' before', '<|eot_id|>', ' Mary', ' bathroom', ' THE', ' Bridge', ' football', 'Answer', '.', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0029702981313069663, 0.0029702981313069663, 0.04987907409667969, 0.009899646043777466]}, 'saliency': {'score': [0.0033572614192962646, 0.00012962861274593715, 0.0006821589036421342, 0.0001222908869324458, 0.00021464669186135998], 'topk_tokens': ['.', ' hallway', '<|eot_id|>', 'assistant', ' the', ' Jackson', '<|start_header_id|>', 'THE', ' THE', '.', ' Bridge', '.', ' bathroom', ':', 'THE', '.\n\n', '<|begin_of_text|>', ' Mary', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0010217328866322835, 0.0010217328866322835, 0.013414278626441956, 0.0013236403465270996]}}, 28: {'grad': {'score': [0.5338356711647727, 0.5065348633691782, 0.47373823686079547, 0.5065744679492862, 0.5777476213980413], 'topk_tokens': [' ', 'ib', ' exact', ' within', ' adj', ' ende', 'na', ' at', ' balance', ' become', ' came', '600', ' dry', ' be', ' on', 'been', ' ', ' endeavor', ' reached', ' ins'], 'evidence_proportions': [0.5022176106770834, 0.5022176106770834, 0.5093994140625, 0.6133626302083334]}, 'weight': {'score': [0.007007929411801425, 0.0024499430285251853, 0.015737186778675426, 0.0024056448255826562, 0.0013268589973449707], 'topk_tokens': [' bedroom', 'Question', '.\n\n', ' bathroom', ' Bridge', ' football', '?', '<|eot_id|>', ' \n', ' before', ' the', '<|eot_id|>', 'Answer', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00012194116910298665, 0.00012194116910298665, 0.011536121368408203, 0.01776111125946045]}, 'saliency': {'score': [0.0005416815931146795, 4.779312842272986e-05, 0.0012048789949128122, 4.376012564215137e-05, 5.384286244710287e-05], 'topk_tokens': ['Bridge', ' during', ' football', ' bathroom', ' Third', '<|begin_of_text|>', ' the', '<|eot_id|>', ' Far', ' Bridge', ' before', 'Answer', '<|eot_id|>', ' bedroom', 'office', '<|end_header_id|>', 'assistant', ' Bridge', ' the', ':'], 'evidence_proportions': [2.2411346435546875e-05, 2.2411346435546875e-05, 0.0021742135286331177, 0.000491867462793986]}}, 29: {'grad': {'score': [0.8182567249644886, 1.0098141453763199, 0.9953197132457386, 1.0101999848360608, 0.6074453160382699], 'topk_tokens': [' mail', 'The', ' Press', ' paper', '\n', ' The', ' The', ' Papers', ' Paul', 'APER', '\n', ' The', ' were', 'ants', ' printed', 'boat', ' press', ' The', 'paper', ' the'], 'evidence_proportions': [0.9249674479166666, 0.9249674479166666, 0.6806640625, 0.696563720703125]}, 'weight': {'score': [0.003678110512820157, 0.0025257342593744247, 0.007419187011140765, 0.0025103719654767024, 0.0010484508846117103], 'topk_tokens': [' bathroom', ' Where', ' football', '?', '<|eot_id|>', ' Does', '.\n\n', '<|eot_id|>', ' in', ' \n', ' before', ' the', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010668238004048665, 0.00010668238004048665, 0.012647628784179688, 0.004841287930806478]}, 'saliency': {'score': [0.0002808381210673939, 5.697844958767872e-05, 0.00026995124238910097, 5.599563303324683e-05, 9.039605873218481e-05], 'topk_tokens': ['Does', ' part', '?', ' the', ' football', ' the', ' in', ' before', '<|eot_id|>', ' Do', ' the', ' to', '<|end_header_id|>', ' Does', 'Answer', ' \n', 'office', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [1.4096498489379883e-05, 1.4096498489379883e-05, 0.0009207427501678467, 0.00038771828015645343]}}, 30: {'grad': {'score': [0.5137079412286932, 0.7728911461743881, 0.6134393865411932, 0.7737926375883579, 0.7682203209918478], 'topk_tokens': ['G', ' Loan', ' men', 'op', ' anx', 'better', ' Third', 'office', ' Paul', ' o', 'ire', ' Was', 'ob', ' Bridge', ' States', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.421173095703125, 0.421173095703125, 0.685821533203125, 0.5840352376302084]}, 'weight': {'score': [0.010407664559104225, 0.0024553993539216134, 0.01552157871650927, 0.0024055610252355906, 0.004528650339098944], 'topk_tokens': ['.', ':', ' the', ' before', '<|eot_id|>', '<|eot_id|>', ' bathroom', '?', 'Question', ' the', ' the', '.\n\n', 'assistant', 'Answer', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0013682246208190918, 0.0013682246208190918, 0.022945404052734375, 0.020128051439921062]}, 'saliency': {'score': [0.0018614720214496958, 0.0001373533981272034, 0.002052348671537457, 0.00012903852773289053, 0.00034288306167160255], 'topk_tokens': [' the', ' Miles', 'IR', ' lounge', ' Mary', '.', ' football', ' the', '?', 'assistant', ' \n', '.\n\n', ' bathroom', 'Question', ' the', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.0006511310736338297, 0.0006511310736338297, 0.007006824016571045, 0.0008519192536671957]}}, 31: {'grad': {'score': [0.8312433416193182, 0.6505782317927887, 0.5081884210759943, 0.6506377804967413, 0.39523238030032837], 'topk_tokens': [' very', ' the', 'editary', ' marriage', ' an', 'IR', ' M', ' M', ' an', ' J', ' B', ' Mr', ' an', ' C', ' H', ' C', ' most', 'Mr', ' an', ' an'], 'evidence_proportions': [1.0013020833333333, 1.0013020833333333, 0.84918212890625, 0.4791666666666667]}, 'weight': {'score': [0.0017411763017827814, 0.002310974796791948, 0.001974385796171246, 0.0023129188030335764, 0.0007918407951576122], 'topk_tokens': [' bathroom', ':', ' the', 'Question', ' football', '.\n\n', ' before', ' Where', '?', '<|eot_id|>', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000457535187403361, 0.000457535187403361, 0.0052661895751953125, 0.001958449681599935]}, 'saliency': {'score': [0.0005100396546450528, 8.458151329048646e-05, 0.0004370302864999482, 8.28555727687209e-05, 2.7777492136195086e-05], 'topk_tokens': [' hallway', 'Question', 'light', ' the', ' Where', ' the', ' Mary', ' the', ' the', ' football', ' the', '<|end_header_id|>', '<|eot_id|>', ' the', ':', ' \n', '<|start_header_id|>', '<|begin_of_text|>', 'office', 'assistant'], 'evidence_proportions': [0.00019541382789611816, 0.00019541382789611816, 0.0018005222082138062, 0.00027896960576375324]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:00:21.984 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:00:21.984 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-1_1-4-5-8.pkl | len: 10 |  size: 9.11 KB
Processing depth (1, 4, 5, 8):   2%|▏         | 2/100 [00:38<31:27, 19.26s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (0, 1, 2, 5):   2%|▏         | 2/100 [00:45<31:27, 19.26s/it]2025-01-22 03:00:29.597 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:00:29.597 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:00:29.598 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:00:29.602 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1542, 1548) --> . Mary journeyed to the
2025-01-22 03:00:29.602 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:00:29.609 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (2506, 2510) -->  Mary dropped the football
2025-01-22 03:00:29.609 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:00:29.626 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5989, 5995) --> . Daniel went back to the
2025-01-22 03:00:29.627 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:00:29.644 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (5989, 5995) --> . Daniel went back to the
2025-01-22 03:00:29.644 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:00:29.678 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (11290, 11296) --> . Sandra journeyed to the
2025-01-22 03:00:29.678 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:00:29.690 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4493, 4499) --> . John went back to the
2025-01-22 03:00:29.691 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:00:29.691 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:00:29.691 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John took the milk.
2025-01-22 03:00:29.692 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (335, 339) -->  John took the milk
2025-01-22 03:00:29.692 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:00:29.703 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4088, 4093) -->  John moved to the bedroom
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:00:31.757 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:00:31.758 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12206])
your chose emoji: ['🧍🏽\u200d♂️', '🍡', '👩🏿\u200d🤝\u200d👩🏽', '😮', '🆒', '🐌', '🆘', '📸', '🚶🏻\u200d♀', '🧎🏾\u200d♂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 207126.12it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.71s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.08it/s][A
 75%|███████▌  | 6/8 [00:04<00:01,  1.81it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.72it/s][A100%|██████████| 8/8 [00:05<00:00,  1.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 22.85it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.93it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.13it/s][A100%|██████████| 8/8 [00:00<00:00, 17.95it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.89it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.61it/s][A
100%|██████████| 8/8 [00:00<00:00, 16.95it/s][A100%|██████████| 8/8 [00:00<00:00, 17.69it/s]
2025-01-22 03:00:40.644 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.39943070845170453, 0.533267399899144, 0.4570858117305871, 0.5337165399980007, 0.9997500101725261], 'topk_tokens': ['Minnesota', ' rep', ' conn', 'con', 'editor', 'enter', ' rept', ' effect', ' Gal', 'sur', ' Bu', ' Minnesota', ' cons', ' res', 'cont', ' cont', 'ire', 'super', 'cont', 'active'], 'evidence_proportions': [0.3548329671223958, 0.2735087076822917, 0.36409759521484375, 0.593505859375]}, 'weight': {'score': [0.039166339419104836, 0.002567095494051592, 0.025965795372471664, 0.0024373052807430233, 0.0018613378206888834], 'topk_tokens': [' bedroom', ' the', ' Married', ' Mary', ' the', '<|start_header_id|>', 'Answer', ' bathroom', '<|eot_id|>', ':', ' football', 'Bridge', '<|eot_id|>', ' bathroom', 'assistant', ' bedroom', '<|end_header_id|>', ' hallway', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.007523377736409505, 0.03750833868980408, 0.10269927978515625, 0.030112008253733318]}, 'saliency': {'score': [0.006515700708736073, 9.445434980592441e-05, 0.0048206629175128355, 6.99968246984784e-05, 0.00014472603797912597], 'topk_tokens': [' kitchen', ' John', ' office', '<|eot_id|>', 'From', ' top', '<|eot_id|>', '<|end_header_id|>', ' the', ' Mary', ' football', ' office', ' bedroom', ' office', 'office', ' Mary', '<|begin_of_text|>', ' Married', ' bedroom', ' hallway'], 'evidence_proportions': [0.00037676095962524414, 0.00921475887298584, 0.01971527934074402, 0.0011558632055918376]}}, 25: {'grad': {'score': [0.6968175714666193, 0.7563209383703309, 0.5546381401293206, 0.7569762997143477, 0.4500630696614583], 'topk_tokens': ['\n', ' corner', '\n', ' returns', ' Minnesota', ' thirst', ' prisoners', ' from', 'ting', 'ing', 'ers', 'ing', ' subscribers', ' extraordinary', 'le', 'ation', 'ised', 'ing', 'ation', 'erc'], 'evidence_proportions': [0.8127034505208334, 0.7733561197916666, 0.672332763671875, 0.5207163492838541]}, 'weight': {'score': [0.02243713899092241, 0.0025406011903895305, 0.008107752510995575, 0.0024894665055222778, 0.0014824380477269491], 'topk_tokens': ['Mary', ' the', '<|start_header_id|>', ' Married', ' the', '.\n\n', ' Mary', ' Daniel', ' \n', 'Answer', ' Dan', '.', ' Mary', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015548467636108398, 0.03486789266268412, 0.046802520751953125, 0.014645089705785116]}, 'saliency': {'score': [0.0017720894380049272, 5.1477848311707984e-05, 0.0003614263101057573, 4.75214791672618e-05, 0.00010525186856587727], 'topk_tokens': ['E', ' random', ' Dan', 'MIN', ' boat', ' Daniel', ' Min', ' Min', ' Mary', ' slept', 'Mary', 'Answer', '.', ' Mary', ':', '<|eot_id|>', ' Married', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.975001017252604e-05, 0.003389964501063029, 0.0035660266876220703, 0.0006805956363677979]}}, 26: {'grad': {'score': [1.0952779596502131, 1.0049796048185615, 1.2875162760416667, 1.0040489478649979, 0.5838963826497395], 'topk_tokens': [' Gal', ' Press', ' gang', ' black', ' mar', ' steam', ' some', ' steam', 'BO', ' Col', ' journey', 'str', ' generally', ' gold', ' generally', ' gold', ' Merch', ' STR', ' Guards', ' str'], 'evidence_proportions': [1.4294840494791667, 1.004198710123698, 0.676483154296875, 1.13134765625]}, 'weight': {'score': [0.008127754384821112, 0.0025062086072849257, 0.008400015758745598, 0.002480028269664256, 0.0012510418891906738], 'topk_tokens': [' before', ' bedroom', ' football', '.\n\n', '?', ' bathroom', 'Bridge', '<|eot_id|>', '<|eot_id|>', ' kitchen', ' hallway', 'Answer', ' \n', '<|start_header_id|>', ' the', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0010246634483337402, 0.010555376609166464, 0.013227462768554688, 0.009403417507807413]}, 'saliency': {'score': [0.0010292855176058683, 6.677274290731038e-05, 0.001382110696850401, 6.145871069110942e-05, 7.113218307495118e-05], 'topk_tokens': [' \n', ' Daniel', ' Mary', ' Father', ' hallway', ' Dan', ' Married', 'Answer', ' Bridge', ' bathroom', '.', 'assistant', ' bathroom', '<|start_header_id|>', ':', ' bedroom', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00020499030749003092, 0.0017859935760498047, 0.0018984675407409668, 0.0005174179871877035]}}, 27: {'grad': {'score': [0.4778927889737216, 0.5195603276923944, 0.3761151631673177, 0.5200252644389703, 0.460492197672526], 'topk_tokens': [' *', ' *', ' earnest', ' *', 'na', ' *', '4', ' Newspaper', 'sur', ' notified', ' *', ' print', '185', 'remember', '      ', 'ile', 'n', 'sur', ' N', ' N'], 'evidence_proportions': [0.4908243815104167, 0.5754801432291666, 0.42215728759765625, 0.4045308430989583]}, 'weight': {'score': [0.020411537452177567, 0.0025528966208741674, 0.013188867857961944, 0.002491687277575182, 0.0021613160769144695], 'topk_tokens': [' Bridge', 'Bridge', ' football', '<|eot_id|>', '<|eot_id|>', ' Mary', ' \n', ' bedroom', ' football', '.', ' bathroom', '<|start_header_id|>', 'Answer', ' hallway', 'assistant', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004528919855753581, 0.028675576051076252, 0.044338226318359375, 0.012078990538914999]}, 'saliency': {'score': [0.0036042115905068135, 0.00011596062158382111, 0.0017982179468328302, 0.00010507804007723646, 0.00022268990675608318], 'topk_tokens': ['RE', '.', ' Daniel', ' THE', ' kitchen', ' Bridge', ' Mary', ' John', 'Mary', 'assistant', ' bedroom', ' bathroom', '.', ':', ' Mary', '<|begin_of_text|>', ' hallway', '.\n\n', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0006990532080332438, 0.00718998908996582, 0.005637705326080322, 0.001567929983139038]}}, 28: {'grad': {'score': [0.5216508345170454, 0.49396691878532517, 0.4230686534534801, 0.4941093195596075, 0.5681467692057292], 'topk_tokens': [' news', ' half', ' firm', '      ', ' ', ' acted', 'been', ' escaping', ' ', ' account', ' become', 'ot', ' returns', 'na', ' balance', ' reached', 'hom', '600', ' endeavor', ' ins'], 'evidence_proportions': [0.3862711588541667, 0.6031494140625, 0.642333984375, 0.4950764973958333]}, 'weight': {'score': [0.013656857338818636, 0.002435514217591692, 0.01765543944907911, 0.002373872887771394, 0.0009105324745178223], 'topk_tokens': [' bedroom', ' football', ' hallway', ' the', '.\n\n', '<|eot_id|>', ' the', ' before', '?', ' the', ' bathroom', '<|eot_id|>', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005249977111816406, 0.006229579448699951, 0.005501747131347656, 0.039652734994888306]}, 'saliency': {'score': [0.0011201013218272817, 5.446378322291343e-05, 0.0012007120883826053, 4.9422211601127376e-05, 4.197061061859131e-05], 'topk_tokens': [' bedroom', ' \n', '<|start_header_id|>', ' hallway', '.\n\n', ' the', '<|eot_id|>', ' bathroom', ' before', 'Answer', ' Bridge', ' the', '<|end_header_id|>', ' Bridge', 'assistant', ' kitchen', '<|begin_of_text|>', ' the', 'office', ':'], 'evidence_proportions': [5.587935447692871e-05, 0.001710077126820882, 0.0007012486457824707, 0.0018735826015472412]}}, 29: {'grad': {'score': [0.7735762162642046, 0.9030978501546117, 0.9222523082386364, 0.903280305419058, 0.685555903116862], 'topk_tokens': [' and', ' the', 'ION', 'ants', 'mail', 'AILY', ' The', 'boat', '\n', ' mail', ' should', ' paper', 'APER', '\n', ' NEW', ' were', 'paper', ' Press', '\n', 'ION'], 'evidence_proportions': [1.209228515625, 0.8075154622395834, 0.43096923828125, 0.5323893229166666]}, 'weight': {'score': [0.007755948738618331, 0.002516665589918784, 0.0085607496174899, 0.002490769185559265, 0.0008841166893641154], 'topk_tokens': ['Question', ':', ' bathroom', ' Where', '<|eot_id|>', '?', '<|eot_id|>', '.\n\n', ' the', ' the', ' before', ' \n', ' Does', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003405610720316569, 0.004827241102854411, 0.014352798461914062, 0.010637094577153524]}, 'saliency': {'score': [0.0006549710577184504, 4.700409690330726e-05, 0.0004570610595471931, 4.4790063092299156e-05, 7.617473602294922e-05], 'topk_tokens': ['Does', 'assistant', ' Where', '<|end_header_id|>', ' bathroom', '.\n\n', '.', ' Daniel', ' from', ' Do', ' to', ' the', ' the', 'Answer', ' \n', ' Does', '<|start_header_id|>', 'office', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0005623201529184977, 0.0005839864412943522, 0.0007464587688446045, 0.0007576147715250651]}}, 30: {'grad': {'score': [0.7134975086558949, 0.6028253997762738, 0.5590949781013258, 0.6027437999671666, 0.6818295796712239], 'topk_tokens': [',', 'Country', ' Paul', ' Crew', 'Third', ' Third', ' Sons', ' RID', ' States', 'G', ' Falls', 'ob', ' Bridge', ' Was', 'ire', ' o', ' O', ' o', ' o', ' o'], 'evidence_proportions': [0.7442626953125, 0.6538569132486979, 0.839324951171875, 0.6584879557291666]}, 'weight': {'score': [0.023608107458461414, 0.002443347502192226, 0.02832381653063225, 0.002334758660179847, 0.005497884750366211], 'topk_tokens': [' Daniel', '<|eot_id|>', ' Where', ':', '<|eot_id|>', ' the', ' the', '?', 'Question', '.\n\n', ' bathroom', 'assistant', ' the', 'Answer', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002232233683268229, 0.01684609055519104, 0.019819259643554688, 0.05427189668019613]}, 'saliency': {'score': [0.003990140828219327, 0.0001362840868386185, 0.0033164331407258005, 0.0001206723229047213, 0.00043299992879231773], 'topk_tokens': [':', ' Married', ' the', '.', ' office', ' lounge', '?', ' the', ' Daniel', ' Mary', 'assistant', '<|end_header_id|>', ' the', ' bathroom', 'Question', ' the', '<|start_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0004904866218566895, 0.0060264964898427325, 0.005434989929199219, 0.004490206638971965]}}, 31: {'grad': {'score': [0.6871369101784446, 0.6084593126689466, 0.46692033247514203, 0.6087012183055126, 0.3524216969807943], 'topk_tokens': ['Mr', 'k', 'IO', ' DAY', ' Mr', ' an', ' marriage', ' J', ' the', ' an', ' M', ' an', ' an', ' an', 'IR', ' C', ' M', ' most', ' H', 'Mr'], 'evidence_proportions': [0.5226643880208334, 0.9175211588541666, 0.89129638671875, 0.4851188659667969]}, 'weight': {'score': [0.0028636591000990434, 0.0023086085075788385, 0.002725429607160164, 0.002306471898566938, 0.000951958696047465], 'topk_tokens': [' was', ' bathroom', ' football', 'Question', ' before', ':', '.\n\n', '?', '<|eot_id|>', ' the', ' Where', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0009145935376485189, 0.0030181705951690674, 0.004167437553405762, 0.003789027531941732]}, 'saliency': {'score': [0.0008479708975011653, 8.849147341042015e-05, 0.0004338116356820771, 8.617894870993788e-05, 3.1911333401997884e-05], 'topk_tokens': ['.', ' the', ' before', ' Where', ' Daniel', ' the', ' Mary', ' hallway', ' football', 'Question', ' the', '<|eot_id|>', ' the', '<|end_header_id|>', ' \n', ':', '<|begin_of_text|>', '<|start_header_id|>', 'assistant', 'office'], 'evidence_proportions': [0.0006933510303497314, 0.0010277827580769856, 0.001209750771522522, 0.000581592321395874]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:00:40.646 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:00:40.646 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-2_0-1-2-5.pkl | len: 10 |  size: 9.18 KB
Processing depth (0, 1, 2, 5):   3%|▎         | 3/100 [00:56<30:41, 18.98s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (1, 2, 5, 9):   3%|▎         | 3/100 [01:04<30:41, 18.98s/it]2025-01-22 03:00:47.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:00:47.948 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1535, 1541) --> . Mary journeyed to the
2025-01-22 03:00:47.948 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:00:47.953 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1535, 1541) --> . Mary journeyed to the
2025-01-22 03:00:47.953 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:00:47.968 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5961, 5965) -->  Mary dropped the football
2025-01-22 03:00:47.969 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:00:47.986 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6215, 6221) --> . Daniel went back to the
2025-01-22 03:00:47.986 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:00:48.004 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (6215, 6221) --> . Daniel went back to the
2025-01-22 03:00:48.004 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:00:48.036 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (11325, 11331) --> . Sandra journeyed to the
2025-01-22 03:00:48.036 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:00:48.048 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4505, 4511) --> . John went back to the
2025-01-22 03:00:48.049 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:00:48.053 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1536, 1542) -->  Mary journeyed to the office
2025-01-22 03:00:48.053 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John took the milk.
2025-01-22 03:00:48.054 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (328, 332) -->  John took the milk
2025-01-22 03:00:48.054 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:00:48.065 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4085, 4090) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:00:50.224 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the football.<|eot_id|>
2025-01-22 03:00:50.224 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12221])
your chose emoji: ['\U0001faf5🏽', '👨🏻\u200d❤\u200d💋\u200d👨🏽', '☮', '🧑🏼\u200d⚕', '🧑🏻\u200d🤝\u200d🧑🏽', '🚵🏼\u200d♂️', '☕', '♀', '🇲🇪', '👁']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 208412.62it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.28s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.99it/s][A100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.54it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.69it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.75it/s][A100%|██████████| 8/8 [00:00<00:00, 20.89it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.46it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.21it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.45it/s][A100%|██████████| 8/8 [00:00<00:00, 20.46it/s]
2025-01-22 03:00:59.233 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7672784978693182, 0.721617928290927, 0.6872688062263258, 0.7216285289235213, 0.7670499674479166], 'topk_tokens': [' from', 'Minnesota', ',', ' the', ' Minneapolis', 'from', 'super', 'Minnesota', ' Minnesota', ' the', ' THE', 'con', ' a', ' the', ' of', ' Min', ' the', ' Minnesota', ' marched', ' Minnesota'], 'evidence_proportions': [0.8872273763020834, 0.8872273763020834, 0.564483642578125, 0.6625773111979166]}, 'weight': {'score': [0.028617154468189587, 0.002563795208160493, 0.004644720843344024, 0.002511046650494344, 0.0015053780873616536], 'topk_tokens': [' office', ' old', ' Bridge', ' football', 'Answer', ' hallway', ' bedroom', '<|eot_id|>', ' bathroom', '<|start_header_id|>', '<|eot_id|>', 'assistant', ' bathroom', 'Bridge', ':', '<|end_header_id|>', '.', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0014701485633850098, 0.0014701485633850098, 0.14398956298828125, 0.005996227264404297]}, 'saliency': {'score': [0.0049146251244978475, 8.491965369332455e-05, 0.0003490032571734804, 7.5471241524312e-05, 7.370154062906901e-05], 'topk_tokens': [' bedroom', ' folded', ' kitchen', '.', ' bedroom', '.', ' hand', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', ' hallway', 'Bridge', '.', ' bedroom', ' office', ' top', ' Mary', ' football', 'office'], 'evidence_proportions': [0.00016936659812927246, 0.00016936659812927246, 0.026074886322021484, 0.00029830137888590497]}}, 25: {'grad': {'score': [0.7080091996626421, 0.5736835701009367, 0.7130136200875947, 0.5730628390440731, 0.4661541748046875], 'topk_tokens': [' head', 'ush', ' location', ' twenty', 'ing', 'ervative', ' location', 'ush', ' office', ' actions', 'op', '\n', ' location', ' population', ' news', ' Key', 'ation', ' moved', 'erc', ' Wood'], 'evidence_proportions': [0.7764078776041666, 0.7764078776041666, 1.0735702514648438, 0.3275044759114583]}, 'weight': {'score': [0.013582061637531628, 0.0025493080659104985, 0.0028346325411941066, 0.002528586806355927, 0.0012592768669128418], 'topk_tokens': [' Minneapolis', ' Senator', ' East', ' Dan', ' bathroom', ' \n', '.', '.\n\n', '.', '<|start_header_id|>', 'Answer', ' Mary', '<|eot_id|>', ':', '.', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0024111966292063394, 0.0024111966292063394, 0.05908966064453125, 0.005585392316182454]}, 'saliency': {'score': [0.0007617311044172807, 6.049997744325783e-05, 0.00022916902195323598, 5.8774701041034146e-05, 9.59467887878418e-05], 'topk_tokens': ['neh', '<|start_header_id|>', ' Anthony', ' Anthony', ' Press', ' Min', 'office', ' Married', ' Minnesota', ' Min', ' Senator', ' Minneapolis', ' Market', '.', 'Answer', ':', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00039836764335632324, 0.00039836764335632324, 0.0021005868911743164, 0.0005958875020345052]}}, 26: {'grad': {'score': [0.6925014149058949, 0.719390122617197, 0.8354325727982954, 0.7191240271794647, 0.47745519002278647], 'topk_tokens': [' some', ' Jul', ' gold', ' Field', ' some', ' Gen', ' generally', ' Gutenberg', ' gang', ' went', ' generally', ' Merch', ' sm', ' some', 'BO', ' Guards', ' worth', 'str', ' STR', ' str'], 'evidence_proportions': [0.7791341145833334, 0.7791341145833334, 0.3104667663574219, 0.77392578125]}, 'weight': {'score': [0.005787161263552579, 0.002519023464264987, 0.004196226596832275, 0.0025085659745411994, 0.0007670839627583821], 'topk_tokens': [' hallway', ' the', '.\n\n', ' Bridge', ' before', '?', ' football', 'Bridge', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' \n', ' the', 'Answer', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00042962034543355304, 0.00042962034543355304, 0.02475452423095703, 0.0038573344548543296]}, 'saliency': {'score': [0.0001406805081800981, 0.00011875245552215913, 0.00023978226112596917, 0.0001183845724893353, 3.2437642415364585e-05], 'topk_tokens': [' Dan', ' the', ' Anthony', ' kitchen', ' the', ' Anthony', ' floats', ' Western', ' North', 'assistant', ' Seventh', ' bathroom', 'Answer', '<|start_header_id|>', ' Bridge', ' the', 'Bridge', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [2.3742516835530598e-05, 2.3742516835530598e-05, 0.00040391087532043457, 0.00019906957944234213]}}, 27: {'grad': {'score': [0.5309406627308239, 0.4951850986613352, 0.36377872120250354, 0.4954768300918589, 0.4381604512532552], 'topk_tokens': [' very', ' too', 'n', ' medicine', 'sur', 'ig', ' product', ' possessed', ' plainly', ' vigorous', '      ', ' B', ' carn', ' remember', ' N', '185', 'ly', 'remember', 'bread', 'ile'], 'evidence_proportions': [0.6024983723958334, 0.6024983723958334, 0.6797332763671875, 0.2886301676432292]}, 'weight': {'score': [0.014180958271026611, 0.0025418217654393227, 0.003322494752479322, 0.002518660751986394, 0.0013673909505208333], 'topk_tokens': [' Jackson', ' football', 'Bridge', '<|eot_id|>', ' \n', ' before', ' Mary', '<|eot_id|>', '.', ' Bridge', ' bathroom', 'Answer', ' football', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.001991202433904012, 0.001991202433904012, 0.06252002716064453, 0.006334424018859863]}, 'saliency': {'score': [0.003414107994599776, 0.00013529787201699224, 0.00045606945500229347, 0.00012849977167713885, 0.00010925451914469401], 'topk_tokens': [' football', ' the', 'Question', '.', ' upper', 'Bridge', '<|start_header_id|>', ' Anthony', ' ST', 'THE', ' Minnesota', 'Minnesota', '.\n\n', ' Jackson', '.', ' bathroom', '<|begin_of_text|>', ' Mary', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0009737610816955566, 0.0009737610816955566, 0.015150785446166992, 0.00047035018603007]}}, 28: {'grad': {'score': [0.46601070057262073, 0.4944886896757138, 0.4607768203272964, 0.494631606132713, 0.6038512166341146], 'topk_tokens': [' July', ' escaping', ' within', ' ', ' on', ' kept', ' take', ' firm', 'been', ' July', ' dry', ' have', ' came', 'hum', ' be', ' reached', 'na', '600', ' endeavor', ' ins'], 'evidence_proportions': [0.3908793131510417, 0.3908793131510417, 0.6210269927978516, 0.5129292805989584]}, 'weight': {'score': [0.007313148541883988, 0.0024489206387272424, 0.007730745907985803, 0.0024258015355258142, 0.0009252341588338216], 'topk_tokens': [' bedroom', ' the', ' the', '.\n\n', ' Bridge', ' the', ' football', '?', '<|eot_id|>', ' bathroom', ' before', ' \n', '<|eot_id|>', 'assistant', 'Answer', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00015017390251159668, 0.00015017390251159668, 0.019235610961914062, 0.013690789540608725]}, 'saliency': {'score': [0.0006791407411748713, 5.145211135132715e-05, 0.00037872249429876155, 4.942966949618388e-05, 2.8040409088134765e-05], 'topk_tokens': [' to', ' \n', '<|eot_id|>', ' bedroom', ' football', '<|eot_id|>', ' the', 'Bridge', ' Bridge', ' before', ' the', ':', '<|begin_of_text|>', '<|start_header_id|>', ' the', 'assistant', ' Far', '<|end_header_id|>', 'office', ' Bridge'], 'evidence_proportions': [1.1593103408813477e-05, 1.1593103408813477e-05, 0.003033459186553955, 0.00044469038645426434]}}, 29: {'grad': {'score': [0.8485939719460227, 1.0877643901354004, 1.1337335759943181, 1.0880721454005897, 0.5767637125651042], 'topk_tokens': [' Paul', ' The', ' the', ' The', '\n', 'The', ' the', ' and', 'The', 'boat', '\n', 'paper', 'ants', ' were', ' the', ' The', ' The', ' press', ' printed', ' the'], 'evidence_proportions': [0.9324137369791666, 0.9324137369791666, 0.79437255859375, 0.71710205078125]}, 'weight': {'score': [0.0038942071524533358, 0.0025255283922783573, 0.0022397276126977167, 0.002523828893018192, 0.0007977835337320963], 'topk_tokens': [' Where', ' the', '?', ' Does', ' football', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' in', ' \n', ' before', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0001761913299560547, 0.0001761913299560547, 0.015550613403320312, 0.0035593012968699136]}, 'saliency': {'score': [0.00020524588498202237, 5.75329359566864e-05, 0.0001781600894349994, 5.693872319013958e-05, 6.698290506998698e-05], 'topk_tokens': [' part', '.\n\n', ' the', ' the', ' the', ' Does', 'ot', ' Where', ' in', '<|end_header_id|>', ' before', ' football', ' Do', 'Answer', ' the', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [9.099642435709635e-06, 9.099642435709635e-06, 0.0005683004856109619, 0.0003555019696553548]}}, 30: {'grad': {'score': [0.6814464222301136, 0.6916834074337315, 0.6802281466397372, 0.6917329831471339, 0.8399808756510416], 'topk_tokens': [',', 'ch', ' Falls', 'con', ' o', 'ire', ' Crew', ' Published', 'office', ' o', ' Bridge', 'ob', ' States', ' o', ' Was', ' o', ' O', ' o', ' o', ' o'], 'evidence_proportions': [0.6278483072916666, 0.6278483072916666, 0.743804931640625, 0.7470703125]}, 'weight': {'score': [0.009819052436135033, 0.002450529003478809, 0.010760988249923244, 0.0024146683303483767, 0.004353421529134115], 'topk_tokens': [' football', ' Min', ' before', ' the', ' the', '<|eot_id|>', '?', 'Question', ' the', '<|eot_id|>', '.\n\n', ' bathroom', 'assistant', 'Answer', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009220441182454427, 0.0009220441182454427, 0.03183746337890625, 0.012934128443400065]}, 'saliency': {'score': [0.0019938864491202616, 0.00012444714462953144, 0.000678123849810976, 0.00011956557033878655, 0.00033913215001424155], 'topk_tokens': [' the', ' Bridge', ' kitchen', ' Third', ' the', ' \n', 'IR', ' lounge', ' football', ' Mary', '?', '.\n\n', 'Question', ' bathroom', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.00039347012837727863, 0.00039347012837727863, 0.008893594145774841, 0.0005949139595031738]}}, 31: {'grad': {'score': [0.8209797252308239, 0.6327138290773542, 0.5518382679332386, 0.6325927774503308, 0.5453529866536458], 'topk_tokens': [' United', ' DAYS', ' J', ' very', ' an', 'k', ' an', ' M', ' M', ' Married', 'IR', ' an', ' marriage', ' Mr', ' DAY', ' most', ' an', ' Mr', 'Mr', ' Mary'], 'evidence_proportions': [0.91131591796875, 0.91131591796875, 1.04327392578125, 0.4921112060546875]}, 'weight': {'score': [0.0019233985380692916, 0.002299441337819518, 0.0010943828207073789, 0.00230338938784082, 0.0006148211161295572], 'topk_tokens': [':', ' bathroom', ' the', 'Question', '?', ' before', ' football', '.\n\n', ' Where', '<|eot_id|>', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000299374262491862, 0.000299374262491862, 0.007088422775268555, 0.0017280975977579753]}, 'saliency': {'score': [0.0006426112218336625, 9.388303557755386e-05, 0.0001478159066402551, 9.274465582388e-05, 3.073851267496745e-05], 'topk_tokens': [' the', 'Answer', ' was', ' Where', ' the', ' hallway', ' the', 'ot', ' Mary', ' football', ' before', ' the', '<|eot_id|>', ' the', ' \n', ':', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.00017233689626057944, 0.00017233689626057944, 0.002651989459991455, 0.0002435743808746338]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:00:59.234 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:00:59.235 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-3_1-2-5-9.pkl | len: 10 |  size: 9.28 KB
Processing depth (1, 2, 5, 9):   4%|▍         | 4/100 [01:15<30:07, 18.83s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (4, 5, 6, 8):   4%|▍         | 4/100 [01:23<30:07, 18.83s/it]2025-01-22 03:01:06.932 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:01:06.943 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3566, 3572) -->  John journeyed to the office
2025-01-22 03:01:06.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:01:06.957 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4879, 4885) --> . Mary journeyed to the
2025-01-22 03:01:06.957 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:01:06.976 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7223, 7227) -->  Mary dropped the football
2025-01-22 03:01:06.976 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:01:06.993 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6221, 6227) --> . Daniel went back to the
2025-01-22 03:01:06.994 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:01:07.011 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (6221, 6227) --> . Daniel went back to the
2025-01-22 03:01:07.012 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:01:07.044 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (11326, 11332) --> . Sandra journeyed to the
2025-01-22 03:01:07.044 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:01:07.057 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4473, 4479) -->  business. John went back to
2025-01-22 03:01:07.057 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:01:07.067 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3565, 3571) --> . John journeyed to the
2025-01-22 03:01:07.067 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John took the milk.
2025-01-22 03:01:07.068 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (315, 319) -->  John took the milk
2025-01-22 03:01:07.068 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:01:07.079 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4021, 4026) -->  sell. John moved to
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:01:09.101 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the football.<|eot_id|>
2025-01-22 03:01:09.102 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12225])
your chose emoji: ['🙅🏼\u200d♂️', '🧑🏾\u200d❤️\u200d🧑🏻', '🙌🏻', '💑🏼', '🇦🇴', '💁🏻\u200d♂', '🌎', '🚶\u200d♂️', '🧛🏽\u200d♂', '🏌🏼\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 209715.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.59s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.09s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.55it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.37it/s][A100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.26it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.19it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.33it/s][A100%|██████████| 8/8 [00:00<00:00, 21.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.69it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.44it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.79it/s][A100%|██████████| 8/8 [00:00<00:00, 20.76it/s]
2025-01-22 03:01:19.061 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.9656344327059659, 0.8190188561942832, 0.8287945371685606, 0.8187273560172039, 0.9092289405533031], 'topk_tokens': [' Min', ' the', ' THE', 'Minnesota', 'Minnesota', ' Ramsey', ' Minnesota', ' THE', ' the', 'irie', ' from', ' marched', ' the', 'Minnesota', ' Phil', ' Minnesota', ' of', ' Minnesota', ' Min', ' Minnesota'], 'evidence_proportions': [1.3760986328125, 0.8841145833333334, 0.8357391357421875, 0.7232869466145834]}, 'weight': {'score': [0.054366331208835945, 0.002556199560261342, 0.006576494737104936, 0.0024516569512320553, 0.0012906865228580523], 'topk_tokens': [' the', ' \n', ' the', ' Bridge', 'Answer', ' football', ' hallway', '<|start_header_id|>', '<|eot_id|>', ' bathroom', 'Bridge', 'assistant', '<|eot_id|>', ' bedroom', ':', ' bathroom', '<|end_header_id|>', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000820547342300415, 0.0018790761629740398, 0.28568267822265625, 0.0061884721120198565]}, 'saliency': {'score': [0.006612024523995139, 9.53435741832331e-05, 0.0003697836037838098, 8.282110442701547e-05, 8.054473732091204e-05], 'topk_tokens': [' Dul', 'Bridge', ' bedroom', ':', ' kitchen', ' top', ' office', '<|end_header_id|>', '<|eot_id|>', 'Question', ' Mary', '.', ' hallway', '<|eot_id|>', ' Mary', ' bathroom', ' bathroom', ' bedroom', ' football', 'office'], 'evidence_proportions': [7.316470146179199e-05, 0.00022488832473754883, 0.03582048416137695, 6.571412086486816e-05]}}, 25: {'grad': {'score': [0.9406876997514204, 0.6724307309131431, 0.8364003499348959, 0.6715013314108296, 0.6768051340610166], 'topk_tokens': [' news', ' location', 'deal', '186', ' heading', 'ing', '186', ' population', ' news', 'erc', ' location', ' head', ' opinion', 'op', '186', ' Wood', ' news', ' actions', 'op', ' Key'], 'evidence_proportions': [1.073486328125, 1.0365982055664062, 1.21441650390625, 0.5294926961263021]}, 'weight': {'score': [0.015485338189385155, 0.0025417889150212516, 0.002734178846532648, 0.0025178728000215898, 0.0008277417738226395], 'topk_tokens': [' football', ' Miss', ' Daniel', '.', ' Mary', ' the', ' \n', '.\n\n', 'Answer', ' football', '<|start_header_id|>', ' Mary', '<|eot_id|>', '<|eot_id|>', '.', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.386738141377766e-05, 0.0011560320854187012, 0.07634735107421875, 0.004631439844767253]}, 'saliency': {'score': [0.0012827461416071112, 6.491541033765326e-05, 0.0001617164322824189, 6.245182918319176e-05, 7.639429237269147e-05], 'topk_tokens': ['Mer', 'Right', ' Dan', ' Met', ' Market', ' Daniel', '.', ' East', ' random', ' directly', 'assistant', 'Answer', ' football', ' Mary', ':', '<|eot_id|>', '<|eot_id|>', '.', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [4.102786382039388e-06, 5.4111083348592125e-05, 0.006351947784423828, 0.0004105567932128906]}}, 26: {'grad': {'score': [0.8244601162997159, 0.6626891066517747, 0.8573631517814867, 0.6618689287269033, 0.5315663784365111], 'topk_tokens': [' kids', ' by', ' some', ' for', ' and', ' Gen', 'Cut', ' generally', 'BO', ' generally', ' sm', ' Gutenberg', ' generally', ' Field', 'ol', ' Guards', ' STR', 'str', ' worth', ' str'], 'evidence_proportions': [0.9420369466145834, 0.9702657063802084, 0.454345703125, 0.8078206380208334]}, 'weight': {'score': [0.008571635593067516, 0.002510058321195855, 0.005049152807755904, 0.002492218622047181, 0.0007217541525635538], 'topk_tokens': [' the', '?', ' the', ' hallway', ' football', ' football', ' Bridge', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'Bridge', ' the', ' \n', 'Answer', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [8.877118428548177e-05, 0.0006476342678070068, 0.04000091552734375, 0.00402564803759257]}, 'saliency': {'score': [0.00042042678052728826, 0.00011721667025088678, 0.00021141586881695372, 0.00011641325290133364, 5.7855738869196255e-05], 'topk_tokens': [' East', ' Gal', ' Seventh', ' football', ' kitchen', ' bathroom', ' hallway', ' bedroom', 'Answer', 'assistant', '.', ' bathroom', ' the', '<|start_header_id|>', '<|begin_of_text|>', ' Bridge', 'Bridge', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [8.056561152140299e-06, 3.0229489008585613e-05, 0.002120375633239746, 8.969505627950032e-05]}}, 27: {'grad': {'score': [0.6849347894841974, 0.6394374226864725, 0.6805644179835464, 0.6392436884674163, 0.5972523749629154], 'topk_tokens': ['.', ' later', '.', ' N', 'fire', ' B', '.', ' accepted', '.', '.', '.', '.', '\n', '.', 'roduced', ' N', ' carn', 'Mal', 'ile', 'bread'], 'evidence_proportions': [0.3434588114420573, 0.4118245442708333, 1.0958251953125, 1.0255940755208333]}, 'weight': {'score': [0.014699150215495716, 0.002532632175232468, 0.0036827614813139944, 0.0025075239215365686, 0.0011264168763462502], 'topk_tokens': ['Bridge', ' THE', ' bedroom', '<|eot_id|>', ' before', ' \n', '<|eot_id|>', ' bathroom', ' Bridge', ' football', 'Answer', ' football', 'assistant', '.', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0003863672415415446, 0.0013802448908487956, 0.0702962875366211, 0.005266080300013225]}, 'saliency': {'score': [0.002526256171139804, 0.00015951267606523545, 0.0007638172669844193, 0.0001535966057076957, 7.40817830532412e-05], 'topk_tokens': [' football', ' upper', ' Daniel', ' Jackson', ' the', ' hallway', ' football', ' Bridge', ':', ' bedroom', ' the', ' THE', ' Mary', '<|begin_of_text|>', '.', ' bathroom', '.\n\n', ' Mary', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00010577837626139323, 0.00045363108317057293, 0.011778593063354492, 0.000851134459177653]}}, 28: {'grad': {'score': [0.544858759099787, 0.6390207043709618, 0.5101265184807054, 0.6395403454266917, 0.585059443606606], 'topk_tokens': [' on', ' acted', ' been', ' been', ' about', ' have', ' out', ' executed', 'na', 'being', ' came', ' office', ' escaping', ' within', 'been', ' take', ' have', ' be', ' ins', ' reached'], 'evidence_proportions': [0.3725433349609375, 0.41638946533203125, 0.66278076171875, 0.76702880859375]}, 'weight': {'score': [0.00798376040025191, 0.002454628994705301, 0.009722724105372574, 0.0024249306682532766, 0.001011310498925704], 'topk_tokens': [' bedroom', 'Question', ' Bridge', '.\n\n', ' football', ' the', ' the', '?', ' bathroom', '<|eot_id|>', ' before', '<|eot_id|>', ' \n', 'Answer', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010042389233907063, 0.0008739133675893148, 0.01919841766357422, 0.015500505765279135]}, 'saliency': {'score': [0.0012250623919747093, 6.108245680354998e-05, 0.0008340748873623935, 5.688295723222174e-05, 4.3472911738142185e-05], 'topk_tokens': ['<|eot_id|>', '?', 'Answer', ' Bridge', '<|eot_id|>', '<|start_header_id|>', ' before', ' the', ' football', ' the', ' the', ' the', ' bedroom', 'assistant', ' Far', 'office', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', ':'], 'evidence_proportions': [5.453824996948242e-06, 9.972850481669109e-05, 0.003795146942138672, 0.00185661514600118]}}, 29: {'grad': {'score': [0.9471631483598189, 1.0390885374069683, 1.1403171654903528, 1.0389802408108695, 0.6534827510012856], 'topk_tokens': [' routes', 'ION', ' the', ' The', ' the', '\n', ' press', ' place', 'The', 'ION', ' The', '\n', ' papers', ' were', ' and', ' paper', ' printed', 'paper', ' press', ' the'], 'evidence_proportions': [1.6105143229166667, 1.0306803385416667, 0.33245849609375, 0.6100978851318359]}, 'weight': {'score': [0.0047383172945542765, 0.0025202415068833798, 0.0029017004099759188, 0.002515198308466455, 0.0007555069802682611], 'topk_tokens': [' in', ' the', '?', 'Question', ' Where', ' football', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' the', ' \n', ' before', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00014428297678629556, 0.0018851558367411296, 0.015995025634765625, 0.00468104084332784]}, 'saliency': {'score': [0.00038964910940690476, 6.988226582552197e-05, 0.0002833658998662775, 6.87255265483173e-05, 5.613701252997676e-05], 'topk_tokens': [' football', ' bathroom', 'nes', ' the', ',', ' bathroom', ' the', ' the', ' the', ' Where', ' Do', ' before', ' the', 'Answer', '<|end_header_id|>', ' \n', ':', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [8.463859558105469e-06, 0.00012561678886413574, 0.0010313242673873901, 0.000607083241144816]}}, 30: {'grad': {'score': [1.0520560524680398, 0.9853706512738203, 1.017856482303504, 0.9851620486407071, 0.9682307182988034], 'topk_tokens': [' States', 'Emp', ' dre', ' an', ' Published', 'op', ' Bridge', ' politically', ' offices', ' o', ' it', ' anx', ' o', ' head', 'office', ' o', ' o', ' o', ' O', ' o'], 'evidence_proportions': [1.0636800130208333, 0.9193318684895834, 0.8613510131835938, 1.30029296875]}, 'weight': {'score': [0.010108145800503817, 0.0024380698328446393, 0.014615609790339615, 0.0023911917117564177, 0.0038416687446304514], 'topk_tokens': [':', '.', ' the', ' the', ' before', '?', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Question', ' the', ' bathroom', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0004932483037312826, 0.0027605692545572915, 0.0285797119140625, 0.014756242434183756]}, 'saliency': {'score': [0.002099985426122492, 0.00012617404866298513, 0.0017645539659442325, 0.00011816465106403754, 0.00026254035249541075], 'topk_tokens': [' football', ' the', ' kitchen', ' the', ' Miles', '.', '.\n\n', '.', '?', ' Mary', ' football', '<|begin_of_text|>', ':', ' the', ' bathroom', 'assistant', 'Question', '<|end_header_id|>', '<|start_header_id|>', 'office'], 'evidence_proportions': [9.91523265838623e-05, 0.0003256400426228841, 0.008916974067687988, 0.0013305048147837322]}}, 31: {'grad': {'score': [0.8060413707386364, 0.7831999302261389, 0.5419459487452651, 0.7838127193895955, 0.5875800410403481], 'topk_tokens': [' DAYS', 'issippi', ' an', ' M', ' members', ' Mr', 'editary', ' Mary', 'Mexico', ' membership', ' M', 'IR', ' an', ' very', ' Mr', ' marriage', ' DAY', 'Mr', ' United', ' most'], 'evidence_proportions': [0.7150065104166666, 0.6940511067708334, 1.48681640625, 0.5552164713541666]}, 'weight': {'score': [0.001750680533322421, 0.002308911115840264, 0.0015325582388675575, 0.0023120248783900085, 0.0008026240747186202], 'topk_tokens': [' bathroom', ' the', ':', 'Question', ' before', '?', '.\n\n', ' football', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0005568663279215494, 0.00040545066197713214, 0.0055255889892578125, 0.0017731189727783203]}, 'saliency': {'score': [0.0004599717530337247, 0.0001093132903221594, 0.0001364458690990101, 0.00010860594056211252, 2.949599978290027e-05], 'topk_tokens': [' the', ' was', '?', '.\n\n', ' Mary', ' Where', 'Answer', ' Mary', ' the', ' football', ' the', ':', '<|eot_id|>', '<|end_header_id|>', ' the', ' \n', '<|begin_of_text|>', '<|start_header_id|>', 'assistant', 'office'], 'evidence_proportions': [3.769000371297201e-05, 0.00011958678563435872, 0.0019202232360839844, 0.00024913748105367023]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:01:19.062 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:01:19.062 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-4_4-5-6-8.pkl | len: 10 |  size: 9.25 KB
Processing depth (4, 5, 6, 8):   5%|▌         | 5/100 [01:35<30:23, 19.20s/it]Processing depth (4, 5, 6, 8):   5%|▌         | 5/100 [01:35<30:16, 19.12s/it]
2025-01-22 03:01:19.344 | INFO     | __main__:<module>:72 - Selected idx: 3
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:74 - Answer: office
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:76 - Needle: [' Sandra journeyed to the bedroom.', ' Mary journeyed to the office.', ' John went back to the bedroom.', ' Mary picked up the apple.', ' John journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:77 - Real Needle: [' Mary journeyed to the office.', ' Mary picked up the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 03:01:19.345 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
Processing depth (0, 1, 3, 5, 6):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:01:26.439 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:01:26.440 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:01:26.440 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:01:26.444 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1498, 1503) -->  tragedy. Mary picked up
2025-01-22 03:01:26.444 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:01:26.455 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3805, 3811) --> . Mary journeyed to the
2025-01-22 03:01:26.456 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:01:26.471 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5929, 5933) -->  Mary dropped the apple
2025-01-22 03:01:26.472 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:01:26.497 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7194, 7200) --> . Daniel went back to the
2025-01-22 03:01:26.498 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:01:26.508 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3592, 3598) --> . Sandra journeyed to the
2025-01-22 03:01:26.508 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:01:26.525 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5983, 5989) --> . John went back to the
2025-01-22 03:01:26.525 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:01:26.526 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:01:26.526 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:01:26.546 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7194, 7200) --> . Daniel went back to the
2025-01-22 03:01:26.546 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:01:26.551 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1546, 1551) --> . John moved to the
2025-01-22 03:01:26.551 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John took the milk.
2025-01-22 03:01:26.579 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10681, 10685) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:01:28.658 | INFO     | test_jbb_retain:begin_test:632 - the bathroom<|eot_id|>
2025-01-22 03:01:28.658 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12249])
your chose emoji: ['👾', '👨🏼\u200d🌾', '🇭🇳', '🏋🏻\u200d♂️', '👱🏾\u200d♀️', '🚶🏾\u200d➡️', '🧑🏿\u200d🦰', '👩🏼\u200d❤️\u200d👨🏻', '🙆🏼\u200d♂', '🙅\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.57s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.09s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.89it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.59it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.74it/s][A100%|██████████| 8/8 [00:00<00:00, 20.88it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.57it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.23it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.32it/s][A100%|██████████| 8/8 [00:00<00:00, 20.38it/s]
2025-01-22 03:01:38.112 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7365784821686922, 0.6627740608547261, 0.5967723962032434, 0.662789263508981, 1.6348752108487217], 'topk_tokens': ['ire', 'Spring', ' upper', ' Ot', 'editor', 'edit', '�', ' utter', 'sur', 'Dub', '�', ' sure', ' effect', ' or', ' Or', 'ent', 'enter', ' rept', ' conn', 'active'], 'evidence_proportions': [0.8037109375, 0.65908203125, 0.5517247517903646, 1.1983642578125, 0.61102294921875]}, 'weight': {'score': [0.021501042224742747, 0.0025602500591071008, 0.011569980419043339, 0.0024939123927671734, 0.003502828153696927], 'topk_tokens': [' bedroom', 'Mary', '.', ' John', 'Answer', ' Bridge', '<|eot_id|>', ' office', 'Bridge', '<|eot_id|>', '<|start_header_id|>', ':', ' Mary', 'assistant', ' hallway', ' bedroom', '<|end_header_id|>', ' bathroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.009577433268229166, 0.04601624011993408, 0.01849024494489034, 0.040386199951171875, 0.0034160117308298745]}, 'saliency': {'score': [0.0034866023946691443, 0.00013440213449808367, 0.0017832824678132029, 0.00012251447491117384, 0.00043849443847482854], 'topk_tokens': ['�', ' Bridge', ' top', '<|start_header_id|>', ' John', '<|end_header_id|>', 'Mary', ' office', ' bedroom', '<|eot_id|>', ' office', ' office', 'Bridge', ' bathroom', ' Mary', ' bedroom', 'office', '<|begin_of_text|>', ' hallway', ' office'], 'evidence_proportions': [0.005339294672012329, 0.008422410488128662, 0.0006766219933827718, 0.0038416385650634766, 9.402632713317871e-05]}}, 25: {'grad': {'score': [1.0522393120659723, 1.1993380004693495, 1.0439989494554924, 1.2000842766788768, 0.611885590986772], 'topk_tokens': [' whistle', ' from', 'MIN', 'REAT', ' much', ' the', ' remin', 'ervative', 'ation', ' principles', ' be', 'ing', 'IVE', 'THE', ' good', ' subscribers', ' thirst', 'ished', 'ation', 'erc'], 'evidence_proportions': [1.35400390625, 0.58692626953125, 1.0203043619791667, 0.79815673828125, 1.3395589192708333]}, 'weight': {'score': [0.020094096660614014, 0.0025257122739462275, 0.004826025529341264, 0.002480575967173357, 0.0029261227358471265], 'topk_tokens': [' apple', ' prior', ' top', '?\n', '.\n\n', ' the', '.', ' to', ' Mary', 'Answer', '<|eot_id|>', 'Mary', '<|start_header_id|>', '<|eot_id|>', ' Mary', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0023575623830159507, 0.0473804235458374, 0.01429283618927002, 0.04197883605957031, 0.006303459405899048]}, 'saliency': {'score': [0.004407282228823061, 7.155973778365728e-05, 0.00022811600656220406, 6.153341802910426e-05, 0.00018395009365948764], 'topk_tokens': [' to', 'Print', ' spring', ' bathroom', ' apple', ' Min', ' top', 'Answer', '<|start_header_id|>', ':', ' Mary', ' apple', 'MIN', ' Mary', '<|eot_id|>', '<|eot_id|>', 'Mary', '<|end_header_id|>', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.0003153681755065918, 0.013724124431610108, 0.0024345715840657554, 0.008185923099517822, 0.00018877784411112467]}}, 26: {'grad': {'score': [0.7823169849537037, 0.7271343602052893, 0.9154348662405303, 0.7265024311127317, 0.5698326500979337], 'topk_tokens': ['graph', ' Gal', 'istributed', 'str', 'prob', ' journey', ' steam', ' broad', ' mar', ' part', ' wh', ' str', 'graph', ' Merch', ' Press', ' Col', ' Marshall', ' Col', ' clear', ' Col'], 'evidence_proportions': [1.088623046875, 0.36820068359375, 1.0433451334635417, 0.396484375, 0.8173014322916666]}, 'weight': {'score': [0.0076044996579488116, 0.002468922139965615, 0.0037726777972597065, 0.00245401897126114, 0.0022394155914133244], 'topk_tokens': [' bedroom', ' kitchen', '�', '.\n\n', ' bedroom', ' Bridge', '<|eot_id|>', 'Bridge', '<|eot_id|>', ' hallway', '?\n', ' bathroom', 'Answer', 'assistant', '<|start_header_id|>', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012748241424560547, 0.01657308340072632, 0.003043373425801595, 0.016132831573486328, 0.0053359270095825195]}, 'saliency': {'score': [0.0009345588860688386, 8.734602138428054e-05, 0.0004766691814769398, 8.441579321353648e-05, 0.00013388964262875643], 'topk_tokens': [' old', 'Mary', ' office', ' Mary', ' bathroom', ' kitchen', 'assistant', ' apple', 'Answer', '<|start_header_id|>', '<|end_header_id|>', ' hallway', ' Bridge', ' bedroom', ' bedroom', 'Bridge', '<|begin_of_text|>', ' the', ':', 'office'], 'evidence_proportions': [0.0002493162949879964, 0.0024409055709838866, 0.0002241035302480062, 0.0016831308603286743, 0.0005759199460347494]}}, 27: {'grad': {'score': [0.4388320357711227, 0.49764611849287815, 0.47747635118889087, 0.4978309747437615, 0.44193211468783294], 'topk_tokens': [' excursion', 'ile', '186', 'conciliation', 'APER', ' ar', ' paper', ' newspaper', ' Moore', ' earnest', 'sur', ' N', ' told', ' EX', 'sur', '\n', ' Newspaper', ' excessive', ' opposition', ' N'], 'evidence_proportions': [0.23926798502604166, 0.50419921875, 0.2636210123697917, 0.4107208251953125, 0.7778752644856771]}, 'weight': {'score': [0.014123881304705585, 0.0025339325090669414, 0.006063085613828717, 0.002498710569116209, 0.005303267728198658], 'topk_tokens': ['NEW', '<|eot_id|>', ' hallway', '?\n', ' Bridge', '�', ' bedroom', ' THE', ' bedroom', ' Mary', 'Mary', 'Answer', 'assistant', ' bathroom', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003952344258626302, 0.039009690284729004, 0.0073669056097666425, 0.02443981170654297, 0.0034369329611460366]}, 'saliency': {'score': [0.00422943079913104, 0.0001318842992423729, 0.0010010311097809763, 0.00012045655744557462, 0.0014502555131912231], 'topk_tokens': ['Answer', '�', '�', ' Dan', '�', ' upper', 'THE', '�', ' bedroom', ' Mary', '<|begin_of_text|>', ' hallway', '<|end_header_id|>', '�', ' Mary', ' bathroom', ':', 'Mary', '.\n\n', 'office'], 'evidence_proportions': [0.0010349055131276448, 0.013375532627105714, 0.0018429954846700032, 0.007390737533569336, 8.110205332438151e-05]}}, 28: {'grad': {'score': [0.7378427010995371, 0.7825913286415395, 0.8315799597537878, 0.7825578275437567, 0.5586171583695845], 'topk_tokens': [' ', ' spoken', ' child', '\n', ' returns', ' PA', 'CH', ' ', ' returns', 'being', ' ', 'hom', ' balance', ' become', 'been', ' be', ' acted', '600', 'na', ' ins'], 'evidence_proportions': [0.7855072021484375, 0.933154296875, 0.44659423828125, 0.8736724853515625, 0.7281138102213541]}, 'weight': {'score': [0.015479575704645228, 0.002417752075444415, 0.009268328999028061, 0.0023702794910406186, 0.0008362253958528692], 'topk_tokens': [' discarded', ' the', ' the', ' to', ' hallway', ' the', '<|eot_id|>', '.\n\n', ' Bridge', ' the', '?\n', '<|eot_id|>', 'Answer', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003604789574940999, 0.005256831645965576, 0.020319233338038128, 0.02585601806640625, 0.027360339959462483]}, 'saliency': {'score': [0.0017004101364700883, 7.086468272263269e-05, 0.0005483013210874615, 6.596326886673731e-05, 5.6022270159287885e-05], 'topk_tokens': ['.\n\n', ' Mary', ' Mary', ' office', ' kitchen', 'Bridge', ' the', ' bedroom', ' apple', ' hallway', ' the', ' bathroom', 'assistant', ' Floral', 'Answer', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [5.894899368286133e-05, 0.0008968830108642579, 0.0033264756202697754, 0.0025192946195602417, 0.0018394887447357178]}}, 29: {'grad': {'score': [1.0429144965277777, 0.9862319912660191, 1.1815067638050427, 0.9855778615690414, 0.8398320458152078], 'topk_tokens': ['\n', ' printed', ' Papers', ' mail', '\n', 'LY', 'AILY', 'mail', ' were', ' THE', '\n', ' the', ' regular', '\n', 'paper', 'ATING', '\n', 'aper', ' Press', 'APER'], 'evidence_proportions': [1.462799072265625, 0.79620361328125, 1.4471638997395833, 0.625732421875, 0.7024943033854166]}, 'weight': {'score': [0.005313599551165545, 0.0024940846316834466, 0.007045046849684281, 0.002475521047398315, 0.0016542747616767883], 'topk_tokens': ['️', 'Question', ':', ' the', '<|eot_id|>', ' Where', ' was', '<|eot_id|>', ' Does', '.\n\n', '?\n', ' to', 'Answer', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0011622707049051921, 0.0016253471374511718, 0.00949069857597351, 0.007250785827636719, 0.007069915533065796]}, 'saliency': {'score': [0.000481108824412028, 6.638647692416232e-05, 0.0008275382446520256, 6.34075981023108e-05, 0.0001449578187682412], 'topk_tokens': ['IVE', ' the', '"The', '.\n\n', ' the', 'Answer', '<|eot_id|>', 'assistant', ' the', 'Does', ' the', 'NEW', '?\n', ' to', '<|end_header_id|>', ' Does', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00017458200454711914, 7.69495964050293e-05, 0.0011310080687204997, 0.0007598400115966797, 0.0002887149651845296]}}, 30: {'grad': {'score': [0.6761135525173612, 0.9702055571994123, 0.7029585404829546, 0.971580311992133, 1.3223130919716575], 'topk_tokens': ['�', ' S', ' an', ' o', '3', ' offices', '�', ' Paul', 'office', ' o', 'op', ' o', ' office', ' o', 'office', ' o', ' o', 'ob', 'op', ' O'], 'evidence_proportions': [0.966796875, 0.35098876953125, 0.6941121419270834, 0.709716796875, 0.615966796875]}, 'weight': {'score': [0.011358378110108551, 0.0024381817705669594, 0.011487659179803097, 0.0023939296127724868, 0.006434068761088632], 'topk_tokens': ['.', '<|eot_id|>', '.', '<|eot_id|>', ' the', ' the', ' Where', ':', 'Question', ' bathroom', ' the', 'Answer', '.\n\n', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002600987752278646, 0.010258376598358154, 0.014974902073542276, 0.015095710754394531, 0.014924357334772745]}, 'saliency': {'score': [0.001902182896931966, 0.0001843635933358916, 0.0012274131630406234, 0.0001777356090034045, 0.0002723668109286915], 'topk_tokens': [',', 'Mary', 'Question', ' location', ' Where', ' Mary', '.\n\n', '.', ' the', 'office', ':', '?\n', ' the', '<|begin_of_text|>', '<|end_header_id|>', ' bathroom', '<|start_header_id|>', ':', ' office', 'office'], 'evidence_proportions': [0.001575191815694173, 0.002157425880432129, 0.0025698343912760415, 0.0019649267196655273, 0.0013069907824198406]}}, 31: {'grad': {'score': [1.2964680989583333, 1.7081843765304874, 0.908578583688447, 1.7112606894382254, 0.7022063515403054], 'topk_tokens': [' be', ' its', ' C', ' M', ' be', ' C', ' S', ' its', ' H', ' two', ' L', ' were', ' an', ' was', ' most', ' B', ' their', ' and', 'editary', ' W'], 'evidence_proportions': [1.0638020833333333, 1.55048828125, 1.222412109375, 1.894775390625, 0.9926350911458334]}, 'weight': {'score': [0.004031925289719193, 0.0022933981134671405, 0.00345825245886138, 0.0022863945512362474, 0.0008512003855271773], 'topk_tokens': [' the', ' to', ' was', ' where', ',', 'Question', ':', '.\n\n', '<|eot_id|>', ' the', ' Where', 'Answer', '<|eot_id|>', '?\n', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0009561975797017416, 0.0036533474922180176, 0.0035007198651631675, 0.0055658817291259766, 0.006931702295939128]}, 'saliency': {'score': [0.0006139830306724265, 0.00013380261423461107, 0.00043999245672514945, 0.00013191030547847878, 3.3100220290097326e-05], 'topk_tokens': ['light', ' John', '<|eot_id|>', ' location', ' office', ' hallway', ' Where', ' the', ',', 'Question', ' office', '?\n', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0009480118751525879, 0.0006819725036621094, 0.0003280043601989746, 0.0009182840585708618, 0.0003064076105753581]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-22 03:01:38.113 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:01:38.113 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-0_0-1-3-5-6.pkl | len: 10 |  size: 9.6 KB
Processing depth (0, 1, 3, 5, 6):   1%|          | 1/100 [00:18<30:49, 18.68s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (1, 3, 4, 8, 9):   1%|          | 1/100 [00:26<30:49, 18.68s/it]2025-01-22 03:01:45.537 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:01:45.542 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 03:01:45.542 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:01:45.553 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3790, 3795) --> . Mary picked up the
2025-01-22 03:01:45.553 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:01:45.557 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 03:01:45.557 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:01:45.583 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9628, 9632) -->  dropped the apple.
2025-01-22 03:01:45.583 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:01:45.613 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10390, 10396) --> . Daniel went back to the
2025-01-22 03:01:45.613 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:01:45.624 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3577, 3583) --> . Sandra journeyed to the
2025-01-22 03:01:45.624 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:01:45.643 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (6033, 6039) --> . John went back to the
2025-01-22 03:01:45.643 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:01:45.648 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1496, 1502) -->  Mary journeyed to the office
2025-01-22 03:01:45.648 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:01:45.677 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10390, 10396) --> . Daniel went back to the
2025-01-22 03:01:45.677 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:01:45.682 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1543, 1548) --> . John moved to the
2025-01-22 03:01:45.682 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John took the milk.
2025-01-22 03:01:45.710 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10681, 10685) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:01:47.798 | INFO     | test_jbb_retain:begin_test:632 - Mary's hand<|eot_id|>
2025-01-22 03:01:47.798 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12205])
your chose emoji: ['🧗\u200d♀️', '🇨🇽', '⏭️', '⬅️', '👩🏻', '🎅', '🏀', '🤹🏽', '🏌', '🦹']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 93990.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.32s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.05s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.97it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.62it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.64it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.75it/s][A100%|██████████| 8/8 [00:00<00:00, 20.88it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.68it/s][A
 50%|█████     | 4/8 [00:00<00:00, 17.37it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 20.26it/s][A100%|██████████| 8/8 [00:00<00:00, 19.19it/s]
2025-01-22 03:01:56.932 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6047159830729166, 0.7606681873105595, 0.5423713452888258, 0.7616078847915123, 1.2268562316894531], 'topk_tokens': ['ire', ' evil', ' upper', ' rep', ' Or', 'fire', 'cont', 'editor', 'con', 'sur', ' election', ' effect', 'super', ' sure', ' election', 'enter', 'active', ' rept', ' up', ' conn'], 'evidence_proportions': [0.69427490234375, 0.431689453125, 0.69427490234375, 0.7774658203125, 0.454620361328125]}, 'weight': {'score': [0.01701983036818328, 0.0025681103243624303, 0.012331679011836197, 0.002509462575299304, 0.0023347437381744385], 'topk_tokens': [' bedroom', 'el', '\n\n', ' Mary', 'THE', '.', '?\n', 'Bridge', '<|eot_id|>', 'Answer', '<|eot_id|>', '<|start_header_id|>', ' bedroom', 'assistant', ' bathroom', ':', ' hallway', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.01431158185005188, 0.02500396966934204, 0.01431158185005188, 0.027019500732421875, 0.009116431077321371]}, 'saliency': {'score': [0.008831728387761998, 0.00016974494626581244, 0.003259844852216316, 0.0001420964034308109, 0.00011543794111772017], 'topk_tokens': ['el', '<|start_header_id|>', ' bedroom', '<|end_header_id|>', '<|eot_id|>', ' Mary', ' Mary', 'Answer', '<|eot_id|>', 'Bridge', ' bedroom', '.', ' bathroom', ' Mary', ' office', '<|begin_of_text|>', ' Mary', ' bedroom', ' hallway', 'office'], 'evidence_proportions': [0.013697455326716105, 0.012991154193878173, 0.013697455326716105, 0.001998305320739746, 0.00018970171610514322]}}, 25: {'grad': {'score': [0.9865796124493634, 1.400090624231998, 1.0195367986505681, 1.4020436227964435, 0.8725148981267755], 'topk_tokens': [' printers', 'been', 'rogate', ' both', 'bel', ' stere', ' thirst', 'ing', ' Republican', 'ate', ' be', ' extraordinary', 'ation', ' printers', 'ervative', 'erc', ' most', ' prisoners', ' subscribers', 'ation'], 'evidence_proportions': [0.991186777750651, 0.443603515625, 0.991186777750651, 0.9412841796875, 1.4600423177083333]}, 'weight': {'score': [0.017682199124936706, 0.002540801973203051, 0.006231676448475231, 0.002497119040727478, 0.0017272423614155161], 'topk_tokens': [' to', ' Miss', ' apple', 'THE', ' Mary', ' Mary', '.\n\n', '?\n', ' Daniel', ' Mary', '<|eot_id|>', '<|start_header_id|>', 'Answer', '<|eot_id|>', '.', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0141468346118927, 0.03200998306274414, 0.0141468346118927, 0.0281219482421875, 0.005853275458017985]}, 'saliency': {'score': [0.0017791147585268373, 7.14427024245682e-05, 0.0005534273205381451, 6.633752107011778e-05, 0.00013979321176355535], 'topk_tokens': ['.', 'office', ' Paul', ' directly', ' Paul', '<|start_header_id|>', ' Anthony', ' Miss', ' Az', ' Mary', ' Daniel', ' apple', 'assistant', ':', ' Paul', '<|eot_id|>', 'Answer', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0020096302032470703, 0.0016951441764831543, 0.0020096302032470703, 0.0034043192863464355, 0.0003045896689097087]}}, 26: {'grad': {'score': [0.5736750849971065, 0.7944077136837471, 0.7894199255741003, 0.7949119022061936, 0.6702929410067472], 'topk_tokens': [' broad', ' of', ' steam', ' by', 'prob', ' part', ' Press', 'BO', ' worth', ' hand', 'ols', ' some', ' went', ' clear', ' STR', ' generally', ' wh', ' generally', 'str', ' str'], 'evidence_proportions': [0.8236490885416666, 0.296844482421875, 0.8236490885416666, 0.15594482421875, 0.5829060872395834]}, 'weight': {'score': [0.008455223507351346, 0.002497735612953169, 0.0049473390434727526, 0.002477838594236128, 0.001310863278128884], 'topk_tokens': [' Anthony', ' Mary', ' Floral', '.\n\n', ' bathroom', ' bedroom', 'Bridge', ' kitchen', '<|eot_id|>', '<|eot_id|>', ' hallway', '?\n', 'assistant', ' the', 'Answer', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.004356980323791504, 0.014999687671661377, 0.004356980323791504, 0.011420249938964844, 0.00922130544980367]}, 'saliency': {'score': [0.0010912484592861598, 0.00010689621115430056, 0.0005794023022507176, 0.00010342455463781566, 6.770681251179089e-05], 'topk_tokens': [' Mary', ' apple', ' the', ' Bridge', ' Anthony', ' bathroom', ' Mary', ' Bridge', ' bedroom', '?\n', 'assistant', ' hallway', '<|start_header_id|>', ' Az', ' kitchen', ' bedroom', 'Bridge', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.0010993778705596924, 0.0024613142013549805, 0.0010993778705596924, 0.00043283402919769287, 0.00037221113840738934]}}, 27: {'grad': {'score': [0.6638511375144676, 0.6793122190392807, 0.6773973522764264, 0.6793517876409391, 0.5165869972922585], 'topk_tokens': [' remember', ' time', 'paper', ' time', ' print', ' told', ' tell', ' plant', '      ', ' time', ' N', '      ', '185', ' Temper', ' time', '186', ' talk', 'ile', ' tell', 'na'], 'evidence_proportions': [0.676177978515625, 0.530670166015625, 0.676177978515625, 0.69189453125, 0.7314860026041666]}, 'weight': {'score': [0.01587594438482214, 0.002541247755594254, 0.00832640402244799, 0.0024958911271431698, 0.002056146209890192], 'topk_tokens': [' to', ' Geo', '<|eot_id|>', ' hallway', '.', '<|eot_id|>', '?\n', ' bedroom', ' bathroom', 'THE', ' Mary', 'Answer', 'THE', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.013208856185277304, 0.037017297744750974, 0.013208856185277304, 0.008129119873046875, 0.008756875991821289]}, 'saliency': {'score': [0.00704338373961272, 0.00015467258346181365, 0.0019238446698044286, 0.00013455421019554375, 0.0001972277056087147], 'topk_tokens': [' Paul', ' Emily', ' upper', ' prior', ' Daniel', '.', ' bathroom', '<|start_header_id|>', ' Mary', ' Mary', ' hallway', '.\n\n', ' Mary', 'THE', '<|end_header_id|>', ':', '<|begin_of_text|>', 'THE', ' Mary', 'office'], 'evidence_proportions': [0.005774438381195068, 0.022285258769989012, 0.005774438381195068, 0.0008649080991744995, 0.000998695691426595]}}, 28: {'grad': {'score': [0.677734375, 0.7456112687648481, 0.6486495046904592, 0.7460255615405215, 0.6210556030273438], 'topk_tokens': ['messages', 'hum', ' returns', 'PA', ' child', ' ', '600', ' be', ' ', 'na', ' become', ' made', ' executed', ' spoken', ' balance', 'hom', ' kept', ' acted', 'CH', ' ins'], 'evidence_proportions': [0.4499104817708333, 1.047412109375, 0.4499104817708333, 0.856964111328125, 0.7058308919270834]}, 'weight': {'score': [0.013245723865650318, 0.00245823473058091, 0.013514498869578043, 0.002404219836102127, 0.0010034631599079478], 'topk_tokens': [' discarded', ' was', ' Mary', ' the', ' to', 'Question', '.\n\n', ' Bridge', '<|eot_id|>', ' the', ' the', '<|eot_id|>', 'Answer', '?\n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009613235791524252, 0.014384698867797852, 0.0009613235791524252, 0.006539344787597656, 0.0413362979888916]}, 'saliency': {'score': [0.0010095503595140246, 9.162533221710047e-05, 0.0008581732258652196, 8.750249890620849e-05, 4.827298901297829e-05], 'topk_tokens': [' Mary', ' during', 'Question', ' the', 'Answer', ' Floral', '?\n', ' the', ' Third', ' Bridge', 'Bridge', ' Mary', ' Mary', ' Bridge', 'office', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.00019878149032592773, 0.0029080986976623537, 0.00019878149032592773, 0.0003353208303451538, 0.0014984508355458577]}}, 29: {'grad': {'score': [0.9483484338831019, 1.0647740858216597, 1.092181581439394, 1.0649584148944313, 0.7547782551158558], 'topk_tokens': ['\n', 'LY', '\n', ' Newspaper', 'aper', ' were', ' printed', ' the', '\n', ' The', 'ION', '\n', ' regular', 'paper', 'AILY', '\n', 'ATING', 'ION', ' NEW', 'APER'], 'evidence_proportions': [0.9796905517578125, 1.07021484375, 0.9796905517578125, 0.92578125, 0.7991536458333334]}, 'weight': {'score': [0.003976062492088035, 0.00251916629281571, 0.00779606176145149, 0.002501592097718539, 0.0010169121352109041], 'topk_tokens': [' the', ' in', 'Does', ' the', 'Question', '.\n\n', ' Where', '<|eot_id|>', '<|eot_id|>', ' Does', ' to', ' the', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00013978282610575357, 0.003945529460906982, 0.00013978282610575357, 0.004494905471801758, 0.011328170696894327]}, 'saliency': {'score': [0.00027972901308978044, 8.159835342303476e-05, 0.0008873975638187293, 7.896882335354765e-05, 9.613010016354647e-05], 'topk_tokens': [' the', 'THE', ' the', 'Answer', ' Where', ' to', 'THE', ' the', ' the', ' the', '?\n', 'Does', 'assistant', ' to', '<|end_header_id|>', ' Does', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.8695990244547527e-05, 0.00019402503967285155, 1.8695990244547527e-05, 0.00040340423583984375, 0.0007907648881276449]}}, 30: {'grad': {'score': [0.8749344437210648, 1.1016343401839108, 0.9084824070785985, 1.1026629819058975, 1.5181857022372158], 'topk_tokens': [' head', ' the', ' account', '2', ' its', 'G', ' Paul', ' an', ' office', ' M', ' o', ' o', ' o', 'op', 'ob', '3', 'office', ' o', ' o', ' O'], 'evidence_proportions': [0.918701171875, 1.0896484375, 0.918701171875, 0.9817657470703125, 0.5372517903645834]}, 'weight': {'score': [0.007862051328023275, 0.0024691366334897817, 0.010501231207992092, 0.0024353284654062236, 0.004053604873743924], 'topk_tokens': [':', ' the', ' Floral', ',', ' Az', ' the', 'Question', '<|eot_id|>', ' Where', ' the', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002330829699834188, 0.006559133529663086, 0.002330829699834188, 0.010807991027832031, 0.01804629961649577]}, 'saliency': {'score': [0.001415901713901096, 0.0001323076433950671, 0.0015437783616961856, 0.0001256199366685009, 0.0002416616136377508], 'topk_tokens': [',', ':', ' Daniel', '<|eot_id|>', ' George', ' Third', ' the', ' nearly', ' the', ' the', '.\n\n', 'assistant', 'Question', ' bathroom', ' office', 'Answer', '<|start_header_id|>', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [0.0014787018299102783, 0.0013112068176269532, 0.0014787018299102783, 0.0009246468544006348, 0.0017050504684448242]}}, 31: {'grad': {'score': [1.0761374014395255, 1.27803751177603, 0.6314988569779829, 1.2802427541064345, 0.6344316655939276], 'topk_tokens': [' U', 'ARCH', ' and', ' C', ' S', ' Mr', ' the', 'Mo', ' W', ' M', '7', ' very', 'UX', ' L', ' United', 'G', ' S', ' M', ' most', 'editary'], 'evidence_proportions': [1.4942220052083333, 0.77713623046875, 1.4942220052083333, 1.10992431640625, 0.46661122639973956]}, 'weight': {'score': [0.0018082503919248229, 0.0023050935572895065, 0.0029601508920842953, 0.0023044183184170786, 0.001313156702301719], 'topk_tokens': [' apple', ' where', ' was', ' the', ':', ',', 'Question', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '<|eot_id|>', '?\n', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0008201797803243002, 0.0022241353988647463, 0.0008201797803243002, 0.0016000866889953613, 0.0035765965779622397]}, 'saliency': {'score': [0.000595251719156901, 0.00016120502205985648, 0.00038701295852661133, 0.000159626778647901, 7.097694006833163e-05], 'topk_tokens': [' the', ' the', ' open', 'light', ' hallway', ' the', ' Mary', ' Miles', 'Question', ' Where', ' the', '?\n', '<|eot_id|>', 'Answer', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':', 'assistant', 'office'], 'evidence_proportions': [0.0005797743797302246, 0.0010852336883544922, 0.0005797743797302246, 0.0002967268228530884, 0.0004169046878814697]}}, 'pred_res': "Mary's hand<|eot_id|>", 'score': 0}
2025-01-22 03:01:56.934 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:01:56.934 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-1_1-3-4-8-9.pkl | len: 10 |  size: 9.6 KB
Processing depth (1, 3, 4, 8, 9):   2%|▏         | 2/100 [00:37<30:38, 18.76s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 2, 3, 8, 9):   2%|▏         | 2/100 [00:44<30:38, 18.76s/it]2025-01-22 03:02:04.250 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:02:04.251 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:04.251 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:02:04.258 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2506, 2511) --> . Mary picked up the
2025-01-22 03:02:04.258 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:02:04.270 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3839, 3845) -->  war. Mary journeyed to
2025-01-22 03:02:04.270 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:02:04.296 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9736, 9740) -->  Mary dropped the apple
2025-01-22 03:02:04.296 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:02:04.326 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10477, 10483) --> . Daniel went back to the
2025-01-22 03:02:04.326 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:02:04.337 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3617, 3623) -->  cold. Sandra journeyed to
2025-01-22 03:02:04.337 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:02:04.355 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (6057, 6063) --> . John went back to the
2025-01-22 03:02:04.356 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:02:04.356 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:04.356 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:02:04.386 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10477, 10483) --> . Daniel went back to the
2025-01-22 03:02:04.386 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:02:04.391 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1546, 1551) --> . John moved to the
2025-01-22 03:02:04.391 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John took the milk.
2025-01-22 03:02:04.420 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10727, 10731) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:02:06.488 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the apple.<|eot_id|>
2025-01-22 03:02:06.489 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12259])
your chose emoji: ['👨🏿\u200d✈️', '📈', '👩🏿\u200d❤️\u200d💋\u200d👨🏽', '🧜🏿\u200d♀️', '👩🏼\u200d❤\u200d💋\u200d👨🏼', '🤾🏽\u200d♀', '🦸🏿', '👨🏿\u200d🦽\u200d➡', '🦈', '💇🏿\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 205855.41it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.52s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.56it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.38it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.69it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.24it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.03it/s][A100%|██████████| 8/8 [00:00<00:00, 16.97it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.42it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.46it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.60it/s][A100%|██████████| 8/8 [00:00<00:00, 16.62it/s]
2025-01-22 03:02:16.478 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7069543909143519, 0.7879044200106027, 0.6124535762902462, 0.7885580982852007, 1.3320726667131697], 'topk_tokens': ['edit', ' effect', ' utter', ' enterprise', 'ors', ' sure', ' offices', ' emb', 'office', 'ire', ' office', 'Official', 'editor', 'ent', ' Or', ' office', ' or', ' office', 'active', 'enter'], 'evidence_proportions': [1.0140584309895833, 0.6634033203125, 0.9508056640625, 0.73834228515625, 0.17136637369791666]}, 'weight': {'score': [0.019883204389501502, 0.0025610476136742324, 0.005953897129405628, 0.002513538207316846, 0.0022424812219580827], 'topk_tokens': ['\n\n', ' kitchen', '?\n', ' bedroom', ' top', ' bedroom', 'Bridge', '<|start_header_id|>', '<|eot_id|>', ' Mary', 'Answer', '<|eot_id|>', ' bedroom', 'assistant', ':', ' bathroom', '<|end_header_id|>', ' hallway', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0025067726771036782, 0.0383794903755188, 0.007364322741826375, 0.055049896240234375, 0.010920484860738119]}, 'saliency': {'score': [0.006254454453786214, 0.00018351031948133468, 0.0008401708169416947, 0.0001682996574009787, 0.00027886762910959674], 'topk_tokens': ['<|start_header_id|>', ' random', ' John', '<|eot_id|>', ' kitchen', '<|end_header_id|>', 'Mary', ' Mary', '<|eot_id|>', ' bathroom', 'Bridge', ' bedroom', ' bedroom', ' bedroom', '<|begin_of_text|>', ' top', 'office', ' Mary', ' hallway', ' office'], 'evidence_proportions': [0.001897980769475301, 0.018927836418151857, 0.0016060471534729004, 0.01263427734375, 0.00044496854146321613]}}, 25: {'grad': {'score': [1.3351236979166667, 1.3734691679920072, 1.3066961115056819, 1.3737346248034228, 0.6576461791992188], 'topk_tokens': [' divide', 'ENCES', ' subscribers', ' printed', 'ing', 'ers', 'THE', 'been', ' returns', 'ised', ' printers', ' prisoners', ' rest', 'ervative', 'age', 'ation', 'erc', ' thirst', 'ation', ' principles'], 'evidence_proportions': [1.7318522135416667, 1.002685546875, 1.2892659505208333, 0.987060546875, 1.4933268229166667]}, 'weight': {'score': [0.018850710656907823, 0.002531058791605258, 0.0027470769304217715, 0.0024943602260004624, 0.0014458329093699552], 'topk_tokens': [' prior', ' to', '?\n', ' apple', 'Mary', ' Miss', '.\n\n', '<|start_header_id|>', '.', ' Mary', 'Answer', ' Mary', ' top', '<|eot_id|>', '<|eot_id|>', 'assistant', 'office', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004946490128835043, 0.02488248348236084, 0.0086535116036733, 0.07149505615234375, 0.007281263669331868]}, 'saliency': {'score': [0.004423309255529333, 9.78162146520774e-05, 0.00017599626020951703, 8.803273349421482e-05, 0.00011236144571888204], 'topk_tokens': [' Bench', ' Paul', ' Paul', 'office', ' Mary', ' Daniel', ' apple', '.', 'assistant', 'MIN', 'Answer', ' directly', ' top', '<|eot_id|>', '<|eot_id|>', ' Mary', '<|end_header_id|>', 'Mary', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [5.7925780614217125e-05, 0.008258354663848878, 0.002012491226196289, 0.015530705451965332, 0.0005987087885538737]}}, 26: {'grad': {'score': [0.8473895037615741, 0.8534520660019574, 1.1315548058712122, 0.8527132985865656, 0.6346871901531609], 'topk_tokens': [' part', ' went', ' steam', ' steam', 'little', ' summer', ' Marshall', ' steam', ' Jul', 'BO', ' some', ' steam', ' hand', ' str', ' Press', 'graph', 'prob', 'graph', ' Col', ' clear'], 'evidence_proportions': [1.3441569010416667, 0.30662841796875, 1.105926513671875, 0.4713134765625, 0.7934366861979166]}, 'weight': {'score': [0.008179567478321216, 0.0024857707342912103, 0.0031361038034612484, 0.002471411788026853, 0.00137799613329829], 'topk_tokens': ['.\n\n', ' bedroom', ' Bridge', ' Floral', ' bedroom', 'Bridge', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' kitchen', '?\n', ' hallway', 'Answer', 'assistant', '<|start_header_id|>', ' the', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00025548537572224933, 0.012435245513916015, 0.0016675194104512532, 0.022887706756591797, 0.009263873100280762]}, 'saliency': {'score': [0.00038496211723045064, 0.00010631323813536498, 0.00034561482342806727, 0.00010504936861235649, 0.00014427486731081592], 'topk_tokens': [' Seventh', 'assistant', ' sample', ' Floral', 'Answer', ' bedroom', ' apple', ' Az', ' Bridge', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', ':', ' bedroom', 'Bridge', ' hallway', ' kitchen', '<|begin_of_text|>', ' the', 'office'], 'evidence_proportions': [2.3325284322102863e-05, 0.0003911733627319336, 0.00015804171562194824, 0.0007276535034179688, 0.0007398823897043864]}}, 27: {'grad': {'score': [0.7069747359664352, 0.6815219496778403, 0.6785509514086174, 0.6814736600059357, 0.6204809072066326], 'topk_tokens': ['ile', 'sur', '\n', 'isc', ' told', ' newspaper', ' several', ' exchanged', ' proportion', ' opposition', ' went', ' West', ' Newspaper', ' Seventh', ' excursion', 'na', ' N', 'bread', ' opposition', ' excessive'], 'evidence_proportions': [0.4010416666666667, 0.783447265625, 0.652374267578125, 0.571258544921875, 1.0942586263020833]}, 'weight': {'score': [0.011697828769683838, 0.0025369059095451917, 0.0043725100430575285, 0.002511668645908633, 0.0022203891861195466], 'topk_tokens': ['<|eot_id|>', '?\n', ' bedroom', '.', ' Mary', ' Bridge', ' THE', ' bedroom', 'THE', ' hallway', 'Answer', ' bathroom', 'THE', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.000917971134185791, 0.025934278964996338, 0.005640983581542969, 0.024469852447509766, 0.008156140645345053]}, 'saliency': {'score': [0.004761980639563667, 0.00015639414863906885, 0.0008089361768780333, 0.00014443736458965895, 0.00045333285720980897], 'topk_tokens': [' office', ' Bridge', '<|start_header_id|>', ' THE', ' old', ' Mary', ' bathroom', '.', ' prior', ' top', ' Mary', 'THE', 'Mary', '<|end_header_id|>', ' hallway', ' Mary', '.\n\n', ':', 'THE', 'office'], 'evidence_proportions': [0.0004953046639760336, 0.011614251136779784, 0.00285110870997111, 0.01042833924293518, 0.001451730728149414]}}, 28: {'grad': {'score': [0.9106219256365741, 0.9186358805358453, 0.8619636766838304, 0.9188068959861688, 0.585161170180963], 'topk_tokens': [' ', 'been', ' ', ' returns', ' membership', ' child', 'criptions', ' balance', ' spoken', ' out', ' account', 'hom', ' ', ' be', ' ', ' executed', 'na', '600', ' acted', ' ins'], 'evidence_proportions': [0.8657430013020834, 1.1220703125, 0.5489908854166666, 1.134674072265625, 0.9915568033854166]}, 'weight': {'score': [0.008809182378980849, 0.002447389173776035, 0.006223255937749689, 0.0024230983763207726, 0.000842615049712512], 'topk_tokens': ['Question', ' the', ' the', ' kitchen', ' discarded', ' was', ' to', '.\n\n', ' Bridge', '<|eot_id|>', '<|eot_id|>', 'Answer', '?\n', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010396043459574382, 0.004489028453826904, 0.0037613113721211753, 0.018983840942382812, 0.019379297892252605]}, 'saliency': {'score': [0.000777337286207411, 0.00010694250684414124, 0.0004914041721459591, 0.0001044191158108023, 5.077707524202308e-05], 'topk_tokens': [' bedroom', ' apple', 'Question', ' during', '<|eot_id|>', ' parade', '?\n', ' Bridge', ' the', ' kitchen', ' Floral', 'Answer', 'office', 'Bridge', '<|end_header_id|>', 'assistant', '<|start_header_id|>', ' Bridge', '<|begin_of_text|>', ':'], 'evidence_proportions': [1.5656153361002605e-05, 0.000610506534576416, 0.000459750493367513, 0.0018790960311889648, 0.0012611250082651775]}}, 29: {'grad': {'score': [1.3111538357204862, 1.1780077264803035, 1.5435023452296401, 1.1767245310563044, 0.7608845769142618], 'topk_tokens': ['.', ' The', 'ants', 'ION', 'The', ' were', 'AILY', ' printed', ' Papers', 'ATING', '\n', ' Printing', ' Newspaper', ' NEW', '\n', ' the', '\n', 'paper', ' Press', 'APER'], 'evidence_proportions': [2.2290852864583335, 1.273095703125, 1.1612447102864583, 0.8460693359375, 0.8849029541015625]}, 'weight': {'score': [0.004398853690535934, 0.0024975785306067637, 0.0040164553757869835, 0.0024892630347286364, 0.001222942556653704], 'topk_tokens': [' the', ' discarded', ':', 'Question', 'Does', ' was', '.\n\n', ' Where', '<|eot_id|>', '<|eot_id|>', ' Does', '?\n', ' the', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002754330635070801, 0.0029265284538269043, 0.0026930272579193115, 0.00795745849609375, 0.009082635243733725]}, 'saliency': {'score': [0.0005143660086172598, 7.659808893828317e-05, 0.0005516915610342314, 7.434435412880045e-05, 0.000162912874805684], 'topk_tokens': ['IVE', ' discarded', ' was', ' the', ' Where', '.', '<|eot_id|>', ' the', 'assistant', ' to', 'inen', ' the', '<|end_header_id|>', '?\n', 'Does', 'office', '<|start_header_id|>', ' Does', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.122918764750163e-05, 0.00010416507720947265, 0.00041233499844868976, 0.0007541775703430176, 0.001291493574778239]}}, 30: {'grad': {'score': [0.9315135037457501, 1.2868391297865183, 1.021988868713379, 1.2883417812510305, 1.5716468655333227], 'topk_tokens': [' Paul', ' office', 'G', 'op', ' office', ' office', ' o', '3', ' office', ' offices', 'ob', ' o', ' office', 'op', ' o', ' o', 'office', ' o', ' o', ' O'], 'evidence_proportions': [1.4032910664876301, 0.72528076171875, 0.8432820638020834, 0.882080078125, 0.7527836163838705]}, 'weight': {'score': [0.008590974189617016, 0.0024562524880002525, 0.006440874302025997, 0.0024318995656327004, 0.0031787303029274456], 'topk_tokens': [' the', ' prior', ' bathroom', '.', '<|eot_id|>', ' Floral', 'Question', ':', '<|eot_id|>', ' Where', ' the', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009257793426513672, 0.006332767009735107, 0.0037387808163960776, 0.022722244262695312, 0.013569355010986328]}, 'saliency': {'score': [0.002305198598791052, 0.00016644061092995535, 0.0008237343845945416, 0.00015992990195502044, 0.0003183155643696688], 'topk_tokens': ['Mary', ' Mary', 'Question', '.', ' Where', ' the', '.\n\n', 'assistant', ' nearly', '<|begin_of_text|>', '?\n', ' artist', ' lounge', ' Mary', ' the', ' office', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.000536958376566569, 0.002640366554260254, 0.0009472171465555826, 0.007473468780517578, 0.0017066001892089844]}}, 31: {'grad': {'score': [1.2064118561921295, 1.634693304736563, 0.7993173310250947, 1.6379005013797217, 0.8916028081154337], 'topk_tokens': [' M', '186', '7', ' the', ' M', ' B', 'Mo', ' C', ' Mr', ' and', ' was', ' most', 'G', ' U', ' L', ' S', ' W', ' C', ' S', 'editary'], 'evidence_proportions': [0.7317708333333334, 1.29345703125, 1.4092203776041667, 2.15673828125, 0.77215576171875]}, 'weight': {'score': [0.0025665826267666286, 0.002313191851242634, 0.002520100636915727, 0.0023120714889062386, 0.0006197557157399703], 'topk_tokens': [' to', ' the', ' where', ' was', ',', ':', 'Question', ' the', '.\n\n', '<|eot_id|>', ' Where', 'Answer', '<|eot_id|>', '?\n', '<|start_header_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00044263402620951336, 0.0032611727714538575, 0.0009982089201609294, 0.006636381149291992, 0.0029668807983398438]}, 'saliency': {'score': [0.0010766320758395726, 0.00020191570487635704, 0.0003884922374378551, 0.00019947538296909238, 4.573014317726602e-05], 'topk_tokens': ['<|eot_id|>', ' the', ' the', ' open', 'Question', ' sample', ' Mary', 'light', ' hallway', ' Where', ' Mary', '<|eot_id|>', 'Answer', '?\n', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0005719165007273356, 0.0015616416931152344, 0.00041922926902770996, 0.0033251047134399414, 0.00033559401830037433]}}, 'pred_res': 'Mary dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:02:16.479 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:02:16.480 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-2_0-2-3-8-9.pkl | len: 10 |  size: 9.75 KB
Processing depth (0, 2, 3, 8, 9):   3%|▎         | 3/100 [00:57<30:54, 19.12s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 2, 3, 5, 8):   3%|▎         | 3/100 [01:04<30:54, 19.12s/it]2025-01-22 03:02:23.764 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:02:23.765 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:23.765 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:02:23.772 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2479, 2484) --> . Mary picked up the
2025-01-22 03:02:23.772 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:02:23.784 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3829, 3835) -->  war. Mary journeyed to
2025-01-22 03:02:23.784 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:02:23.800 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5991, 5995) -->  Mary dropped the apple
2025-01-22 03:02:23.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:02:23.828 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9676, 9682) -->  Daniel went back to the hallway
2025-01-22 03:02:23.828 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:02:23.839 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3607, 3613) -->  cold. Sandra journeyed to
2025-01-22 03:02:23.839 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:02:23.856 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (6051, 6057) -->  John went back to the bedroom
2025-01-22 03:02:23.856 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:02:23.856 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:23.857 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:02:23.884 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9677, 9683) -->  went back to the hallway.
2025-01-22 03:02:23.884 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:02:23.889 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1546, 1551) --> . John moved to the
2025-01-22 03:02:23.889 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John took the milk.
2025-01-22 03:02:23.917 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10709, 10713) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:02:26.018 | INFO     | test_jbb_retain:begin_test:632 - Mary dropped the apple.<|eot_id|>
2025-01-22 03:02:26.018 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12230])
your chose emoji: ['🛠', '🧎🏼\u200d➡', '🧑🏿\u200d❤\u200d🧑🏼', '\U0001fadc', '🏋🏻\u200d♀', '🦹\u200d♂️', '🔪', '🦹🏽\u200d♀', '🙎🏾\u200d♂', '⏹️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 200924.74it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.74s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.07it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.20it/s][A100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.20it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 21.04it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.93it/s][A100%|██████████| 8/8 [00:00<00:00, 22.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.90it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.45it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.60it/s][A100%|██████████| 8/8 [00:00<00:00, 21.78it/s]
2025-01-22 03:02:34.651 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7490621496129919, 0.7151276493010137, 0.7331600767193418, 0.7150034871573046, 1.1684858902640964], 'topk_tokens': [' offices', '800', 'enter', ' Or', ' office', ' office', ' office', 'door', 'Official', ' office', ' office', ' office', 'active', ' or', ' offices', ' office', 'office', ' office', ' office', ' office'], 'evidence_proportions': [1.2240397135416667, 0.5416748046875, 0.75653076171875, 0.8880615234375, 0.3467725118001302]}, 'weight': {'score': [0.03300604555341932, 0.0025683908855470665, 0.011476840033675686, 0.002476721850226594, 0.0011906321497930996], 'topk_tokens': ['\n\n', '?\n', '.', ' Bridge', ' THE', ' hallway', ' Mary', '<|start_header_id|>', '<|eot_id|>', 'Answer', 'assistant', '<|eot_id|>', 'Bridge', ':', ' Mary', ' bedroom', '<|end_header_id|>', ' bathroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0033686955769856772, 0.07881555557250977, 0.007909874121348063, 0.07049560546875, 0.024571935335795086]}, 'saliency': {'score': [0.008164831885585078, 0.00014781349620607027, 0.0038920099085027523, 0.00011987905830605188, 0.00014712499535602072], 'topk_tokens': [' bedroom', ' John', '<|start_header_id|>', ' bathroom', '<|eot_id|>', ' top', 'office', ' Mary', '<|end_header_id|>', ' office', ' office', '<|eot_id|>', 'Bridge', ' office', ' hallway', '<|begin_of_text|>', ' Mary', ' bedroom', 'office', ' office'], 'evidence_proportions': [0.004996250073115031, 0.015978574752807617, 0.0008105635643005371, 0.011242866516113281, 0.01012420654296875]}}, 25: {'grad': {'score': [1.2383490668402777, 0.9607591448148299, 1.0919466885653408, 0.9597877254229097, 0.6573698624320652], 'topk_tokens': [' subscribers', 'age', ' be', ' mentioned', 'erc', 'op', 'THE', 'op', ' at', ' circulated', 'itable', ' subscribers', ' Wood', 'ation', ' Tribune', ' rest', ' thirst', 'ised', 'ation', ' principles'], 'evidence_proportions': [1.4615478515625, 0.8683349609375, 1.2904866536458333, 0.8834228515625, 1.5079752604166667]}, 'weight': {'score': [0.020737067416862206, 0.0025362127976451846, 0.0031170303171331234, 0.0024942648800587598, 0.001057643821274025], 'topk_tokens': [' prior', '.\n\n', ' top', ' to', '?\n', ' apple', ' Daniel', 'Mary', ' Mary', '<|start_header_id|>', 'Answer', '<|eot_id|>', '<|eot_id|>', ' Mary', ' THE', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006490151087443033, 0.04135282039642334, 0.008013000090916952, 0.0673980712890625, 0.005262056986490886]}, 'saliency': {'score': [0.007468947657832393, 9.208356266009254e-05, 0.00023611747857296106, 7.532966438579277e-05, 7.321437199910481e-05], 'topk_tokens': [' April', 'Answer', ' Dan', ' counting', 'assistant', ' directly', ' Paul', ' top', ' Mary', '<|start_header_id|>', 'MIN', '<|eot_id|>', ' Daniel', ' apple', '<|eot_id|>', ' Mary', 'Mary', '<|end_header_id|>', '<|begin_of_text|>', ' Mary'], 'evidence_proportions': [0.000136643648147583, 0.01971014738082886, 0.002842793862024943, 0.021018028259277344, 0.00019368529319763184]}}, 26: {'grad': {'score': [0.9225780345775463, 0.6910396398790059, 0.927550575949929, 0.6898848257525516, 0.5533948151961617], 'topk_tokens': [' neighboring', ' steam', ' week', ' not', ' hand', 'leading', 'BO', 'graph', ' steam', 'str', ' some', ' Press', 'prob', ' searched', ' Col', ' Marshall', ' str', ' Jul', ' part', ' clear'], 'evidence_proportions': [1.0721028645833333, 0.6851806640625, 0.948211669921875, 1.0020751953125, 0.8922526041666666]}, 'weight': {'score': [0.010593743235976607, 0.0024897712285881965, 0.0047045368136781635, 0.0024657904112609327, 0.0007247371950011322], 'topk_tokens': [' the', ' apple', ' hallway', ' Mary', ' kitchen', ' Bridge', '<|eot_id|>', ' bedroom', '<|eot_id|>', ' the', 'Bridge', ' bathroom', '?\n', 'Answer', 'assistant', '<|start_header_id|>', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00046771764755249023, 0.018309688568115233, 0.0015108088652292888, 0.0214691162109375, 0.01612250010172526]}, 'saliency': {'score': [0.001670749099166305, 8.889091334520718e-05, 0.001297569636142615, 8.210512885050753e-05, 5.070368448893229e-05], 'topk_tokens': [' sale', ' Dan', ' Daily', ' Moore', ' bathroom', 'Answer', ' bedroom', ' old', ' apple', ' kitchen', '<|start_header_id|>', ' hallway', ' Bridge', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', ' the', ':', 'Bridge', 'office'], 'evidence_proportions': [4.621346791585287e-05, 0.001043999195098877, 8.578101793924968e-05, 0.0014849603176116943, 0.005526403586069743]}}, 27: {'grad': {'score': [0.6696799949363426, 0.680288830015533, 0.6024891246448864, 0.6805232885124415, 0.5271938158118207], 'topk_tokens': [' July', ' several', ' accepted', ' proportion', ' told', ' exchanged', 'sp', ' newspaper', ' EX', ' it', ' West', ' excursion', ' went', 'SP', ' interruption', ' Newspaper', ' opposition', ' excessive', ' N', 'bread'], 'evidence_proportions': [0.3842112223307292, 0.735467529296875, 0.6449788411458334, 0.649017333984375, 0.9388020833333334]}, 'weight': {'score': [0.01561075007473981, 0.0025390958068727903, 0.005731418277278091, 0.002501445354461043, 0.001177619332852571], 'topk_tokens': [' to', 'THE', '<|eot_id|>', '<|eot_id|>', 'Mary', 'NEW', ' Bridge', '?\n', ' Mary', 'Answer', ' bedroom', ' bathroom', 'assistant', '.\n\n', '<|start_header_id|>', ' THE', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0014566977818806965, 0.04243266582489014, 0.0038508077462514243, 0.033274173736572266, 0.0073975324630737305]}, 'saliency': {'score': [0.005791231437965676, 0.00015328157914862124, 0.0012031432354088986, 0.00013792912426490044, 9.197860524274301e-05], 'topk_tokens': [' top', 'LY', ' prior', '<|begin_of_text|>', ' old', '<|start_header_id|>', ' Dan', 'Answer', 'RE', ' Bridge', ' bedroom', ' Mary', ' Daniel', '<|end_header_id|>', 'Mary', '.\n\n', ' Mary', ':', ' THE', 'office'], 'evidence_proportions': [0.0007279813289642334, 0.018688809871673585, 0.0015384058157602947, 0.010149568319320679, 0.00145376722017924]}}, 28: {'grad': {'score': [0.99822998046875, 0.9562043014735938, 0.9229902787642046, 0.9562011277483672, 0.6302532251330389], 'topk_tokens': [' reflecting', ' kept', ' child', ' ', ' become', ' spoken', 'hom', ' acted', ' out', 'being', ' came', 'CH', ' ', 'na', ' ', 'been', '600', ' be', ' acted', ' ins'], 'evidence_proportions': [0.9893798828125, 1.142578125, 0.6643473307291666, 1.33868408203125, 0.9937032063802084]}, 'weight': {'score': [0.008528786676901358, 0.0024488772185820235, 0.004499361370549057, 0.002429831578392282, 0.0005379371021104896], 'topk_tokens': [' apple', ' was', '.\n\n', 'Question', ' to', ' discarded', ' the', ' the', ' Bridge', ' the', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002342661221822103, 0.0075801372528076175, 0.007583985726038615, 0.021909713745117188, 0.009638031323750814]}, 'saliency': {'score': [0.0008186499277750651, 0.000107525297829807, 0.0003922112060315681, 0.00010517605366457696, 3.53947929714037e-05], 'topk_tokens': [' Mary', ' the', ' office', ' bedroom', 'Question', '?\n', ' apple', 'Bridge', ' kitchen', ' Floral', ' the', ' bathroom', 'Answer', 'assistant', '<|end_header_id|>', 'office', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [5.147854487101237e-05, 0.0008637428283691407, 0.0009448528289794922, 0.0020735859870910645, 0.0005854169527689616]}}, 29: {'grad': {'score': [1.0587384259259258, 1.1016820953748365, 1.2566121419270833, 1.1013573161716568, 0.7573481020720109], 'topk_tokens': [' printed', ' The', ' mail', ' printer', 'mail', ' were', ' Papers', ' paper', ' printed', ' regular', ' Newspaper', ' Printing', 'aper', '\n', '\n', ' the', '\n', 'paper', 'APER', ' Press'], 'evidence_proportions': [1.8546346028645833, 0.939013671875, 0.9608357747395834, 0.7926025390625, 0.637939453125]}, 'weight': {'score': [0.004297121807380959, 0.0025115970539220257, 0.00363015586679632, 0.00250460382280403, 0.0005974674570387688], 'topk_tokens': ['Question', ' was', 'NEW', '.\n\n', 'Does', ' to', ' the', '<|eot_id|>', '<|eot_id|>', ' Where', '.', ' Does', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005545715490976969, 0.007971715927124024, 0.002850492795308431, 0.009837150573730469, 0.0027307868003845215]}, 'saliency': {'score': [0.0004527944105642813, 9.270074751113583e-05, 0.0003993438951896899, 9.107063308656902e-05, 4.2530937471251555e-05], 'topk_tokens': [' Bridge', ' the', '"The', '<|eot_id|>', ' Where', ' the', ' the', 'IVE', '.', ' to', 'Answer', '?\n', 'NEW', 'Does', '<|end_header_id|>', ' Does', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [7.466475168863933e-05, 0.0006465673446655274, 0.00026541948318481445, 0.001280665397644043, 0.00030490756034851074]}}, 30: {'grad': {'score': [0.9300209327980324, 1.2255522464130968, 1.1446473092743845, 1.2264271403001477, 1.3758483002151267], 'topk_tokens': [' o', ' office', 'op', ' office', 'office', ' office', ' office', ' an', ' offices', ' o', 'ob', ' o', 'op', ' office', ' o', ' o', 'office', ' o', ' o', ' O'], 'evidence_proportions': [1.389404296875, 0.65458984375, 0.6989695231119791, 0.8607177734375, 0.9774169921875]}, 'weight': {'score': [0.008686814043256972, 0.002465533617667079, 0.006787085171901818, 0.0024400172051809903, 0.0023805585460386415], 'topk_tokens': [',', ' the', ' the', '.', ':', ' the', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' Where', 'Question', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.001207272211710612, 0.00945967435836792, 0.004427552223205566, 0.020948410034179688, 0.011607170104980469]}, 'saliency': {'score': [0.0017420980665418836, 0.0001777336829779895, 0.0010314926956639145, 0.00017194894047266082, 0.00012033918629521909], 'topk_tokens': ['<|eot_id|>', ':', ' Daniel', ' Mary', 'NEW', ' Mary', ' Where', '?\n', ' lounge', ' the', '<|begin_of_text|>', 'office', ' bathroom', 'assistant', 'Question', '<|start_header_id|>', ' office', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [0.0007381141185760498, 0.002690136432647705, 0.0006089309851328532, 0.0039153993129730225, 0.0016403496265411377]}}, 31: {'grad': {'score': [1.3776403356481481, 1.9296003183248855, 1.0872506806344697, 1.9331084071825964, 0.9324282217716825], 'topk_tokens': [' C', ' most', ' M', ' L', ' their', ' were', '186', '7', ' and', ' B', 'G', ' C', ' S', ' its', ' was', ' U', ' S', ' W', 'editary', ' S'], 'evidence_proportions': [0.9510498046875, 1.4677734375, 1.6485595703125, 2.08056640625, 0.9895833333333334]}, 'weight': {'score': [0.0030482610066731772, 0.00230859275578361, 0.002915958563486735, 0.002305305365508534, 0.0006806582644365836], 'topk_tokens': [' to', ' where', ' was', ' the', ',', ':', '.\n\n', ' the', 'Question', '<|eot_id|>', ' Where', 'Answer', '<|eot_id|>', '?\n', '<|start_header_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0005232095718383789, 0.0030068159103393555, 0.001547396183013916, 0.00598907470703125, 0.005148172378540039]}, 'saliency': {'score': [0.0009282430013020834, 0.000181062148239505, 0.0005316860748059822, 0.00017845415673364043, 2.45527944703033e-05], 'topk_tokens': [' the', ' hallway', ' Mary', '.\n\n', ' open', 'light', ' Mary', ' the', '<|eot_id|>', ' Where', 'Question', '<|eot_id|>', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0007317264874776205, 0.001161956787109375, 0.0005275209744771322, 0.001590728759765625, 0.0008890628814697266]}}, 'pred_res': 'Mary dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:02:34.652 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:02:34.652 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-3_0-2-3-5-8.pkl | len: 10 |  size: 9.76 KB
Processing depth (0, 2, 3, 5, 8):   4%|▍         | 4/100 [01:15<29:59, 18.75s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 2, 4, 7, 8):   4%|▍         | 4/100 [01:22<29:59, 18.75s/it]2025-01-22 03:02:42.072 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:02:42.073 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:42.073 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:02:42.080 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2453, 2458) --> . Mary picked up the
2025-01-22 03:02:42.080 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:02:42.094 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4875, 4881) --> . Mary journeyed to the
2025-01-22 03:02:42.094 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:02:42.117 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8419, 8423) -->  Mary dropped the apple
2025-01-22 03:02:42.117 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:02:42.145 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9704, 9710) --> . Daniel went back to the
2025-01-22 03:02:42.145 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:02:42.155 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3536, 3542) -->  Sandra journeyed to the bedroom
2025-01-22 03:02:42.155 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:02:42.173 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (6038, 6044) --> . John went back to the
2025-01-22 03:02:42.173 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:02:42.173 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:02:42.173 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:02:42.200 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9704, 9710) --> . Daniel went back to the
2025-01-22 03:02:42.201 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:02:42.205 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1555, 1560) -->  ranks. John moved to
2025-01-22 03:02:42.205 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John took the milk.
2025-01-22 03:02:42.233 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10695, 10699) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:02:44.397 | INFO     | test_jbb_retain:begin_test:632 - Mary's bathroom<|eot_id|>
2025-01-22 03:02:44.398 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12212])
your chose emoji: ['🙇🏻', '⛹️\u200d♀', '🍊', '👲', '🪦', '🏄🏻\u200d♀️', '👯\u200d♂', '🧑🏾\u200d🚒', '💐', '☀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 138654.68it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.87s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.04it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.75it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.68it/s][A100%|██████████| 8/8 [00:05<00:00,  1.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.40it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.46it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.17it/s][A100%|██████████| 8/8 [00:00<00:00, 21.53it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.58it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.25it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.16it/s][A100%|██████████| 8/8 [00:00<00:00, 21.40it/s]
2025-01-22 03:02:53.111 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.8779703776041666, 0.8367683057096365, 0.7849269057765151, 0.836817533145595, 1.5503504136029411], 'topk_tokens': [' office', ' office', ' editor', 'eff', ' office', ' offices', 'Official', ' office', ' editor', ' or', ' office', ' effect', '�', ' office', 'enter', 'editor', 'active', ' office', ' office', 'office'], 'evidence_proportions': [1.4136555989583333, 0.5217041015625, 0.7235921223958334, 1.3818359375, 0.4576416015625]}, 'weight': {'score': [0.016467352708180744, 0.0025640384292415145, 0.00908070622068463, 0.0025154585771229544, 0.0012178596328286564], 'topk_tokens': [' front', ' Floral', ' Fort', ' bedroom', ' Bench', '<|eot_id|>', ' Mary', 'Answer', ' bedroom', '<|eot_id|>', 'assistant', 'Bridge', ':', '<|start_header_id|>', ' bathroom', ' hallway', ' Bridge', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003222227096557617, 0.011814284324645995, 0.009939869244893393, 0.06694412231445312, 0.006466339031855266]}, 'saliency': {'score': [0.005635577219503897, 0.00017178148416756842, 0.002527115922985655, 0.0001532486125750822, 0.00020037211623846315], 'topk_tokens': [' location', ' front', ' Bench', ' Mary', ' Bridge', '<|start_header_id|>', ' office', 'Mary', ' Market', '<|eot_id|>', 'Bridge', ' bedroom', 'office', ' bedroom', ' top', ' Mary', '<|begin_of_text|>', 'office', ' hallway', ' office'], 'evidence_proportions': [0.004645228385925293, 0.0049550414085388185, 0.0009738107522328695, 0.02303338050842285, 0.000256270170211792]}}, 25: {'grad': {'score': [1.4656575520833333, 1.262178567499386, 1.3727241284919507, 1.261426392405053, 0.8350195791207108], 'topk_tokens': [' good', 'ATION', ' grand', ' the', ' rest', ' excited', ' thirst', ' be', ' the', ' her', ' principal', 'ised', 'ished', ' OF', ' of', 'itable', ' IN', 'erc', ' principles', 'THE'], 'evidence_proportions': [1.936767578125, 0.972314453125, 1.7669270833333333, 0.999755859375, 1.4149983723958333]}, 'weight': {'score': [0.019779556327395968, 0.002537702526381739, 0.004406425085934726, 0.0024943260332854224, 0.0012556162534975539], 'topk_tokens': ['THE', ' the', ' Anthony', ' April', '.', ' to', ' the', 'Mary', 'Answer', ' top', '<|eot_id|>', ' Daniel', '<|eot_id|>', '<|start_header_id|>', ' Mary', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006987253824869791, 0.012802410125732421, 0.0077991485595703125, 0.0811920166015625, 0.015713443358739216]}, 'saliency': {'score': [0.004411317684032299, 0.0001205589682649469, 0.0003335927471970067, 0.00011044866712701072, 0.00011593337152518478], 'topk_tokens': ['count', 'MIN', ' April', ' counting', ' Merch', 'assistant', 'Answer', ':', ' Ramsey', ' top', '<|start_header_id|>', ' Mary', '<|eot_id|>', '<|eot_id|>', ' Daniel', ' Bench', 'Mary', '<|end_header_id|>', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.00013585885365804037, 0.0057448863983154295, 0.0015990932782491047, 0.018400460481643677, 0.0010615984598795574]}}, 26: {'grad': {'score': [1.0174393830475983, 0.9671069379912396, 1.2064264470880681, 0.9663453352417977, 0.7774777879901961], 'topk_tokens': [' steam', ' STR', ' part', ' well', 'BO', ' hand', 'graph', ' Married', ' several', ' some', ' Press', ' wh', ' neighboring', ' nearly', ' Col', ' Marshall', ' clear', ' Col', ' Jul', ' James'], 'evidence_proportions': [1.5362955729166667, 0.19256134033203126, 1.4686686197916667, 0.8028564453125, 0.8778076171875]}, 'weight': {'score': [0.008575152467798304, 0.002493190125121371, 0.006042364871863163, 0.00247004253997288, 0.0009624034750695322], 'topk_tokens': [' bedroom', ' Mary', ' Anthony', '<|eot_id|>', ' bathroom', '<|eot_id|>', 'Bridge', ' Floral', ' kitchen', '?\n', 'assistant', 'Answer', ' hallway', ' Bridge', ' the', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00033010045687357586, 0.004398560523986817, 0.003033717473347982, 0.031604766845703125, 0.010489056507746378]}, 'saliency': {'score': [0.0015112934289155183, 0.0001452229837748976, 0.001664302565834739, 0.00013806373346826836, 7.82924539902631e-05], 'topk_tokens': [' apple', 'Answer', ' the', ' Bench', ' Az', ' Mary', ' Seventh', ' old', ' old', ' bedroom', ' bedroom', ' kitchen', '<|end_header_id|>', ':', 'Bridge', ' hallway', ' the', ' Bridge', '<|begin_of_text|>', 'office'], 'evidence_proportions': [8.526444435119629e-05, 0.000515139102935791, 0.0006518065929412842, 0.0050830841064453125, 0.002245744069417318]}}, 27: {'grad': {'score': [0.8251501012731481, 0.8304941275124856, 0.774658203125, 0.8306576024354122, 0.7161057416130515], 'topk_tokens': [' talk', 'sp', ' newspaper', ' propriet', ' earnest', 'apers', 'sp', ' N', ' paper', ' news', ' paper', 'papers', ' Papers', ' paper', 'bread', 'SP', 'APER', ' newspaper', ' newspaper', ' Newspaper'], 'evidence_proportions': [1.0388997395833333, 0.4982421875, 0.846435546875, 0.7701416015625, 0.8992106119791666]}, 'weight': {'score': [0.011520692595729121, 0.0025504533721788904, 0.007905584393125591, 0.0025159860541990407, 0.0013107657432556152], 'topk_tokens': [' hallway', ' Floral', '?\n', ' bedroom', ' bathroom', ' Mary', ' bedroom', ' the', 'THE', 'Answer', ' THE', 'assistant', '.\n\n', ' Bridge', 'THE', ':', '<|end_header_id|>', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000966330369313558, 0.012619924545288087, 0.006169557571411133, 0.042578697204589844, 0.005804826815923055]}, 'saliency': {'score': [0.0009867990458453143, 0.00014677233365703688, 0.0010074377059936523, 0.0001425693816645906, 0.0001964896332983877], 'topk_tokens': [' office', ' top', ' hallway', ' Floral', ' Dan', ' bedroom', ' the', 'RE', '.', '.', ' prior', 'looking', ' bathroom', ' Daniel', ':', '<|start_header_id|>', '<|begin_of_text|>', ' Bridge', ' THE', 'office'], 'evidence_proportions': [0.0004938046137491862, 0.0007165908813476562, 0.0012060503164927165, 0.002781987190246582, 0.00028892358144124347]}}, 28: {'grad': {'score': [1.0819656937210649, 0.9118883811609628, 0.9464092832623106, 0.9114168263470359, 0.7790826535692402], 'topk_tokens': [' first', ' membership', ' executed', ' reflecting', ' kept', 'CH', ' came', 'hom', ' ', ' hands', 'hum', ' ', 'been', ' out', ' become', ' be', '600', ' acted', 'na', ' ins'], 'evidence_proportions': [0.8539632161458334, 1.30908203125, 1.0977681477864583, 1.1912841796875, 1.0320231119791667]}, 'weight': {'score': [0.011110129179777923, 0.0024452496808621896, 0.006318310896555583, 0.0024154847667114073, 0.0003629291758817785], 'topk_tokens': [' to', ' the', 'looking', ' the', ' Floral', ' Mary', ' to', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', ' Bridge', 'assistant', ' the', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0001934667428334554, 0.003152322769165039, 0.006186882654825847, 0.03132057189941406, 0.02010791500409444]}, 'saliency': {'score': [0.0017268503153765643, 0.00012884730302664815, 0.0007517265551017992, 0.00012360613989912406, 2.4448422824635226e-05], 'topk_tokens': [' front', ' the', 'office', ' kitchen', ' the', ' the', '<|eot_id|>', 'Answer', 'SSION', ' Daniel', ' Bridge', ' Mary', 'Bridge', ' Floral', 'assistant', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', '<|start_header_id|>', ':'], 'evidence_proportions': [4.4951836268107094e-05, 0.0006926178932189942, 0.0012769599755605061, 0.005608320236206055, 0.002132852872212728]}}, 29: {'grad': {'score': [1.4474577727141205, 1.390509705819142, 1.6838138464725378, 1.389586829856683, 0.9229615155388328], 'topk_tokens': [' months', 'aper', ' Papers', 'published', ' routes', 'ing', ' printed', ' Newspaper', 'The', 'ants', ' printed', 'mail', 'ION', ' mail', ' Press', '\n', ' the', 'ATING', 'paper', 'APER'], 'evidence_proportions': [2.2738444010416665, 1.3947021484375, 1.4212849934895833, 1.0787353515625, 0.9370218912760416]}, 'weight': {'score': [0.006862582983794036, 0.0025078405115087896, 0.0049403382070136795, 0.002491561881370287, 0.000706794215183632], 'topk_tokens': [' place', 'THE', ' the', ' Where', 'Does', '<|eot_id|>', '<|eot_id|>', '?\n', ' where', ' in', ' Does', 'Answer', ' was', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0004209280014038086, 0.002098405361175537, 0.009099006652832031, 0.011392593383789062, 0.012017955382664999]}, 'saliency': {'score': [0.0006734618434199581, 0.000131820859389042, 0.0004348158836364746, 0.00012979493028183447, 8.410565993365119e-05], 'topk_tokens': [' Mary', ' the', ' Bridge', '.', 'assistant', ' the', 'IVE', '<|eot_id|>', '?\n', ' the', ' where', ' in', 'Does', 'office', ' was', '<|end_header_id|>', ' Does', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.251169204711914e-05, 0.00016936063766479493, 0.0013103087743123372, 0.0018783807754516602, 0.00027437011400858563]}}, 30: {'grad': {'score': [0.8706297697844328, 1.095015757404413, 0.8182423909505209, 1.096265711556027, 1.2208970013786764], 'topk_tokens': [' Paul', ' office', ' office', ' an', ' office', ' o', ' offices', 'op', ' o', ' o', 'ob', ' office', ' office', ' o', 'op', ' o', 'office', ' o', ' o', ' O'], 'evidence_proportions': [1.165008544921875, 0.94052734375, 0.8792317708333334, 0.84466552734375, 0.5267105102539062]}, 'weight': {'score': [0.015385682936067934, 0.0024367823272050454, 0.008506052421801018, 0.002391537450657327, 0.002200799829819623], 'topk_tokens': [' Anthony', ' Daniel', '<|eot_id|>', ' the', ' Where', ' bathroom', ' Floral', '.', ' the', ' Mary', '.\n\n', ' the', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0011148452758789062, 0.004757201671600342, 0.010567982991536459, 0.04594230651855469, 0.022960205872853596]}, 'saliency': {'score': [0.0035373237397935656, 0.0002602930445317377, 0.0010784737991564202, 0.0002507916627911845, 0.00016123407027300666], 'topk_tokens': [' Geo', '?\n', ' location', ' the', ' nearly', ' Where', ' Congress', ' the', ' the', '<|begin_of_text|>', 'assistant', ' the', ' Daniel', 'office', ' Mary', '<|end_header_id|>', ' office', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.000798642635345459, 0.0016251683235168457, 0.0022468666235605874, 0.012882709503173828, 0.0029296676317850747]}}, 31: {'grad': {'score': [1.1313205295138888, 1.5849179220566563, 0.9013903068773674, 1.5877814682883142, 0.6375603769339767], 'topk_tokens': [' its', ' the', ' M', ' and', ' were', ' M', ' H', ' was', 'G', '7', ' B', ' S', ' C', ' most', ' U', ' C', ' L', ' S', 'editary', ' W'], 'evidence_proportions': [0.89862060546875, 1.201904296875, 0.9701131184895834, 1.798828125, 1.0214029947916667]}, 'weight': {'score': [0.002918333918960006, 0.002316826744947844, 0.0030883659016002307, 0.0023133956534662037, 0.0007405818677416034], 'topk_tokens': [' to', ' where', ' the', ' was', ',', ':', 'Question', '.\n\n', '<|eot_id|>', ' Where', ' the', 'Answer', '<|eot_id|>', '?\n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007129112879435221, 0.0034194111824035643, 0.0013294219970703125, 0.008040904998779297, 0.002880056699117025]}, 'saliency': {'score': [0.0010975820046884041, 0.0001886906982229485, 0.0004810799251903187, 0.00018587772226733793, 4.836274128334195e-05], 'topk_tokens': ['Mary', ' Mary', ' Ramsey', ' open', ' Miles', 'light', ' Where', ' the', ' Ramsey', ' Market', ' Mary', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '?\n', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0008191267649332682, 0.0013341426849365235, 0.0004242261250813802, 0.003030627965927124, 0.0005635619163513184]}}, 'pred_res': "Mary's bathroom<|eot_id|>", 'score': 0}
2025-01-22 03:02:53.112 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:02:53.113 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-4_0-2-4-7-8.pkl | len: 10 |  size: 9.72 KB
Processing depth (0, 2, 4, 7, 8):   5%|▌         | 5/100 [01:33<29:31, 18.64s/it]Processing depth (0, 2, 4, 7, 8):   5%|▌         | 5/100 [01:34<29:55, 18.90s/it]
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:72 - Selected idx: 4
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:76 - Needle: [' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 03:02:53.961 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (0, 1, 6, 7):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:03:01.003 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:03:01.004 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:03:01.004 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:03:01.008 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1537, 1542) --> . Daniel went to the
2025-01-22 03:03:01.009 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:03:01.029 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7253, 7257) -->  Daniel left the apple
2025-01-22 03:03:01.029 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:03:01.054 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8443, 8449) -->  affair. Mary journeyed to
2025-01-22 03:03:01.054 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:03:01.079 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8797, 8802) --> . Mary got the football
2025-01-22 03:03:01.079 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:03:01.087 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3023, 3028) -->  Mary moved to the bathroom
2025-01-22 03:03:01.087 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:03:01.098 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3484, 3490) --> . John went back to the
2025-01-22 03:03:01.098 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:03:01.115 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6178, 6184) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:03:03.278 | INFO     | test_jbb_retain:begin_test:632 - The apple was left in the apple.<|eot_id|>
2025-01-22 03:03:03.278 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12205])
your chose emoji: ['👨🏻\u200d⚕', '👯', '🏋️\u200d♀️', '🚵🏿\u200d♀', '🏊🏻\u200d♂', '💇🏾\u200d♂', '🇹🇰', '👩🏼\u200d🦯', '🏄', '👩🏿\u200d✈']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 236298.82it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:31,  4.49s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.20s/it][A
 62%|██████▎   | 5/8 [00:04<00:01,  1.63it/s][A
 88%|████████▊ | 7/8 [00:04<00:00,  2.62it/s][A100%|██████████| 8/8 [00:04<00:00,  1.63it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.94it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.85it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.25it/s][A100%|██████████| 8/8 [00:00<00:00, 17.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.87it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 22.61it/s][A100%|██████████| 8/8 [00:00<00:00, 22.80it/s]
2025-01-22 03:03:12.468 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2061495099748884, 0.20533427967156526, 0.20777875726873224, 0.20532845161209076, 0.28666713139782213], 'topk_tokens': [' turtle', 'est', 'itter', 'MIN', 'Mer', ' and', ' absence', 'adv', ' emb', 'vent', ' STE', 'remark', ' Merch', 'adj', ' compl', ' Arr', 'ols', 'deal', ' Do', 'consider'], 'evidence_proportions': [0.28873697916666663, 0.1445159912109375, 0.06793403625488281, 0.26706695556640625]}, 'weight': {'score': [0.033353084609622045, 0.0025772060198052684, 0.02246477116237987, 0.002488112728927932, 0.000422745943069458], 'topk_tokens': [' the', ' bedroom', ' garden', ' bedroom', '<|start_header_id|>', ' apple', ':', '<|eot_id|>', ' Bridge', 'assistant', ' bathroom', 'b', ' Bridge', '\n\n', '<|eot_id|>', ' the', 'Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0169370969136556, 0.008013206720352174, 0.1362009048461914, 0.002320423722267151]}, 'saliency': {'score': [0.001499448503766741, 4.834928311807925e-05, 0.0006204586137424816, 4.480966216391906e-05, 8.811689402959118e-06], 'topk_tokens': [' Press', ' office', ':', ' Daniel', '<|eot_id|>', ' Bench', ' Daniel', ' bathroom', 'b', ' the', ' the', ' garden', ' apple', ' bedroom', ' bedroom', '<|begin_of_text|>', ' Bridge', 'athroom', ' Bridge', 'Bridge'], 'evidence_proportions': [0.0004922598600387573, 0.001027899980545044, 0.005772814154624939, 5.0683816274007164e-05]}}, 25: {'grad': {'score': [0.47542898995535715, 0.4578785827556725, 0.44879826632412995, 0.45786470757361747, 0.3134335557075396], 'topk_tokens': [' a', ' at', ' appropriated', ' for', ' self', ' grand', ' bogus', ' the', 'ivery', ' inverted', 'antic', ' black', ' a', ' a', ' for', ' a', ' no', ' at', ' free', ' of'], 'evidence_proportions': [0.544677734375, 0.48230590820312497, 0.5045433044433594, 0.38103993733723956]}, 'weight': {'score': [0.02061287562052409, 0.002499194163951149, 0.009666192260655489, 0.0024549638911426385, 0.0005389950046800588], 'topk_tokens': [' the', ' Daniel', ' Daniel', ' bathroom', '.\n\n', 'b', 'Answer', '<|start_header_id|>', ' \n', ' the', ' Bench', ' apple', '<|eot_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004154284795125325, 0.01362593173980713, 0.07847142219543457, 0.004321555296579996]}, 'saliency': {'score': [0.0018852282138097855, 2.7381298699822683e-05, 0.00024078515442934903, 2.378823087874921e-05, 1.2832145168356699e-05], 'topk_tokens': [' Daniel', 'Dub', ' there', ':', ' Anthony', ' Daniel', 'b', ' St', ' apple', 'Den', 'assistant', ' bathroom', ' Dan', ' Bench', '<|eot_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [0.0001456141471862793, 0.00041570663452148435, 0.009109392762184143, 3.333389759063721e-05]}}, 26: {'grad': {'score': [0.36743091401599703, 0.3869368026953381, 0.3486175970597701, 0.3870397740217868, 0.41679643604853384], 'topk_tokens': [' Eagle', ' Bench', ' favor', 'hue', ' vastly', ' satisf', ' compl', 'ree', ' Milwaukee', ' bend', 'b', ' when', 'issippi', 'b', 'b', 'b', 'RI', ' Press', ' bitter', 'itter'], 'evidence_proportions': [0.3156331380208333, 0.42351074218749996, 0.3157691955566406, 0.4069366455078125]}, 'weight': {'score': [0.026300583566938127, 0.0024628145510372295, 0.02218319069255482, 0.0023860006238323544, 0.0005631385600730164], 'topk_tokens': ['Bridge', ' garden', ' Bridge', ' apple', '<|eot_id|>', ' the', '<|eot_id|>', 'Answer', 'b', ' \n', 'assistant', ' apple', ' bathroom', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ' the', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01235694686571757, 0.006836014986038208, 0.10624217987060547, 0.003170296549797058]}, 'saliency': {'score': [0.0015369909150259836, 3.636907175827776e-05, 0.00043301284313201904, 3.30612853481796e-05, 1.77287075617542e-05], 'topk_tokens': [' Anthony', ' apple', ' garden', ' Bridge', ' Daniel', ' the', ' bathroom', '<|start_header_id|>', 'assistant', 'Bridge', ' Daniel', ' Bridge', ' apple', ' the', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.00020222365856170654, 0.0007107019424438477, 0.0067369937896728516, 9.366373221079509e-05]}}, 27: {'grad': {'score': [0.15323402768089658, 0.2431724318358095, 0.17090329256924716, 0.2434583856008007, 0.20081274476769853], 'topk_tokens': ['sylvania', '-n', ' conventions', ' was', ' designated', ' would', ' convention', ' be', 'ARCH', ' sentinel', 'lin', 'str', 'ly', ' Franklin', ' was', ' would', ' accepted', 'Republicans', ' STR', ' step'], 'evidence_proportions': [0.12583541870117188, 0.14003677368164064, 0.20346832275390625, 0.15814081827799478]}, 'weight': {'score': [0.03224457303682963, 0.0025268543125917526, 0.0264369628646157, 0.0024323130481977108, 0.0007880852647023658], 'topk_tokens': [' Bridge', ' bathroom', ' Daniel', ' garden', ' apple', ' \n', 'Answer', ' the', ' the', ' apple', 'assistant', '<|start_header_id|>', '.\n\n', 'b', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.019444366296132408, 0.014590513706207276, 0.11858606338500977, 0.0021955023209253945]}, 'saliency': {'score': [0.0026420212927318757, 3.883612206691527e-05, 0.0010116046125238593, 3.2583117926017865e-05, 3.2961368560791016e-05], 'topk_tokens': ['<|start_header_id|>', ' the', ' the', 'Dub', ' the', ' \n', ' the', 'assistant', ' Daniel', ' Bridge', 'NEW', ' bathroom', '<|end_header_id|>', ':', ' apple', '<|begin_of_text|>', ' the', 'athroom', 'b', '.\n\n'], 'evidence_proportions': [0.00038273632526397705, 0.001502758264541626, 0.011310189962387085, 7.191300392150879e-05]}}, 28: {'grad': {'score': [0.22218540736607142, 0.23714685002598193, 0.23214314200661398, 0.23718172646431193, 0.27413214069523223], 'topk_tokens': [' inside', ' returns', 'about', ' became', ' lie', ' Cedar', 'ien', ' balance', 'nes', 'nes', 'arp', ' a', 'dent', 'half', 'S', ' RID', '.', 'ot', ' half', 'ball'], 'evidence_proportions': [0.24529520670572916, 0.20560150146484377, 0.17264270782470703, 0.2459239959716797]}, 'weight': {'score': [0.013285645416804723, 0.002376954958604485, 0.019357570193030617, 0.002327414799559719, 0.0003855440714587904], 'topk_tokens': [' the', 'Answer', '<|eot_id|>', ' \n', ' Bridge', '?', ' the', ' bathroom', 'assistant', ' garden', ' before', ' apple', 'b', '<|start_header_id|>', '<|end_header_id|>', ' the', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004918570319811503, 0.004010647535324097, 0.05076932907104492, 0.004392762978871664]}, 'saliency': {'score': [0.00030140791620527, 3.0081963281331493e-05, 0.00038051469759507614, 2.897983708599199e-05, 5.77062776643936e-06], 'topk_tokens': [' near', '?', ' the', 'Bridge', ' bathroom', 'athroom', ' before', 'b', ' apple', ' the', 'assistant', '<|start_header_id|>', ' Bridge', ' garden', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', '\n\n', ':', ' the'], 'evidence_proportions': [0.00016631682713826496, 0.00020595192909240724, 0.0009162724018096924, 0.00010613600413004556]}}, 29: {'grad': {'score': [0.19960721333821616, 0.25048505150631245, 0.1788886460390958, 0.2507023598106124, 0.3718641620792755], 'topk_tokens': ['Engine', 't', ' M', ' faithfully', ' Get', ' Dr', ' a', ' THE', 'goods', '\u200d', ' not', ' enough', ' Gen', 'cret', 'Emp', '\u200d', ' Far', 'ys', 'tal', ' ga'], 'evidence_proportions': [0.2277984619140625, 0.13818092346191407, 0.17133522033691406, 0.2414525349934896]}, 'weight': {'score': [0.006475255602882022, 0.0024456922379889933, 0.0058409910310398445, 0.00243259586280528, 0.000491648504178818], 'topk_tokens': [' Does', ' Where', ' was', ' the', 'Answer', ' \n', ' apple', '<|eot_id|>', '?', '.\n\n', ' before', 'b', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.002145295341809591, 0.005581903457641602, 0.021161198616027832, 0.001759047309557597]}, 'saliency': {'score': [0.00019457084792000906, 1.689415000291202e-05, 0.0001321909102526578, 1.6378922762323984e-05, 2.5400560196131878e-05], 'topk_tokens': ['Does', ':', ' garden', ' Where', 'Answer', ' before', 'NEW', ' in', '<|eot_id|>', ' the', 'b', ' Does', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', 'assistant', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00010814766089121501, 0.00021407604217529296, 0.00047697871923446655, 7.646779219309488e-05]}}, 30: {'grad': {'score': [0.2812518165225074, 0.2733776372507078, 0.2335232821377841, 0.27343611962208275, 0.27088869107912666], 'topk_tokens': [' THAT', 'IT', ' OCC', ' Million', 'Emp', ' its', ' C', 'b', ' Times', 'deal', ' forb', '2', ' United', 'AM', 'SSION', ' Times', 'ire', ' account', ' LINE', 'b'], 'evidence_proportions': [0.2107518513997396, 0.2365081787109375, 0.37213706970214844, 0.3284479777018229]}, 'weight': {'score': [0.01748585275241307, 0.002457059460886184, 0.019356028600172562, 0.0024005545714339553, 0.001765606746281663], 'topk_tokens': [' apple', ' the', '<|eot_id|>', '<|eot_id|>', 'Question', ' garden', '?', ' bathroom', '.\n\n', 'Answer', 'assistant', 'b', ' \n', ' the', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005121707916259766, 0.006737172603607178, 0.0648660659790039, 0.007220422228177388]}, 'saliency': {'score': [0.0015323914232708159, 6.37112256154164e-05, 0.002212859012863853, 5.72892333860511e-05, 4.0723444664315e-05], 'topk_tokens': [' \n', ' the', 'Bridge', ' bathroom', '.', ' the', ' Daniel', 'assistant', ' the', ' the', ' Bridge', ' apple', '<|start_header_id|>', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', ' bathroom', 'b', ':', 'athroom'], 'evidence_proportions': [0.000810931126276652, 0.0007044196128845215, 0.005035027861595154, 0.0006087372700373331]}}, 31: {'grad': {'score': [0.203973270597912, 0.20192383532442865, 0.20333732258189807, 0.20191774121342104, 0.09586479280092945], 'topk_tokens': [' the', ' the', '      ', ' was', ' apple', ' location', 'did', 'membership', ' the', 'nes', ' could', ' the', ' location', ' location', 'nes', ' August', ' the', ' population', ' location', ' department'], 'evidence_proportions': [0.15092150370279947, 0.3238637208938599, 0.24932551383972168, 0.12688150008519491]}, 'weight': {'score': [0.003729454108646938, 0.0022646222327046125, 0.004483768885785883, 0.002258080293061162, 0.0008875727653503418], 'topk_tokens': ['Question', ':', ' before', '.\n\n', ' the', '?', ' Where', '<|eot_id|>', 'Answer', ' the', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'b', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011252164840698242, 0.0019727706909179687, 0.007309317588806152, 0.005411018927892049]}, 'saliency': {'score': [0.0001885692278544108, 1.5695329125711834e-05, 6.143884225325151e-05, 1.531417752997747e-05, 7.567340380524936e-06], 'topk_tokens': ['?', ' Daniel', '<|eot_id|>', ' Where', ' Daniel', 'Question', ' garden', ' Bridge', ' the', ' apple', 'Answer', ' \n', ':', ' the', 'athroom', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'b', 'assistant'], 'evidence_proportions': [1.5338261922200523e-05, 0.00022417306900024414, 0.0006322786211967468, 3.632406393686931e-05]}}, 'pred_res': 'The apple was left in the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:03:12.469 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:03:12.469 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-0_0-1-6-7.pkl | len: 10 |  size: 9.14 KB
Processing depth (0, 1, 6, 7):   1%|          | 1/100 [00:18<30:23, 18.42s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 3, 4, 9):   1%|          | 1/100 [00:25<30:23, 18.42s/it]2025-01-22 03:03:19.918 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:03:19.919 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:03:19.919 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:03:19.930 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3837, 3842) --> . Daniel went to the
2025-01-22 03:03:19.930 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:03:19.943 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4955, 4959) -->  Daniel left the apple
2025-01-22 03:03:19.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:03:19.975 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10767, 10773) --> . Mary journeyed to the
2025-01-22 03:03:19.975 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:03:20.000 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8734, 8739) --> . Mary got the football
2025-01-22 03:03:20.000 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:03:20.008 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3013, 3018) -->  Mary moved to the bathroom
2025-01-22 03:03:20.008 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:03:20.018 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3474, 3480) --> . John went back to the
2025-01-22 03:03:20.018 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:03:20.036 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6188, 6194) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:03:22.101 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:03:22.101 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12214])
your chose emoji: ['🏃🏽\u200d♀\u200d➡️', '🐻\u200d❄', '😟', '♉', '🧑🏿\u200d🤝\u200d🧑🏾', '🧗🏼', '🇸🇪', '💂🏾\u200d♀️', '👩🏽\u200d🤝\u200d👨🏼', '👨🏿']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 219310.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.46s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.07s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.93it/s][A100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.63it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.69it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.52it/s][A100%|██████████| 8/8 [00:00<00:00, 20.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.44it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.32it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.68it/s][A100%|██████████| 8/8 [00:00<00:00, 16.54it/s]
2025-01-22 03:03:31.708 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.25647553943452384, 0.23144446818949774, 0.25272165645252576, 0.23136283925587425, 0.285672211065525], 'topk_tokens': [' appearance', ' first', ' Do', ' Cl', ' hundred', ' or', 'oggle', 'est', 'yster', 'remark', 'itter', ' malignant', ' Arr', 'adj', ' absence', ' turtle', ' STE', ' compl', 'ols', 'consider'], 'evidence_proportions': [0.3021647135416667, 0.244805908203125, 0.14630126953125, 0.2939605712890625]}, 'weight': {'score': [0.018578972135271345, 0.002577736167536143, 0.023921986872499638, 0.0025115623979590418, 0.000362278484716648], 'topk_tokens': ['bed', ':', ' the', ' the', ' apple', ' top', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'Bridge', '\n\n', ' lounge', 'b', '<|eot_id|>', ' bathroom', ' bedroom', ' Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.015956759452819824, 0.015286636352539063, 0.05179619789123535, 0.0017999807993570962]}, 'saliency': {'score': [0.0005509455998738607, 3.570129167451068e-05, 0.0007429664785211736, 3.353438149028093e-05, 5.383680506450374e-06], 'topk_tokens': [' Father', ',', ' Press', '<|eot_id|>', ' the', ' bathroom', ' bedroom', ':', 'bed', ' garden', ' the', ' bathroom', ' top', ' lounge', '<|begin_of_text|>', 'b', 'Bridge', ' Bridge', ' bedroom', 'athroom'], 'evidence_proportions': [0.0008030285437901815, 0.00017921924591064451, 0.001424551010131836, 2.6231010754903156e-05]}}, 25: {'grad': {'score': [0.4422916230701265, 0.5146460394583368, 0.38848423957824707, 0.5149988407020951, 0.29394653948341926], 'topk_tokens': [' bogus', ' at', ' a', ' grand', ' self', ' as', ' inverted', ' with', ' for', ' the', ' black', ' a', ' a', ' for', 'posit', ' a', ' free', ' for', ' no', ' of'], 'evidence_proportions': [0.5147603352864584, 0.43487396240234377, 0.5072517395019531, 0.3326975504557292]}, 'weight': {'score': [0.01892188759077163, 0.0025028540380283844, 0.008424108678644354, 0.002463830930857269, 0.0004257087300463421], 'topk_tokens': ['door', ' Press', ' Daniel', ' Bench', ' \n', ' bathroom', ' Dan', 'Answer', 'b', '<|start_header_id|>', ' the', ':', ' apple', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.003966102997461955, 0.013162082433700562, 0.07200503349304199, 0.003288745880126953]}, 'saliency': {'score': [0.0022451267355964298, 2.5755600247395218e-05, 0.00020338594913482666, 2.1606211261207177e-05, 7.996835359712926e-06], 'topk_tokens': [' Press', ' Daniel', 'Daniel', ' Bridge', ' Met', ',', ' boat', ' bathroom', 'door', 'b', ' Bench', ' top', ' Dan', '<|eot_id|>', '<|eot_id|>', '\n\n', 'athroom', '<|end_header_id|>', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [0.000109975536664327, 0.0003826618194580078, 0.011108115315437317, 2.3672978083292644e-05]}}, 26: {'grad': {'score': [0.23138900030226933, 0.29248138799930423, 0.23281357505104758, 0.2926945991071159, 0.3093432449712986], 'topk_tokens': ['!', 'ire', ' Clean', ' West', 'ente', 'ike', ' when', 'rich', 'UX', ' Empire', ',', 'UX', 'hue', ' Milwaukee', ' and', ' and', 'issippi', 'ub', ' Eagle', ' Press'], 'evidence_proportions': [0.17241160074869794, 0.3478515625, 0.2004985809326172, 0.21390787760416669]}, 'weight': {'score': [0.01563181337856111, 0.0024855224104787066, 0.019297687844796615, 0.0024324634528735873, 0.0005005376368034176], 'topk_tokens': [' apple', ' barric', ' Bridge', ' apple', ' the', ' bedroom', 'Answer', '<|eot_id|>', ' \n', 'b', '<|eot_id|>', 'assistant', ' bathroom', '<|start_header_id|>', '\n\n', ' the', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.011939004063606262, 0.008863317966461181, 0.0455782413482666, 0.005000750223795573]}, 'saliency': {'score': [0.0004383453301021031, 2.228496770787729e-05, 0.00021417574449019, 2.12204971394947e-05, 1.4556253828653475e-05], 'topk_tokens': [' Saturday', '<|eot_id|>', ' bathroom', ' apple', ' the', '<|eot_id|>', ' Bridge', ' Father', '<|start_header_id|>', 'assistant', 'Bridge', ' apple', ' bedroom', 'athroom', ' the', '\n\n', '<|end_header_id|>', ':', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0001068363587061564, 0.00011981725692749024, 0.0016312599182128906, 0.00024001797040303546]}}, 27: {'grad': {'score': [0.21439135642278762, 0.2240512707702382, 0.22001890702681107, 0.22407522100875074, 0.1905435440016956], 'topk_tokens': ['185', ' STR', ' intended', ' Thanksgiving', ' platform', 'ers', ' concerned', 'reader', ' dre', ' restrictions', ' short', 'direction', 'ided', 'event', ' staff', 'str', 'successful', 'Republicans', ' accepted', ' step'], 'evidence_proportions': [0.18519020080566406, 0.261822509765625, 0.22177749872207642, 0.1991424560546875]}, 'weight': {'score': [0.02575765195347014, 0.0025305084855503602, 0.023911637338725002, 0.0024518034709622087, 0.0005964284263006071], 'topk_tokens': [' bathroom', ' garden', ' apple', ' \n', 'Answer', '<|start_header_id|>', ' the', 'assistant', ' Bridge', ' apple', ' bedroom', '.\n\n', 'b', ' bathroom', ' lounge', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.019318605462710064, 0.017493784427642822, 0.08064079284667969, 0.002494494120279948]}, 'saliency': {'score': [0.0017982451688675653, 4.1211947702372086e-05, 0.0010191540826450694, 3.6413818524352666e-05, 1.869768631167528e-05], 'topk_tokens': [' apple', ' top', '\n\n', ' Daniel', ' the', ' Daniel', 'assistant', ' lounge', 'NEW', ' bathroom', ' the', '<|end_header_id|>', ' apple', '<|begin_of_text|>', ' bedroom', ' Bridge', ':', 'athroom', '.\n\n', 'b'], 'evidence_proportions': [0.00043884913126627606, 0.0013705670833587646, 0.007034808397293091, 2.2997458775838215e-05]}}, 28: {'grad': {'score': [0.2116793677920387, 0.27651910343936725, 0.19304067438299005, 0.2767818075536957, 0.3062603764417695], 'topk_tokens': ['nes', '\n', ' a', ' half', '\n', '185', "'", 'ot', 'ien', 'IO', 'E', 'nes', 'arp', '.', 'S', ' ', '.', ' RID', 'dent', ' Cedar'], 'evidence_proportions': [0.309814453125, 0.16090087890625, 0.10530328750610352, 0.2267770767211914]}, 'weight': {'score': [0.011480654988970076, 0.002399380960157697, 0.01899511841210452, 0.00235372522017512, 0.0003648167703209854], 'topk_tokens': [' the', 'Answer', ' Bridge', '<|eot_id|>', '?', ' before', ' the', ' garden', ' bathroom', '<|eot_id|>', ' apple', 'assistant', '<|start_header_id|>', 'b', '<|end_header_id|>', ' the', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005803152918815613, 0.010684770345687865, 0.01596987247467041, 0.014828582604726156]}, 'saliency': {'score': [0.00016921332904270718, 2.423884328633115e-05, 0.0003404725681651722, 2.3417288649546332e-05, 4.51650561355963e-06], 'topk_tokens': [' soon', ' garden', ' the', '?', ' before', 'b', '<|start_header_id|>', ' Bridge', ' the', 'Bridge', 'athroom', ' garden', ':', '<|end_header_id|>', ' apple', 'assistant', '\n\n', ' the', ' Bridge', '<|begin_of_text|>'], 'evidence_proportions': [9.206930796305338e-05, 0.000310903787612915, 0.00025391578674316406, 7.181366284688313e-05]}}, 29: {'grad': {'score': [0.24083964029947916, 0.3014762324495324, 0.24273681640625, 0.30168697958178997, 0.35571633606422237], 'topk_tokens': [' foreign', ' be', 'ib', 'Emp', ' face', 'adv', 'incre', ' arms', 'eff', 'pend', ' not', 'sim', ' facilities', 'goods', 'ys', ' enough', ' faithfully', 'tal', 'cret', ' ga'], 'evidence_proportions': [0.21569315592447916, 0.26763916015625, 0.20145225524902344, 0.2699114481608073]}, 'weight': {'score': [0.010006935823531378, 0.0024722045363874045, 0.00654501806605946, 0.0024518471144486164, 0.00038980565420011196], 'topk_tokens': [' the', ' Where', ' \n', '<|eot_id|>', 'Answer', ' apple', ' the', '?', '<|eot_id|>', '.\n\n', ' before', ' the', 'b', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006608595450719198, 0.005816555023193359, 0.029149413108825684, 0.004135608673095703]}, 'saliency': {'score': [0.0002830298173995245, 2.6205367227032715e-05, 0.00017346170815554532, 2.549623687102404e-05, 1.3339446812141233e-05], 'topk_tokens': [' Bridge', ' garden', 'Answer', ' Where', ' soon', '<|eot_id|>', ' Does', 'NEW', ' the', '      ', ':', '      ', '<|eot_id|>', '<|end_header_id|>', 'assistant', 'b', 'athroom', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.830692291259766e-05, 0.00015659332275390624, 0.0010739639401435852, 7.582704226175944e-05]}}, 30: {'grad': {'score': [0.17744645618257068, 0.2653852033275546, 0.20579944957386365, 0.26564457578300277, 0.27020273848277765], 'topk_tokens': ['\n', ' C', ' United', ' Loan', ' THAT', '-the', ' itself', ' Times', ' OF', ' Million', ' account', ' Times', 'b', 'AM', 'SSION', ' LINE', 'ire', 'deal', 'b', 'b'], 'evidence_proportions': [0.21297200520833334, 0.13186721801757814, 0.2681732177734375, 0.11941909790039062]}, 'weight': {'score': [0.016058667784645445, 0.0024683671232974304, 0.017977367747913708, 0.0024168972425984104, 0.0014392104817599785], 'topk_tokens': [' before', 'Question', '.', ' garden', '<|eot_id|>', '<|eot_id|>', ' bathroom', '.\n\n', '?', 'Answer', 'b', 'assistant', ' \n', ' the', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004645938674608867, 0.01867462396621704, 0.04475688934326172, 0.00615928570429484]}, 'saliency': {'score': [0.0013389459678104945, 5.9146569151237085e-05, 0.002984037453478033, 5.1653273042560073e-05, 3.739064786492324e-05], 'topk_tokens': [' Bridge', ' Daniel', ' bedroom', ' bedroom', 'Bridge', ' the', ' the', 'assistant', '.', ' bathroom', '<|start_header_id|>', ' apple', ' the', '<|end_header_id|>', '<|begin_of_text|>', ' Bridge', ' bathroom', 'athroom', ':', 'b'], 'evidence_proportions': [0.0011819551388422649, 0.0011774539947509766, 0.0035634636878967285, 0.00014750162760416666]}}, 31: {'grad': {'score': [0.19223992029825845, 0.19111908419858006, 0.1924363930117, 0.19111477022190995, 0.11135425545820375], 'topk_tokens': [' the', ' the', ' having', ' location', ' location', ' the', ' had', ' the', ' the', ' the', ' location', 'If', ' department', ' apple', ' Press', ' the', ' population', ' January', 'nes', 'd'], 'evidence_proportions': [0.14830462137858072, 0.23462657928466799, 0.2566998600959778, 0.15787971019744873]}, 'weight': {'score': [0.002399241640454247, 0.0022725286934178073, 0.0037832937457344747, 0.0022695799663733905, 0.0008547080726158329], 'topk_tokens': [' was', ' before', ':', '.\n\n', ' apple', '?', ' Where', '<|eot_id|>', 'Answer', ' \n', ' the', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006843209266662598, 0.002931052446365356, 0.004317134618759155, 0.002392391363779704]}, 'saliency': {'score': [0.00011301750228518532, 1.1893541652676009e-05, 6.57100569118153e-05, 1.1621850630088219e-05, 5.0351387116967174e-06], 'topk_tokens': ['.\n\n', ' bedroom', ' Where', ' garden', ' apple', 'Question', '<|eot_id|>', '<|begin_of_text|>', ' Bridge', 'Answer', ' \n', ' the', ' apple', 'athroom', ' the', ':', '<|start_header_id|>', 'b', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [1.231829325358073e-05, 0.00014869570732116698, 0.00032047927379608154, 4.5677026112874344e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:03:31.709 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:03:31.710 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-1_0-3-4-9.pkl | len: 10 |  size: 9.14 KB
Processing depth (0, 3, 4, 9):   2%|▏         | 2/100 [00:37<30:52, 18.90s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (1, 2, 5, 7):   2%|▏         | 2/100 [00:44<30:52, 18.90s/it]2025-01-22 03:03:38.899 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:03:38.903 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1488, 1494) --> . Daniel journeyed to the
2025-01-22 03:03:38.903 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:03:38.910 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2489, 2494) --> . Daniel went to the
2025-01-22 03:03:38.911 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:03:38.927 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5988, 5992) -->  Daniel left the apple
2025-01-22 03:03:38.927 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:03:38.951 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8387, 8393) --> . Mary journeyed to the
2025-01-22 03:03:38.951 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:03:38.975 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8764, 8769) --> . Mary got the football
2025-01-22 03:03:38.975 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:03:38.983 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2869, 2874) --> . Mary moved to the
2025-01-22 03:03:38.983 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:03:38.993 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3456, 3462) --> . John went back to the
2025-01-22 03:03:38.993 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:03:39.011 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6132, 6138) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:03:41.166 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:03:41.166 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12192])
your chose emoji: ['🤣', '👨🏻\u200d💼', '🚜', '📵', '🤸', '🛫', '🇬🇦', '🧏🏾\u200d♀', '👨🏻\u200d❤️\u200d💋\u200d👨🏾', '\U0001faa9']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 260111.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.36s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.43s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.73it/s][A100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.39it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.44it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.52it/s][A100%|██████████| 8/8 [00:00<00:00, 20.66it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.29it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.94it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.11it/s][A100%|██████████| 8/8 [00:00<00:00, 20.14it/s]
2025-01-22 03:03:50.674 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.18221755254836308, 0.19941236082282698, 0.22414325584064831, 0.19939730250184046, 0.32058385213216145], 'topk_tokens': ['itter', ' turned', '<|start_header_id|>', 'remark', ' S', ' minds', ' first', ' compl', ' S', ' S', ' malignant', ' and', '�', '�', ' STE', 'est', 'consider', 'ols', ' or', ' or'], 'evidence_proportions': [0.16733233133951822, 0.17352905273437502, 0.1978168487548828, 0.19394365946451825]}, 'weight': {'score': [0.012482123715536935, 0.002581301709089479, 0.009806684472344139, 0.0025511111492699428, 0.0009160916010538737], 'topk_tokens': [' office', ' Broadway', ' bedroom', ' football', ' garden', ' THE', ' Bridge', '<|eot_id|>', ':', 'assistant', '<|start_header_id|>', 'Bridge', 'b', '<|eot_id|>', ' bathroom', '\n\n', '.', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011212279399236043, 0.0033758223056793214, 0.04360389709472656, 0.010683755079905193]}, 'saliency': {'score': [0.0011859039465586345, 3.66340068677939e-05, 0.00026517700065266, 3.423418670676899e-05, 1.5978018442789713e-05], 'topk_tokens': [' Wild', '<|eot_id|>', ' Market', ' THE', '<|eot_id|>', ' bedroom', '.', ' bathroom', ' office', ' Dan', ' bedroom', 'b', ' bathroom', ' Daniel', ' Bridge', 'athroom', '.', ' Bench', '<|begin_of_text|>', 'Bridge'], 'evidence_proportions': [3.83456548055013e-05, 0.0006000876426696778, 0.005087636411190033, 0.00022048751513163248]}}, 25: {'grad': {'score': [0.49418858119419645, 0.5071772939409082, 0.4805986231023615, 0.5072478579407542, 0.31063308715820315], 'topk_tokens': ['posit', ' with', ' the', ' with', ' for', ' the', ' bogus', ' An', ' a', ' old', ' this', ' self', ' a', ' for', ' York', ' old', ' old', ' black', ' of', ' no'], 'evidence_proportions': [0.4880854288736979, 0.4959716796875, 0.5522174835205078, 0.46011988321940106]}, 'weight': {'score': [0.03896192425773257, 0.0024907841355940414, 0.004337875680489974, 0.0024244139943372426, 0.0010309000809987387], 'topk_tokens': [' bathroom', '.\n\n', ' apple', 'b', ' Dan', 'Answer', ' THE', '<|start_header_id|>', '.', ' Bench', ' Daniel', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', ' the', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007405678431193033, 0.005713671445846558, 0.0715099573135376, 0.08319146931171417]}, 'saliency': {'score': [0.0023983418941497803, 3.201787028951985e-05, 0.0001308254220268943, 2.774971931525325e-05, 1.522749662399292e-05], 'topk_tokens': [':', 'b', ' East', ' Ramsey', ' Anthony', ' Az', ' apple', ' the', '<|end_header_id|>', ' Ramsey', 'athroom', '<|eot_id|>', ' Daniel', '<|eot_id|>', ' Bench', '\n\n', ' THE', ' Daniel', ' Dan', '<|begin_of_text|>'], 'evidence_proportions': [8.975466092427571e-05, 0.0015197157859802247, 0.00897572934627533, 0.001054192582766215]}}, 26: {'grad': {'score': [0.26054727463495164, 0.30427853692599427, 0.27888003262606537, 0.30440009087618436, 0.32593911488850913], 'topk_tokens': [' or', ' favorable', ',\n', ' and', '\n', ' bonds', ' bend', ' bonds', ',\n', ' Press', ' I', ',', '\n', ' and', 'hue', ' he', 'itter', ',\n', ' when', ' bitter'], 'evidence_proportions': [0.2476069132486979, 0.34505233764648435, 0.19461822509765625, 0.24701944986979166]}, 'weight': {'score': [0.02062295022464934, 0.002495675597243643, 0.006133047017184171, 0.0024577645588619605, 0.0007483919461568196], 'topk_tokens': [' garden', ' bathroom', ' Bench', ' apple', ' Daniel', '<|eot_id|>', 'b', ' \n', 'Answer', '<|eot_id|>', ' bathroom', 'assistant', ' the', ' the', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003750026226043701, 0.0017747759819030763, 0.06582021713256836, 0.026446198423703514]}, 'saliency': {'score': [0.0029613645303817023, 3.456756344287304e-05, 0.00021004270423542369, 2.919205411081647e-05, 1.0331471761067709e-05], 'topk_tokens': [' the', ' bathroom', ' Bench', 'assistant', 'athroom', ' the', '<|start_header_id|>', ' apple', ' apple', '.', ' Bridge', ' Dan', 'Bridge', ' Anthony', '\n\n', ':', '<|end_header_id|>', 'b', ' Daniel', '<|begin_of_text|>'], 'evidence_proportions': [2.3926297823588055e-05, 0.00038936138153076175, 0.014321163296699524, 0.00046893954277038574]}}, 27: {'grad': {'score': [0.17466479256039574, 0.23135995837497436, 0.18202278830788352, 0.23154725398257658, 0.2691780646642049], 'topk_tokens': [' was', ' be', ' Besides', ' even', '.', ' were', ' dre', ' were', ' would', ' was', ' were', ' during', ' medicine', ' entirely', ' seldom', ' STR', ' Bottle', ' was', 'str', 'ly'], 'evidence_proportions': [0.2909513314565023, 0.1631908416748047, 0.12212467193603516, 0.10296662648518881]}, 'weight': {'score': [0.01602303414117722, 0.002532383182489662, 0.004563675685362382, 0.002505392390422871, 0.0009089559316635132], 'topk_tokens': [' Daniel', ' Broadway', ' Bridge', ' \n', ' apple', 'Answer', ' bathroom', ' garden', 'assistant', '.', '<|start_header_id|>', 'b', ' bathroom', '.\n\n', ' THE', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004952400922775269, 0.003428763151168823, 0.05206573009490967, 0.018017590045928955]}, 'saliency': {'score': [0.0010804306893121628, 4.0733200841968596e-05, 0.00023829936981201172, 3.857881448785281e-05, 3.465563058853149e-05], 'topk_tokens': [' Bridge', 'THE', 'NEW', ' the', ' Wild', ' bathroom', ' the', '\n\n', 'assistant', ' garden', '<|begin_of_text|>', ' bathroom', ' Daniel', '.', ' THE', 'athroom', '<|end_header_id|>', '.\n\n', ':', 'b'], 'evidence_proportions': [1.9301970799764e-05, 0.00026838183403015136, 0.004140317440032959, 0.0007783422867457072]}}, 28: {'grad': {'score': [0.25959705171130953, 0.22934338584108496, 0.1960508173162287, 0.2293513770790929, 0.2796326001485189], 'topk_tokens': ['dent', ' in', 'E', ' RID', ' returns', 'about', ' ins', ' prepared', ' in', '185', 'arp', ' ', 'S', ' Cedar', '.', '.', ' half', 'half', ' inside', ' half'], 'evidence_proportions': [0.29799906412760413, 0.24544067382812498, 0.31308555603027344, 0.19733301798502603]}, 'weight': {'score': [0.03456289569536845, 0.0024302904505180307, 0.006902659481221979, 0.0023666649708589337, 0.000506076713403066], 'topk_tokens': [' the', ' Bridge', '<|eot_id|>', '?', 'Answer', ' \n', ' the', ' before', ' garden', '<|eot_id|>', 'assistant', ' apple', 'b', '<|start_header_id|>', '<|end_header_id|>', ' the', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00017047921816507977, 0.0001802504062652588, 0.013068675994873047, 0.11193699638048807]}, 'saliency': {'score': [0.0022666738146827335, 2.8372316782764844e-05, 0.00010100413452495228, 2.4372791482713208e-05, 7.316966851552328e-06], 'topk_tokens': [' to', ' Daniel', '<|eot_id|>', '?', ' Bridge', ' garden', ' before', 'assistant', 'b', ' the', 'Bridge', ' apple', '<|start_header_id|>', ' the', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', ' Bridge', ':', ' the'], 'evidence_proportions': [1.0465582211812338e-05, 1.2940168380737306e-05, 0.0007411986589431763, 0.0074179768562316895]}}, 29: {'grad': {'score': [0.2389688037690662, 0.23113074245976833, 0.2230773839083585, 0.2311317772359892, 0.2831955273946126], 'topk_tokens': ['cret', 'ans', ' M', 's', ' not', ' extra', ' a', ' face', 'Emp', ' both', 'goods', ' NEW', 'sim', ' faithfully', ' enough', 'incre', 'ys', ' not', 'tal', ' ga'], 'evidence_proportions': [0.2556101481119792, 0.22904052734375002, 0.21682453155517578, 0.2453638712565104]}, 'weight': {'score': [0.018562014613832747, 0.00247480033508526, 0.003497661514715715, 0.002445148060084802, 0.000684236486752828], 'topk_tokens': [' the', ' Does', '.\n\n', ' the', '?', ' apple', '<|eot_id|>', ' \n', 'Answer', ' before', 'b', ' the', 'assistant', ' the', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00010881821314493814, 0.0003662586212158203, 0.013214588165283203, 0.05574329197406769]}, 'saliency': {'score': [0.0002463928290775844, 2.8110501120060182e-05, 0.0001381297003139149, 2.753410618347585e-05, 2.5724371274312336e-05], 'topk_tokens': [' the', 'THE', 'THE', ' garden', ' THE', ' the', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', ' before', ' Does', '<|end_header_id|>', 'assistant', 'athroom', 'b', '\n\n', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.7120113372802734e-06, 7.68899917602539e-06, 0.00033731013536453247, 0.0006283819675445557]}}, 30: {'grad': {'score': [0.2407772427513486, 0.32467338558883635, 0.2750244140625, 0.3249082519789916, 0.2551184813181559], 'topk_tokens': ['moment', ' August', '186', ' the', ' the', ' Republicans', ' soon', ' Times', 'ar', ' Loan', ' account', 'ire', ' LINE', ' the', ' Europe', ' of', ' its', '2', ' account', 'deal'], 'evidence_proportions': [0.28510284423828125, 0.26976547241210935, 0.39757204055786133, 0.06776491800944011]}, 'weight': {'score': [0.034411834818976264, 0.002480126913085537, 0.006872425025159662, 0.0024169935667648214, 0.0026783789197603863], 'topk_tokens': [' the', ' bathroom', ' the', ' before', '.\n\n', ' garden', '<|eot_id|>', '?', '<|eot_id|>', 'Answer', 'b', ' \n', 'assistant', '<|end_header_id|>', ' the', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000562782088915507, 0.001046907901763916, 0.03153181076049805, 0.09798500935236612]}, 'saliency': {'score': [0.0066401490143367225, 5.2031230711457965e-05, 0.0003267770463770086, 4.014883428282295e-05, 6.813804308573405e-05], 'topk_tokens': [' apple', '.', ' \n', ' Dan', ' Daniel', ' Broadway', 'Bridge', ' the', '.\n\n', ' Bench', 'assistant', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>', ' Bridge', '<|end_header_id|>', 'b', ':', 'athroom', ' the'], 'evidence_proportions': [2.9658277829488117e-05, 6.437301635742188e-05, 0.0018134638667106628, 0.021948243180910744]}}, 31: {'grad': {'score': [0.35558214641752695, 0.3000863235179593, 0.32484324411912396, 0.29994560063003, 0.1562884251276652], 'topk_tokens': ['had', ' Press', ' the', ' the', ' the', 'If', ' the', ' the', ' the', ' the', ' the', ' the', ' evening', ' population', ' the', ' department', ' that', ' the', ' the', ' the'], 'evidence_proportions': [0.31504376729329425, 0.40720672607421876, 0.4497652053833008, 0.290311336517334]}, 'weight': {'score': [0.0025977549098786853, 0.0022851376222655048, 0.0028459456833926115, 0.0022835820972173914, 0.0008927926421165467], 'topk_tokens': [' Where', ':', ' the', ' apple', ' before', '.\n\n', ' the', '<|eot_id|>', '?', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', 'b', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00023590028285980225, 0.0002098381519317627, 0.0055779218673706055, 0.004962762196858724]}, 'saliency': {'score': [0.00010739053998674666, 1.8056370577591276e-05, 4.527514631097967e-05, 1.785271433798242e-05, 1.0364751021067302e-05], 'topk_tokens': ['Question', ' Key', ' Emily', ' Daniel', ' Market', ' garden', ' before', ' the', 'Answer', ' apple', ' \n', ' the', ' the', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>', 'b', 'athroom', 'assistant'], 'evidence_proportions': [1.589953899383545e-05, 2.2602081298828128e-05, 0.0002889111638069153, 0.00014852484067281085]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:03:50.676 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:03:50.676 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-2_1-2-5-7.pkl | len: 10 |  size: 9.04 KB
Processing depth (1, 2, 5, 7):   3%|▎         | 3/100 [00:56<30:37, 18.94s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]
Processing depth (2, 6, 7, 8):   3%|▎         | 3/100 [01:04<30:37, 18.94s/it]2025-01-22 03:03:58.478 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:03:58.485 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2442, 2448) --> . Daniel journeyed to the
2025-01-22 03:03:58.486 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:03:58.505 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7190, 7195) --> . Daniel went to the
2025-01-22 03:03:58.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:03:58.528 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8375, 8379) -->  Daniel left the apple
2025-01-22 03:03:58.528 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:03:58.556 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9596, 9602) --> . Mary journeyed to the
2025-01-22 03:03:58.556 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:03:58.580 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8705, 8710) --> . Mary got the football
2025-01-22 03:03:58.581 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:03:58.589 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2849, 2854) --> . Mary moved to the
2025-01-22 03:03:58.589 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:03:58.599 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3436, 3442) --> . John went back to the
2025-01-22 03:03:58.599 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:03:58.616 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6107, 6113) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:04:00.704 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:04:00.704 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12184])
your chose emoji: ['👔', '🍺', '🔰', '🫒', '🧑🏿\u200d🎨', '🧘🏻\u200d♂️', '✋🏿', '📅', '🗂️', '💁🏿\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 161319.38it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.33s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.05s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.97it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.66it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.81it/s][A100%|██████████| 8/8 [00:00<00:00, 20.93it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.63it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.31it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.60it/s][A100%|██████████| 8/8 [00:00<00:00, 20.60it/s]
2025-01-22 03:04:10.349 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.15081260317847842, 0.24819765742793293, 0.2011591304432262, 0.2484512751595619, 0.2483671445112962], 'topk_tokens': [' OUT', ' emb', 'itter', 'adj', ' comparison', ' communication', 'vent', ' EAR', ' Cl', 'est', ' absence', ' announced', ' Arr', 'deal', ' Do', ' compl', 'remark', ' STE', 'Mer', 'consider'], 'evidence_proportions': [0.15382893880208334, 0.16956787109375, 0.08362579345703125, 0.1769580841064453]}, 'weight': {'score': [0.015381923743656703, 0.0025782650485640917, 0.006928885524923151, 0.0025482427755834556, 0.0017877226838698755], 'topk_tokens': ['.', ' Market', ' bathroom', 'Answer', ' bedroom', 'Bridge', '.', ' Bridge', ':', '<|eot_id|>', 'assistant', 'b', ' Fort', ' Broadway', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00017546614011128744, 0.020220577716827393, 0.05020427703857422, 0.003341267506281535]}, 'saliency': {'score': [0.0007459322611490885, 2.7372791190588358e-05, 0.0001898800784891302, 2.5835825677602816e-05, 4.164186807779165e-05], 'topk_tokens': ['<|eot_id|>', 'Bridge', '.', ' Broadway', ' Temper', ' Shak', ' Anthony', ' Dan', ':', '<|begin_of_text|>', ' Fort', ' Daniel', '<|eot_id|>', ' Daniel', ' garden', '<|start_header_id|>', ' Market', 'b', ' bedroom', 'athroom'], 'evidence_proportions': [2.806385358174642e-06, 0.0012853324413299562, 0.002231009304523468, 4.95066245396932e-05]}}, 25: {'grad': {'score': [0.34976822989327566, 0.393991674578316, 0.33336361971768463, 0.39417798135905713, 0.2420353522667518], 'topk_tokens': [' bogus', ' free', ' this', 'iously', ' the', ' of', ' the', ' self', ' for', ' at', ' a', ' black', ' for', ' a', ' of', ' a', ' for', ' printing', ' of', ' no'], 'evidence_proportions': [0.3652165730794271, 0.28528175354003904, 0.4153709411621094, 0.34432347615559894]}, 'weight': {'score': [0.02295473359879993, 0.002503456196480176, 0.005493940277533097, 0.002462673301615891, 0.0022601324778336743], 'topk_tokens': [' Ramsey', ' \n', '.', 'b', ' apple', ' Daniel', '.', ' Anthony', 'Answer', ' Bench', ' Daniel', ':', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0002018858989079793, 0.02722601890563965, 0.08043384552001953, 0.003828768928845723]}, 'saliency': {'score': [0.001418660084406535, 2.624070398858586e-05, 0.0001266734166578813, 2.365092083085353e-05, 4.8730235833388106e-05], 'topk_tokens': [' Ramsey', '<|start_header_id|>', 'Minnesota', ':', ' Seventh', ' Fort', '.', ' Daniel', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', ' Bench', ' Daniel', ' Anthony', 'athroom', ' Ramsey', ' Dan', ' Ramsey', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [1.520415147145589e-05, 0.0009309113025665283, 0.00616426020860672, 6.483991940816244e-05]}}, 26: {'grad': {'score': [0.34058416457403273, 0.3436472914593932, 0.34941517223011365, 0.34364213930916565, 0.3889599580031175], 'topk_tokens': [' bend', ' favorable', ' when', ' compl', ' I', 'b', 'b', ' bonds', 'ree', ',', ' blot', ' Press', ' bonds', ' favor', ' broad', 'b', 'RI', 'b', 'itter', ' bitter'], 'evidence_proportions': [0.3988037109375, 0.3381744384765625, 0.323455810546875, 0.2957916259765625]}, 'weight': {'score': [0.016337722539901733, 0.002490882594953014, 0.003441755067218434, 0.002465215365602408, 0.0018615814355703501], 'topk_tokens': [' apple', ' Fort', ' bathroom', ' Ramsey', ' apple', '<|eot_id|>', 'b', '<|eot_id|>', ' \n', ' Anthony', 'Answer', ' the', 'assistant', ' the', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [2.524256706237793e-05, 0.01449715495109558, 0.061696529388427734, 0.003944804271062215]}, 'saliency': {'score': [0.0014044557298932756, 3.759082811170443e-05, 0.00015752017498016357, 3.500991502141136e-05, 4.203560260625986e-05], 'topk_tokens': [' Seventh', ' the', ' Broadway', ' Fort', ' Bridge', 'assistant', ' Charles', ' Ramsey', ' apple', ' Shak', ' Daniel', '<|start_header_id|>', ' Daniel', 'athroom', ':', '\n\n', '<|end_header_id|>', ' Anthony', 'b', '<|begin_of_text|>'], 'evidence_proportions': [1.1126200358072915e-06, 0.0016747653484344483, 0.005124092102050781, 0.00010278324286142984]}}, 27: {'grad': {'score': [0.19297899518694198, 0.24145301473906622, 0.17585615678267044, 0.24165567327707968, 0.274660770709698], 'topk_tokens': [' was', ' was', ' was', ' step', ' was', ' seldom', ' would', ' was', ' ideas', ' medicine', ' be', ' was', ' were', '.', ' would', 'ly', ' was', ' STR', 'str', '.'], 'evidence_proportions': [0.21777057647705078, 0.20070343017578127, 0.24610519409179688, 0.12633291880289713]}, 'weight': {'score': [0.018221684864589145, 0.0025334033072410504, 0.0033148134296590633, 0.0025048587638124018, 0.0023897777383144084], 'topk_tokens': [' Bridge', '<|eot_id|>', 'THE', ' Daniel', ' \n', 'Answer', ' apple', ' Broadway', ' garden', ' Anthony', 'assistant', ' bathroom', 'b', '.\n\n', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [6.57339890797933e-05, 0.020811498165130615, 0.06591320037841797, 0.0024251143137613935]}, 'saliency': {'score': [0.0016751473858242943, 4.1041500292350986e-05, 0.0001857822591608221, 3.795351278483475e-05, 0.00010063785773057204], 'topk_tokens': [' Jackson', ' Bridge', 'THE', ' Daniel', 'THE', 'assistant', ' \n', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>', ' Daniel', ' apple', ' the', '\n\n', ' Anthony', '<|end_header_id|>', ':', 'athroom', '.\n\n', 'b'], 'evidence_proportions': [2.0613272984822593e-06, 0.0015807807445526122, 0.006724998354911804, 6.0304999351501465e-05]}}, 28: {'grad': {'score': [0.23380542936779203, 0.2650979801815587, 0.19780601154674182, 0.2652739985344124, 0.2587947111863356], 'topk_tokens': ['<|end_header_id|>', 'about', ' ', ' a', 'ew', 'nes', 'Kent', 'ien', 'S', '.', 'G', ' RID', ' inside', ' half', ' Cedar', 'arp', 'dent', ' half', 'half', ' in'], 'evidence_proportions': [0.2794532775878906, 0.13729248046875, 0.19040918350219727, 0.297515869140625]}, 'weight': {'score': [0.012602689720335462, 0.0024316630743384665, 0.004751731048930775, 0.0024098718148681956, 0.0012478576256678654], 'topk_tokens': ['<|eot_id|>', ' garden', ' the', ' the', 'Answer', ' \n', '?', ' the', ' before', '<|eot_id|>', 'assistant', ' apple', ' garden', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [1.163780689239502e-05, 0.015344023704528809, 0.03318643569946289, 0.009186799327532452]}, 'saliency': {'score': [0.0003408009097689674, 3.299572914435297e-05, 5.8282505382191054e-05, 3.2417647962670707e-05, 2.39484585248507e-05], 'topk_tokens': ['?', ' Third', ' the', ' Market', ' to', 'Bridge', ' before', ' apple', ' Bridge', 'athroom', 'b', ' garden', 'assistant', ' the', '<|start_header_id|>', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [1.7384688059488931e-07, 0.0004045724868774414, 0.0009585246443748474, 0.0002164691686630249]}}, 29: {'grad': {'score': [0.18546821957542783, 0.22827831558474193, 0.19867217540740967, 0.22840597921287903, 0.35364590929104733], 'topk_tokens': [' M', 'd', 's', ' a', '.', ' l', ' Get', 'Emp', 'ION', ' com', 'cret', 'st', 'ib', 's', 'tal', ' a', ' Bu', 'ys', ' THE', ' ga'], 'evidence_proportions': [0.2240422566731771, 0.07242202758789062, 0.21332168579101562, 0.22253036499023438]}, 'weight': {'score': [0.008462592249824888, 0.0024925387501374256, 0.0035559778863733463, 0.002480288520848013, 0.0014838765446956342], 'topk_tokens': [' the', '<|eot_id|>', '<|eot_id|>', ' Does', ' apple', '?', ' \n', ' the', ' before', '.\n\n', 'Answer', 'assistant', ' the', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.5601515769958496e-05, 0.006228697299957275, 0.02978372573852539, 0.00455707311630249]}, 'saliency': {'score': [0.000130910248983474, 2.4199365953273432e-05, 0.00015498968687924472, 2.3777897277054265e-05, 6.552384449885442e-05], 'topk_tokens': [' the', ' the', 'E', ' before', ' the', ' garden', 'THE', 'THE', '<|eot_id|>', 'Does', ':', 'Answer', 'b', 'athroom', 'assistant', '<|end_header_id|>', ' Does', '\n\n', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [3.8246313730875653e-07, 7.399320602416993e-05, 0.0004891529679298401, 7.004042466481526e-05]}}, 30: {'grad': {'score': [0.25775300888788133, 0.31607846962551667, 0.312526442787864, 0.316185763702091, 0.2826148042312035], 'topk_tokens': ['ar', 'Emp', ' EVENTS', 'CO', 'AM', ' Million', ' Loan', 'SSION', 'b', ' its', ' Europe', ' Times', ' soon', ' forb', 'ire', ' LINE', 'b', '2', ' account', 'deal'], 'evidence_proportions': [0.24178568522135419, 0.2971904754638672, 0.42757606506347656, 0.12764040629069012]}, 'weight': {'score': [0.015613235178447905, 0.0024805193392649603, 0.006238501180302013, 0.0024510016652592244, 0.004312769724772527], 'topk_tokens': [' Fort', ' the', ' Broadway', ' the', ' Anthony', ' garden', '?', '.\n\n', 'b', '<|eot_id|>', 'Answer', 'assistant', ' \n', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [7.947782675425211e-05, 0.017093324661254884, 0.049283504486083984, 0.007466738422711691]}, 'saliency': {'score': [0.0006373609815325056, 6.063397546744556e-05, 0.0003430138934742321, 5.912511303961984e-05, 0.00010968400881840632], 'topk_tokens': [' Daniel', ' the', ' midnight', ' the', ' apple', ' Bench', ' the', ' Fort', 'assistant', ' Bridge', ' Anthony', '.\n\n', ' Broadway', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', '<|start_header_id|>', 'b', ':', 'athroom'], 'evidence_proportions': [5.97536563873291e-06, 0.0007862746715545654, 0.0018891692161560059, 0.0003101130326588949]}}, 31: {'grad': {'score': [0.27793376786368235, 0.2601157561136523, 0.2753835916519165, 0.2600572852120883, 0.1417871736563169], 'topk_tokens': [' August', ' the', ' location', ' January', 'did', ' population', 'membership', ' the', ' the', ' location', ' the', ' the', ' apple', ' the', ' location', ' location', ' the', ' location', ' department', ' population'], 'evidence_proportions': [0.3151947657267252, 0.3143300056457519, 0.32031261920928955, 0.1820900042851766]}, 'weight': {'score': [0.0031170078686305453, 0.002292408312845046, 0.0029227232391184025, 0.0022898405000115886, 0.0014663779964813818], 'topk_tokens': [' Where', ':', 'Question', ' the', ' before', ' the', '.\n\n', '?', '<|eot_id|>', 'Answer', ' \n', 'b', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [8.425613244374594e-05, 0.004218244552612304, 0.006885528564453125, 0.0027197152376174927]}, 'saliency': {'score': [0.00012897877466110956, 1.6659737865792e-05, 4.419278014789928e-05, 1.6415631582614463e-05, 1.4229462696955754e-05], 'topk_tokens': [' Where', ' before', ' was', ' the', 'Question', ' \n', ' Market', ' garden', '<|begin_of_text|>', 'Answer', ' the', ' apple', ' the', ' the', ':', 'b', 'athroom', '<|end_header_id|>', '<|start_header_id|>', 'assistant'], 'evidence_proportions': [6.139278411865234e-06, 0.00022628307342529297, 0.0003280267119407654, 3.8032730420430504e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:04:10.350 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:04:10.350 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-3_2-6-7-8.pkl | len: 10 |  size: 9.1 KB
Processing depth (2, 6, 7, 8):   4%|▍         | 4/100 [01:16<30:45, 19.22s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 2, 4, 6):   4%|▍         | 4/100 [01:23<30:45, 19.22s/it]2025-01-22 03:04:17.682 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:04:17.687 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1498, 1504) -->  tragedy. Daniel journeyed to
2025-01-22 03:04:17.687 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:04:17.694 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2481, 2486) -->  Daniel went to the garden
2025-01-22 03:04:17.694 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:04:17.707 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4876, 4880) -->  Daniel left the apple
2025-01-22 03:04:17.708 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:04:17.729 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7203, 7209) --> . Mary journeyed to the
2025-01-22 03:04:17.729 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:04:17.753 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8772, 8777) --> . Mary got the football
2025-01-22 03:04:17.753 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:04:17.761 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2859, 2864) --> . Mary moved to the
2025-01-22 03:04:17.761 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:04:17.771 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3446, 3452) --> . John went back to the
2025-01-22 03:04:17.771 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:04:17.789 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6122, 6128) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:04:19.780 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:04:19.780 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12212])
your chose emoji: ['👶🏽', '😱', '🚵🏾\u200d♂️', '👩🏿\u200d🦼\u200d➡️', '🧑\u200d✈️', '👩🏿\u200d🤝\u200d👨🏼', '🈷', '🧏🏿\u200d♂️', '🤦🏻\u200d♂', '☹']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 239674.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.53s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.55it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.34it/s][A100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.02it/s][A
 50%|█████     | 4/8 [00:00<00:00, 16.52it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 15.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 13.94it/s][A100%|██████████| 8/8 [00:00<00:00, 14.78it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.68it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.73it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.80it/s][A100%|██████████| 8/8 [00:00<00:00, 16.85it/s]
2025-01-22 03:04:29.504 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2591579073951358, 0.24087936395121265, 0.24637055397033691, 0.240837903583751, 0.30014901161193847], 'topk_tokens': ['deal', ' miles', ' latter', ' It', 'adv', 'ounding', ' emb', ' Arr', ' STE', ' malignant', '202', ' and', 'ols', ' hundred', 'est', ' Do', 'itter', ' turtle', ' compl', 'consider'], 'evidence_proportions': [0.2750622431437174, 0.21392364501953126, 0.2138652801513672, 0.3111438751220703]}, 'weight': {'score': [0.020399977763493855, 0.0025779048780705956, 0.009401261806488037, 0.002534824251795614, 0.0006798282265663147], 'topk_tokens': [' Daniel', ' the', ' the', ' part', ':', ' garden', ' Bridge', ' barric', 'b', '<|eot_id|>', ' bedroom', '<|start_header_id|>', 'assistant', ' bathroom', 'Bridge', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006442765394846599, 0.029813575744628906, 0.049457550048828125, 0.01293929914633433]}, 'saliency': {'score': [0.0008964822405860538, 4.197367868411751e-05, 0.0003225437619469383, 3.999230983497828e-05, 1.398622989654541e-05], 'topk_tokens': [' the', ' the', '<|eot_id|>', '<|start_header_id|>', ' garden', ' Press', ' Father', ' Bench', ' Bridge', '<|eot_id|>', ' the', '<|begin_of_text|>', '.', ' bathroom', ' bedroom', ' Bridge', ' garden', 'b', 'Bridge', 'athroom'], 'evidence_proportions': [8.702278137207031e-06, 0.0013009488582611084, 0.0019936859607696533, 0.0007157375415166219]}}, 25: {'grad': {'score': [0.5570151919410342, 0.6558813206738641, 0.48005537553267047, 0.656369684089613, 0.3237075448036194], 'topk_tokens': [' black', ' large', ' York', ' for', ' as', ' bogus', ' the', ' a', ' a', ' severe', ' with', 'antic', ' for', ' a', ' for', ' free', ' self', ' of', ' a', ' no'], 'evidence_proportions': [0.6117960611979166, 0.5857513427734375, 0.5754718780517578, 0.46598307291666663]}, 'weight': {'score': [0.022519077573503767, 0.00250496639856002, 0.0035476196895946155, 0.0024685521110906984, 0.0009337421506643295], 'topk_tokens': [' Dan', 'b', ' Ot', ' part', ' the', 'Answer', '.', ' \n', ' Bench', ' apple', ':', ' Daniel', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00043910245100657147, 0.03881616592407227, 0.05292844772338867, 0.010745232303937275]}, 'saliency': {'score': [0.0012057508741106307, 3.1416844599470466e-05, 5.7659365914084695e-05, 2.9343368581670935e-05, 2.428591251373291e-05], 'topk_tokens': ['b', 'Answer', ' bathroom', ' Ear', 'Bridge', 'assistant', ' Ot', ' Dan', ' barric', ' Capt', '<|eot_id|>', '<|eot_id|>', ' Daniel', '.', 'athroom', ' Bench', ' apple', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [9.372830390930176e-06, 0.001620316505432129, 0.0039441511034965515, 0.00023105740547180176]}}, 26: {'grad': {'score': [0.2875823974609375, 0.29028077471026914, 0.26963390003551135, 0.29032274785889556, 0.34844322204589845], 'topk_tokens': [' when', '�', 'ub', ' prof', ' Press', ' Eagle', ' Empire', 'bec', 'occ', ' Moore', 'b', 'issippi', 'UX', ' broad', 'b', 'b', ' Press', ' bitter', 'b', 'itter'], 'evidence_proportions': [0.35465494791666663, 0.30974121093749996, 0.24573135375976562, 0.22994486490885416]}, 'weight': {'score': [0.010158834003266833, 0.0024840256155781505, 0.004583125764673407, 0.002466990520324983, 0.0009446967393159867], 'topk_tokens': ['Bridge', '?', ' the', ' bathroom', '<|eot_id|>', ' \n', 'Answer', ' garden', 'b', '<|eot_id|>', 'assistant', ' the', ' barric', ' the', '\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00013754268487294516, 0.015845489501953126, 0.022564411163330078, 0.007170860966046651]}, 'saliency': {'score': [0.00042135374886648994, 2.6893481532911565e-05, 0.00014229118824005127, 2.6004357710896925e-05, 3.207474946975708e-05], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' apple', ' Dan', ' bedroom', ' Bridge', ' the', ' West', ' bathroom', ' apple', 'assistant', 'Bridge', ':', ' garden', '<|end_header_id|>', ' the', '\n\n', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [1.685818036397298e-05, 0.000458294153213501, 0.001407809555530548, 0.00013742844263712564]}}, 27: {'grad': {'score': [0.19401513962518602, 0.24158463355121776, 0.25138785622336646, 0.2416489852168158, 0.24179842472076415], 'topk_tokens': ['ides', 'arr', ' DAYS', ' designated', 'lin', ' sentinel', '.', '-n', 'UG', ' conventions', 'sur', ' accepted', ' Thanksgiving', ' conventions', ' STR', 'Republicans', ' convention', ' restrictions', 'str', ' step'], 'evidence_proportions': [0.21504720052083331, 0.1869171142578125, 0.13446617126464844, 0.218597412109375]}, 'weight': {'score': [0.017549870979218257, 0.0025331054512614166, 0.005089469931342385, 0.002502577017589968, 0.0009622599929571152], 'topk_tokens': [' apple', '<|eot_id|>', ' apple', ' Daniel', ' \n', ' garden', 'Answer', ' Bridge', ' bedroom', 'assistant', '.\n\n', '<|start_header_id|>', ' bathroom', 'b', ' barric', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00027560194333394367, 0.03410537242889404, 0.041147589683532715, 0.005296076337496439]}, 'saliency': {'score': [0.0013988926297142392, 3.700510332245735e-05, 0.00043289769779552114, 3.3939931195228065e-05, 2.893880009651184e-05], 'topk_tokens': [' apple', '\n\n', ' \n', ' garden', '<|start_header_id|>', ' apple', ' bathroom', 'assistant', ':', ' the', '.', ' bedroom', '.', 'Bridge', '<|begin_of_text|>', ' Daniel', ' Bridge', '.\n\n', 'athroom', 'b'], 'evidence_proportions': [9.710590044657389e-06, 0.003242397308349609, 0.003151416778564453, 8.347133795420329e-05]}}, 28: {'grad': {'score': [0.2232799530029297, 0.2962487638069868, 0.22612632404674182, 0.29650139597110203, 0.3118212789297104], 'topk_tokens': ['nes', 'ot', ' as', ' inside', '.', 'nes', 'S', '.', ' returns', ' Cedar', ' ', ' half', ' in', 'nes', 'arp', 'dent', 'nes', ' RID', ' half', 'half'], 'evidence_proportions': [0.2711569468180339, 0.1356201171875, 0.23978805541992188, 0.23744742075602213]}, 'weight': {'score': [0.021580728746595838, 0.0024237668275149983, 0.011264550414952364, 0.0023747368045751098, 0.0006154999136924744], 'topk_tokens': ['Answer', ' Bridge', ' Bridge', ' \n', '?', ' apple', '<|eot_id|>', ' the', ' before', ' garden', 'assistant', ' the', 'b', ' the', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [5.758305390675862e-05, 0.020649814605712892, 0.003138899803161621, 0.05617418885231018]}, 'saliency': {'score': [0.0015409404323214577, 3.27351880864047e-05, 0.00031971525062214244, 2.9614429665050576e-05, 1.083947718143463e-05], 'topk_tokens': [' before', ' Far', ' back', '?', ' the', '\n\n', '<|start_header_id|>', ' apple', 'athroom', 'Bridge', 'assistant', '<|end_header_id|>', ':', ' the', ' garden', ' Bridge', ' the', ' the', ' Bridge', '<|begin_of_text|>'], 'evidence_proportions': [4.872679710388184e-06, 0.00037088990211486816, 0.00010512024164199829, 0.005009263753890991]}}, 29: {'grad': {'score': [0.23024858747209823, 0.32698081692271286, 0.2862322547219016, 0.3272213562906787, 0.4239219129085541], 'topk_tokens': [' idea', ' not', 'incre', 't', '�', ' extra', ' arms', 'ra', 'pend', ' awake', 'Emp', 'tal', ' facilities', 'goods', 'assistant', ' enough', 'ys', ' faithfully', 'cret', ' ga'], 'evidence_proportions': [0.28167724609375, 0.16307983398437498, 0.2902069091796875, 0.1948216756184896]}, 'weight': {'score': [0.008231083552042643, 0.0024889962769803872, 0.003878990357572382, 0.0024765773070042673, 0.000765041634440422], 'topk_tokens': [' part', 'ot', ' the', ' apple', ' \n', ' the', '.\n\n', '?', '<|eot_id|>', 'Answer', ' before', 'b', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.471442222595215e-05, 0.00991806983947754, 0.019286155700683594, 0.00761158267656962]}, 'saliency': {'score': [0.00023350119590759277, 2.7579935271379845e-05, 0.0001593868840824474, 2.6986433846206992e-05, 2.994909882545471e-05], 'topk_tokens': ['Does', ' Where', ' Daniel', 'ot', ' garden', ' the', '<|eot_id|>', 'Answer', ' Does', '<|eot_id|>', ' the', 'b', ':', ' part', 'athroom', '<|end_header_id|>', 'assistant', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.6259492238362627e-06, 0.0006311237812042236, 0.000300481915473938, 8.73704751332601e-05]}}, 30: {'grad': {'score': [0.3213624500093006, 0.3201476925862797, 0.3253010403026234, 0.32013628250119563, 0.29806635677814486], 'topk_tokens': [' Europe', ' Times', ' Loan', ' the', '2', ' of', ' B', ' Million', ' soon', 'SSION', ' itself', ' Times', ' forb', 'ire', 'b', ' LINE', ' account', 'b', 'deal', 'b'], 'evidence_proportions': [0.32029469807942706, 0.18681488037109376, 0.5438060760498047, 0.28625742594401044]}, 'weight': {'score': [0.010185450315475464, 0.0024702288408626847, 0.009930277412587946, 0.002443434499789334, 0.002672753110527992], 'topk_tokens': [' barric', 'Question', ' the', ' before', '.\n\n', '<|eot_id|>', ' garden', '<|eot_id|>', '?', 'Answer', ' the', 'b', ' \n', 'assistant', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00038412710030873615, 0.00958113670349121, 0.014353036880493164, 0.0177119771639506]}, 'saliency': {'score': [0.000920735654376802, 6.718129297772539e-05, 0.00040352073582735926, 6.510077134347691e-05, 4.6458840370178225e-05], 'topk_tokens': ['.', ' barric', ' before', ' Bench', ' garden', '.\n\n', 'Bridge', ' the', '<|start_header_id|>', ' the', 'assistant', ' bathroom', ' the', '<|end_header_id|>', ' Bridge', ' Bridge', '<|begin_of_text|>', ':', 'athroom', 'b'], 'evidence_proportions': [2.551575501759847e-05, 0.0007101655006408692, 0.00067158043384552, 0.0021575341622034707]}}, 31: {'grad': {'score': [0.3132732482183547, 0.280585369183497, 0.2509761723605069, 0.28058249018829273, 0.1661892428994179], 'topk_tokens': ['did', ' Press', ' the', ' the', ' evening', ' location', ' location', ' apple', 'If', ' the', ' location', ' the', ' the', ' the', ' the', ' population', ' the', ' the', ' the', ' the'], 'evidence_proportions': [0.23015721638997394, 0.4121752738952637, 0.3942872881889343, 0.2599615653355916]}, 'weight': {'score': [0.001976150841940017, 0.002276597587095144, 0.002510003068230369, 0.002276694075845003, 0.0008405856788158417], 'topk_tokens': [' the', ' was', ':', ' Where', '.\n\n', ' before', ' the', '?', '<|eot_id|>', 'Answer', ' \n', 'b', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00015007456143697104, 0.0012893915176391602, 0.001613430678844452, 0.004616340001424153]}, 'saliency': {'score': [4.720262118748256e-05, 1.5069330221700024e-05, 3.361160104924982e-05, 1.4980377784262666e-05, 6.669014692306519e-06], 'topk_tokens': ['Bridge', ' Bridge', '.\n\n', ' garden', ' Bridge', '<|eot_id|>', '<|begin_of_text|>', ' ', ' the', ' the', ' apple', ' the', 'Answer', ':', ' \n', '<|start_header_id|>', 'b', 'athroom', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [6.2783559163411456e-06, 3.711581230163574e-05, 7.371604442596436e-05, 7.88569450378418e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:04:29.506 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:04:29.506 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-4_1-2-4-6.pkl | len: 10 |  size: 9.07 KB
Processing depth (1, 2, 4, 6):   5%|▌         | 5/100 [01:35<30:23, 19.20s/it]Processing depth (1, 2, 4, 6):   5%|▌         | 5/100 [01:35<30:20, 19.16s/it]
2025-01-22 03:04:29.874 | INFO     | __main__:<module>:72 - Selected idx: 5
2025-01-22 03:04:29.874 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:04:29.874 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:04:29.874 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:04:29.875 | INFO     | __main__:<module>:76 - Needle: [' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 03:04:29.875 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 03:04:29.875 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.54it/s]
Processing depth (2, 3, 4, 5, 6):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:04:37.338 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:04:37.345 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2492, 2498) --> . Daniel journeyed to the
2025-01-22 03:04:37.345 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:04:37.356 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3831, 3835) -->  Daniel grabbed the milk
2025-01-22 03:04:37.356 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:04:37.369 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4908, 4913) --> . Daniel went to the
2025-01-22 03:04:37.370 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:04:37.385 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5977, 5981) -->  Daniel left the milk
2025-01-22 03:04:37.386 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:04:37.406 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7240, 7246) --> . Mary journeyed to the
2025-01-22 03:04:37.406 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:04:37.431 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8751, 8757) --> . Sandra journeyed to the
2025-01-22 03:04:37.432 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football there.
2025-01-22 03:04:37.458 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (9584, 9589) --> . Mary got the football
2025-01-22 03:04:37.458 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:04:37.472 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4912, 4918) -->  the garden. John went back
2025-01-22 03:04:37.472 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:04:37.478 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1927, 1932) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:04:39.639 | INFO     | test_jbb_retain:begin_test:632 - The garden.<|eot_id|>
2025-01-22 03:04:39.639 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12250])
your chose emoji: ['🪝', '📰', '👩🏿\u200d🦼\u200d➡', '👩🏻\u200d🤝\u200d👨🏼', '👩🏻\u200d🤝\u200d👩🏿', '🏋🏻\u200d♂️', '\U0001f7f0', '👨🏻\u200d🍳', '👩🏼\u200d🌾', '👨🏿\u200d❤️\u200d💋\u200d👨🏾']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136956.87it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.40s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.44s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.72it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.33it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.53it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.61it/s][A100%|██████████| 8/8 [00:00<00:00, 20.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.43it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.14it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.28it/s][A100%|██████████| 8/8 [00:00<00:00, 20.32it/s]
2025-01-22 03:04:48.853 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2524280548095703, 0.2802967795992563, 0.23181161013516513, 0.2804412488642041, 0.39144310584435094], 'topk_tokens': [' Arr', ' agreement', '�', ' compl', ' It', 'st', ' whistle', ' Do', '�', ' and', '�', ' out', ' turtle', '.', ' first', ' examined', ' absence', '�', 'ols', 'consider'], 'evidence_proportions': [0.25994936625162757, 0.18020105361938477, 0.275506591796875, 0.19566726684570312, 0.31166648864746094]}, 'weight': {'score': [0.027000367641448975, 0.0025618536116434176, 0.02911790121685375, 0.002463934891500884, 0.0007995200844911429], 'topk_tokens': [' the', ' office', ' milk', ' milk', 'Answer', ' football', '<|eot_id|>', 'assistant', ' bedroom', '<|eot_id|>', ' bathroom', '<|start_header_id|>', ':', 'b', ' garden', '\n\n', '<|end_header_id|>', 'Bridge', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002753143509229024, 0.047210693359375, 0.030137413740158082, 0.06435012817382812, 0.010259995857874554]}, 'saliency': {'score': [0.0019777536392211912, 5.1592283226330115e-05, 0.0021286390044472432, 4.390351853955872e-05, 2.5288989910712608e-05], 'topk_tokens': ['c', ' Daniel', ' the', '<|eot_id|>', 'b', '<|begin_of_text|>', ' bedroom', '<|eot_id|>', ':', ' football', ' Bridge', ' milk', '<|start_header_id|>', ' bathroom', ' office', ' milk', ' bedroom', ' garden', 'athroom', 'Bridge'], 'evidence_proportions': [6.982684135437012e-05, 0.004294440150260925, 0.001006162166595459, 0.006314188241958618, 0.00025992592175801593]}}, 25: {'grad': {'score': [0.5257160949707032, 0.540413950760783, 0.5193991227583452, 0.5404819314760712, 0.4360705338991605], 'topk_tokens': [' of', ' black', ' for', ' set', ' the', ' at', ' l', ' hate', ' money', ' for', ' bogus', ' for', 'ivery', ' a', ' a', ' free', ' inverted', ' no', ' of', ' self'], 'evidence_proportions': [0.5056851704915365, 0.5548324584960938, 0.6315887451171875, 0.5446128845214844, 0.4255110422770182]}, 'weight': {'score': [0.024558331966400147, 0.0024820957574554045, 0.011159783059900457, 0.0024212392093743455, 0.0012554523463432605], 'topk_tokens': ['.', ' Bench', ' bathroom', ' Daniel', ' garden', ' milk', ' Daniel', ' the', 'b', 'Answer', '?\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013143867254257202, 0.043915748596191406, 0.029106986522674558, 0.04956483840942383, 0.0144357830286026]}, 'saliency': {'score': [0.000973658561706543, 2.7592971374927654e-05, 0.00037462061101740056, 2.5029785414709334e-05, 3.139731975702139e-05], 'topk_tokens': ['<|start_header_id|>', '.', ' THE', ' Geo', ' garden', '\n\n', 'Bridge', ' Min', ':', 'Print', ' milk', 'Answer', 'athroom', ' Dan', ' milk', ' Bench', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.49879264831543e-05, 0.0018271654844284058, 0.0008513808250427246, 0.0025690123438835144, 0.0003816535075505575]}}, 26: {'grad': {'score': [0.46444350242614746, 0.49474038644235085, 0.4505337801846591, 0.49488211734830484, 0.436394948225755], 'topk_tokens': [' Press', 'graph', 'UX', 'rich', 'oggle', ' and', ' Marshall', ',', ' Milwaukee', ' Marshall', ' Gutenberg', 'graph', 'UX', 'hue', 'graph', ' Press', ' Eagle', 'ub', 'itter', ' bitter'], 'evidence_proportions': [0.46272655328114826, 0.5821380615234375, 0.513623046875, 0.43951416015625, 0.3633340199788412]}, 'weight': {'score': [0.009651850461959838, 0.0024669948893255734, 0.010385323654521595, 0.0024380071273929034, 0.0009637073828623845], 'topk_tokens': [' bathroom', '.\n\n', ' garden', ' barric', ' bathroom', '<|eot_id|>', '<|eot_id|>', '?\n', 'b', 'Answer', ' the', 'assistant', 'Bridge', ' the', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.000568881630897522, 0.014122724533081055, 0.011088371276855469, 0.01978898048400879, 0.0077990492184956866]}, 'saliency': {'score': [0.0003680753707885742, 3.71559828064991e-05, 0.0009496157819574529, 3.483359215592783e-05, 2.6019719930795523e-05], 'topk_tokens': [' Daniel', ' Merch', ' the', ' bathroom', 'assistant', ' carrier', ' Dan', 'Answer', '?\n', ' the', 'Bridge', ' bathroom', '<|end_header_id|>', ' garden', '<|start_header_id|>', ':', 'athroom', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [2.3156404495239258e-05, 0.0007311329245567322, 0.00027893781661987306, 0.00072488933801651, 0.00030736128489176434]}}, 27: {'grad': {'score': [0.2601823043823242, 0.2878607654439219, 0.2557225227355957, 0.28797538144147417, 0.3401025441976694], 'topk_tokens': ['.', ' without', ' ideas', ' prev', ' Thanksgiving', 'str', ' dre', 'arr', ' time', 'ides', ' medicine', ' sentinel', ' business', ' be', ' step', '10', ' was', '\n', ' accepted', ' Bottle'], 'evidence_proportions': [0.18752463658650714, 0.26421356201171875, 0.295562744140625, 0.4147796630859375, 0.19760386149088544]}, 'weight': {'score': [0.021427918672561646, 0.0025149587999993145, 0.02666940607807853, 0.0024326859967114395, 0.0011477201030804561], 'topk_tokens': [' THE', '<|eot_id|>', ' Daniel', '?\n', ' barric', ' bedroom', 'Answer', ' bathroom', 'Bridge', 'assistant', '.\n\n', ' bathroom', 'b', ' garden', '\n\n', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0013524939616521199, 0.042469143867492676, 0.025705695152282715, 0.04057812690734863, 0.011144240697224935]}, 'saliency': {'score': [0.002087535858154297, 3.813287778493386e-05, 0.0012233460491353815, 3.179912681992125e-05, 3.724401960006127e-05], 'topk_tokens': ['Bridge', ' Mary', 'NEW', ' Daniel', ' bathroom', ' THE', ' Daniel', ' Daniel', ' the', ' bathroom', ' the', ':', ' THE', '<|begin_of_text|>', ' garden', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.0001629789670308431, 0.0026914402842521667, 0.0037562012672424316, 0.004025839269161224, 0.0009267330169677734]}}, 28: {'grad': {'score': [0.2739874267578125, 0.32150877511502285, 0.2974925474687056, 0.32164939372203166, 0.3727847612821139], 'topk_tokens': ['600', ' probably', 'antic', '\n', ' the', ' returns', ' the', ' ', ' a', 'nes', 'half', ' lie', ' spring', '.', '.', ' in', 'nes', ' half', 'dent', 'ball'], 'evidence_proportions': [0.28720855712890625, 0.27251434326171875, 0.18724670410156252, 0.22622299194335938, 0.365875244140625]}, 'weight': {'score': [0.010493754148483277, 0.002385751691086773, 0.00871336189183322, 0.002357740263440424, 0.0009942733897612644], 'topk_tokens': ['.\n\n', ' the', ' bathroom', ' the', ' the', ' bathroom', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', '?\n', ' the', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.000500763456026713, 0.006989240646362305, 0.01183573007583618, 0.013477206230163574, 0.019715805848439533]}, 'saliency': {'score': [0.00040685772895812986, 3.526642606042993e-05, 0.000341481783173301, 3.395342258435868e-05, 1.938211230131296e-05], 'topk_tokens': [' garden', ' the', '.\n\n', ' Bridge', '?\n', ' milk', ' the', ' the', 'b', 'athroom', ' Bridge', ' the', '<|start_header_id|>', ' the', 'Bridge', 'assistant', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', ':'], 'evidence_proportions': [1.3247132301330566e-05, 0.0002340376377105713, 0.00042057037353515625, 0.00036973506212234497, 0.0009290029605229695]}}, 29: {'grad': {'score': [0.5301082611083985, 0.3941265141381192, 0.5404728976162997, 0.39358422640169644, 0.3570588552034818], 'topk_tokens': [' LO', 're', ' Pioneer', 'Spring', 'A', ' An', ' In', 'y', 'ION', ' THE', ' ga', 'ION', ' a', 'ION', ' M', 'UL', ' Pioneer', ' The', ' The', ' THE'], 'evidence_proportions': [0.5958760579427084, 0.42374515533447266, 0.48873291015625003, 0.5419230461120605, 0.5618521372477213]}, 'weight': {'score': [0.006352487802505493, 0.0024536469513315538, 0.006239626895297657, 0.002438837629682643, 0.0011803338734003215], 'topk_tokens': [' the', ' Where', ' Does', 'Question', '.', '<|eot_id|>', '.\n\n', ' the', '<|eot_id|>', '?\n', 'Answer', 'b', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005110949277877808, 0.0026186704635620117, 0.012392973899841307, 0.008091568946838379, 0.00848996639251709]}, 'saliency': {'score': [0.0002531611919403076, 2.7529171104091806e-05, 0.00019501149654388428, 2.6765168836307042e-05, 5.958000054726234e-05], 'topk_tokens': ['.', '      ', 'Question', ' the', ' the', '<|eot_id|>', ' the', 'Does', '<|start_header_id|>', '<|eot_id|>', '      ', ' Does', 'assistant', '<|end_header_id|>', 'Answer', 'athroom', '\n\n', 'b', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.3320317268371582e-05, 8.954107761383057e-05, 0.00045950412750244145, 0.0003734678030014038, 0.00033992528915405273]}}, 30: {'grad': {'score': [0.41590087890625, 0.4135018711935342, 0.42763519287109375, 0.41347148382095317, 0.3892914790373582], 'topk_tokens': ['2', 'B', ' months', ' the', ' the', ' his', 'itter', ' two', ' itself', ' of', '3', ' Burb', ' account', 'b', 'deal', ' forb', ' B', ' B', 'b', 'b'], 'evidence_proportions': [0.47822316487630206, 0.43917083740234375, 0.331561279296875, 0.4562530517578125, 0.38144683837890625]}, 'weight': {'score': [0.015762989521026612, 0.0024722255045838584, 0.012694009325721047, 0.0024265800560768873, 0.004147347922508533], 'topk_tokens': [' the', 'Gov', ' barric', 'Bridge', ' the', ' the', 'Question', '<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', 'b', 'assistant', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0014842450618743896, 0.01567554473876953, 0.020294839143753053, 0.023758411407470703, 0.020993207891782124]}, 'saliency': {'score': [0.0006802058219909667, 6.189936522531536e-05, 0.00037211586128581655, 6.0073834803189174e-05, 6.275050915204562e-05], 'topk_tokens': [' fifteen', ' Wide', ' the', ' Bench', ' the', 'Answer', 'assistant', ' barric', ' milk', ' Bridge', ' the', ' bathroom', '.\n\n', 'Bridge', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', ':', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.00012006858984629312, 0.001033879816532135, 0.0007769286632537842, 0.000930391252040863, 0.0007571677366892496]}}, 31: {'grad': {'score': [0.41312040328979494, 0.5185296007207897, 0.45643147555264557, 0.5188574221765879, 0.37522655381606174], 'topk_tokens': [' the', ' an', ' August', ' the', ' they', ' the', ' the', ' the', ' the', ' was', ' had', ' location', ' the', ' location', ' the', ' the', 'did', ' he', ' the', ' the'], 'evidence_proportions': [0.3871908187866211, 0.3750190734863281, 0.44988250732421875, 0.4690380096435547, 0.39653738339742023]}, 'weight': {'score': [0.0038774585723876953, 0.002238663324947446, 0.00424602898684415, 0.0022316887283762733, 0.0012319583732348222], 'topk_tokens': [' was', ' the', ',', ':', 'Question', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003899584213892619, 0.004697591066360474, 0.0016988694667816162, 0.0040441155433654785, 0.008522590001424154]}, 'saliency': {'score': [5.461812019348145e-05, 2.8413239376093333e-05, 8.15527005629106e-05, 2.826378909209e-05, 1.7704012302251962e-05], 'topk_tokens': ['Just', ' dropped', ' left', ' the', ' discarded', '.\n\n', 'Bridge', ' Market', ' the', '<|eot_id|>', '\n\n', ' the', '?\n', '<|begin_of_text|>', ':', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.138448715209961e-05, 6.870925426483154e-05, 1.582503318786621e-05, 2.1226704120635986e-05, 0.0001430461804072062]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:04:48.855 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:04:48.855 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-0_2-3-4-5-6.pkl | len: 10 |  size: 9.51 KB
Processing depth (2, 3, 4, 5, 6):   1%|          | 1/100 [00:18<31:09, 18.89s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.11it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]
Processing depth (0, 4, 6, 7, 9):   1%|          | 1/100 [00:26<31:09, 18.89s/it]2025-01-22 03:04:57.007 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:04:57.008 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:04:57.008 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:04:57.030 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4873, 4877) -->  Daniel grabbed the milk
2025-01-22 03:04:57.030 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:04:57.063 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7219, 7224) --> . Daniel went to the
2025-01-22 03:04:57.064 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:04:57.101 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8415, 8419) -->  Daniel left the milk
2025-01-22 03:04:57.102 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:04:57.153 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10791, 10797) --> . Mary journeyed to the
2025-01-22 03:04:57.154 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:04:57.195 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8690, 8696) -->  Sandra journeyed to the bedroom
2025-01-22 03:04:57.195 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football there.
2025-01-22 03:04:57.224 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (9446, 9451) --> . Mary got the football
2025-01-22 03:04:57.224 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:04:57.238 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4876, 4882) -->  milk. John went back to
2025-01-22 03:04:57.238 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:04:57.244 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1942, 1947) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:04:59.319 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:04:59.319 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12244])
your chose emoji: ['🙆\u200d♂', '🧑🏽\u200d🦯', '🧍🏿', '🧑🏿\u200d❤\u200d💋\u200d🧑🏻', '〽️', '🧑🏻\u200d✈', '👩\u200d🦯\u200d➡', '👩🏿\u200d❤️\u200d👨🏼', '🧑🏻\u200d🤝\u200d🧑🏿', '⭕']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132104.06it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.34s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.05s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.97it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.00it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.92it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.89it/s][A100%|██████████| 8/8 [00:00<00:00, 16.87it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.73it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.27it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.06it/s][A100%|██████████| 8/8 [00:00<00:00, 17.09it/s]
2025-01-22 03:05:08.954 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.26930110931396484, 0.2806058690393566, 0.22636751695112747, 0.2807268413950185, 0.38003096288564253], 'topk_tokens': ['.', '.', ' examined', '�', ' out', ' state', 'user', ' war', ' Do', '.', 'low', ' S', ' out', ' absence', ' first', ' St', '�', '�', '.', ' turtle'], 'evidence_proportions': [0.2572046915690104, 0.21022629737854004, 0.2850362777709961, 0.2412402629852295, 0.3263753255208333]}, 'weight': {'score': [0.02313068151473999, 0.0025635347314160086, 0.013402865691618486, 0.002501842604308832, 0.0004971623420715332], 'topk_tokens': [' bathroom', ' barric', ' milk', 'Answer', ' the', 'Bridge', ' bedroom', '<|eot_id|>', ' bathroom', ':', 'assistant', '<|eot_id|>', ' Bridge', 'b', '<|start_header_id|>', '\n\n', ' garden', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01828487714131673, 0.03198385238647461, 0.0383393406867981, 0.03483724594116211, 0.0015961130460103354]}, 'saliency': {'score': [0.002106863260269165, 3.975996769057631e-05, 0.0008574466813694347, 3.404958326308454e-05, 1.5710993688933702e-05], 'topk_tokens': ['<|eot_id|>', ':', 'Answer', ' Daniel', ' milk', 'Daniel', '<|eot_id|>', ' Daniel', ' bedroom', 'Bridge', '<|begin_of_text|>', '<|start_header_id|>', ' Bridge', 'b', ' Daniel', ' bathroom', ' bedroom', ' bathroom', 'athroom', ' garden'], 'evidence_proportions': [0.0021626303593317666, 0.004028484225273132, 0.002189403772354126, 0.003080010414123535, 5.24669885635376e-05]}}, 25: {'grad': {'score': [0.6522269439697266, 0.7319984613578836, 0.5386949018998579, 0.7325105077712262, 0.45378696675203284], 'topk_tokens': [' first', ' im', ' the', ' the', ' favored', ' with', 'ivery', ' a', ' by', ' no', ' a', ' free', ' with', ' containing', ' bogus', ' money', ' of', ' inverted', ' l', ' no'], 'evidence_proportions': [0.8297322591145833, 0.6689605712890625, 0.6185932159423828, 0.6143035888671875, 0.516876220703125]}, 'weight': {'score': [0.018071895837783812, 0.002485670926201594, 0.007565421136942777, 0.002444571735428982, 0.00045712292194366455], 'topk_tokens': [' discarded', ' prior', ' bathroom', ' to', ' Daniel', ' Bench', 'b', '?\n', 'Answer', ' garden', ' Daniel', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004712879657745361, 0.04277229309082031, 0.021319478750228882, 0.03329277038574219, 0.002110411723454793]}, 'saliency': {'score': [0.0005253815650939942, 3.628211429462322e-05, 0.00033579089424826884, 3.473976352175728e-05, 1.4773436955043248e-05], 'topk_tokens': [' to', ' Mary', 'assistant', '<|start_header_id|>', ' bathroom', 'Answer', ' Daniel', 'Gov', ':', ' THE', ' garden', ' Geo', ' Dan', ' Bench', '<|eot_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00027112166086832684, 0.0016020014882087708, 0.00027534961700439455, 0.0008754134178161621, 3.6900242169698075e-05]}}, 26: {'grad': {'score': [0.3677424621582031, 0.4117674545296807, 0.39818815751509234, 0.4118821569348945, 0.39482891316316565], 'topk_tokens': [' general', 'char', 'agle', ' into', 'graph', ' Gutenberg', 'graph', 'hue', ' Milwaukee', 'issippi', 'rich', ' bitter', ' Eagle', 'ers', 'itter', 'ente', 'ers', 'UX', ' Marshall', 'UX'], 'evidence_proportions': [0.36436971028645837, 0.5076980590820312, 0.3543720245361328, 0.34418487548828125, 0.30465857187906903]}, 'weight': {'score': [0.01533677339553833, 0.0024628195751343123, 0.01039811833338304, 0.002422128967085823, 0.0003720382038427859], 'topk_tokens': [' bedroom', '.\n\n', ' Bridge', ' bathroom', ' barric', '<|eot_id|>', '<|eot_id|>', ' the', ' bathroom', '?\n', ' garden', 'Answer', 'b', 'assistant', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.017317180832227073, 0.015656471252441406, 0.021521621942520143, 0.02252674102783203, 0.0031958818435668945]}, 'saliency': {'score': [0.0007239353656768799, 4.2905756490244386e-05, 0.000264513221654025, 4.111058399325511e-05, 1.287886074611119e-05], 'topk_tokens': [' Dan', ' Daniel', ' Daniel', ' bedroom', '185', 'Bridge', '?\n', 'Answer', 'assistant', ' Bridge', ' bathroom', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', '\n\n', ' garden', ':', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0013093352317810059, 0.0007857605814933777, 0.0004994332790374756, 0.0009227916598320007, 0.00015183289845784503]}}, 27: {'grad': {'score': [0.3109169006347656, 0.3500323799801737, 0.31518216566606, 0.35017537929972664, 0.31377113108732263], 'topk_tokens': ['-n', ' one', 'str', '\n', 'roduced', ' short', 'ides', ' excessive', ' designated', 'prev', ' conventions', '\n', 'ers', ' received', ' Bottle', ' exchanged', ' convention', '\n', ' step', ' accepted'], 'evidence_proportions': [0.2883402506510417, 0.32608795166015625, 0.32457962036132815, 0.4236030578613281, 0.23686981201171875]}, 'weight': {'score': [0.019483906030654908, 0.0025206994375735195, 0.00852439891208302, 0.0024751124250107126, 0.0004970592503644982], 'topk_tokens': ['<|eot_id|>', ' Bridge', ' Daniel', ' barric', ' bathroom', ' bedroom', ' THE', '?\n', 'Answer', 'assistant', '.\n\n', ' bathroom', 'b', '\n\n', ' garden', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0224502831697464, 0.031421661376953125, 0.021916139125823977, 0.02650165557861328, 0.001853664716084798]}, 'saliency': {'score': [0.0011792576313018799, 4.629672396141159e-05, 0.00031617554751309484, 4.348841602685022e-05, 1.661449062580965e-05], 'topk_tokens': [' Daniel', ' dropped', ' the', ' to', ' prior', 'assistant', ' bathroom', ' the', '<|begin_of_text|>', 'Gov', 'NEW', ' Bridge', ' THE', '<|start_header_id|>', ':', '<|end_header_id|>', ' garden', '.\n\n', 'athroom', 'b'], 'evidence_proportions': [0.0008228421211242676, 0.0013712495565414429, 0.002383506298065186, 0.0017333179712295532, 3.476440906524658e-05]}}, 28: {'grad': {'score': [0.2910301208496094, 0.33610645679490386, 0.35295226357199927, 0.3361684486514232, 0.3809218990559481], 'topk_tokens': [' before', ' probably', '600', ' half', ' lie', ' summer', ' returns', ' summer', ' balance', 'about', ' spring', 'half', ' summer', ' summer', ' platform', ' summer', ' half', ' summer', 'dent', 'ball'], 'evidence_proportions': [0.389312744140625, 0.2792234420776367, 0.249114990234375, 0.230560302734375, 0.2758611043294271]}, 'weight': {'score': [0.015264571905136108, 0.00238391106553279, 0.009749706495891918, 0.0023442336868067255, 0.0002853286509611169], 'topk_tokens': [' Bridge', ' the', ' the', ' to', ' the', '<|eot_id|>', '?\n', ' garden', 'Answer', '<|eot_id|>', ' bathroom', ' the', 'assistant', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0029612233241399126, 0.006512999534606934, 0.03669952154159546, 0.026439189910888672, 0.00809009869893392]}, 'saliency': {'score': [0.00039043068885803224, 4.4516914278436724e-05, 0.0004264766519719904, 4.3119294721572124e-05, 6.680585900131537e-06], 'topk_tokens': [' the', ' milk', ' the', '.\n\n', '?\n', 'Answer', ' the', ' garden', ' Bridge', 'Bridge', ' bathroom', 'b', 'athroom', '\n\n', 'assistant', ' Bridge', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':'], 'evidence_proportions': [0.000186920166015625, 8.688867092132568e-05, 0.0008893966674804687, 0.0003078281879425049, 0.00043556590874989826]}}, 29: {'grad': {'score': [0.552706298828125, 0.4307461935831224, 0.46177153153852984, 0.4304403280039303, 0.3262875809961436], 'topk_tokens': [' Paul', ' The', ' Pioneer', ' P', ' The', ' The', ' Ch', ' THE', 'Spring', ' Paul', ' The', ' In', 'ER', 'UL', 'ION', ' Pioneer', ' M', ' In', ' THE', 'ION'], 'evidence_proportions': [0.692169189453125, 0.4114837646484375, 0.437481689453125, 0.5098609924316406, 0.6319758097330729]}, 'weight': {'score': [0.007114882469177246, 0.002439690851003012, 0.005464767867868597, 0.0024246554833943727, 0.00039236490823784656], 'topk_tokens': [' milk', ' Does', ' where', ' to', '.\n\n', '?\n', ' the', ' was', '<|eot_id|>', ' the', 'Answer', 'b', ' the', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00038808584213256836, 0.004677772521972656, 0.010851049423217773, 0.022062182426452637, 0.0023880799611409502]}, 'saliency': {'score': [0.00019329547882080078, 3.3361559703786723e-05, 0.00011525641788135876, 3.288614701052181e-05, 1.993988241468157e-05], 'topk_tokens': ['athroom', '<|eot_id|>', ' place', 'NEW', ' a', 'Does', ' was', '<|end_header_id|>', ' to', '<|eot_id|>', ' where', ' Does', 'assistant', 'Answer', ' the', 'b', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.0469228426615398e-05, 4.903227090835571e-05, 0.00037072896957397457, 0.0005955174565315247, 4.62879737218221e-05]}}, 30: {'grad': {'score': [0.28786773681640626, 0.3642564220526583, 0.30644989013671875, 0.3645171975307777, 0.34310808960272343], 'topk_tokens': ['moment', ' of', ' B', ' the', ' S', ' itself', 'itter', ' Buchanan', ' the', 'B', ' account', ' Burb', '3', ' forb', 'b', 'deal', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.24419275919596356, 0.33734130859375, 0.3161041259765625, 0.39992523193359375, 0.20032501220703125]}, 'weight': {'score': [0.014108016490936279, 0.002455101642373475, 0.0124618492343209, 0.0024131777638294657, 0.001409779093703445], 'topk_tokens': [' to', ' barric', ':', ' the', 'Gov', 'Question', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', 'assistant', 'Answer', '?\n', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0036582350730895996, 0.01927328109741211, 0.024887192249298095, 0.027185916900634766, 0.0034130414326985674]}, 'saliency': {'score': [0.00098313570022583, 7.44132866341211e-05, 0.00040966949679634786, 7.19465901617144e-05, 1.851241199337706e-05], 'topk_tokens': [' Daniel', ' the', 'Answer', ' the', ' garden', ' Bench', 'Gov', ' the', '?\n', '.\n\n', ' bathroom', 'assistant', ' Bridge', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ':', 'b', '<|start_header_id|>', 'athroom'], 'evidence_proportions': [0.0011639346679051716, 0.001044541597366333, 0.0013285458087921142, 0.001625753939151764, 4.5145551363627114e-05]}}, 31: {'grad': {'score': [0.36806556820869446, 0.4751982914337559, 0.3917033022100275, 0.4755683904373255, 0.30214437294979485], 'topk_tokens': [' that', ' the', ' the', '7', ' paper', ' the', ' the', ' the', 'membership', ' August', ' they', ' the', ' was', ' had', ' the', ' the', ' the', ' he', ' an', ' the'], 'evidence_proportions': [0.27513249715169275, 0.4037252143025398, 0.4621417999267578, 0.36017894744873047, 0.36408642927805585]}, 'weight': {'score': [0.004420356750488281, 0.0022364462444984137, 0.004988477988676591, 0.0022270083378572933, 0.0004819452154393099], 'topk_tokens': [' where', ' to', 'Question', ' was', ':', '.\n\n', ' Where', ' the', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007982800404230753, 0.005068928003311157, 0.005790644884109497, 0.011427879333496094, 0.0017964641253153482]}, 'saliency': {'score': [4.576444625854492e-05, 2.0757730010870457e-05, 9.550289674238725e-05, 2.0571700373633963e-05, 4.990976683947505e-06], 'topk_tokens': ['light', ' a', 'Just', ' left', ' Where', ' bedroom', '.\n\n', ' the', 'Question', ' prior', '<|start_header_id|>', '<|eot_id|>', ' the', '?\n', '<|begin_of_text|>', 'Answer', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.7111500104268394e-05, 6.327033042907715e-05, 8.491873741149903e-05, 5.251169204711914e-05, 2.5620063145955403e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:05:08.956 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:05:08.956 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-1_0-4-6-7-9.pkl | len: 10 |  size: 9.6 KB
Processing depth (0, 4, 6, 7, 9):   2%|▏         | 2/100 [00:38<32:00, 19.60s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.17it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]
Processing depth (1, 2, 3, 6, 9):   2%|▏         | 2/100 [00:46<32:00, 19.60s/it]2025-01-22 03:05:16.585 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:05:16.591 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1472, 1478) --> . Daniel journeyed to the
2025-01-22 03:05:16.591 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:05:16.598 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2454, 2458) -->  Daniel grabbed the milk
2025-01-22 03:05:16.598 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:05:16.609 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3787, 3792) --> . Daniel went to the
2025-01-22 03:05:16.609 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:05:16.628 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7206, 7210) -->  Daniel left the milk
2025-01-22 03:05:16.628 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:05:16.659 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10753, 10759) --> . Mary journeyed to the
2025-01-22 03:05:16.659 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:05:16.685 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8693, 8699) --> . Sandra journeyed to the
2025-01-22 03:05:16.685 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football there.
2025-01-22 03:05:16.710 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (9434, 9439) --> . Mary got the football
2025-01-22 03:05:16.711 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:05:16.724 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4870, 4876) --> . John went back to the
2025-01-22 03:05:16.724 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:05:16.730 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1931, 1936) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:05:18.748 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:05:18.748 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12194])
your chose emoji: ['🧑🏼', '🖐🏿', '🦧', '🌨', '🔑', '🚵\u200d♀', '👵🏿', '👸🏽', '🆔', '🤦🏻\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 64403.90it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.26s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.03s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.64it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.40it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.79it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.93it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.59it/s][A100%|██████████| 8/8 [00:00<00:00, 17.65it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.05it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.07it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.11it/s][A100%|██████████| 8/8 [00:00<00:00, 21.38it/s]
2025-01-22 03:05:27.994 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.21438613891601563, 0.21014937234822087, 0.23655371232466263, 0.2100928443940088, 0.3073275883992513], 'topk_tokens': [' appearance', '.', ' compromised', ' and', ' communication', ' S', 'ols', '�', ' advantage', '�', ' Hor', ' turtle', ' absence', ' compl', ' comparison', ' appearance', 'able', '�', ' examined', 'consider'], 'evidence_proportions': [0.13164583841959637, 0.1929769515991211, 0.22482070922851563, 0.22837448120117188, 0.29337819417317706]}, 'weight': {'score': [0.017263227701187135, 0.002575023942299824, 0.0038152729923074894, 0.002542555582376174, 0.0009342264384031296], 'topk_tokens': ['Answer', ' Daniel', ' garden', ' Market', '<|eot_id|>', ' bathroom', ' Bench', 'assistant', ' Broadway', '<|eot_id|>', ':', ' bathroom', '.', 'Bridge', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009947434067726135, 0.014657020568847656, 0.009176242351531982, 0.0635533332824707, 0.002195576826731364]}, 'saliency': {'score': [0.0007336497306823731, 3.357970627583548e-05, 7.247112014076926e-05, 3.206881230750692e-05, 1.3856838146845499e-05], 'topk_tokens': [' Miles', ' milk', '<|eot_id|>', '.', ' Fort', 'b', ':', 'Official', '<|eot_id|>', ' Daniel', '<|end_header_id|>', '<|begin_of_text|>', ' garden', ' Market', 'Bridge', ' bathroom', '<|start_header_id|>', ' bathroom', ' Bench', 'athroom'], 'evidence_proportions': [0.0002497086922327677, 0.0006776303052902222, 0.00037263035774230956, 0.003014206886291504, 3.5415093104044594e-05]}}, 25: {'grad': {'score': [0.39185943603515627, 0.4274609102776297, 0.3901598670265891, 0.4276017053235215, 0.3981706003348033], 'topk_tokens': [' a', ' for', ' the', 'posit', ' incorporated', ' the', ' little', ' "', ' the', ' York', ' old', ' for', ' self', ' lie', ' inverted', ' old', ' of', ' for', ' a', ' no'], 'evidence_proportions': [0.3748575846354167, 0.4998893737792969, 0.4254302978515625, 0.40485191345214844, 0.30020395914713544]}, 'weight': {'score': [0.017776613235473634, 0.0024968086524782603, 0.0043786398389122705, 0.0024619612943978957, 0.0010735349108775456], 'topk_tokens': [' bathroom', ' Paul', '.', ' Daniel', ' Dan', ' the', ' Market', '?\n', 'b', '<|eot_id|>', ' Daniel', '<|eot_id|>', 'assistant', ':', ' Bench', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006725187102953592, 0.014719009399414062, 0.019184809923171998, 0.04691886901855469, 0.010264774163564045]}, 'saliency': {'score': [0.00044104933738708496, 2.7361102423590108e-05, 0.000131877985867587, 2.6320643385741936e-05, 1.927092671394348e-05], 'topk_tokens': ['b', ' Minnesota', ' bathroom', ' bathroom', ' Anthony', ':', ' Gov', ' George', 'Gov', ' Market', ' Ramsey', '?\n', ' Ramsey', 'athroom', ' Dan', ' Gov', '<|end_header_id|>', '\n\n', ' Bench', '<|begin_of_text|>'], 'evidence_proportions': [0.00012229382991790771, 0.0006268247961997986, 0.00045601129531860354, 0.0011474639177322388, 0.00015254318714141846]}}, 26: {'grad': {'score': [0.2968335723876953, 0.28759280226118616, 0.28159956498579547, 0.2875846402831529, 0.35300515095392865], 'topk_tokens': [' prepared', ' steam', 'itter', ' and', ' steam', 'graph', ' Giants', ' Eagle', 'AM', 'ub', ' Press', '-in', 'ente', ',', 'UX', 'UX', 'oth', ' bitter', ' Gutenberg', ' Marshall'], 'evidence_proportions': [0.33437601725260413, 0.3787345886230469, 0.37194366455078126, 0.22695159912109375, 0.1886866887410482]}, 'weight': {'score': [0.013646667003631591, 0.002469155772197361, 0.0034000697461041536, 0.0024444711723445372, 0.0011357540885607402], 'topk_tokens': [' Father', 'Bridge', '<|eot_id|>', ' bathroom', ' Daniel', '<|eot_id|>', '?\n', ' the', 'assistant', 'Answer', ' Bench', 'b', ' bathroom', ' the', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004142875472704569, 0.0048288702964782715, 0.0076791346073150635, 0.04809713363647461, 0.011034955581029255]}, 'saliency': {'score': [0.0009773933887481689, 3.8758011893228377e-05, 0.00012854689901525325, 3.666408268021948e-05, 2.1690502762794495e-05], 'topk_tokens': [' Daniel', ' Broadway', ' Hotel', '?\n', ' Charles', ' Dan', ' Anthony', ' Bench', '<|end_header_id|>', 'Answer', ' the', ' bathroom', ' Father', 'athroom', ' Daniel', '\n\n', '<|start_header_id|>', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0002397596836090088, 0.00038585811853408813, 0.0006298363208770753, 0.00413794070482254, 0.0002919832865397135]}}, 27: {'grad': {'score': [0.20958797454833986, 0.20658236612518704, 0.17112766612659802, 0.20664037955641257, 0.2592001458009084], 'topk_tokens': [' mile', ' short', ' Cyrus', ' avenue', '.', ' time', ' excessive', ' dre', '\n', ' medicine', 'atter', ' business', ' accepted', ' was', 'ree', ' step', ' Bre', ' Thanksgiving', 'CE', ' Bottle'], 'evidence_proportions': [0.2905006408691406, 0.19511008262634277, 0.20489196777343752, 0.18379592895507812, 0.15943527221679688]}, 'weight': {'score': [0.016046605110168456, 0.002528942949783414, 0.003198513930494135, 0.0024999164382126108, 0.001319010431567828], 'topk_tokens': [' Daniel', ' garden', 'Bridge', ' Bench', '?\n', 'Answer', ' Broadway', '.', ' Daniel', 'assistant', '.\n\n', ' bathroom', 'b', '\n\n', ':', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006022835771242778, 0.00916290283203125, 0.015182489156723022, 0.055441856384277344, 0.005116105079650879]}, 'saliency': {'score': [0.0007295155525207519, 4.089906331583292e-05, 0.00022265179590745404, 3.915305736133591e-05, 5.354173481464386e-05], 'topk_tokens': [' Ramsey', 'NEW', ' Broadway', 'Bridge', ' Daniel', ' the', ' THE', ' Daniel', 'Gov', 'THE', ' bathroom', ' bathroom', '<|begin_of_text|>', ':', '<|end_header_id|>', '.', 'athroom', '.\n\n', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.00011186798413594565, 0.00048045814037323, 0.0009841799736022949, 0.002591051161289215, 5.9957305590311684e-05]}}, 28: {'grad': {'score': [0.33382843017578123, 0.36464744268249877, 0.27390783483331854, 0.36487515845907076, 0.37423038482666016], 'topk_tokens': ['nes', ' the', ' ', ',', ' the', '.', ' lie', 'antic', 'en', ' the', ' a', 'nes', 'ien', 'ew', ' the', 'ivery', ' lie', 'ien', '.', 'dent'], 'evidence_proportions': [0.326934814453125, 0.33623313903808594, 0.33968658447265626, 0.256134033203125, 0.3860333760579427]}, 'weight': {'score': [0.01365684986114502, 0.00239508459096347, 0.0035041502930901265, 0.002369904049630028, 0.0006858153889576594], 'topk_tokens': [' to', ' Daniel', ' bathroom', 'Answer', ' garden', '<|eot_id|>', ' discarded', ' the', '<|eot_id|>', '?\n', 'assistant', ' the', ' the', 'b', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008078763882319132, 0.0015566349029541016, 0.0028653919696807863, 0.03134346008300781, 0.031774441401163735]}, 'saliency': {'score': [0.0006265246868133545, 2.9806493295649384e-05, 8.316608992489901e-05, 2.848205988299209e-05, 1.3512248794237772e-05], 'topk_tokens': [' to', ' Floral', ' Market', ' Broadway', ' Dr', 'b', 'athroom', ' the', 'Bridge', ' the', ' Bridge', ' Daniel', ' the', 'assistant', '<|end_header_id|>', ' Bridge', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.3064355850219727e-05, 7.259100675582886e-05, 0.0001910865306854248, 0.0018775910139083862, 0.0011080950498580933]}}, 29: {'grad': {'score': [0.2434925079345703, 0.2458788534065754, 0.28107140280983667, 0.24582004044772177, 0.2458726167678833], 'topk_tokens': ['ball', 'ION', 'ION', ' not', 's', ' Pioneer', ' The', ' a', ' extra', ' S', ' S', 'Spring', 're', ' M', ' B', 'b', ' B', ' B', 'b', ' ga'], 'evidence_proportions': [0.20405260721842447, 0.27320098876953125, 0.2434112548828125, 0.2565956115722656, 0.25445906321207684]}, 'weight': {'score': [0.005483349561691284, 0.002465124490701479, 0.003092607313936407, 0.0024577779681594285, 0.0009149741381406784], 'topk_tokens': [' was', ' was', '<|eot_id|>', ' Does', 'Does', '.\n\n', ' the', '<|eot_id|>', ' the', 'Answer', '?\n', 'assistant', 'b', ' the', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005505780378977458, 0.0011736154556274414, 0.0009991943836212159, 0.007741451263427734, 0.015520672003428142]}, 'saliency': {'score': [0.00021907687187194824, 2.7748307820528347e-05, 0.0001314810731194236, 2.716679877214471e-05, 2.7275333801905315e-05], 'topk_tokens': ['Does', ' Mary', ' the', ' the', 'THE', 'THE', '<|eot_id|>', ' Does', ' the', '<|eot_id|>', 'Answer', ' the', '<|end_header_id|>', 'assistant', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [1.270572344462077e-05, 4.353374242782593e-05, 6.519556045532226e-05, 0.0003510117530822754, 0.0005827546119689941]}}, 30: {'grad': {'score': [0.268258056640625, 0.3548388773958888, 0.30966359918767755, 0.3550988260904948, 0.304377943277359], 'topk_tokens': ['ar', ' account', 'b', ' S', ' hour', ' need', ' of', 'b', ' soon', ' the', ' years', 'moment', ' Europe', '2', ' the', ' itself', ' forb', '3', ' account', 'deal'], 'evidence_proportions': [0.2620671590169271, 0.4773101806640625, 0.21562080383300783, 0.35225868225097656, 0.12294483184814453]}, 'weight': {'score': [0.00941685438156128, 0.002479162354581501, 0.0060856315222653475, 0.0024583570358684523, 0.005188959961136182], 'topk_tokens': [' the', '.', ' Anthony', ' Daniel', ' the', ' Miles', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' Broadway', 'Answer', 'assistant', 'b', '?\n', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0021748393774032593, 0.0023417770862579346, 0.005237513780593872, 0.03199958801269531, 0.009803215662638346]}, 'saliency': {'score': [0.00043892383575439454, 4.5348474058135764e-05, 0.00013679672371257436, 4.437306290300785e-05, 0.00012945197522640228], 'topk_tokens': ['.', ' milk', ' Fort', ' Ramsey', ' Bridge', 'Gov', 'Bridge', ' Daniel', '.\n\n', ' the', ' Bridge', ' bathroom', ' Bench', 'Official', ':', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'b', '<|start_header_id|>'], 'evidence_proportions': [8.469820022583008e-05, 7.453560829162598e-05, 0.0002723217010498047, 0.001654118299484253, 0.0003647804260253906]}}, 31: {'grad': {'score': [0.2694572448730469, 0.3155915142327359, 0.23083626140247693, 0.3158399070143209, 0.17845407128334045], 'topk_tokens': [' office', ' the', ' they', ' location', ' could', 'membership', ' evening', ' he', ' the', ' location', ' is', ' the', ' August', ' the', ' paper', ' the', 'did', ' the', ' the', 'If'], 'evidence_proportions': [0.2797826131184896, 0.22642850875854492, 0.31609792709350587, 0.28101205825805664, 0.24124725659688315]}, 'weight': {'score': [0.0031179165840148925, 0.0022807176400278695, 0.0029809380119497127, 0.002277727119716597, 0.0014706434061129887], 'topk_tokens': [' Market', 'Question', ':', ' the', ',', ' milk', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', '?\n', 'b', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005455414454142252, 0.0006708875298500061, 0.0024461090564727784, 0.008949041366577148, 0.003994067509969075]}, 'saliency': {'score': [6.925702095031739e-05, 2.152861176215792e-05, 5.6292523037303575e-05, 2.1367458159050334e-05, 2.251441280047099e-05], 'topk_tokens': [' Daniel', ' Bre', ' Key', ' the', ' the', ' S', ' discarded', '<|eot_id|>', '<|eot_id|>', ' Emily', ' Market', ' the', '<|begin_of_text|>', '?\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.7156203587849934e-05, 2.1539628505706787e-05, 0.00014352798461914062, 0.00014799833297729492, 3.878275553385417e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:05:27.996 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:05:27.997 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-2_1-2-3-6-9.pkl | len: 10 |  size: 9.56 KB
Processing depth (1, 2, 3, 6, 9):   3%|▎         | 3/100 [00:58<31:16, 19.35s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.19it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.22it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]
Processing depth (0, 3, 4, 5, 9):   3%|▎         | 3/100 [01:05<31:16, 19.35s/it]2025-01-22 03:05:35.810 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:05:35.811 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:05:35.811 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:05:35.821 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3752, 3756) -->  Daniel grabbed the milk
2025-01-22 03:05:35.821 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:05:35.835 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4872, 4877) --> . Daniel went to the
2025-01-22 03:05:35.835 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:05:35.851 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5968, 5972) -->  Daniel left the milk
2025-01-22 03:05:35.851 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:05:35.882 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10779, 10785) --> . Mary journeyed to the
2025-01-22 03:05:35.882 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:05:35.907 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8747, 8753) --> . Sandra journeyed to the
2025-01-22 03:05:35.907 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football there.
2025-01-22 03:05:35.934 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (9573, 9578) --> . Mary got the football
2025-01-22 03:05:35.934 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:05:35.948 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4876, 4882) -->  the garden. John went back
2025-01-22 03:05:35.949 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:05:35.954 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1933, 1938) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:05:37.964 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:05:37.965 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12220])
your chose emoji: ['🚴🏼\u200d♀️', '🚶🏼\u200d♂', '🤜🏽', '🙅🏿\u200d♀', '🙍🏿', '🏇🏻', '💠', '🙋🏽', '🙇🏾\u200d♀️', '\U0001faf1🏿\u200d\U0001faf2🏻']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 88301.14it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.55s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.48s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.46it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.52it/s][A100%|██████████| 8/8 [00:00<00:00, 20.68it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.70it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.17it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.17it/s][A100%|██████████| 8/8 [00:00<00:00, 17.74it/s]
2025-01-22 03:05:47.280 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.19411754608154297, 0.2234559238985928, 0.20155772295865146, 0.22355572842110472, 0.3390566465016958], 'topk_tokens': ['ob', ' the', ' out', '�', ' first', ' S', ' and', ' examined', '�', ' appearance', '�', ' sop', '.', ' absence', ' first', ' S', ' turtle', 'consider', 'ols', ' Do'], 'evidence_proportions': [0.21587689717610675, 0.12273788452148438, 0.16513195037841796, 0.20670127868652344, 0.23571014404296875]}, 'weight': {'score': [0.03258224248886108, 0.0025689771012901764, 0.02318282425403595, 0.0024701074994464113, 0.0003653414346076347], 'topk_tokens': ['Answer', ' composing', ' bedroom', ' milk', '<|eot_id|>', ' Daniel', ' Daniel', 'assistant', ' the', '<|eot_id|>', ':', 'Bridge', '<|start_header_id|>', '\n\n', 'b', ' garden', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.018982648849487305, 0.07015037536621094, 0.007470846176147461, 0.09238338470458984, 0.0021951496601104736]}, 'saliency': {'score': [0.0021643614768981933, 3.798970280444184e-05, 0.00205131010575728, 2.9986060950195273e-05, 6.1380702096062735e-06], 'topk_tokens': ['<|eot_id|>', ' milk', ':', ' milk', ' bedroom', '<|eot_id|>', ' building', ' composing', ' bathroom', ' Bridge', ' bedroom', ' Daniel', '<|begin_of_text|>', ' the', 'b', ' Daniel', 'athroom', 'Bridge', ' bathroom', ' garden'], 'evidence_proportions': [0.0016062905391057334, 0.004047006368637085, 0.00023189187049865723, 0.0067269280552864075, 3.6016106605529785e-05]}}, 25: {'grad': {'score': [0.4663222122192383, 0.5146214627872402, 0.45587678389115766, 0.5148267735789546, 0.37420002189842433], 'topk_tokens': [' a', ' NEW', ' the', ' bogus', ' locom', ' new', ' free', ' little', ' the', ' incorporated', ' for', ' money', ' the', ' im', ' a', ' of', ' a', ' bogus', ' no', ' inverted'], 'evidence_proportions': [0.6258036295572916, 0.48229217529296875, 0.3397216796875, 0.43534159660339355, 0.4223483403523763]}, 'weight': {'score': [0.019543037414550782, 0.002486709486118639, 0.011607736349105835, 0.0024352089285302257, 0.0005040031832617682], 'topk_tokens': [' THE', '.\n\n', ' Bench', ' bathroom', ' garden', ' Daniel', 'Answer', '?\n', 'b', ' the', '<|eot_id|>', ' Daniel', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006367305914560953, 0.0520172119140625, 0.010653495788574219, 0.04092741012573242, 0.004221022129058838]}, 'saliency': {'score': [0.0008446216583251953, 2.2930547608434302e-05, 0.00019144740971651944, 2.093895359280545e-05, 1.4564475497683963e-05], 'topk_tokens': ['.', 'Answer', '<|start_header_id|>', ' bathroom', ' milk', ' composing', ' Dan', ' Daniel', ':', 'Print', '<|eot_id|>', '<|eot_id|>', ' Geo', '\n\n', ' THE', ' Bench', 'athroom', ' Daniel', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002337197462717692, 0.002848714590072632, 0.0002848029136657715, 0.0016729235649108887, 3.377596537272136e-05]}}, 26: {'grad': {'score': [0.3717230987548828, 0.4036300473173014, 0.3541663776744496, 0.40378493188088577, 0.4007637951825116], 'topk_tokens': [' Empire', 'graph', ' Press', 'ian', ' Marshall', 'ente', 'issippi', ' Gutenberg', 'graph', 'ers', ' medicine', 'rich', ' Eagle', 'UX', ' Milwaukee', ' Press', 'itter', 'hue', ' bitter', 'UX'], 'evidence_proportions': [0.33109537760416663, 0.4817657470703125, 0.40533447265625, 0.38507080078125, 0.3020811080932617]}, 'weight': {'score': [0.01661080598831177, 0.002456668238002774, 0.012403790246356617, 0.0024096338976659225, 0.00036404020077473407], 'topk_tokens': [' discarded', 'Bridge', ' barric', ' garden', ' Daniel', ' bathroom', '<|eot_id|>', '<|eot_id|>', '?\n', ' bathroom', 'Answer', 'b', 'assistant', '\n\n', '<|start_header_id|>', ' the', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.018117035428682964, 0.028293132781982422, 0.006009888648986816, 0.03122854232788086, 0.006405298908551534]}, 'saliency': {'score': [0.0005759239196777344, 3.164149116769532e-05, 0.00040579113093289464, 2.984793394138246e-05, 5.961672679798023e-06], 'topk_tokens': [' Geo', ' Col', ' Daniel', ' Daniel', ' the', ' Dan', ' bathroom', ' bathroom', '?\n', 'Bridge', ' garden', ' the', 'Answer', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0007150620222091675, 0.0006341785192489624, 0.0002922236919403076, 0.001279786229133606, 0.00016512473424275717]}}, 27: {'grad': {'score': [0.20839088439941406, 0.24624745393066605, 0.2110030000860041, 0.24638886262176854, 0.22701389725143845], 'topk_tokens': [' like', 'str', ' also', ' was', 'am', ' than', ' avenue', '-n', ' short', ' were', 'ly', ' step', ' accepted', ' were', ' be', ' business', 'CE', ' was', ' was', ' Bottle'], 'evidence_proportions': [0.20071156819661456, 0.2268695831298828, 0.24499855041503904, 0.23695755004882812, 0.15420023600260416]}, 'weight': {'score': [0.02674789071083069, 0.0025233694977078785, 0.020946918563409286, 0.0024403429610970456, 0.0005234246318404739], 'topk_tokens': [' Daniel', 'Daniel', ' barric', '?\n', ' the', ' bathroom', ' THE', 'Answer', ' Daniel', 'assistant', '.\n\n', ' garden', 'b', '\n\n', ' bathroom', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.027951205770174663, 0.0576174259185791, 0.009539198875427247, 0.049611568450927734, 0.004063010215759277]}, 'saliency': {'score': [0.001396433115005493, 3.610569521983497e-05, 0.0009144666520032016, 3.1725592842709845e-05, 1.8964748124818544e-05], 'topk_tokens': [' prior', 'Gov', ' dropped', ' Dan', 'Daniel', ' Daniel', ' Bridge', 'Bridge', ':', ' garden', ' bathroom', ' the', 'NEW', '<|start_header_id|>', '<|begin_of_text|>', ' THE', 'b', '<|end_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.0008006195227305095, 0.002325303852558136, 0.0007038116455078125, 0.004193194210529327, 8.567670981089275e-05]}}, 28: {'grad': {'score': [0.2646775817871094, 0.2914499368506095, 0.26574091477827594, 0.29155135828311435, 0.2798549355687322], 'topk_tokens': [' the', ' the', '<|end_header_id|>', ' the', 'nes', ' the', 'in', 'ivery', 'ien', 'ew', 'nes', ' half', '\n', 'antic', ' lively', ' a', ' lie', ' lie', 'dent', ','], 'evidence_proportions': [0.3138097127278646, 0.25630974769592285, 0.19546203613281252, 0.15337753295898438, 0.35300366083780926]}, 'weight': {'score': [0.012834123373031615, 0.002377455610793185, 0.012101754546165466, 0.002338415591851484, 0.00027673550554223965], 'topk_tokens': ['.\n\n', ' to', ' Bridge', ' the', '<|eot_id|>', ' the', ' the', 'Answer', '?\n', '<|eot_id|>', ' bathroom', 'assistant', 'b', ' the', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0034902642170588174, 0.009508132934570312, 0.004671239852905273, 0.029879093170166016, 0.019834399223327637]}, 'saliency': {'score': [0.0003745365142822266, 3.2271104544379226e-05, 0.0004202127456665039, 3.086741274509104e-05, 5.174730275128339e-06], 'topk_tokens': ['athroom', 'Answer', 'During', ' the', ' garden', ' milk', ' the', '?\n', ' the', ' Bridge', 'b', 'assistant', '<|start_header_id|>', '\n\n', 'Bridge', ' Bridge', '<|end_header_id|>', ' the', ':', '<|begin_of_text|>'], 'evidence_proportions': [6.6414475440979e-05, 0.00037561357021331787, 0.00024124383926391602, 0.000635780394077301, 0.0006188551584879557]}}, 29: {'grad': {'score': [0.32358642578125, 0.27131875166182606, 0.25656110590154474, 0.2712380992598665, 0.24121982986862595], 'topk_tokens': [' Pioneer', ' Paul', ' B', 'b', ' B', ' a', 'UL', ' ga', ' Paul', ' In', ' THE', ' The', ' In', 're', ' P', ' Ch', ' M', ' The', 'ION', 'ION'], 'evidence_proportions': [0.406280517578125, 0.2123870849609375, 0.2743408203125, 0.23978424072265625, 0.4119313557942708]}, 'weight': {'score': [0.0073868858814239505, 0.0024437542721613293, 0.008330908688631926, 0.002422967832657887, 0.0003163282935683792], 'topk_tokens': [' where', ' to', ' Does', ' the', '<|eot_id|>', ' was', ' the', '?\n', '.\n\n', '<|eot_id|>', 'Answer', 'b', 'assistant', ' the', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.001973355809847514, 0.005912899971008301, 0.012755393981933594, 0.015492916107177734, 0.0039052963256835938]}, 'saliency': {'score': [0.00023131489753723146, 2.295835790780058e-05, 0.00016356733712283048, 2.227649924867883e-05, 1.3911643543758908e-05], 'topk_tokens': [' the', '.', ' to', 'IVE', ' where', ' place', '<|eot_id|>', 'athroom', 'NEW', '<|start_header_id|>', ' Does', '<|end_header_id|>', '<|eot_id|>', 'assistant', 'Answer', '      ', ':', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00010149180889129639, 0.00022388994693756104, 0.0003326952457427979, 0.0004838034510612488, 0.00011327862739562988]}}, 30: {'grad': {'score': [0.23906486511230468, 0.3268852492922858, 0.32013450969349255, 0.3270777612728766, 0.2790419088827597], 'topk_tokens': [' S', 'g', ' the', ' years', 'moment', ' Burb', ' Times', ' its', ' account', ' of', '2', ' the', ' B', ' B', 'b', ' forb', 'b', 'b', ' account', 'deal'], 'evidence_proportions': [0.2199529012044271, 0.2705230712890625, 0.22510185241699218, 0.35640907287597656, 0.17061106363932294]}, 'weight': {'score': [0.015316171646118164, 0.0024602146144000266, 0.014483324506066063, 0.0024120947603092245, 0.0012508642834586066], 'topk_tokens': [' Daniel', ' Where', ' the', ':', 'Question', ' barric', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Answer', ' the', 'assistant', '?\n', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004748046398162842, 0.02514934539794922, 0.007500076293945312, 0.04040813446044922, 0.009114285310109457]}, 'saliency': {'score': [0.001188225746154785, 5.320610834652416e-05, 0.0003368353301828558, 5.0363193282003316e-05, 2.489420207771095e-05], 'topk_tokens': ['.', ' Bench', ' garden', 'Gov', ' the', ' milk', ' Daniel', ' bathroom', '?\n', 'assistant', 'Bridge', ' Bridge', ' Daniel', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ':', 'b', 'athroom', '<|start_header_id|>'], 'evidence_proportions': [0.0008902698755264282, 0.0028161853551864624, 0.00027788877487182616, 0.002552032470703125, 0.0002502848704655965]}}, 31: {'grad': {'score': [0.28771079778671266, 0.32223498726655736, 0.2868986292318864, 0.3223697199056637, 0.20888153403191953], 'topk_tokens': [' they', ' an', ' department', ' paper', ' location', ' he', ' the', 'If', ' the', '7', ' could', ' the', 'did', ' was', ' the', ' August', ' the', 'membership', ' the', ' the'], 'evidence_proportions': [0.2055851618448893, 0.3044545650482178, 0.3482107281684875, 0.3222341537475586, 0.2852417429288228]}, 'weight': {'score': [0.003995715379714966, 0.002260067610579544, 0.004509692842310125, 0.0022524392474614053, 0.0005967911030795123], 'topk_tokens': [' bedroom', ' where', ' was', 'Question', ':', '.\n\n', ' the', '<|eot_id|>', ' Where', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008704413970311482, 0.004120782017707825, 0.001401257514953613, 0.01058506965637207, 0.0048067569732666016]}, 'saliency': {'score': [0.0001249074935913086, 2.0300016595914888e-05, 3.69074669751254e-05, 2.00552275984422e-05, 8.2133589564143e-06], 'topk_tokens': [' prior', ' the', ' the', ' Geo', ' was', ' discarded', ' bedroom', ' Daniel', '<|eot_id|>', ' Daniel', ' the', ':', 'Answer', '?\n', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.3416012128194174e-05, 0.00024484843015670776, 6.601810455322265e-05, 0.0003604218363761902, 4.850327968597412e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:05:47.282 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:05:47.282 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-3_0-3-4-5-9.pkl | len: 10 |  size: 9.54 KB
Processing depth (0, 3, 4, 5, 9):   4%|▍         | 4/100 [01:17<30:55, 19.33s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.14it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.18it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]
Processing depth (0, 3, 5, 7, 8):   4%|▍         | 4/100 [01:25<30:55, 19.33s/it]2025-01-22 03:05:55.100 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:05:55.101 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:05:55.101 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:05:55.111 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3754, 3758) -->  Daniel grabbed the milk
2025-01-22 03:05:55.111 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:05:55.128 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5957, 5962) --> . Daniel went to the
2025-01-22 03:05:55.128 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:05:55.151 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8381, 8385) -->  Daniel left the milk
2025-01-22 03:05:55.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:05:55.179 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9670, 9676) -->  Mary journeyed to the office
2025-01-22 03:05:55.179 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:05:55.203 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8678, 8684) -->  business. Sandra journeyed to
2025-01-22 03:05:55.204 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football there.
2025-01-22 03:05:55.230 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (9447, 9452) --> . Mary got the football
2025-01-22 03:05:55.230 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:05:55.244 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4874, 4880) --> . John went back to the
2025-01-22 03:05:55.244 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:05:55.251 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1920, 1925) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:05:57.268 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:05:57.269 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12229])
your chose emoji: ['🧰', '👨🏽\u200d❤️\u200d💋\u200d👨🏽', '🖥️', '👩🏿\u200d❤\u200d👨🏻', '💃', '👨🏿\u200d🦰', '🦹🏻\u200d♂', '🥭', '👨🏿\u200d❤\u200d👨🏻', '🗝']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 208412.62it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.86s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.30s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.89it/s][A100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:00<00:00,  7.73it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 12.71it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 15.59it/s][A
100%|██████████| 8/8 [00:00<00:00, 19.04it/s][A100%|██████████| 8/8 [00:00<00:00, 16.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.48it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.04it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.95it/s][A100%|██████████| 8/8 [00:00<00:00, 21.20it/s]
2025-01-22 03:06:06.283 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3137885284423828, 0.3494817360777571, 0.300679857080633, 0.3496430796582923, 0.4773108987923128], 'topk_tokens': [' of', ' were', 'low', '.', ' out', ' type', ' S', '�', ' St', ' out', 'user', '�', '.', ' Do', ' first', '�', '�', '�', '.', ' turtle'], 'evidence_proportions': [0.3275763193766276, 0.2854270935058594, 0.32621116638183595, 0.2987022399902344, 0.31861368815104163]}, 'weight': {'score': [0.02842177152633667, 0.002569461897980395, 0.006178809837861495, 0.00250990396647557, 0.0009463282234697457], 'topk_tokens': [' Daniel', ' bathroom', 'Answer', ' bedroom', ' bedroom', ' the', 'Bridge', ' Bridge', '<|eot_id|>', ':', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'b', ' garden', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.021675586700439453, 0.014273405075073242, 0.049887692928314215, 0.05220937728881836, 0.01085352897644043]}, 'saliency': {'score': [0.0026976609230041503, 4.8130840803767595e-05, 0.00028299472548744893, 4.2270745808441325e-05, 4.3535807046545555e-05], 'topk_tokens': ['.', ':', 'NEW', ' office', '<|eot_id|>', 'Answer', ' milk', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', ' Bench', ' bedroom', ' bedroom', ' Daniel', ' Daniel', ' bathroom', 'b', ' bathroom', ' garden', 'athroom'], 'evidence_proportions': [0.0027944097916285195, 0.001483425498008728, 0.003492468595504761, 0.005177587270736694, 0.0010947783788045247]}}, 25: {'grad': {'score': [0.8335948944091797, 0.9569753643108241, 0.8128160996870561, 0.957488783725618, 0.6034550494458302], 'topk_tokens': [' by', ' the', 'ivery', ' a', ' with', ' M', ' a', ' of', ' ideas', ' containing', ' for', ' lie', ' no', ' M', ' free', ' money', ' im', ' l', ' inverted', ' no'], 'evidence_proportions': [0.99578857421875, 0.77996826171875, 0.8060180664062501, 0.8509063720703125, 0.7185920079549154]}, 'weight': {'score': [0.023006265163421632, 0.002491420277432728, 0.0046198449351570825, 0.0024454870427491286, 0.0009925803506230734], 'topk_tokens': [' milk', '.', ' THE', ' Daniel', ' garden', ' bathroom', ' Daniel', ' the', 'Answer', ' Bench', 'b', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.008518020311991373, 0.015856504440307617, 0.03389319181442261, 0.06063532829284668, 0.00810253620147705]}, 'saliency': {'score': [0.001144866943359375, 4.7029426688068904e-05, 0.00017831271344965154, 4.453995847095463e-05, 3.887910440743688e-05], 'topk_tokens': [' bathroom', ' Daniel', ' Daniel', ' garden', 'Answer', ' Peter', 'b', '<|start_header_id|>', ' there', '<|eot_id|>', ':', '<|eot_id|>', ' THE', ' bathroom', 'athroom', '.', ' Bench', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007612754901250203, 0.0011338144540786743, 0.0015930294990539552, 0.0024991557002067566, 0.00025949875513712567]}}, 26: {'grad': {'score': [0.5465233612060547, 0.5600294932732791, 0.5693815404718573, 0.5600403187360048, 0.5396510204636907], 'topk_tokens': [' in', ' Marshall', ' ice', 'graph', 'hue', 'graph', ' Gutenberg', 'ente', ' into', 'issippi', 'ers', ' Milwaukee', 'ub', 'rich', ' bitter', ' Marshall', 'UX', 'ers', 'UX', 'itter'], 'evidence_proportions': [0.5358721415201823, 0.6952171325683594, 0.589971923828125, 0.43260717391967773, 0.4977823893229167]}, 'weight': {'score': [0.02168003797531128, 0.0024586443498759896, 0.004375925118272955, 0.00241574611290097, 0.0007265511765537491], 'topk_tokens': [' bedroom', ' barric', ' the', '?\n', ' the', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' garden', 'Answer', 'assistant', ' bathroom', ' the', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.023474226395289104, 0.004821658134460449, 0.038120722770690924, 0.038950443267822266, 0.005910595258076986]}, 'saliency': {'score': [0.0010474801063537597, 5.355598423276718e-05, 0.00017098676074634898, 5.1304726117352845e-05, 3.1715775110635416e-05], 'topk_tokens': ['185', ' Daniel', ' Merch', ' old', ' the', ' Daniel', ' Merch', 'assistant', '?\n', 'Answer', '<|start_header_id|>', ' bathroom', ' bathroom', '<|end_header_id|>', ':', ' garden', 'athroom', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.002040281891822815, 0.00026144087314605713, 0.0015253305435180664, 0.0011843442916870117, 8.925298849741618e-05]}}, 27: {'grad': {'score': [0.41461395263671874, 0.46753754001415343, 0.3766463886607777, 0.46781022733579564, 0.3698986237307629], 'topk_tokens': ['ments', '10', '\n', 'ition', ' Bottle', ' conventions', ' one', '\n', ' designated', 'arch', ' convention', ' excessive', ' exchanged', ' received', 'ers', '\n', 'roduced', 'ides', ' step', ' accepted'], 'evidence_proportions': [0.3524805704752604, 0.4354248046875, 0.46863861083984376, 0.5401725769042969, 0.33414713541666663]}, 'weight': {'score': [0.023322325944900513, 0.0025249534061798635, 0.0037565773183649235, 0.0024800596811461262, 0.0008214564926653024], 'topk_tokens': ['<|eot_id|>', ' Daniel', ' bedroom', 'NEW', '?\n', ' the', ' THE', ' bathroom', 'Answer', 'assistant', '.\n\n', 'b', '\n\n', ' bathroom', ' garden', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.027760634819666546, 0.013481378555297852, 0.029595273733139037, 0.04629397392272949, 0.004902760187784831]}, 'saliency': {'score': [0.0017515933513641358, 5.755870998334542e-05, 0.00032220645384355026, 5.3605233052738585e-05, 3.2078429877039896e-05], 'topk_tokens': [' dropped', ' the', ' bathroom', ' the', ' prior', '<|end_header_id|>', 'Bridge', 'THE', ' bathroom', ' the', '<|begin_of_text|>', ' Bridge', ' THE', '<|start_header_id|>', 'NEW', '.\n\n', ':', ' garden', 'athroom', 'b'], 'evidence_proportions': [0.0012727975845336914, 0.0005250647664070129, 0.004028773307800293, 0.003179222345352173, 0.00019867221514383954]}}, 28: {'grad': {'score': [0.43159423828125, 0.41656721675216646, 0.4584680037064986, 0.41646073395764677, 0.4045522873660168], 'topk_tokens': [' proof', '50', '.', 'nes', 'half', 'in', ' as', ' summer', ' preceding', ' lie', ' probably', 'ball', ' spring', ' scale', ' returns', ' half', ' lie', '600', ' platform', 'dent'], 'evidence_proportions': [0.5248311360677084, 0.3205146789550781, 0.4443115234375, 0.2904930114746094, 0.495880126953125]}, 'weight': {'score': [0.0172482168674469, 0.0023730785102919084, 0.003862762993032282, 0.002339869522392921, 0.0005567307213702833], 'topk_tokens': ['.\n\n', ' Bridge', ' the', ' garden', '?\n', ' the', ' the', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', ' bathroom', 'b', ' the', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003505989909172058, 0.003252744674682617, 0.036161398887634276, 0.03703880310058594, 0.01136604944864909]}, 'saliency': {'score': [0.0007568514347076416, 5.3800050439672424e-05, 0.00011599741198799826, 5.224530061933903e-05, 1.2265989579350115e-05], 'topk_tokens': ['?\n', ' bedroom', '.\n\n', 'Answer', ' the', ' the', ' Bridge', ' the', 'athroom', ' garden', '\n\n', 'assistant', ' bathroom', 'Bridge', 'b', '<|begin_of_text|>', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', ':'], 'evidence_proportions': [0.00027908881505330407, 7.91698694229126e-05, 0.001949399709701538, 0.0007533133029937744, 0.0006949702898661296]}}, 29: {'grad': {'score': [0.65583740234375, 0.4698518063212782, 0.5395434986461293, 0.46934439088166324, 0.46071132981633567], 'topk_tokens': ['THE', 'THE', ' Ch', ' Pioneer', 'Spring', 're', ' PA', 's', ' THE', 'ION', ' In', ' The', ' Paul', ' The', 'ER', ' P', 'UL', ' THE', 'ION', ' M'], 'evidence_proportions': [0.82952880859375, 0.494842529296875, 0.590087890625, 0.5711746215820312, 0.7007090250651041]}, 'weight': {'score': [0.011832902431488037, 0.002437422055680429, 0.002308096398006786, 0.002418378818509615, 0.0005480819437877241], 'topk_tokens': [' milk', '<|eot_id|>', '?\n', ' was', ' the', ' where', ' the', '.\n\n', '<|eot_id|>', ' the', 'Answer', 'b', ' the', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005631645520528158, 0.0018506646156311035, 0.018557488918304443, 0.040204524993896484, 0.005239228407541911]}, 'saliency': {'score': [0.00034798622131347655, 5.0742564110135316e-05, 7.593089883977717e-05, 5.0087230930477076e-05, 2.6704676180000766e-05], 'topk_tokens': ['      ', ' place', 'IVE', ' ', '<|eot_id|>', ' where', ' the', 'Does', 'NEW', 'athroom', 'assistant', ' Does', '<|eot_id|>', '<|end_header_id|>', 'Answer', 'b', ':', '\n\n', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [3.4506122271219894e-05, 7.385015487670898e-05, 0.0005930840969085693, 0.0009637922048568726, 0.00022943814595540363]}}, 30: {'grad': {'score': [0.27544891357421875, 0.39261475507537863, 0.3591146469116211, 0.3929156289709167, 0.38553951447268564], 'topk_tokens': [' the', ' of', ' Europe', ' B', 'itter', ' Buchanan', 'B', ' itself', ' of', ' the', '3', ' account', ' Burb', ' forb', 'deal', 'b', ' B', ' B', 'b', 'b'], 'evidence_proportions': [0.2956034342447917, 0.336395263671875, 0.239971923828125, 0.34300994873046875, 0.1991869608561198]}, 'weight': {'score': [0.018782280683517456, 0.0024503332247836985, 0.0062290172685276375, 0.0024100025119867586, 0.001515150429254555], 'topk_tokens': ['Question', ':', ' the', ' milk', ' barric', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', 'assistant', '?\n', 'Answer', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004715621471405029, 0.010413527488708496, 0.029497379064559938, 0.04449653625488281, 0.01235602299372355]}, 'saliency': {'score': [0.0011739683151245116, 6.596197327263921e-05, 0.0003982403061606667, 6.308874538016212e-05, 2.725835306098662e-05], 'topk_tokens': ['?\n', ' office', ' Daniel', ' the', ' bathroom', ' to', '.', ' the', ' Bench', 'assistant', '.\n\n', ' the', ' Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', ':', ' bathroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0009343971808751425, 0.0006874948740005493, 0.0011664628982543945, 0.00159386545419693, 0.001464178164800008]}}, 31: {'grad': {'score': [0.42439550399780274, 0.5003457200379057, 0.3570238229903308, 0.5007603147967115, 0.35253218283136206], 'topk_tokens': [' their', ' the', ' the', ' the', ' the', ' the', ' would', ' the', ' the', ' was', ' the', ' had', ' paper', ' they', ' the', ' the', ' the', ' he', ' the', ' an'], 'evidence_proportions': [0.2736024856567383, 0.44074463844299316, 0.5500674247741699, 0.4363226890563965, 0.4516110420227051]}, 'weight': {'score': [0.005413278341293335, 0.0022506261667135574, 0.0030139671130613847, 0.002242759133050517, 0.0007634837943387319], 'topk_tokens': [' was', ' bedroom', ' Peter', 'Question', ' the', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009374270836512247, 0.0038195550441741943, 0.005077922344207763, 0.015627145767211914, 0.004421830177307129]}, 'saliency': {'score': [3.7115812301635745e-05, 2.3437916082521916e-05, 6.097148765217174e-05, 2.3342086293436143e-05, 9.258827531194113e-06], 'topk_tokens': [' bedroom', ' left', 'light', ' Market', ':', ' the', ' Where', '.\n\n', '?\n', ' the', ' Peter', '.', '<|eot_id|>', '<|begin_of_text|>', 'Answer', '<|start_header_id|>', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [1.8974145253499348e-05, 3.3892691135406494e-05, 4.2438507080078125e-05, 3.247708082199097e-05, 5.606313546498617e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:06:06.285 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:06:06.285 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-4_0-3-5-7-8.pkl | len: 10 |  size: 9.52 KB
Processing depth (0, 3, 5, 7, 8):   5%|▌         | 5/100 [01:36<30:24, 19.21s/it]Processing depth (0, 3, 5, 7, 8):   5%|▌         | 5/100 [01:36<30:34, 19.31s/it]
2025-01-22 03:06:06.522 | INFO     | __main__:<module>:72 - Selected idx: 6
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:76 - Needle: [' John went back to the bedroom.', ' Mary journeyed to the office.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' Mary got the football there.', ' John journeyed to the office.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 03:06:06.523 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.22it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.22it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]
Processing depth (3, 6, 7, 9):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:06:13.868 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:06:13.879 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3790, 3796) --> . Daniel journeyed to the
2025-01-22 03:06:13.880 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:06:13.899 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7212, 7217) --> . Daniel went to the
2025-01-22 03:06:13.900 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:06:13.922 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8424, 8428) -->  Daniel left the apple
2025-01-22 03:06:13.922 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John moved to the bedroom.
2025-01-22 03:06:13.952 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10784, 10789) -->  John moved to the bedroom
2025-01-22 03:06:13.952 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:06:13.957 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1679, 1685) --> . John went back to the
2025-01-22 03:06:13.957 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 03:06:13.961 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1252, 1258) -->  John journeyed to the office
2025-01-22 03:06:13.961 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:06:13.993 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11307, 11312) --> . Mary moved to the
2025-01-22 03:06:13.993 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary got the football there.
2025-01-22 03:06:13.997 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1566, 1571) --> . Mary got the football
2025-01-22 03:06:13.997 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John journeyed to the office.
2025-01-22 03:06:14.001 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1251, 1257) --> . John journeyed to the
2025-01-22 03:06:14.001 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 03:06:14.006 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1620, 1626) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:06:16.069 | INFO     | test_jbb_retain:begin_test:632 - Anthony street<|eot_id|>
2025-01-22 03:06:16.069 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12233])
your chose emoji: ['👩🏾\u200d🏫', '🕸️', '🦶🏻', '🧑🏾\u200d❤\u200d🧑🏽', '🤸🏿\u200d♂️', '👨🏿\u200d❤️\u200d💋\u200d👨🏾', '👩\u200d👦', '👨🏽\u200d🦼', '🌿', '💁\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 187454.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.20s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.39s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.78it/s][A100%|██████████| 8/8 [00:05<00:00,  1.45it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.68it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.44it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.15it/s][A100%|██████████| 8/8 [00:00<00:00, 21.54it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.54it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.13it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.00it/s][A100%|██████████| 8/8 [00:00<00:00, 21.26it/s]
2025-01-22 03:06:25.023 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.17969045639038086, 0.29275692269162207, 0.2720510819379021, 0.2930003414990142, 0.24954257228157736], 'topk_tokens': [' out', 'G', ' appearance', ' communication', ' absence', 'remark', ' describing', ' said', ' appearance', ' Cl', ' appearance', ' EAR', ' comparison', ' Ear', ' Do', ' STE', ' EAR', ' announced', ' OUT', 'consider'], 'evidence_proportions': [0.22865064938863117, 0.21984558105468752, 0.1092597246170044, 0.137127685546875]}, 'weight': {'score': [0.016734863817691802, 0.002563911623797334, 0.0016350640970117906, 0.0025432386449788166, 0.0006789537993344394], 'topk_tokens': [' Bench', ' \n', ' Fort', 'Answer', ' Daniel', ' Bridge', 'Bridge', ' Anthony', '<|eot_id|>', ' Broadway', 'assistant', ':', '.', 'b', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00246712068716685, 0.008407020568847656, 0.06333351135253906, 0.004905080795288086]}, 'saliency': {'score': [0.001226186752319336, 2.9648644115925613e-05, 7.963092888102812e-05, 2.7544707172395677e-05, 1.4675611799413508e-05], 'topk_tokens': [' Fort', '\n\n', ' Broadway', ' Shak', '<|eot_id|>', ' Bridge', ' Miles', '<|start_header_id|>', ' Market', '.', ' Ramsey', ':', 'Bridge', '<|end_header_id|>', ' Fort', ' Bench', 'athroom', 'b', ' Daniel', ' Anthony'], 'evidence_proportions': [5.527834097544353e-05, 0.00022153854370117189, 0.005493395030498505, 0.00022215843200683593]}}, 25: {'grad': {'score': [0.46211090087890627, 0.37977784160852196, 0.473178414737477, 0.37938198865565775, 0.272494684566151], 'topk_tokens': [' with', ' with', ' to', ' morning', 'a', ' were', ' for', ' bogus', ' of', ' no', ' a', ' black', ' the', ' for', ' the', ' a', ' the', ' for', ' at', ' printing'], 'evidence_proportions': [0.458465576171875, 0.37152099609375, 0.441680908203125, 0.573419189453125]}, 'weight': {'score': [0.019014327228069304, 0.0024895456032879254, 0.0016835007597418392, 0.0024646654433950455, 0.0013444613326679576], 'topk_tokens': [' Broadway', ' Ramsey', ' Ramsey', '.\n\n', 'b', ' \n', 'Answer', '.', ' Anthony', ' Daniel', ' Bench', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0023764818906784058, 0.008208942413330079, 0.07673835754394531, 0.0036059021949768066]}, 'saliency': {'score': [0.0027140453457832336, 3.077807565575606e-05, 2.5675577275893267e-05, 2.638701832053736e-05, 2.145665613087741e-05], 'topk_tokens': [' Merch', 'b', 'Answer', ' apple', ' Daniel', ' Dan', '<|start_header_id|>', ' Bench', ':', ' Ramsey', '.', ' Anthony', 'athroom', ' apple', '<|end_header_id|>', '\n\n', ' Ramsey', ' Ramsey', '<|begin_of_text|>', ' Daniel'], 'evidence_proportions': [0.00033357739448547363, 0.0006901204586029052, 0.0121101513504982, 7.764697074890136e-05]}}, 26: {'grad': {'score': [0.4108644723892212, 0.43692111376649845, 0.4273618810317096, 0.4369905725368587, 0.46415537053888495], 'topk_tokens': [' branches', ' emb', ' bonds', ' compl', ' bonds', 'bread', ' com', ' favor', ' bend', 'ree', 'b', 'b', ' bonds', ' bonds', 'b', 'RI', 'b', ' broad', 'itter', ' bitter'], 'evidence_proportions': [0.412524938583374, 0.3123928070068359, 0.4491729736328125, 0.47669677734375]}, 'weight': {'score': [0.014659453928470612, 0.00247386500273471, 0.0010216332533780265, 0.0024579122938990024, 0.0006562281738628041], 'topk_tokens': [' garden', ' barric', ' bathroom', ' Daniel', '?', ' Bench', ' apple', '<|eot_id|>', 'b', 'Answer', 'assistant', ' \n', '<|eot_id|>', ' Anthony', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007797926664352417, 0.005409431457519531, 0.05934715270996094, 0.004814910888671875]}, 'saliency': {'score': [0.0015392109751701355, 4.2723282932962375e-05, 5.7424692546620086e-05, 4.022536783104087e-05, 1.4393167062239213e-05], 'topk_tokens': [' garden', ' Charles', ' Fort', ' Shak', '?', ' apple', ' Seventh', ' Bench', 'assistant', ' Ramsey', ' Ramsey', 'athroom', '<|start_header_id|>', '\n\n', ' Daniel', 'b', '<|end_header_id|>', ':', ' Anthony', '<|begin_of_text|>'], 'evidence_proportions': [4.823505878448486e-05, 0.0003803253173828125, 0.007089495658874512, 4.7039985656738284e-05]}}, 27: {'grad': {'score': [0.2055363655090332, 0.3042352731884909, 0.2776959433275111, 0.30447138510516, 0.3561142466285012], 'topk_tokens': [' heard', ' even', ' producing', 'e', ' was', ' was', ',', ' as', 'ly', ' be', ' idea', '\n', ' would', ' ideas', ' much', ',', 'str', ' heard', ' was', ' STR'], 'evidence_proportions': [0.2961565653483073, 0.16245994567871094, 0.16917800903320312, 0.16895523071289062]}, 'weight': {'score': [0.014721514284610748, 0.002513674904061673, 0.0009988379829070147, 0.0024978603964035118, 0.000737164169549942], 'topk_tokens': [' bend', ' barric', ' Daniel', 'THE', ' \n', ' garden', ' Broadway', 'Answer', ' apple', ' bathroom', 'assistant', 'b', '.\n\n', ' Anthony', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0016029725472132366, 0.0045437335968017585, 0.05854082107543945, 0.005586099624633789]}, 'saliency': {'score': [0.0022101983428001403, 4.3511178786554304e-05, 6.140330258537742e-05, 3.990404732296608e-05, 4.603205756707625e-05], 'topk_tokens': [' apple', ' apple', ' the', 'THE', ' bathroom', 'NEW', ' Ramsey', '<|end_header_id|>', ' \n', 'assistant', '.', '<|begin_of_text|>', '\n\n', '<|start_header_id|>', 'athroom', ' Daniel', ' Anthony', ':', '.\n\n', 'b'], 'evidence_proportions': [0.00015699366728464764, 0.0005604207515716553, 0.009984724223613739, 0.00010420083999633788]}}, 28: {'grad': {'score': [0.2963665008544922, 0.38706331231228547, 0.4056069991167854, 0.3871604598149782, 0.3653873530301181], 'topk_tokens': ['\n', ' Cedar', '\n', '\n', '\n', 'S', '\n', 'antic', '\n', 'dent', ' the', '\n', '\n', 'P', 'arp', '\n', 'ien', '\n', '.', ' RID'], 'evidence_proportions': [0.38377507527669275, 0.26417846679687496, 0.17423248291015625, 0.3213714599609375]}, 'weight': {'score': [0.008989515900611877, 0.0024055039504650723, 0.0018579363822937012, 0.0023962228027319325, 0.0007035664536736228], 'topk_tokens': [' garden', ' to', ' bathroom', '<|eot_id|>', 'Answer', ' the', ' \n', '<|eot_id|>', 'b', 'assistant', ' garden', ' apple', '?', ' before', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0014402369658152263, 0.0056514739990234375, 0.024429798126220703, 0.009034466743469239]}, 'saliency': {'score': [0.00019421577453613282, 3.559849413441224e-05, 2.432570737950942e-05, 3.5369543973653116e-05, 1.2050298127261074e-05], 'topk_tokens': [' apple', ' \n', 'Bridge', ' the', 'Answer', ' garden', ' Anthony', '?', ' before', 'athroom', ' Bridge', 'assistant', 'b', ' Bridge', ' garden', '<|start_header_id|>', ':', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.714362780253092e-05, 0.0001901090145111084, 0.0005967095494270325, 6.481409072875977e-05]}}, 29: {'grad': {'score': [0.23006258010864258, 0.2747930032281791, 0.22540292319129496, 0.27500428800766075, 0.46843719482421875], 'topk_tokens': [' difficulty', ' locom', ' West', ' fireworks', ' H', 'Emp', ' face', ' faithfully', 't', ' Gen', 'ys', ' Get', ' a', ' Far', ' Bu', 'cret', ' He', '�', 'tal', ' ga'], 'evidence_proportions': [0.2799580891927083, 0.18510475158691406, 0.1401844024658203, 0.28704833984375]}, 'weight': {'score': [0.006879584491252899, 0.0024688973384731226, 0.0006578932790195241, 0.0024667105378628636, 0.0007362873716787858], 'topk_tokens': [' Does', 'Does', ' the', ' the', ' apple', '.\n\n', '<|eot_id|>', ' apple', ' \n', ' before', 'Answer', '?', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005751599868138632, 0.0027509212493896486, 0.026825428009033203, 0.00261688232421875]}, 'saliency': {'score': [7.506608963012696e-05, 1.880719444259938e-05, 8.839894743526683e-06, 1.874264923540991e-05, 2.866678617217324e-05], 'topk_tokens': [' apple', 'Question', ' Where', ' garden', ' \n', ' the', '<|eot_id|>', '<|eot_id|>', ' before', 'b', 'Answer', 'Does', 'athroom', ' Does', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', '<|begin_of_text|>'], 'evidence_proportions': [1.6873081525166828e-05, 5.043745040893555e-05, 0.00024339556694030762, 3.4862756729125975e-05]}}, 30: {'grad': {'score': [0.4278566360473633, 0.33540751771156835, 0.3899527157054228, 0.3351035020246116, 0.28788053447549994], 'topk_tokens': ['ar', 'AM', ' account', ' two', ' EVENTS', 'b', '186', 'CO', ' LINE', ' its', '2', ' an', ' first', 'g', ' United', ' of', ' forb', 'b', 'deal', 'b'], 'evidence_proportions': [0.3565266927083333, 0.36938934326171874, 0.6004905700683594, 0.43381271362304685]}, 'weight': {'score': [0.010840483009815216, 0.00246648002816556, 0.0021564513444900513, 0.002453597161280974, 0.002950110218741677], 'topk_tokens': [' Broadway', ' bathroom', '.\n\n', ' before', ' apple', ' garden', ' Anthony', '<|eot_id|>', 'b', 'assistant', '?', 'Answer', '<|eot_id|>', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002064054210980733, 0.008580589294433593, 0.03172016143798828, 0.006928348541259765]}, 'saliency': {'score': [0.00041593313217163085, 6.965880263042668e-05, 7.039659163531135e-05, 6.908824185083465e-05, 4.077397964217446e-05], 'topk_tokens': [' Ramsey', ' Merch', ' the', ' Miles', ' Third', ' the', ' Daniel', '.\n\n', ' Bridge', ' Bench', ' Bridge', ' bathroom', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ' Anthony', 'b', ':', 'athroom'], 'evidence_proportions': [0.00011453529198964438, 0.00034387707710266114, 0.0013224706053733826, 0.000124436616897583]}}, 31: {'grad': {'score': [0.31995769739151003, 0.260261361058844, 0.15229450955110438, 0.26046468943059004, 0.15532170642505994], 'topk_tokens': [' after', ' the', ' St', ' down', ' answer', ' population', ' cord', ' August', 'ian', ' organized', 'IT', ' hours', 'nes', ' could', ' department', ' Press', '4', ' the', ' apple', ' Democratic'], 'evidence_proportions': [0.28178564707438153, 0.3379315376281738, 0.36432331800460815, 0.3122978210449219]}, 'weight': {'score': [0.004251299798488617, 0.002278868611944622, 0.0012742973425809074, 0.0022784341019649375, 0.0010653541169383309], 'topk_tokens': ['Question', ':', ' the', ' Where', '.\n\n', ' apple', ' before', '<|eot_id|>', '?', 'Answer', ' \n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011337945858637493, 0.0029535293579101562, 0.0071451663970947266, 0.006974983215332031]}, 'saliency': {'score': [0.0002830371260643005, 2.0729147090207287e-05, 2.5274122462553135e-05, 2.0285813586501686e-05, 1.3914297927509654e-05], 'topk_tokens': [' Bre', 'ot', '<|eot_id|>', ' How', ':', ' garden', ' before', ' was', ' Daniel', 'Answer', ' apple', '?', ' Anthony', '<|begin_of_text|>', 'athroom', ':', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant'], 'evidence_proportions': [0.00012437502543131512, 0.0002213120460510254, 0.0008180513978004456, 0.00010714530944824218]}}, 'pred_res': 'Anthony street<|eot_id|>', 'score': 0}
2025-01-22 03:06:25.024 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:06:25.024 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-0_3-6-7-9.pkl | len: 10 |  size: 9.09 KB
Processing depth (3, 6, 7, 9):   1%|          | 1/100 [00:18<30:21, 18.40s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.23it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.47it/s]
Processing depth (1, 3, 6, 7):   1%|          | 1/100 [00:25<30:21, 18.40s/it]2025-01-22 03:06:32.675 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:06:32.680 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1537, 1543) --> . Daniel journeyed to the
2025-01-22 03:06:32.680 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:06:32.702 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3873, 3878) --> . Daniel went to the
2025-01-22 03:06:32.702 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:06:32.752 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7347, 7351) -->  Daniel left the apple
2025-01-22 03:06:32.753 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John moved to the bedroom.
2025-01-22 03:06:32.798 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8497, 8502) --> . John moved to the
2025-01-22 03:06:32.798 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:06:32.808 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1737, 1743) --> . John went back to the
2025-01-22 03:06:32.808 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 03:06:32.815 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1256, 1262) -->  John journeyed to the office
2025-01-22 03:06:32.816 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:06:32.849 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11280, 11285) --> . Mary moved to the
2025-01-22 03:06:32.849 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary got the football there.
2025-01-22 03:06:32.854 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1611, 1616) --> . Mary got the football
2025-01-22 03:06:32.854 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John journeyed to the office.
2025-01-22 03:06:32.858 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1255, 1261) --> . John journeyed to the
2025-01-22 03:06:32.858 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 03:06:32.863 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1642, 1648) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:06:35.077 | INFO     | test_jbb_retain:begin_test:632 - The apple was left in the garden.<|eot_id|>
2025-01-22 03:06:35.077 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12210])
your chose emoji: ['🇦🇬', '🧘🏿\u200d♀️', '🤸🏾\u200d♂', '🗞', '🧍🏻\u200d♀️', '🤟', '🤷🏿', '🙆🏻\u200d♀️', '🐢', '🧍🏿\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 198546.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.54s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.56it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.38it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.34it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.41it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.26it/s][A100%|██████████| 8/8 [00:00<00:00, 17.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.77it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.10it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.03it/s][A100%|██████████| 8/8 [00:00<00:00, 17.05it/s]
2025-01-22 03:06:44.605 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.1861642837524414, 0.2340251260743547, 0.22901265761431525, 0.2341178673173911, 0.295123291015625], 'topk_tokens': ['G', 'use', 'Bridge', ' and', ' first', 'remark', 'C', ' absence', 'est', ' compl', 'itter', 'deal', ' turtle', ' first', ' Cl', ' STE', ' Arr', ' Do', 'Mer', 'consider'], 'evidence_proportions': [0.1362323760986328, 0.21635589599609376, 0.0967254638671875, 0.2874420166015625]}, 'weight': {'score': [0.02217240780591965, 0.00257207876261823, 0.006057986441780539, 0.0025300911450544875, 0.0013046613106360802], 'topk_tokens': [' Bench', ' Daniel', ' bathroom', ' apple', ' the', '.', ' bedroom', 'Answer', ' apple', ':', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'b', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'Bridge', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011707097291946411, 0.006898760795593262, 0.08962249755859375, 0.008688020706176757]}, 'saliency': {'score': [0.0011734798550605773, 5.0919465211259053e-05, 0.00017453993068021888, 4.8727319177627094e-05, 1.696302340580867e-05], 'topk_tokens': ['\n\n', '<|eot_id|>', ' bedroom', ' garden', ' bedroom', ':', '<|eot_id|>', ' Market', ' office', '<|end_header_id|>', ' bathroom', ' Bench', '<|begin_of_text|>', '.', ' Bridge', ' Bridge', ' Daniel', 'b', 'athroom', 'Bridge'], 'evidence_proportions': [7.18832015991211e-05, 0.00026177167892456056, 0.005066335201263428, 0.0002928197383880615]}}, 25: {'grad': {'score': [0.410467529296875, 0.41245080805289447, 0.3977374469532686, 0.4124952130082779, 0.33574120448185846], 'topk_tokens': [' to', ' for', ' inverted', ' a', 'antic', ' of', ' the', ' with', ' incorporated', ' no', ' for', ' printing', ' at', ' black', 'a', ' free', ' with', ' of', ' a', ' a'], 'evidence_proportions': [0.3720715840657552, 0.43349609375, 0.4307518005371094, 0.4172866821289063]}, 'weight': {'score': [0.02112036496400833, 0.0025063576944667136, 0.0031749088974560007, 0.002473870574942701, 0.0018820308721982516], 'topk_tokens': [' Bottle', ' bottle', ' bathroom', 'Bridge', ' Daniel', 'b', ' \n', '.\n\n', 'Answer', '<|start_header_id|>', ' apple', ':', ' Bench', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0016435086727142334, 0.008455199003219605, 0.08350229263305664, 0.007252216339111328]}, 'saliency': {'score': [0.001511119306087494, 3.086149199612358e-05, 4.223777967340806e-05, 2.839484588518887e-05, 3.8735224650456353e-05], 'topk_tokens': [' Bottle', 'Bridge', ' Minnesota', ' Ramsey', ' Daniel', ' Geo', ' Ramsey', ' Daniel', '<|end_header_id|>', '.', ' Ramsey', ' Anthony', '<|eot_id|>', '<|eot_id|>', 'athroom', ' Bench', '\n\n', ' apple', ' Dan', '<|begin_of_text|>'], 'evidence_proportions': [0.00021179020404815674, 0.0008573174476623536, 0.006037406623363495, 0.00010308623313903809]}}, 26: {'grad': {'score': [0.4249305725097656, 0.39553362987633567, 0.41105304044835705, 0.39544187900766903, 0.401480219914363], 'topk_tokens': [' began', ' Eagle', ' bend', ' blot', ' and', 'b', ' favor', ' when', ' Press', ' favorable', ' bonds', ' I', 'RI', 'b', 'b', ' Press', ' broad', 'b', 'itter', ' bitter'], 'evidence_proportions': [0.43570963541666663, 0.459454345703125, 0.3577690124511719, 0.43120117187499996]}, 'weight': {'score': [0.01937723010778427, 0.0024766872021506365, 0.0024257091914906223, 0.002449030519384765, 0.0013266645945035494], 'topk_tokens': [' the', ' apple', ' garden', ' Bench', ' apple', 'b', 'assistant', '<|eot_id|>', ' \n', '<|eot_id|>', 'Answer', ' the', 'Bridge', ' bathroom', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005493958791097006, 0.002658277750015259, 0.08527088165283203, 0.00597466230392456]}, 'saliency': {'score': [0.0015099033713340758, 3.9640814981013136e-05, 0.00011006348273333381, 3.702549942622736e-05, 2.821546334486741e-05], 'topk_tokens': [' Jackson', ' La', '.', ' bathroom', ' Bridge', ' Bench', ' apple', ' bathroom', 'assistant', '<|start_header_id|>', ' Daniel', ' Anthony', 'athroom', ' apple', ':', 'Bridge', '\n\n', '<|end_header_id|>', 'b', '<|begin_of_text|>'], 'evidence_proportions': [9.28093989690145e-05, 0.0002686619758605957, 0.006674177944660187, 0.0003202378749847412]}}, 27: {'grad': {'score': [0.1767589569091797, 0.2578471830413801, 0.19997568691478057, 0.25814238786011096, 0.23926761333759014], 'topk_tokens': ['str', ' be', ' were', ' was', ' was', ' was', ' was', ' was', ' would', ' medicine', ' would', 'ad', '185', ' could', ' was', 'ly', ' was', ' would', ' would', ' was'], 'evidence_proportions': [0.2358385721842448, 0.182696533203125, 0.10417556762695312, 0.1579925537109375]}, 'weight': {'score': [0.02175602614879608, 0.002524289707679533, 0.0022566914558410645, 0.0024934042410901898, 0.0017017520391024077], 'topk_tokens': ['<|eot_id|>', ' apple', '<|eot_id|>', ' \n', ' garden', ' apple', 'assistant', ' Daniel', 'Answer', ' bathroom', 'Bridge', 'b', '<|start_header_id|>', '.\n\n', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009846687316894531, 0.00797763466835022, 0.0901033878326416, 0.0057821571826934814]}, 'saliency': {'score': [0.002499818801879883, 4.2607333222253895e-05, 0.0001478326671263751, 3.8271294837033673e-05, 6.015667548546424e-05], 'topk_tokens': [' apple', ' \n', 'Dub', ' the', 'NEW', ' bathroom', ' Dub', '.', 'assistant', ' apple', 'Bridge', ' bathroom', '<|begin_of_text|>', '\n\n', '<|end_header_id|>', ':', ' Daniel', 'athroom', '.\n\n', 'b'], 'evidence_proportions': [5.738933881123861e-05, 0.0009704113006591798, 0.010953649878501892, 0.00019707679748535157]}}, 28: {'grad': {'score': [0.21751003265380858, 0.252565755448216, 0.2834036770988913, 0.2525371860855847, 0.2291595092186561], 'topk_tokens': ['\n', ' became', ' ', '\n', ' in', ' a', 'nes', 'arp', '.', 'na', 'G', '185', 'dent', 'ot', ' half', 'half', ' RID', ' Cedar', ' half', 'ball'], 'evidence_proportions': [0.2773590087890625, 0.19773941040039061, 0.15300559997558594, 0.2170654296875]}, 'weight': {'score': [0.008236357569694519, 0.002413800919777266, 0.003213169820168439, 0.0024019882973895985, 0.001122443492595966], 'topk_tokens': [' Bridge', '.\n\n', '<|eot_id|>', 'Answer', ' \n', ' bathroom', ' the', '?', 'assistant', ' garden', '<|eot_id|>', ' before', ' apple', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0002749512592951457, 0.0033206701278686527, 0.02806997299194336, 0.006838840246200562]}, 'saliency': {'score': [0.00014877766370773314, 3.2036769383273606e-05, 6.165136309231028e-05, 3.1761934769193795e-05, 1.0715081141545223e-05], 'topk_tokens': [' Far', ' Dr', ' bathroom', ' to', '?', 'athroom', ' the', ' apple', ' before', ' garden', 'assistant', 'b', '<|start_header_id|>', ' Bridge', 'Bridge', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [7.907549540201822e-06, 4.826188087463379e-05, 0.0005653649568557739, 8.50677490234375e-05]}}, 29: {'grad': {'score': [0.19910316467285155, 0.2570599979434056, 0.2899963715497185, 0.2570632301140443, 0.36798612154447113], 'topk_tokens': [' not', 'adv', 'cret', 'doll', ' face', 'goods', 'pend', ' THE', ' extra', 't', ' enough', ' Gen', ' faithfully', ' Get', ' epith', ' Far', 'Emp', 'ys', 'tal', ' ga'], 'evidence_proportions': [0.2366154988606771, 0.150225830078125, 0.2034149169921875, 0.19951629638671875]}, 'weight': {'score': [0.0043416827917099, 0.0024831627077624126, 0.001364925328422995, 0.0024832325876224825, 0.0016774365535149208], 'topk_tokens': [' Where', ' the', '<|eot_id|>', ' \n', ' apple', ' before', ' the', '?', ' Does', '.\n\n', 'Answer', '<|eot_id|>', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [8.606910705566406e-05, 0.0014891386032104491, 0.012316346168518066, 0.0059212327003479006]}, 'saliency': {'score': [5.4270029067993164e-05, 2.2661553684054394e-05, 2.8725932626163258e-05, 2.2592604067168922e-05, 5.9301119584303635e-05], 'topk_tokens': ['NEW', 'E', ' garden', ' Where', 'THE', 'Answer', '<|eot_id|>', 'Does', ' before', ' the', '<|eot_id|>', '\n\n', 'b', ':', 'athroom', 'assistant', '<|end_header_id|>', ' Does', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [2.2699435551961264e-06, 2.6178359985351565e-05, 0.00013899058103561401, 7.698535919189453e-05]}}, 30: {'grad': {'score': [0.38680076599121094, 0.36032117235956307, 0.36307256362017465, 0.360269923147005, 0.3690841014568622], 'topk_tokens': [' OCC', ' of', 'SSION', 'CO', ' Million', ' United', 'ire', ' Europe', 'AM', ' soon', ' Times', ' the', ' forb', '2', ' Times', ' two', 'b', ' LINE', 'deal', ' account'], 'evidence_proportions': [0.42412567138671875, 0.236407470703125, 0.5835666656494141, 0.334991455078125]}, 'weight': {'score': [0.011237770318984985, 0.0024829175151517913, 0.004324285423054415, 0.0024633679169162987, 0.00613133769768935], 'topk_tokens': [' before', ' Anthony', 'Question', ' apple', ' garden', '.\n\n', '?', 'b', ' bathroom', 'assistant', 'Answer', '<|eot_id|>', ' \n', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007688403129577637, 0.003990447521209717, 0.03798103332519531, 0.009653198719024658]}, 'saliency': {'score': [0.0009142547845840454, 7.485978327911785e-05, 0.00017321460387286018, 7.320405797882294e-05, 0.00013696322074303261], 'topk_tokens': [' the', ' \n', ' garden', ' Daniel', ' bathroom', ' apple', 'assistant', ' Bench', ' the', ' Bridge', '.\n\n', '<|begin_of_text|>', 'Bridge', '<|start_header_id|>', '<|end_header_id|>', ' Bridge', ' bathroom', 'b', ':', 'athroom'], 'evidence_proportions': [0.00010276337464650473, 0.0002472221851348877, 0.003540724515914917, 0.0004539012908935547]}}, 31: {'grad': {'score': [0.3252019390463829, 0.29507387630244314, 0.22876465495894938, 0.29520973881340623, 0.15395755034226638], 'topk_tokens': [' down', ' the', ' Press', ' the', ' the', 'membership', ' population', 'nes', ' location', ' location', ' was', ' Democratic', ' could', ' the', ' August', ' location', ' department', ' the', ' the', ' apple'], 'evidence_proportions': [0.3450937271118164, 0.3323033571243286, 0.3721984252333641, 0.25663318634033205]}, 'weight': {'score': [0.0025908857583999633, 0.0022811397701479885, 0.0013307648546555463, 0.0022832877944395998, 0.0016889260365412786], 'topk_tokens': [' the', ' the', ':', ' before', 'Question', '.\n\n', ' Where', '?', '<|eot_id|>', ' \n', 'Answer', 'b', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003556261459986369, 0.0011622905731201172, 0.006149947643280029, 0.0038545429706573486]}, 'saliency': {'score': [0.0001830771565437317, 2.0483177904706714e-05, 2.296794863308177e-05, 2.020878348266993e-05, 1.984697121840257e-05], 'topk_tokens': [' garden', '<|eot_id|>', ' the', ' Market', ' Bridge', ' the', ' Where', ' Daniel', ' apple', 'Question', ' \n', ' the', 'Answer', '<|begin_of_text|>', ':', 'athroom', '<|end_header_id|>', 'b', '<|start_header_id|>', 'assistant'], 'evidence_proportions': [4.1365623474121094e-05, 8.73863697052002e-05, 0.0006317943334579468, 8.984804153442382e-05]}}, 'pred_res': 'The apple was left in the garden.<|eot_id|>', 'score': 0}
2025-01-22 03:06:44.607 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:06:44.607 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-1_1-3-6-7.pkl | len: 10 |  size: 9.14 KB
Processing depth (1, 3, 6, 7):   2%|▏         | 2/100 [00:37<31:11, 19.09s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]
Processing depth (0, 4, 5, 8):   2%|▏         | 2/100 [00:45<31:11, 19.09s/it]2025-01-22 03:06:51.967 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:06:51.968 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:06:51.968 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:06:51.982 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4928, 4933) --> . Daniel went to the
2025-01-22 03:06:51.982 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:06:51.998 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5990, 5994) -->  Daniel left the apple
2025-01-22 03:06:51.998 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John moved to the bedroom.
2025-01-22 03:06:52.025 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9636, 9641) --> . John moved to the
2025-01-22 03:06:52.025 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:06:52.030 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1679, 1685) --> . John went back to the
2025-01-22 03:06:52.030 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 03:06:52.034 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1252, 1258) -->  John journeyed to the office
2025-01-22 03:06:52.034 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:06:52.066 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11299, 11304) --> . Mary moved to the
2025-01-22 03:06:52.066 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary got the football there.
2025-01-22 03:06:52.070 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1566, 1571) --> . Mary got the football
2025-01-22 03:06:52.070 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John journeyed to the office.
2025-01-22 03:06:52.074 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1251, 1257) --> . John journeyed to the
2025-01-22 03:06:52.074 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 03:06:52.079 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1620, 1626) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:06:54.176 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:06:54.176 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12230])
your chose emoji: ['🏃🏾\u200d♂\u200d➡️', '🧑🏿\u200d❤️\u200d💋\u200d🧑🏽', '🌯', '🏹', '🛌🏾', '🏃\u200d♀\u200d➡️', '🔴', '🧑🏽\u200d💼', '👨\u200d👦\u200d👦', '🏃🏾\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 220752.84it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.35s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.43s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.73it/s][A100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.29it/s][A
 50%|█████     | 4/8 [00:00<00:00, 18.25it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 21.00it/s][A100%|██████████| 8/8 [00:00<00:00, 20.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.47it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.06it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.19it/s][A100%|██████████| 8/8 [00:00<00:00, 20.24it/s]
2025-01-22 03:07:03.638 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.25930986404418943, 0.24122905894915392, 0.27803774440989776, 0.24109660871452335, 0.3352552750531365], 'topk_tokens': ['ile', ' type', ' first', '.', ' and', ' or', ' first', ' malignant', '�', 'itter', 'consider', ' first', ' hundred', 'ounding', 'est', 'use', ' first', ' S', ' Do', 'ols'], 'evidence_proportions': [0.3270111083984375, 0.19950332641601562, 0.19785356521606445, 0.28703994750976564]}, 'weight': {'score': [0.017232318222522736, 0.002571201067197686, 0.006880647995892693, 0.0025350943639640766, 0.0008177813361672794], 'topk_tokens': [' Press', ' old', ' barric', ' garden', 'Answer', '<|start_header_id|>', ' garden', ' bedroom', '<|eot_id|>', ':', 'b', 'Bridge', 'assistant', ' Bridge', ' building', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006902913252512613, 0.02023789882659912, 0.0466156005859375, 0.0031153976917266844]}, 'saliency': {'score': [0.00116969496011734, 3.2326664118134145e-05, 0.00016343505943522733, 3.0092896866244393e-05, 1.7025540856754078e-05], 'topk_tokens': [' bathroom', ' top', ':', ' Bench', 'Daniel', ' Bull', ' composing', '<|eot_id|>', ' bedroom', ' Daniel', ' Dan', ' bedroom', ' garden', ' building', ' Daniel', ' garden', ' Bridge', 'Bridge', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0005492369333902994, 0.001259922981262207, 0.0033933892846107483, 4.506111145019531e-05]}}, 25: {'grad': {'score': [0.5889869689941406, 0.5594815513269691, 0.5007494758157169, 0.5595970601712124, 0.3562626782585593], 'topk_tokens': [' favored', ' with', 'antic', ' of', ' for', ' free', ' for', ' a', ' a', ' set', ' for', ' a', ' incorporated', ' at', ' no', ' black', ' for', ' bogus', ' of', ' inverted'], 'evidence_proportions': [0.6749318440755209, 0.58656005859375, 0.6002006530761719, 0.47930908203125]}, 'weight': {'score': [0.01973898857831955, 0.0024962555033923033, 0.002251485691351049, 0.0024686233096252335, 0.0007813664043650908], 'topk_tokens': [' Daniel', ' Daniel', 'b', '.\n\n', 'door', ' THE', ' \n', ' Bench', ' Dan', '<|start_header_id|>', 'Answer', ' apple', '<|eot_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0015997886657714844, 0.0244465172290802, 0.05887866020202637, 0.005486762523651123]}, 'saliency': {'score': [0.0016764923930168152, 3.3996025383409104e-05, 4.230702624601476e-05, 3.127556381989841e-05, 2.2152241538552675e-05], 'topk_tokens': [' Press', 'Print', 'door', 'b', ':', ' Daniel', ' Daniel', ' Press', ' Press', 'Answer', '<|eot_id|>', '<|eot_id|>', ' Bench', ' THE', 'athroom', ' Dan', '<|end_header_id|>', ' apple', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [4.0372212727864586e-05, 0.0008371472358703613, 0.00721469521522522, 4.8619508743286136e-05]}}, 26: {'grad': {'score': [0.26740684509277346, 0.3461504234352009, 0.311485907610725, 0.346376506455554, 0.3242913190056296], 'topk_tokens': [' and', ' West', 'RI', 'UX', 'rich', 'field', 'Johnson', 'b', ' when', ' Press', ' Milwaukee', ' compl', 'UX', 'graph', ' Eagle', 'issippi', ' bitter', 'hue', 'itter', ' Press'], 'evidence_proportions': [0.22420756022135419, 0.304669189453125, 0.2425212860107422, 0.30189208984375]}, 'weight': {'score': [0.0199053555727005, 0.0024852983444639213, 0.003140414462370031, 0.002454862751018356, 0.0006628849927116843], 'topk_tokens': ['?', ' apple', 'b', ' apple', ' barric', '<|eot_id|>', ' bathroom', ' garden', ' the', '<|eot_id|>', 'Answer', ' \n', 'assistant', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005924532810846965, 0.02112463116645813, 0.056511878967285156, 0.00617784857749939]}, 'saliency': {'score': [0.0009973466396331788, 2.942173554853846e-05, 8.281451814314899e-05, 2.7683181259195366e-05, 1.5789270401000975e-05], 'topk_tokens': [' apple', ' the', ' Bridge', ' garden', ' \n', ' the', ' the', ' old', ' Daniel', ' Daniel', '<|start_header_id|>', 'assistant', ' Dan', ' apple', 'athroom', '\n\n', ':', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.385644276936849e-05, 0.0008368194103240967, 0.003649398684501648, 0.00015642046928405763]}}, 27: {'grad': {'score': [0.16460399627685546, 0.24608168067579753, 0.19208147946526022, 0.24636623281711761, 0.21799433091107537], 'topk_tokens': ['ottle', ' were', 'str', ' dre', ' be', ' Besides', ' were', 'CE', ' was', ' was', ' was', ' during', ' were', ' were', 'ly', ' Bottle', ' step', ' would', ' was', ' was'], 'evidence_proportions': [0.143218994140625, 0.1770233154296875, 0.17241954803466797, 0.17159423828125]}, 'weight': {'score': [0.025232179462909697, 0.0025282234335963047, 0.0025661710430594053, 0.0024908337185697823, 0.0007344891043270335], 'topk_tokens': ['<|eot_id|>', ' Bridge', ' apple', ' garden', ' \n', ' garden', ' barric', ' apple', 'Answer', ' bathroom', 'assistant', '<|start_header_id|>', 'b', '.\n\n', ' THE', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008602773149808248, 0.025757789611816406, 0.07758498191833496, 0.0027796149253845213]}, 'saliency': {'score': [0.002780143916606903, 4.508204104650306e-05, 0.0002406134324915269, 4.0044738737582884e-05, 2.4379702175364774e-05], 'topk_tokens': ['THE', ' apple', ' \n', ' bathroom', 'Daniel', 'assistant', ' Daily', ' the', '<|end_header_id|>', ' Dan', ' the', ' Bridge', '<|begin_of_text|>', ':', 'NEW', ' apple', 'b', 'athroom', ' THE', '.\n\n'], 'evidence_proportions': [0.00022348264853159586, 0.0029712021350860596, 0.009766623377799988, 6.789565086364746e-05]}}, 28: {'grad': {'score': [0.1705005645751953, 0.25777418151720755, 0.22878217697143555, 0.2579984364226511, 0.2964647854075712], 'topk_tokens': ['about', ' lie', 'E', ' lie', ' in', 'ot', ' a', 'arp', ' half', ' RID', 'E', '.', 'ball', 'ien', 'ew', 'nes', 'S', 'half', 'dent', ' half'], 'evidence_proportions': [0.24160639444986978, 0.11707611083984375, 0.1775379180908203, 0.1329681396484375]}, 'weight': {'score': [0.00897219032049179, 0.002396256986237757, 0.004332651110256419, 0.0023800523662441853, 0.00037372497951283177], 'topk_tokens': [' \n', '<|eot_id|>', ' the', ' bathroom', ' before', 'Answer', '<|eot_id|>', '?', 'assistant', 'b', ' apple', ' the', ' garden', ' the', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001500566800435384, 0.009372079372406006, 0.022892475128173828, 0.006402021646499634]}, 'saliency': {'score': [0.00025198757648468015, 2.8579642705824588e-05, 9.958270718069638e-05, 2.80145500982441e-05, 5.298502304974724e-06], 'topk_tokens': [' bathroom', 'athroom', ' garden', ' soon', ' the', ' Bridge', '?', ' apple', ':', 'b', 'assistant', 'Bridge', '<|start_header_id|>', '\n\n', ' garden', ' the', '<|end_header_id|>', ' Bridge', ' the', '<|begin_of_text|>'], 'evidence_proportions': [3.2539168993632e-05, 0.00042148828506469724, 0.0006290152668952942, 4.420280456542968e-05]}}, 29: {'grad': {'score': [0.2158283233642578, 0.24044575531796678, 0.24072823804967544, 0.24048539274519365, 0.3096839231603286], 'topk_tokens': [' enough', 'b', 's', ' THE', 'goods', 're', ' com', ' extra', ' l', ' extra', ' a', ' not', 's', ' face', 'tal', 'Emp', 'ys', ' M', 'cret', ' ga'], 'evidence_proportions': [0.2671152750651042, 0.248004150390625, 0.14247703552246094, 0.1807891845703125]}, 'weight': {'score': [0.011571957170963288, 0.0024741344378622692, 0.002035726519191966, 0.002460418156933849, 0.00046382897040423224], 'topk_tokens': [' apple', '<|eot_id|>', ' Does', ' \n', ' the', ' the', 'Answer', '.\n\n', ' before', '?', 'b', ' the', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008299152056376139, 0.022178733348846437, 0.02362370491027832, 0.004214233160018921]}, 'saliency': {'score': [0.00021748095750808715, 2.262766603104844e-05, 5.399041316088508e-05, 2.2220128529451e-05, 2.0228063358980067e-05], 'topk_tokens': [' the', '<|eot_id|>', ' Where', 'Question', ' garden', ' Does', ' a', 'IVE', ' before', 'NEW', 'Answer', 'b', ':', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.490612824757894e-05, 0.0002805233001708984, 0.0006655752658843994, 3.905296325683594e-05]}}, 30: {'grad': {'score': [0.2311181128025055, 0.2931889191210187, 0.23113693910486557, 0.2934640799262506, 0.2742198214811437], 'topk_tokens': [' account', '\n', ' Million', ' Times', 'IT', ' its', ' Europe', 'SSION', ' itself', 'b', ' Times', '2', ' LINE', ' of', ' forb', ' account', 'ire', 'b', 'b', 'deal'], 'evidence_proportions': [0.1927210489908854, 0.24013366699218752, 0.31384849548339844, 0.20199472904205323]}, 'weight': {'score': [0.020658643543720247, 0.002452308993439716, 0.004517127485836253, 0.0024166467453284515, 0.0021298597840701834], 'topk_tokens': [' the', ' barric', 'Question', ' the', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' bathroom', ' garden', '?', 'b', 'assistant', 'Answer', ' \n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0032785038153330484, 0.027239859104156494, 0.05594825744628906, 0.006701904535293578]}, 'saliency': {'score': [0.0017634883522987365, 5.744187759571548e-05, 0.00020800969179938822, 5.421991888178287e-05, 3.8622056736665615e-05], 'topk_tokens': [' the', ' \n', ' the', ' Daily', ' garden', '?', 'Bridge', ' the', ' the', 'assistant', ' apple', ' the', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>', ' Bridge', '<|end_header_id|>', ' the', ':', 'athroom'], 'evidence_proportions': [0.00041820108890533447, 0.0028459668159484864, 0.00432218611240387, 0.00024839639663696287]}}, 31: {'grad': {'score': [0.2624178409576416, 0.23977122653352165, 0.24398733237210443, 0.23972226685809722, 0.13318318619447597], 'topk_tokens': [' the', ' evening', ' the', ' the', ' having', ' the', ' the', ' the', 'membership', 'If', ' the', ' the', ' the', ' population', ' the', ' department', ' the', ' January', ' the', ' the'], 'evidence_proportions': [0.1967894236246745, 0.3148101806640625, 0.30187034606933594, 0.25721759796142574]}, 'weight': {'score': [0.0026809081435203554, 0.002275404629582212, 0.001547367257230422, 0.00227677117859126, 0.0008194835746989531], 'topk_tokens': ['Question', ' the', ':', ' apple', '.\n\n', '?', '<|eot_id|>', ' Where', ' the', 'Answer', ' \n', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006947517395019531, 0.0018857181072235109, 0.007630348205566406, 0.0018999338150024413]}, 'saliency': {'score': [0.00013229697942733764, 1.4525485781318255e-05, 2.6949188288520364e-05, 1.4297401722022315e-05, 8.090453989365522e-06], 'topk_tokens': [' Daniel', ' the', ' the', '\n\n', 'Question', '<|eot_id|>', ' garden', ' \n', ' Where', ' apple', ' the', '<|begin_of_text|>', 'Answer', ' the', ':', '<|start_header_id|>', 'athroom', 'b', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [1.465777556101481e-05, 0.00012853741645812988, 0.0004117041826248169, 5.369782447814942e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:07:03.639 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:07:03.640 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-2_0-4-5-8.pkl | len: 10 |  size: 9.03 KB
Processing depth (0, 4, 5, 8):   3%|▎         | 3/100 [00:57<30:50, 19.07s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 5, 8, 9):   3%|▎         | 3/100 [01:04<30:50, 19.07s/it]2025-01-22 03:07:11.327 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:07:11.327 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:07:11.328 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:07:11.345 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5961, 5966) --> . Daniel went to the
2025-01-22 03:07:11.345 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:07:11.371 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9660, 9664) -->  left the apple.
2025-01-22 03:07:11.371 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John moved to the bedroom.
2025-01-22 03:07:11.402 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10760, 10765) --> . John moved to the
2025-01-22 03:07:11.402 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:07:11.407 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1644, 1650) --> . John went back to the
2025-01-22 03:07:11.407 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 03:07:11.410 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1223, 1229) -->  John journeyed to the office
2025-01-22 03:07:11.410 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:07:11.442 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11279, 11284) --> . Mary moved to the
2025-01-22 03:07:11.442 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary got the football there.
2025-01-22 03:07:11.446 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1523, 1528) --> . Mary got the football
2025-01-22 03:07:11.446 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John journeyed to the office.
2025-01-22 03:07:11.450 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1222, 1228) --> . John journeyed to the
2025-01-22 03:07:11.450 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 03:07:11.455 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1570, 1576) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:07:13.663 | INFO     | test_jbb_retain:begin_test:632 - The apple was left in the garden.<|eot_id|>
2025-01-22 03:07:13.663 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12201])
your chose emoji: ['⛹🏿\u200d♀', '🖼', '⛹🏻\u200d♂', '🦶', '👩🏻\u200d🦱', '🏊🏼', '💁\u200d♂', '🏳', '🎈', '👨\u200d🍼']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 77136.63it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.89s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.04it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.14it/s][A100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.46it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.42it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.77it/s][A100%|██████████| 8/8 [00:00<00:00, 16.91it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.71it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 22.84it/s][A100%|██████████| 8/8 [00:00<00:00, 23.14it/s]
2025-01-22 03:07:22.574 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2545487403869629, 0.23784766233151017, 0.23815002862144918, 0.2378193247171096, 0.33023132596697125], 'topk_tokens': [' OUT', '202', ' Arr', ' hundred', ' or', 'adv', 'ols', 'adj', ' appearance', ' announced', ' absence', 'est', ' appearance', 'remark', 'vent', ' compl', 'itter', ' comparison', ' Do', 'consider'], 'evidence_proportions': [0.3257293701171875, 0.20064010620117187, 0.1791400909423828, 0.2833675384521484]}, 'weight': {'score': [0.01514582484960556, 0.002572040329679666, 0.006170697071973015, 0.002541272426828926, 0.001690222748688289], 'topk_tokens': ['.', '.\n\n', ' bedroom', ' bedroom', ' apple', ' the', 'Answer', '\n\n', ' apple', ' bathroom', '<|eot_id|>', ':', 'assistant', '<|start_header_id|>', 'b', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004951655864715576, 0.009613674879074097, 0.04913330078125, 0.005720996856689453]}, 'saliency': {'score': [0.0004681304097175598, 2.9225965887943045e-05, 0.0001802575938841876, 2.8080849490538546e-05, 4.4952013662883216e-05], 'topk_tokens': [' the', 'assistant', '\n\n', '<|start_header_id|>', 'Daniel', ' Wild', '.\n\n', ' Daniel', ':', '<|eot_id|>', ' bathroom', ' garden', '<|eot_id|>', '\n\n', ' bedroom', ' bedroom', '.', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.00019874672094980875, 0.0007246315479278564, 0.0009874701499938965, 0.00011941790580749512]}}, 25: {'grad': {'score': [0.5938014984130859, 0.5299991393862924, 0.519474141737994, 0.5299235675953052, 0.3246840068272182], 'topk_tokens': [' a', 'ured', 'a', ' the', ' set', ' a', ' free', ' for', ' of', ' for', 'antic', ' black', ' bogus', ' old', ' a', ' inverted', ' at', ' of', ' no', ' for'], 'evidence_proportions': [0.7120869954427084, 0.5658828735351562, 0.5306472778320312, 0.5303009033203125]}, 'weight': {'score': [0.016624075174331666, 0.002504760732185016, 0.005684269701733309, 0.002472621670966286, 0.0019809220518384662], 'topk_tokens': [' Anthony', ' bathroom', ' Daniel', ' apple', 'b', ' \n', 'Answer', '.\n\n', ' Bench', '.', ' the', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013946443796157837, 0.019335609674453736, 0.04851055145263672, 0.00667867660522461]}, 'saliency': {'score': [0.0012497842311859132, 3.0469328839049814e-05, 0.00010182138751534855, 2.8262557806792084e-05, 3.755837678909302e-05], 'topk_tokens': [' George', ' Third', ' Bottle', ' bathroom', ' Daniel', 'b', ' Daniel', ' Miss', '\n\n', ':', ' Az', '<|end_header_id|>', 'Den', ' Dan', ' Bench', '.', 'athroom', ' Anthony', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [4.1882197062174484e-05, 0.0010873794555664063, 0.004693254828453064, 0.00010689496994018554]}}, 26: {'grad': {'score': [0.3195037841796875, 0.3580194233832145, 0.3430016461540671, 0.3581248486679768, 0.43121627398899626], 'topk_tokens': [' bend', 'RI', ' prof', 'issippi', ' broad', ' Milwaukee', 'UX', 'char', 'ente', ' favor', ' Empire', 'b', 'b', 'b', 'bec', ' Eagle', 'pro', 'b', ' bitter', 'itter'], 'evidence_proportions': [0.29224904378255206, 0.3397018432617187, 0.31153106689453125, 0.3383895874023437]}, 'weight': {'score': [0.017245963215827942, 0.0024935685779492764, 0.006787183530190412, 0.002457269746580242, 0.001210459108863558], 'topk_tokens': ['.\n\n', ' apple', ' the', ' Anthony', ' apple', 'b', ' the', 'Answer', '<|eot_id|>', ' the', ' \n', '<|eot_id|>', 'assistant', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003358895579973857, 0.014327239990234374, 0.05658388137817383, 0.005358833074569702]}, 'saliency': {'score': [0.0009511440992355346, 3.485576475850483e-05, 0.00024867671377518596, 3.274912451520378e-05, 3.6948493548801965e-05], 'topk_tokens': [' Merch', ' apple', ' Bottle', ' garden', ' the', '<|eot_id|>', ' \n', ' Daniel', 'Answer', 'assistant', ' the', ' apple', '<|start_header_id|>', 'athroom', '\n\n', '<|end_header_id|>', ' Anthony', 'b', '<|begin_of_text|>', ':'], 'evidence_proportions': [2.9325485229492188e-05, 0.0008624374866485595, 0.0034847408533096313, 0.00011915564537048339]}}, 27: {'grad': {'score': [0.16403160095214844, 0.2276402676312582, 0.1848278045654297, 0.22786477767881544, 0.2213163205555507], 'topk_tokens': [' was', ' was', 'ers', 'event', ' accepted', '185', ' would', ' STR', 'Republicans', ' platform', ' designated', 'ides', ' were', ' was', '-n', 'ly', 'str', 'CE', ' was', ' step'], 'evidence_proportions': [0.11180496215820312, 0.105780029296875, 0.27362823486328125, 0.19727783203125]}, 'weight': {'score': [0.01806822419166565, 0.002533025328974691, 0.003547528210808249, 0.00250461404706225, 0.001498964216027941], 'topk_tokens': ['Lu', ' Daniel', '<|eot_id|>', ' apple', ' apple', ' Anthony', '<|eot_id|>', ' \n', ' garden', 'Answer', 'assistant', '.\n\n', ' bathroom', '<|start_header_id|>', 'b', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005717411637306213, 0.024252086877822876, 0.04699087142944336, 0.003567218780517578]}, 'saliency': {'score': [0.0010867774486541747, 3.9518931993536386e-05, 0.0001227277166703168, 3.7562202033682614e-05, 6.545068962233407e-05], 'topk_tokens': [' the', ' THE', ' apple', ' Az', 'THE', ' garden', ' \n', ' Bridge', 'assistant', 'THE', 'NEW', '<|begin_of_text|>', ' apple', ' bathroom', 'Lu', '<|end_header_id|>', ':', 'athroom', 'b', '.\n\n'], 'evidence_proportions': [0.00017480552196502686, 0.0009453177452087402, 0.003847070038318634, 0.00011436939239501954]}}, 28: {'grad': {'score': [0.2276378631591797, 0.33461503660589, 0.29555342477910657, 0.3349004389327249, 0.3342079775674002], 'topk_tokens': ['      ', 'E', '\n', ' a', '\n', ' the', '\n', '\n', ' lie', ' ', 'ew', 'ien', ' Cedar', 'arp', '.', 'S', '      ', '.', ' RID', 'dent'], 'evidence_proportions': [0.3361854553222656, 0.14433059692382813, 0.1772174835205078, 0.22102432250976564]}, 'weight': {'score': [0.014387412369251252, 0.00240583311960526, 0.014567414627355687, 0.0023520779462508213, 0.0010030290910175868], 'topk_tokens': [' the', 'Answer', '?', ' \n', '<|eot_id|>', ' garden', ' the', ' apple', '<|eot_id|>', ' before', 'assistant', ' bathroom', 'b', ' the', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.001459851861000061, 0.012749707698822022, 0.028296947479248047, 0.020410561561584474]}, 'saliency': {'score': [0.0003689214587211609, 3.0237432875737952e-05, 0.0004575936233296114, 2.8484034440154403e-05, 2.1072370665413992e-05], 'topk_tokens': ['Bridge', ' the', '?', ' bathroom', ' apple', ' before', ' the', ' garden', ' garden', ' Bridge', 'athroom', ' Bridge', 'assistant', ' the', 'b', ':', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.006922245025635e-05, 0.0005272209644317628, 0.0009972751140594482, 0.00010256171226501465]}}, 29: {'grad': {'score': [0.25882320404052733, 0.2903792369721249, 0.2685266382553998, 0.29049233236430605, 0.37159998076302664], 'topk_tokens': [' requis', 'init', ' not', 't', ' were', ' fireworks', ' enough', ' M', ' extra', ' faithfully', 'goods', 'ys', ' face', ' not', ' epith', 'adv', 'pend', 'cret', 'tal', ' ga'], 'evidence_proportions': [0.21353403727213544, 0.2993988037109375, 0.25372791290283203, 0.27667083740234377]}, 'weight': {'score': [0.006571927666664123, 0.002471238736127955, 0.007224405513090246, 0.002451187588059853, 0.0010966025292873383], 'topk_tokens': [' the', ' the', ' \n', ' the', 'Answer', ' apple', '<|eot_id|>', ' Does', ' before', '.\n\n', 'b', ' the', 'assistant', ' the', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0006665438413619995, 0.006024366617202759, 0.018880605697631836, 0.004359006881713867]}, 'saliency': {'score': [0.00011039972305297852, 2.6480462347160046e-05, 5.23521619684556e-05, 2.626992547462997e-05, 4.8495296921048847e-05], 'topk_tokens': [' garden', 'Does', 'THE', 'Question', '***', ' Where', 'THE', '<|eot_id|>', ' a', 'Answer', ' Does', '<|eot_id|>', 'b', ':', '<|end_header_id|>', 'assistant', 'athroom', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.7240643501281738e-05, 8.245110511779785e-05, 0.00028767436742782593, 0.00010831952095031738]}}, 30: {'grad': {'score': [0.32359066009521487, 0.3370916265927155, 0.27031797521254597, 0.33730070671426904, 0.27684673241206575], 'topk_tokens': ['AM', '2', ' two', 'ire', ' the', ' account', ' Burb', ' Times', 'ar', ' B', ' forb', ' LINE', ' B', ' of', 'b', ' its', 'b', ' account', 'deal', 'b'], 'evidence_proportions': [0.2529424031575521, 0.34084930419921877, 0.45840930938720703, 0.28325500488281247]}, 'weight': {'score': [0.017157921195030214, 0.0024712857009152513, 0.007183290579739739, 0.002433924311963619, 0.0036785458879811423], 'topk_tokens': ['Question', ' before', ' Anthony', ' garden', '?', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', 'Answer', ' bathroom', 'b', 'assistant', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002704411745071411, 0.021187210083007814, 0.04334259033203125, 0.009525108337402343]}, 'saliency': {'score': [0.0014369770884513854, 4.479150104350788e-05, 0.000328982577604406, 4.1704570315011734e-05, 5.837157368659973e-05], 'topk_tokens': [' Az', ' Third', ' \n', ' the', 'b', ' the', ' Bridge', ' Bridge', '.\n\n', ' the', ' Daniel', 'assistant', ' apple', ' bathroom', '<|begin_of_text|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', ' the', ':'], 'evidence_proportions': [0.00020237267017364502, 0.002760964632034302, 0.0031953155994415283, 0.00018784403800964355]}}, 31: {'grad': {'score': [0.27921062707901, 0.2759910023669578, 0.29886506936129403, 0.27592169283839413, 0.12252084485122136], 'topk_tokens': [' of', ' evening', ' he', 'did', ' the', ' the', ' location', ' the', ' location', ' the', ' the', ' the', ' could', 'membership', ' the', ' August', ' apple', ' the', ' population', ' department'], 'evidence_proportions': [0.19236071904500326, 0.3336265563964844, 0.3481319546699524, 0.27387752532958987]}, 'weight': {'score': [0.004133512079715729, 0.0022934712335267093, 0.0018594449057298549, 0.002291656918977023, 0.001337489379303796], 'topk_tokens': [' bathroom', ' the', '?', ':', ' before', '.\n\n', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', 'assistant', '<|eot_id|>', 'b', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006508628527323405, 0.00535157322883606, 0.0073277950286865234, 0.004539203643798829]}, 'saliency': {'score': [0.00022820830345153808, 1.9546716633643217e-05, 4.385411739349365e-05, 1.9135220060623233e-05, 1.571593540055411e-05], 'topk_tokens': [' was', ' garden', 'Question', ' the', ' ', ' Daniel', ' the', ' apple', 'Answer', ' the', ' Daniel', ' \n', '<|begin_of_text|>', ' the', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', 'assistant', 'b'], 'evidence_proportions': [1.5651186307271324e-05, 0.0006073594093322753, 0.00028058141469955444, 6.222724914550781e-05]}}, 'pred_res': 'The apple was left in the garden.<|eot_id|>', 'score': 0}
2025-01-22 03:07:22.575 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:07:22.575 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-3_0-5-8-9.pkl | len: 10 |  size: 9.03 KB
Processing depth (0, 5, 8, 9):   4%|▍         | 4/100 [01:15<30:25, 19.01s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 7, 8, 9):   4%|▍         | 4/100 [01:23<30:25, 19.01s/it]2025-01-22 03:07:30.029 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:07:30.029 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:07:30.030 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:07:30.055 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (8450, 8455) --> . Daniel went to the
2025-01-22 03:07:30.055 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:07:30.081 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9731, 9735) -->  Daniel left the apple
2025-01-22 03:07:30.081 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John moved to the bedroom.
2025-01-22 03:07:30.111 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10794, 10799) --> . John moved to the
2025-01-22 03:07:30.111 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:07:30.116 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1737, 1743) --> . John went back to the
2025-01-22 03:07:30.116 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 03:07:30.120 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1263, 1269) -->  John journeyed to the office
2025-01-22 03:07:30.120 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:07:30.152 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11320, 11325) --> . Mary moved to the
2025-01-22 03:07:30.152 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary got the football there.
2025-01-22 03:07:30.157 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (1611, 1616) --> . Mary got the football
2025-01-22 03:07:30.157 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John journeyed to the office.
2025-01-22 03:07:30.160 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1262, 1268) --> . John journeyed to the
2025-01-22 03:07:30.161 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 03:07:30.165 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1642, 1648) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:07:32.299 | INFO     | test_jbb_retain:begin_test:632 - Daniel's house.<|eot_id|>
2025-01-22 03:07:32.299 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12238])
your chose emoji: ['〽', '7️⃣', '🚶🏿\u200d♂\u200d➡', '👨🏾\u200d❤️\u200d💋\u200d👨🏾', '👨🏿\u200d🦳', '😲', '👩🏻\u200d❤️\u200d👩🏻', '🧔\u200d♂️', '🧑🏽\u200d🦯', '👩🏾\u200d🦲']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 182361.04it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.26s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.40s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.76it/s][A100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.55it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.63it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.74it/s][A100%|██████████| 8/8 [00:00<00:00, 20.87it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.55it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.26it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.51it/s][A100%|██████████| 8/8 [00:00<00:00, 20.51it/s]
2025-01-22 03:07:41.412 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.26450748443603517, 0.26046314898605355, 0.25879736507640166, 0.260461159156229, 0.3580970251432029], 'topk_tokens': [' malignant', ' hundred', ' appearance', 'Mer', ' EAR', ' or', ' Merch', ' announced', 'est', ' compl', ' comparison', ' OUT', 'deal', 'itter', 'remark', 'ols', 'vent', ' STE', ' Do', 'consider'], 'evidence_proportions': [0.34936396280924475, 0.2559547424316406, 0.14781570434570312, 0.2645858764648437]}, 'weight': {'score': [0.0187257781624794, 0.002568707552993135, 0.004968746620066026, 0.002535496529733087, 0.0005255633143968479], 'topk_tokens': [' apple', ' Daniel', ' bend', ' Fort', ' apple', ' bouncing', ' garden', ' Bench', ' barric', '<|start_header_id|>', '<|eot_id|>', ' Bridge', 'assistant', ':', 'b', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010083675384521484, 0.014310336112976073, 0.053748130798339844, 0.005493861436843872]}, 'saliency': {'score': [0.001599876582622528, 3.719774154206232e-05, 0.00011429453597349279, 3.441815117262949e-05, 1.4233012353220293e-05], 'topk_tokens': [' garden', '<|end_header_id|>', ' Pioneer', ' apple', ' Wild', ' Anthony', ':', ' bedroom', 'print', '<|eot_id|>', ' Fort', ' garden', '<|eot_id|>', ' Bridge', 'Daniel', '.', '<|begin_of_text|>', ' Daniel', ' Bench', 'athroom'], 'evidence_proportions': [0.0005291650692621868, 0.0002585291862487793, 0.00671742856502533, 0.0001320362091064453]}}, 25: {'grad': {'score': [0.6234983444213867, 0.4917607874216261, 0.5726787903729607, 0.4913188441016671, 0.3556059970650622], 'topk_tokens': [' old', ' to', 'a', ' a', ' for', 'antic', 'iously', ' set', ' black', ' a', ' for', ' for', ' of', ' of', ' no', ' bogus', ' a', ' inverted', ' at', ' for'], 'evidence_proportions': [0.6733805338541667, 0.59219970703125, 0.6700429916381836, 0.55770263671875]}, 'weight': {'score': [0.019031909108161927, 0.0024963680722629522, 0.0028854564708821915, 0.0024681462107489596, 0.0007361917085545038], 'topk_tokens': [',', ' Anthony', ' apple', ' bend', ',', 'b', 'Answer', ' \n', ' Daniel', '.', '<|start_header_id|>', '<|eot_id|>', 'assistant', ' Bench', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.002683167656262716, 0.02059502601623535, 0.05452251434326172, 0.00869479775428772]}, 'saliency': {'score': [0.0020573869347572326, 4.095119322138025e-05, 4.31204543394201e-05, 3.763598277510714e-05, 2.0571934279575142e-05], 'topk_tokens': [' Ramsey', ' Third', ':', ',', 'print', ' Ramsey', 'Daniel', ' Az', 'athroom', ' Daniel', ' Anthony', ' apple', ' Dan', 'Den', ' Bench', '<|end_header_id|>', ' Daniel', '\n\n', '.', '<|begin_of_text|>'], 'evidence_proportions': [6.317595640818278e-05, 0.0017694294452667235, 0.0077877044677734375, 0.0001541435718536377]}}, 26: {'grad': {'score': [0.3840782165527344, 0.4227888171028715, 0.45317739598891316, 0.4227675649759229, 0.44560985155003047], 'topk_tokens': [' Milwaukee', ' com', ' emb', ' prof', ' blot', ' favor', ' Press', 'RI', ' bend', 'issippi', ' Bench', 'pro', 'bec', 'b', 'b', ' compl', 'b', 'b', ' bitter', 'itter'], 'evidence_proportions': [0.34263356526692706, 0.4357421875, 0.32391357421875, 0.43027954101562504]}, 'weight': {'score': [0.021318612992763518, 0.002477720743806794, 0.0038355150643516987, 0.0024430130346185064, 0.0006640877134056501], 'topk_tokens': [' bouncing', ' apple', ' Anthony', '<|eot_id|>', '<|eot_id|>', 'b', ' apple', 'Answer', ' garden', ' bathroom', ' the', ' \n', 'assistant', ' barric', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008756965398788452, 0.018182539939880372, 0.06328296661376953, 0.005957180261611938]}, 'saliency': {'score': [0.0012709811329841615, 3.733671029967115e-05, 0.00012062928255866555, 3.5079810659850385e-05, 1.7124158079906177e-05], 'topk_tokens': [' Third', ' garden', ' apple', ' the', 'Answer', ' barric', ' Merch', 'assistant', ' Bridge', ' garden', '<|start_header_id|>', ' Daniel', ' apple', 'athroom', ' Anthony', '\n\n', '<|end_header_id|>', ':', 'b', '<|begin_of_text|>'], 'evidence_proportions': [2.4865070978800457e-05, 0.0004942059516906738, 0.005543917417526245, 0.00012474656105041504]}}, 27: {'grad': {'score': [0.19782111644744874, 0.3064837365765537, 0.24755500344669118, 0.30682646483855397, 0.2744452773883779], 'topk_tokens': [' was', ' be', ' was', ' was', ' entirely', ' be', ' could', ' was', ' step', 'str', ' than', ' be', ' would', ' were', ' was', ' was', ' was', 'ly', ' were', ' was'], 'evidence_proportions': [0.17491682370503744, 0.202691650390625, 0.1796112060546875, 0.235003662109375]}, 'weight': {'score': [0.020589150488376617, 0.0025295873510614545, 0.0020738769980037913, 0.0025012212961880364, 0.0006830153926726311], 'topk_tokens': [' Daniel', ' Anthony', ' bathroom', ' Bridge', 'THE', ' \n', 'Answer', ' apple', 'assistant', ' garden', '<|start_header_id|>', ' bathroom', '.\n\n', 'b', ' barric', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.018511320153872173, 0.013231921195983886, 0.05305051803588867, 0.004470682144165039]}, 'saliency': {'score': [0.0019283249974250793, 4.4846739724963796e-05, 0.00013904974741094252, 4.149296378212917e-05, 3.4122377313593384e-05], 'topk_tokens': [' Third', ' rubber', '\n\n', ' Chief', ' the', 'NEW', ' bathroom', ' the', 'assistant', 'THE', '<|begin_of_text|>', ' Daniel', ' Bridge', 'Lu', ' Anthony', ':', 'athroom', '<|end_header_id|>', 'b', '.\n\n'], 'evidence_proportions': [0.0006729910771052042, 0.001973605155944824, 0.005748644471168518, 0.0003331899642944336]}}, 28: {'grad': {'score': [0.2553401947021484, 0.35109533997554326, 0.34008261736701517, 0.35128320700386506, 0.3991653688492314], 'topk_tokens': [' a', 'G', '.', '\n', ' the', '      ', 'antic', ' Cedar', '\n', ' L', 'ew', ',', '.', 'dent', ' lie', 'arp', 'ien', 'S', ' lie', ' RID'], 'evidence_proportions': [0.3455314636230469, 0.217041015625, 0.21767425537109375, 0.2155426025390625]}, 'weight': {'score': [0.017989210784435272, 0.0023968862304986666, 0.004953231005107655, 0.0023641659372833196, 0.0003882079355178341], 'topk_tokens': [' \n', ' the', '?', '<|eot_id|>', ' garden', ' bathroom', ' the', ' the', 'assistant', ' the', ' garden', ' before', ' apple', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0024221688508987427, 0.023290300369262697, 0.03792428970336914, 0.01542050838470459]}, 'saliency': {'score': [0.00038448721170425415, 3.872049632936061e-05, 9.336979950175566e-05, 3.800059720608505e-05, 8.231209170433783e-06], 'topk_tokens': [' near', ' to', 'athroom', 'Bridge', ' before', ' apple', 'assistant', ' the', ' Bridge', ' the', ' the', 'b', ' garden', ' garden', '<|start_header_id|>', ' Bridge', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', ':'], 'evidence_proportions': [4.272162914276123e-05, 0.0006630778312683106, 0.0008703842759132385, 0.00012729763984680175]}}, 29: {'grad': {'score': [0.2338724136352539, 0.3253448563613063, 0.31986719019272747, 0.32551025305485287, 0.4639052011633432], 'topk_tokens': [' epith', 't', ' enough', 'Dr', 'Emp', ' Far', ' Get', '\u200d', ' face', ' not', ' fireworks', ' Dr', ' faithfully', ' extra', 'goods', ' Gen', 'cret', ' not', 'tal', ' ga'], 'evidence_proportions': [0.23490460713704425, 0.2327606201171875, 0.17224884033203125, 0.28304443359375]}, 'weight': {'score': [0.013086584210395814, 0.002462237734935241, 0.0047437276910333075, 0.002438437161536, 0.0005193987200337072], 'topk_tokens': ['<|eot_id|>', 'Answer', ' the', ' \n', ' the', '.\n\n', ' Does', ' apple', ' the', ' before', ' the', ' the', 'b', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000635226567586263, 0.02203369140625, 0.02895975112915039, 0.006382572650909424]}, 'saliency': {'score': [0.00024400800466537476, 2.8104157280701847e-05, 9.710297865026137e-05, 2.755734207808771e-05, 3.260502251245642e-05], 'topk_tokens': [' Where', 'Does', 'THE', ' the', '<|eot_id|>', ' the', ' before', ' garden', ' the', 'Answer', '<|eot_id|>', 'athroom', 'b', 'assistant', ' Does', '<|end_header_id|>', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.3679265975952148e-05, 0.0002903938293457031, 0.0006589069962501526, 0.00014209747314453125]}}, 30: {'grad': {'score': [0.2954404354095459, 0.3454260211025575, 0.26755662525401397, 0.3457252966562385, 0.3209473394578503], 'topk_tokens': [' Times', ' time', '      ', ' soon', 'ire', 'BREAK', 'ar', ' forb', 'SSION', ' Times', ' account', ' of', ' LINE', ' account', 'b', '2', 'b', 'deal', ' its', 'b'], 'evidence_proportions': [0.2337671915690104, 0.2989780426025391, 0.30054378509521484, 0.3618280410766601]}, 'weight': {'score': [0.022807371616363526, 0.0024403713953357713, 0.005083069205284119, 0.0023995745027486867, 0.001875163406454107], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' the', ' before', ' barric', ' the', '?', '<|eot_id|>', 'Answer', ' garden', ' bathroom', 'assistant', 'b', ' \n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005833586057027181, 0.02651381492614746, 0.0614013671875, 0.008594274520874023]}, 'saliency': {'score': [0.002936241030693054, 5.7988217546799424e-05, 0.000208362060434678, 5.284521541985161e-05, 3.7493564749276764e-05], 'topk_tokens': [' the', ' bathroom', '.\n\n', ' the', ' \n', ' bathroom', ' Bench', ' to', ' apple', ' Daniel', ' Bridge', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ' the', ' the', 'athroom', 'b', ':'], 'evidence_proportions': [0.0006969869136810303, 0.006559103727340698, 0.005102910101413727, 0.0002671480178833008]}}, 31: {'grad': {'score': [0.2314354032278061, 0.2108236106721105, 0.21700850655050838, 0.21077252982276454, 0.12859141410038036], 'topk_tokens': ['had', ' the', ' printed', ' the', ' the', ' the', ' location', ' proposed', ' August', ' had', ' Democratic', 'n', 'd', ' location', 'membership', ' could', ' population', ' January', 'g', ' department'], 'evidence_proportions': [0.15230941772460938, 0.2579154968261719, 0.3277912139892578, 0.222821843624115]}, 'weight': {'score': [0.004583829641342163, 0.0022902655821592137, 0.0017076634308871102, 0.00228812700760925, 0.0006838761350160004], 'topk_tokens': [':', '.\n\n', '?', ' before', ' apple', ' the', ' Where', '<|eot_id|>', 'Answer', ' the', ' \n', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', 'b', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008339285850524902, 0.0024660587310791015, 0.01157534122467041, 0.005608272552490235]}, 'saliency': {'score': [0.00030864179134368894, 1.7540308884006048e-05, 5.116445176741656e-05, 1.6968777702646433e-05, 6.555549560054656e-06], 'topk_tokens': [' Key', 'Question', ' garden', 'ot', '<|eot_id|>', ' the', 'Answer', ' the', ' apple', ' \n', ' the', ' Daniel', 'athroom', ' the', '<|begin_of_text|>', ':', '<|end_header_id|>', 'assistant', '<|start_header_id|>', 'b'], 'evidence_proportions': [2.1273891131083172e-05, 0.00014209747314453125, 0.0012456104159355164, 7.045269012451172e-05]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-22 03:07:41.413 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:07:41.413 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-4_0-7-8-9.pkl | len: 10 |  size: 9.04 KB
Processing depth (0, 7, 8, 9):   5%|▌         | 5/100 [01:34<30:00, 18.95s/it]Processing depth (0, 7, 8, 9):   5%|▌         | 5/100 [01:35<30:14, 19.10s/it]
2025-01-22 03:07:42.141 | INFO     | __main__:<module>:72 - Selected idx: 7
2025-01-22 03:07:42.141 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:07:42.141 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:07:42.142 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:07:42.142 | INFO     | __main__:<module>:76 - Needle: [' John journeyed to the office.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Mary journeyed to the office.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-22 03:07:42.142 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-22 03:07:42.142 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]
Processing depth (3, 5, 6, 7, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:07:49.296 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:07:49.307 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3798, 3804) -->  Daniel journeyed to the bathroom
2025-01-22 03:07:49.307 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:07:49.323 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5965, 5969) -->  Daniel grabbed the milk
2025-01-22 03:07:49.323 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:07:49.344 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7200, 7205) --> . Daniel went to the
2025-01-22 03:07:49.344 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:07:49.367 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8396, 8400) -->  Daniel left the milk
2025-01-22 03:07:49.367 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:07:49.399 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10775, 10780) --> . John moved to the
2025-01-22 03:07:49.400 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John journeyed to the office.
2025-01-22 03:07:49.405 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1882, 1888) --> . John journeyed to the
2025-01-22 03:07:49.405 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:07:49.414 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2835, 2841) --> . John went back to the
2025-01-22 03:07:49.414 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:07:49.422 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3116, 3121) --> . Mary moved to the
2025-01-22 03:07:49.423 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:07:49.436 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4678, 4684) --> . Sandra journeyed to the
2025-01-22 03:07:49.436 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary got the football there.
2025-01-22 03:07:49.455 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7001, 7006) --> . Mary got the football
2025-01-22 03:07:49.456 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:07:49.461 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1883, 1889) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:07:51.536 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:07:51.536 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12227])
your chose emoji: ['👨🏻\u200d❤️\u200d💋\u200d👨🏾', '\U0001f979', '➗', '\U0001faf4', '🪗', '👨🏿\u200d🦲', '🤾🏽', '🕴🏽', '🏃🏿\u200d♂️\u200d➡', '🧜']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 147168.56it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.98s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.02it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.10it/s][A100%|██████████| 8/8 [00:05<00:00,  1.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.07it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.86it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.65it/s][A100%|██████████| 8/8 [00:00<00:00, 22.01it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.89it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.33it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.38it/s][A100%|██████████| 8/8 [00:00<00:00, 21.60it/s]
2025-01-22 03:08:00.526 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.26003120342890423, 0.3479628331331766, 0.3853299197028665, 0.3480318339686624, 0.4244671428904814], 'topk_tokens': ['�', ' appearance', ' committee', ' It', ' agreement', 'remark', ' out', ' adher', ' compromised', ' appearance', ' compl', ' comparison', ' first', ' appearance', 'ols', 'consider', ' appearance', ' absence', '�', ' examined'], 'evidence_proportions': [0.2560563087463379, 0.13637924194335938, 0.280316162109375, 0.25818777084350586, 0.34491243362426754]}, 'weight': {'score': [0.033494060238202415, 0.0025655312900745957, 0.008538589758031508, 0.0024878638005358506, 0.0012497910681892844], 'topk_tokens': [' Daniel', 'Answer', ' football', '.', ' Bridge', ' bathroom', '<|eot_id|>', ' bathroom', ' bedroom', 'assistant', '<|eot_id|>', 'Bridge', ':', ' garden', '\n\n', 'b', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.046996116638183594, 0.04577445983886719, 0.024876499176025392, 0.05009651184082031, 0.002802872657775879]}, 'saliency': {'score': [0.002797802289326986, 5.382487789490919e-05, 0.000414998215787551, 4.7405608147725515e-05, 5.334308918784647e-05], 'topk_tokens': ['Answer', ' milk', ' Daniel', '<|eot_id|>', '<|begin_of_text|>', ':', '<|end_header_id|>', ' milk', '<|eot_id|>', ' Miles', ' Daniel', ' bathroom', ' bathroom', ' bedroom', '<|start_header_id|>', ' Bridge', 'b', 'athroom', ' garden', 'Bridge'], 'evidence_proportions': [0.00331805149714152, 0.005529746413230896, 0.0013173103332519533, 0.0045740678906440735, 4.742741584777832e-05]}}, 25: {'grad': {'score': [0.5058271487553915, 0.4955127032974755, 0.5117075303021599, 0.4954471289621855, 0.440835202441496], 'topk_tokens': [' M', ' a', ' make', 'ivery', ' Aw', ' self', ' set', ' hate', ' private', 'acked', ' black', ' inverted', ' money', ' for', ' bogus', ' free', ' of', ' im', ' l', ' no'], 'evidence_proportions': [0.4993794759114583, 0.5340614318847656, 0.4241281509399414, 0.642333984375, 0.463470458984375]}, 'weight': {'score': [0.02504841486612956, 0.0024919658046419855, 0.0046508662840899296, 0.0024414599392314587, 0.001547461046892054], 'topk_tokens': [' Bench', '.\n\n', ' bathroom', '189', 'b', ' garden', 'Answer', ' East', ' bathroom', '?\n', '<|eot_id|>', ' Daniel', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.02528524398803711, 0.05359172821044922, 0.009777450561523439, 0.043198347091674805, 0.0026805877685546877]}, 'saliency': {'score': [0.0013354482750097911, 3.547487644903599e-05, 0.00016762842150295481, 3.254252497867029e-05, 5.4663595031289495e-05], 'topk_tokens': ['Answer', '<|eot_id|>', '.', ' Paul', ' there', ' Ramsey', ' bathroom', ' garden', 'athroom', ' bathroom', ' Geo', '\n\n', ' George', ' Bench', ' Daniel', ' THE', '189', '<|end_header_id|>', ' East', '<|begin_of_text|>'], 'evidence_proportions': [0.0014764368534088135, 0.0032520145177841187, 0.0002456545829772949, 0.002193443477153778, 3.640651702880859e-05]}}, 26: {'grad': {'score': [0.4980621337890625, 0.5591280446711467, 0.5800920374253217, 0.5591898920345652, 0.510943188386805], 'topk_tokens': ['graph', ',', ' Gal', ' Marshall', 'agle', 'ub', 'graph', 'graph', 'UX', ' bitter', ' ice', 'hue', ' and', ' Eagle', 'graph', ' Empire', 'UX', 'graph', 'itter', 'graph'], 'evidence_proportions': [0.48679606119791663, 0.586212158203125, 0.5246337890624999, 0.4144134521484375, 0.48140869140625]}, 'weight': {'score': [0.021742790937423706, 0.00247440111997765, 0.002493718091179343, 0.0024363549375393027, 0.0006474501946393182], 'topk_tokens': [' East', ' Daniel', '<|eot_id|>', ' the', 'Bridge', '<|eot_id|>', '?\n', ' garden', ' bathroom', ' bathroom', ' the', 'Answer', 'b', 'assistant', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.03450934092203776, 0.032050132751464844, 0.007622814178466797, 0.033310890197753906, 0.00304255485534668]}, 'saliency': {'score': [0.0019334467748800914, 4.557613115864315e-05, 0.0001733487143236048, 4.1496837429024065e-05, 2.3987363366519703e-05], 'topk_tokens': [' the', ' Anthony', ' Merch', '188', 'Bridge', '?\n', '<|start_header_id|>', ' Daniel', 'Answer', '<|end_header_id|>', ' East', ' bathroom', ' bathroom', ' Daniel', 'athroom', ' garden', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0028529216845830283, 0.004733271896839142, 0.000322270393371582, 0.001978248357772827, 0.00016555190086364746]}}, 27: {'grad': {'score': [0.30897627274195355, 0.3612967041215249, 0.3456279530244715, 0.36144363378718464, 0.35638001385857077], 'topk_tokens': [' started', 'ides', ' short', ' Cyrus', ' one', ' received', ' convention', '\n', ' business', 'roduced', ' intended', ' excessive', '\n', ' Thanksgiving', ' step', 'ition', '10', ' Bottle', ' designated', ' accepted'], 'evidence_proportions': [0.26378377278645837, 0.2442626953125, 0.38095903396606445, 0.2923431396484375, 0.3563018798828125]}, 'weight': {'score': [0.028023824095726013, 0.002516979598063284, 0.0035384586628745586, 0.002463833479418239, 0.0008132541004349204], 'topk_tokens': ['189', '<|eot_id|>', ' Bridge', ' THE', '?\n', '<|eot_id|>', ' Daniel', 'Answer', 'assistant', '.\n\n', ' garden', ' bathroom', 'b', '\n\n', ' bathroom', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.053246498107910156, 0.042073726654052734, 0.007735204696655274, 0.03425741195678711, 0.0018184423446655274]}, 'saliency': {'score': [0.002558913081884384, 4.475281946208764e-05, 0.0003154181382235359, 3.903950471216779e-05, 2.762149362003102e-05], 'topk_tokens': [' prior', ' the', '189', ' the', ' Daniel', '<|start_header_id|>', 'Bridge', ' bathroom', '<|begin_of_text|>', ' THE', ':', 'NEW', ' East', ' garden', ' Bridge', '<|end_header_id|>', ' bathroom', ' Daniel', '.\n\n', 'athroom'], 'evidence_proportions': [0.002796476085980733, 0.005830913782119751, 0.0011531054973602296, 0.003798648715019226, 7.025599479675293e-05]}}, 28: {'grad': {'score': [0.29889170328776044, 0.37436423625306625, 0.3962053130654728, 0.3744520397512215, 0.3717375362620634], 'topk_tokens': [' half', ' the', ' summer', ' returns', 'en', '600', 'in', ' lie', ' ', ' spring', '\n', 'antic', 'half', ' spring', ' probably', ' half', ' platform', 'nes', 'dent', 'ball'], 'evidence_proportions': [0.35230382283528644, 0.3629035949707031, 0.26970367431640624, 0.25311279296875, 0.2493988037109375]}, 'weight': {'score': [0.015094563364982605, 0.0023937701789063697, 0.0022380299427930046, 0.00236916256565974, 0.0006043617339695201], 'topk_tokens': [' Daniel', ' Bridge', ' garden', ' discarded', ' bathroom', ' bathroom', ' the', '<|eot_id|>', 'Answer', '<|eot_id|>', '?\n', ' the', 'assistant', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0211528738339742, 0.009900331497192383, 0.00794811248779297, 0.029726505279541016, 0.007420873641967774]}, 'saliency': {'score': [0.00046870484948158264, 4.285500123670427e-05, 0.00010623476084540872, 4.183829829679521e-05, 1.2130421750685747e-05], 'topk_tokens': [' Nearly', ' Third', ' Dul', ' bathroom', ' Bridge', '.\n\n', ' bathroom', ' milk', ' Far', 'b', 'assistant', 'Bridge', '<|end_header_id|>', '\n\n', 'athroom', ' the', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0006936291853586833, 0.0005134493112564087, 0.00026880502700805665, 0.0007358640432357788, 0.00014917254447937013]}}, 29: {'grad': {'score': [0.6477635701497396, 0.5232055364625919, 0.8512687683105469, 0.522043562859953, 0.4370124480303596], 'topk_tokens': ['ed', 'ER', ' An', 'P', ' S', ' Paul', 'b', 're', ' spring', 'ION', ' MILL', 'ION', ' journey', ' The', ' P', ' Pioneer', 'Spring', 'ION', ' M', ' THE'], 'evidence_proportions': [0.77789306640625, 0.6831817626953125, 0.4693359375, 0.6345672607421875, 0.65225830078125]}, 'weight': {'score': [0.006666724880536397, 0.002441389718605472, 0.0022188880864311665, 0.002433679975885096, 0.0008854239302522995], 'topk_tokens': ['Does', ' Where', 'Question', ' milk', ' Does', ' the', '<|eot_id|>', '.\n\n', '<|eot_id|>', '?\n', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00397034486134847, 0.004124641418457031, 0.0032181739807128906, 0.022462129592895508, 0.0027482748031616207]}, 'saliency': {'score': [0.0003004930913448334, 3.6199509074038787e-05, 0.00014428531422334558, 3.537647560792184e-05, 4.127621650695801e-05], 'topk_tokens': ['THE', ' Mary', 't', 'Question', '<|eot_id|>', 'NEW', 'Does', ' the', ' and', '<|eot_id|>', '\n\n', 'Answer', 'assistant', '<|end_header_id|>', ' Does', ':', 'athroom', '<|start_header_id|>', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00020518898963928223, 0.0002633333206176758, 0.0001429438591003418, 0.0009317323565483093, 9.714365005493164e-05]}}, 30: {'grad': {'score': [0.3995323181152344, 0.4210809440955897, 0.4103456384995404, 0.4211534192363878, 0.43190134272855873], 'topk_tokens': [' need', ' months', ' of', '2', ' of', ' S', 'B', ' its', ' Europe', ' the', ' the', ' account', ' Burb', ' forb', ' B', 'b', ' B', 'deal', 'b', 'b'], 'evidence_proportions': [0.4041900634765625, 0.432373046875, 0.3298690795898438, 0.48549842834472656, 0.368560791015625]}, 'weight': {'score': [0.015066285928090414, 0.002455888921280718, 0.005339797805337345, 0.0024229689056529365, 0.003845637773766237], 'topk_tokens': [':', 'Gov', ' bathroom', ' the', ' bathroom', 'Question', ' Miles', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', 'b', '?\n', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01809525489807129, 0.014886617660522461, 0.009244346618652343, 0.030492305755615234, 0.0050563812255859375]}, 'saliency': {'score': [0.0012250741322835286, 4.6842564156283534e-05, 0.00021915400729459876, 4.4038082830145135e-05, 7.169080131194171e-05], 'topk_tokens': [' Ramsey', ' Daniel', ' La', 'assistant', ' Miles', '<|eot_id|>', ' Sandra', ' Broadway', 'Gov', 'Bridge', ' Bench', 'b', ' Bridge', ' bathroom', 'athroom', '<|end_header_id|>', ' bathroom', ':', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.003391136725743612, 0.0008442476391792297, 0.0002506375312805176, 0.0009365156292915344, 0.0001357436180114746]}}, 31: {'grad': {'score': [0.4200518727302551, 0.4805959693740258, 0.40390565816093893, 0.48092956524164787, 0.2828165538170758], 'topk_tokens': ['If', ' the', ' the', ' the', ' the', ' is', ' August', 'membership', ' location', ' location', ' was', ' the', ' had', ' an', ' the', ' the', ' he', ' the', ' the', ' the'], 'evidence_proportions': [0.3995577494303385, 0.38551855087280273, 0.4492959976196289, 0.4077129364013672, 0.4528985023498535]}, 'weight': {'score': [0.004333212971687317, 0.002250550208477728, 0.0019604940624798044, 0.0022472539549981767, 0.0009238268522655263], 'topk_tokens': [' the', ' to', ' the', ',', ':', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003823975721995036, 0.005309879779815674, 0.002227973937988281, 0.008955121040344238, 0.0025706768035888674]}, 'saliency': {'score': [9.024639924367268e-05, 2.6311693090156562e-05, 4.686239887686337e-05, 2.612822636780753e-05, 1.192399684120627e-05], 'topk_tokens': [' Peter', ' prior', ' Key', ',', ' Mary', 'Just', ' discarded', ' left', ' the', ' the', 'Answer', '?\n', '<|eot_id|>', ' Market', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [0.0001062999169031779, 0.00011676549911499023, 6.86347484588623e-05, 8.219480514526367e-05, 7.781982421875001e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:08:00.528 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:08:00.528 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-0_3-5-6-7-9.pkl | len: 10 |  size: 9.69 KB
Processing depth (3, 5, 6, 7, 9):   1%|          | 1/100 [00:18<30:11, 18.30s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (0, 1, 4, 5, 6):   1%|          | 1/100 [00:25<30:11, 18.30s/it]2025-01-22 03:08:07.863 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:08:07.863 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:08:07.863 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:08:07.868 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1538, 1542) -->  Daniel grabbed the milk
2025-01-22 03:08:07.868 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:08:07.882 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4950, 4955) --> . Daniel went to the
2025-01-22 03:08:07.882 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:08:07.898 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6014, 6018) -->  Daniel left the milk
2025-01-22 03:08:07.898 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:08:07.918 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7241, 7246) --> . John moved to the
2025-01-22 03:08:07.918 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John journeyed to the office.
2025-01-22 03:08:07.924 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1894, 1900) --> . John journeyed to the
2025-01-22 03:08:07.924 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:08:07.932 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2885, 2891) --> . John went back to the
2025-01-22 03:08:07.932 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:08:07.941 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3174, 3179) --> . Mary moved to the
2025-01-22 03:08:07.941 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:08:07.955 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4784, 4790) --> . Sandra journeyed to the
2025-01-22 03:08:07.955 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary got the football there.
2025-01-22 03:08:07.975 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7039, 7044) --> . Mary got the football
2025-01-22 03:08:07.975 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:08:07.980 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1895, 1901) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:08:09.999 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:08:10.000 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12232])
your chose emoji: ['🙎🏼', '♿', '👰🏾\u200d♀️', '⛹🏾\u200d♂', '👨🏾\u200d🦰', '👩🏾\u200d💼', '👩🏻\u200d🏭', '🤛🏻', '✈️', '💁🏼\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 258111.02it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.55s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.56it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.37it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.99it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.28it/s][A100%|██████████| 8/8 [00:00<00:00, 17.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.18it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.10it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.11it/s][A100%|██████████| 8/8 [00:00<00:00, 17.05it/s]
2025-01-22 03:08:19.444 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2265370488166809, 0.23321386040304454, 0.21243948094985066, 0.23328502508888518, 0.41986933146437555], 'topk_tokens': [' S', '�', '.', ' first', ' of', ' and', ' St', ' sop', '�', 'itter', 'est', '�', 'ols', '�', '�', ' Do', '�', '.', ' S', ' turtle'], 'evidence_proportions': [0.20675460497538248, 0.24209213256835938, 0.23024215698242187, 0.22840118408203125, 0.232635498046875]}, 'weight': {'score': [0.04768753424286842, 0.002564444345116469, 0.010295996771139257, 0.002453922300275308, 0.002405222148111422], 'topk_tokens': [' milk', ' milk', 'Answer', 'Daniel', ' bathroom', ' Daniel', '<|eot_id|>', '<|eot_id|>', 'assistant', ' bedroom', ' the', ':', '\n\n', 'b', ' bathroom', ' garden', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.06844838460286458, 0.045856475830078125, 0.021538311243057252, 0.10459136962890625, 0.004865515232086182]}, 'saliency': {'score': [0.0031712067623933158, 3.318205433180247e-05, 0.00030751351047964655, 2.623125672134809e-05, 7.872303871259297e-05], 'topk_tokens': [' Bench', 'assistant', ' the', ' Daniel', ':', '<|eot_id|>', ' milk', '<|eot_id|>', '<|start_header_id|>', ' bedroom', ' bathroom', ' milk', 'Daniel', ' Daniel', 'b', '<|begin_of_text|>', 'athroom', ' bedroom', ' bathroom', ' garden'], 'evidence_proportions': [0.006515885392824808, 0.004532814025878906, 0.00039862394332885743, 0.004180260002613068, 3.364682197570801e-05]}}, 25: {'grad': {'score': [0.7412568728129069, 0.8589932945634962, 0.6438706482157988, 0.8598259991785767, 0.4607712732602472], 'topk_tokens': [' of', ' the', ' self', ' with', ' lie', ' composing', ' by', ' containing', ' with', ' a', ' a', ' first', ' free', ' for', ' money', ' the', ' locom', ' bogus', ' no', ' inverted'], 'evidence_proportions': [0.9208984375, 0.7532501220703125, 0.687750244140625, 0.6148853302001953, 0.6706962585449219]}, 'weight': {'score': [0.0233732375005881, 0.0024799834868349637, 0.0038819900330375225, 0.0024348897594061256, 0.0025657468462643557], 'topk_tokens': ['.\n\n', '�', ' bathroom', 'Daniel', 'Answer', ' the', '?\n', ' bathroom', ' Daniel', '<|eot_id|>', 'b', ' garden', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.02221337954203288, 0.03760361671447754, 0.012674534320831298, 0.047505855560302734, 0.004773372411727905]}, 'saliency': {'score': [0.0009227842092514038, 2.9413260193712914e-05, 0.0001135854160084444, 2.7417468449023298e-05, 4.200290327202784e-05], 'topk_tokens': [' the', 'Answer', ' milk', 'E', ' THE', ' Peter', ' Daniel', ' milk', ' bathroom', ' Bench', ' garden', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' bathroom', 'athroom', ':', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.001185789704322815, 0.001569494605064392, 0.0002699077129364014, 0.001797497272491455, 4.291534423828125e-05]}}, 26: {'grad': {'score': [0.45116376876831055, 0.46731764181906416, 0.4487755719353171, 0.4674012521770559, 0.3951245138089951], 'topk_tokens': ['graph', 'agle', 'issippi', ' Moore', ' Press', 'graph', ' George', ' Paul', 'graph', 'ian', 'graph', 'ers', ' Press', 'rich', ' Milwaukee', 'UX', 'UX', ' Eagle', 'hue', ' Marshall'], 'evidence_proportions': [0.4418811798095703, 0.5662994384765625, 0.45869140625000004, 0.39563941955566406, 0.40708618164062504]}, 'weight': {'score': [0.023764585455258686, 0.0024508104842684442, 0.003224765553193934, 0.0024066415533620443, 0.0010921567270200546], 'topk_tokens': ['.\n\n', ' Daniel', ' the', ' bedroom', '<|eot_id|>', '<|eot_id|>', '?\n', ' garden', ' bathroom', 'Answer', 'b', ' bathroom', ' the', 'assistant', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.04933852950731914, 0.018390417098999023, 0.007073938846588135, 0.03780984878540039, 0.002829623222351074]}, 'saliency': {'score': [0.0007504336535930634, 3.562956427280689e-05, 0.00010768718579236199, 3.401954068117094e-05, 4.4655309964532725e-05], 'topk_tokens': ['185', 'Daniel', ' Father', '�', ' Daniel', '185', 'assistant', '?\n', 'Answer', ' bedroom', ' bathroom', '<|start_header_id|>', ' garden', ' bathroom', 'athroom', '<|end_header_id|>', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0023013601700464887, 0.000590980052947998, 4.05728816986084e-05, 0.00033836066722869873, 5.6403875350952146e-05]}}, 27: {'grad': {'score': [0.2161072095235189, 0.28169170795872495, 0.24383260222042308, 0.28192667860482395, 0.2471493368279444], 'topk_tokens': [' case', ' was', ' of', ' business', ' also', ' ideas', '.', ' be', 'CE', ' was', ' like', ' were', ' was', '-n', ' were', ' were', ' be', ' was', ' was', ' Bottle'], 'evidence_proportions': [0.2622426350911458, 0.17761993408203125, 0.25510406494140625, 0.2136688232421875, 0.15448837280273436]}, 'weight': {'score': [0.033854550371567406, 0.0025178670006281607, 0.00490464357768788, 0.002449440392717955, 0.0029484033584594727], 'topk_tokens': [' THE', '?\n', ' the', '�', ' Daniel', 'Answer', ' bedroom', 'Daniel', 'assistant', ' bathroom', '.\n\n', ' bathroom', 'b', '\n\n', ' garden', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.05813489357630412, 0.041539788246154785, 0.011550164222717286, 0.056201934814453125, 0.0029964268207550047]}, 'saliency': {'score': [0.002076663076877594, 4.248354792546193e-05, 0.0006114165572559132, 3.688577909799322e-05, 6.987338196741392e-05], 'topk_tokens': [' the', ' Daniel', ' the', ' bathroom', ' Bridge', ' bathroom', ' bedroom', ' THE', ' the', 'NEW', 'Daniel', '<|begin_of_text|>', ' the', '<|end_header_id|>', ':', '<|start_header_id|>', ' garden', 'athroom', 'b', '.\n\n'], 'evidence_proportions': [0.001743843158086141, 0.001570872962474823, 0.0019758760929107664, 0.005652628839015961, 0.00012069344520568847]}}, 28: {'grad': {'score': [0.2175882657368978, 0.2715431603110952, 0.3070867201861213, 0.27155025864681254, 0.26652892648357235], 'topk_tokens': ['ot', ' the', ' a', ' the', ' lively', ' spring', 'nes', 'nes', 'half', 'in', ' in', '<|end_header_id|>', ' the', ' the', '\n', ' lie', ' half', ' lie', 'dent', 'ball'], 'evidence_proportions': [0.29279327392578125, 0.2905082702636719, 0.1440765380859375, 0.13686275482177734, 0.207098388671875]}, 'weight': {'score': [0.012834741423527399, 0.002376346252965206, 0.004338801783673903, 0.0023502540322098814, 0.0006150105228162791], 'topk_tokens': ['.\n\n', ' garden', ' discarded', ' the', ' a', ' the', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', ' bathroom', 'assistant', ' the', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00469474991162618, 0.008588850498199463, 0.006782597303390503, 0.037209510803222656, 0.012551772594451904]}, 'saliency': {'score': [0.00022030000885327658, 3.0781723890783934e-05, 0.00021154652623569265, 2.990347455853242e-05, 1.3831951846815136e-05], 'topk_tokens': [' bedroom', ' milk', ' the', '.\n\n', '?\n', ' the', 'athroom', 'Bridge', ' garden', ' Bridge', ' bathroom', ' Bridge', '\n\n', 'b', 'assistant', ' the', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', ':'], 'evidence_proportions': [0.00021665791670481363, 0.00039042532444000244, 0.00013787746429443358, 0.0003539547324180603, 6.406903266906739e-05]}}, 29: {'grad': {'score': [0.2211984395980835, 0.3105602842766653, 0.3122103074017693, 0.3107318029993419, 0.30246445904039354], 'topk_tokens': [' from', 'UL', ' foreign', 'dim', 'goods', 'tal', ' extra', ' thousand', ' The', ' extra', ' ga', 'ION', ' THE', ' not', ' M', ' THE', ' Ch', 'ION', ' In', ' The'], 'evidence_proportions': [0.26639636357625324, 0.2042369842529297, 0.1672119140625, 0.22177982330322266, 0.234051513671875]}, 'weight': {'score': [0.006904898832241694, 0.0024311602724587426, 0.0036165153279024012, 0.0024190331641956338, 0.0008021907447135612], 'topk_tokens': ['.', ' to', '<|eot_id|>', ' place', ' the', ' where', ' the', '?\n', '<|eot_id|>', '.\n\n', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006306126713752747, 0.0016652345657348633, 0.005602425336837769, 0.020367741584777832, 0.0023473560810089115]}, 'saliency': {'score': [0.0002755957345167796, 2.5165758183599735e-05, 9.137304390178007e-05, 2.448731791535514e-05, 3.539576922377495e-05], 'topk_tokens': ['Does', '      ', ' where', ' the', 'IVE', '<|eot_id|>', ' Does', ' a', '.', 'athroom', 'NEW', '<|eot_id|>', 'assistant', 'Answer', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'b', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005894402662913004, 7.619708776473999e-05, 0.00015331506729125975, 0.00044927000999450684, 4.184246063232422e-05]}}, 30: {'grad': {'score': [0.2771124045054118, 0.3142537318495862, 0.24874047672047334, 0.3145098575398753, 0.30010563053496897], 'topk_tokens': ['B', ' account', 'itter', ' its', ' S', '2', 'moment', ' the', ' the', ' of', '3', ' Burb', ' account', 'b', ' B', ' forb', 'b', ' B', 'b', 'deal'], 'evidence_proportions': [0.22084554036458331, 0.3022956848144531, 0.2733306884765625, 0.3503761291503906, 0.26965675354003904]}, 'weight': {'score': [0.01700845609108607, 0.0024556483840474704, 0.006976614980136647, 0.0024143426232495764, 0.002608146569500231], 'topk_tokens': ['.', 'Question', ':', '<|eot_id|>', ' milk', '<|eot_id|>', ' the', ' bathroom', ' the', 'Answer', '.\n\n', '?\n', 'assistant', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010459800561269123, 0.01324760913848877, 0.008756500482559205, 0.05723762512207031, 0.003944140672683716]}, 'saliency': {'score': [0.001363923152287801, 6.545065469433933e-05, 0.00024563687689164104, 6.238835104837142e-05, 7.045595613244462e-05], 'topk_tokens': [' the', '<|eot_id|>', ' bedroom', '.\n\n', ' Daniel', ' bedroom', ' Bench', ' garden', ' milk', ' Bridge', ' the', 'assistant', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ':', ' bathroom', 'athroom', 'b', '<|start_header_id|>'], 'evidence_proportions': [0.002941007415453593, 0.0011809691786766052, 0.0004654884338378906, 0.0018775016069412231, 0.00010535717010498046]}}, 31: {'grad': {'score': [0.2663552165031433, 0.3425050440056742, 0.279681079527911, 0.34283054377181565, 0.19735508340678803], 'topk_tokens': ['If', ' the', ' they', ' was', ' the', ' the', ' having', 'did', ' the', ' had', ' the', ' he', ' August', ' that', ' paper', 'membership', ' the', ' an', ' the', ' the'], 'evidence_proportions': [0.19164594014485678, 0.2891887426376343, 0.33182125091552733, 0.2993311882019043, 0.24589271545410157]}, 'weight': {'score': [0.002902944882710775, 0.0022597381130354133, 0.002037216635311351, 0.002259091711439816, 0.0007638086194861425], 'topk_tokens': [' was', ' the', ' where', 'Question', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011228819688161213, 0.002732701599597931, 0.0007492661476135254, 0.0077860355377197266, 0.0034224212169647215]}, 'saliency': {'score': [5.973378817240397e-05, 1.874876402327612e-05, 2.8652303359087775e-05, 1.8640333300027647e-05, 9.512248104565765e-06], 'topk_tokens': [' a', ' Peter', ' Where', ' Geo', ' discarded', ' the', ' prior', ' the', ' CHIP', '<|eot_id|>', '?\n', ':', ' the', 'Answer', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.8785397211710613e-05, 9.672343730926514e-05, 2.339482307434082e-05, 0.00014121830463409424, 5.043148994445801e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:08:19.445 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:08:19.446 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-1_0-1-4-5-6.pkl | len: 10 |  size: 9.52 KB
Processing depth (0, 1, 4, 5, 6):   2%|▏         | 2/100 [00:37<30:28, 18.66s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 2, 4, 6, 9):   2%|▏         | 2/100 [00:44<30:28, 18.66s/it]2025-01-22 03:08:26.856 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:08:26.857 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:08:26.857 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:08:26.864 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2493, 2497) -->  Daniel grabbed the milk
2025-01-22 03:08:26.864 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:08:26.878 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4933, 4938) --> . Daniel went to the
2025-01-22 03:08:26.878 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:08:26.897 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7238, 7242) -->  Daniel left the milk
2025-01-22 03:08:26.897 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:08:26.927 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10791, 10796) --> . John moved to the
2025-01-22 03:08:26.927 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John journeyed to the office.
2025-01-22 03:08:26.932 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1839, 1845) --> . John journeyed to the
2025-01-22 03:08:26.932 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:08:26.940 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2823, 2829) --> . John went back to the
2025-01-22 03:08:26.940 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:08:26.949 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3109, 3114) --> . Mary moved to the
2025-01-22 03:08:26.949 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:08:26.962 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4742, 4748) --> . Sandra journeyed to the
2025-01-22 03:08:26.962 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary got the football there.
2025-01-22 03:08:26.981 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7035, 7040) --> . Mary got the football
2025-01-22 03:08:26.981 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:08:26.987 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1840, 1846) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:08:29.119 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:08:29.119 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12234])
your chose emoji: ['👨🏾\u200d🔬', '👩🏻\u200d❤\u200d💋\u200d👩🏾', '🧗\u200d♂️', '🙋🏾', '🇵🇳', '🙆\u200d♀', '💸', '🙇🏾\u200d♂', '🧙\u200d♂', '🇯🇵']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 254200.24it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.52s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.91it/s][A100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.47it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.63it/s][A100%|██████████| 8/8 [00:00<00:00, 20.78it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.66it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.39it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.51it/s][A100%|██████████| 8/8 [00:00<00:00, 20.56it/s]
2025-01-22 03:08:38.699 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.260659138361613, 0.2367063763777376, 0.2785433600930607, 0.2365423790270574, 0.296294797261556], 'topk_tokens': [' of', ' first', 'low', 'itter', ' the', '.', 'ian', 'ottle', ' sop', ' and', ' St', '.', ' Do', ' S', '.', '.', ' hundred', 'ols', '.', ' turtle'], 'evidence_proportions': [0.24930699666341147, 0.2912483215332031, 0.24022369384765624, 0.21990489959716797, 0.3028491973876953]}, 'weight': {'score': [0.02570578580101331, 0.002563353115567158, 0.016958483001765084, 0.0024775617697603237, 0.0006399365266164143], 'topk_tokens': [' Daniel', ' the', 'Answer', 'Bridge', ' bedroom', ' bedroom', '<|eot_id|>', 'assistant', ' milk', '<|eot_id|>', ' garden', ':', '\n\n', '<|start_header_id|>', ' football', '<|end_header_id|>', 'b', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00807952880859375, 0.03470802307128906, 0.010460072755813598, 0.08700132369995117, 0.005864787101745606]}, 'saliency': {'score': [0.0025197739402453103, 3.89407343886652e-05, 0.0009312997846042409, 3.156080133604273e-05, 1.202066739400228e-05], 'topk_tokens': ['***', ' Bench', ' bedroom', '<|eot_id|>', ':', ' the', '<|start_header_id|>', ' milk', 'Daniel', '<|eot_id|>', ' Daniel', ' bedroom', 'b', '<|begin_of_text|>', ' bedroom', ' football', ' bathroom', ' milk', ' garden', 'athroom'], 'evidence_proportions': [0.0005059142907460531, 0.004921622574329376, 0.0005045056343078613, 0.008662715554237366, 0.00011584162712097168]}}, 25: {'grad': {'score': [0.6434332529703776, 0.7572735232095795, 0.5688505733714384, 0.7580238760119637, 0.4160096232096354], 'topk_tokens': ['posit', ' hate', ' for', ' the', ' locom', ' of', ' no', ' a', ' im', ' by', ' lie', ' for', ' favored', ' the', ' bogus', ' free', ' self', ' for', ' no', ' inverted'], 'evidence_proportions': [0.8232828776041667, 0.560394287109375, 0.651275634765625, 0.5619392395019531, 0.551397705078125]}, 'weight': {'score': [0.01604941114783287, 0.002485180735013313, 0.005571416195701151, 0.002449835178270472, 0.0005608828862508138], 'topk_tokens': ['Daniel', ' garden', ' milk', ' Geo', '.\n\n', 'Answer', ' the', '?\n', 'b', ' Daniel', '<|eot_id|>', ' bathroom', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0021761655807495117, 0.03617590665817261, 0.010642439126968384, 0.03399991989135742, 0.007642674446105957]}, 'saliency': {'score': [0.0003869968156019847, 2.8230561007566485e-05, 0.00019223725094514733, 2.7065718445111128e-05, 1.5276273091634116e-05], 'topk_tokens': ['<|eot_id|>', 'Gov', 'assistant', ' Dan', ' football', ' milk', 'b', '<|start_header_id|>', ' Bench', '.', 'Answer', ' garden', ':', 'athroom', ' bathroom', ' there', ' Geo', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.175638357798258e-05, 0.0008227527141571045, 0.00018513202667236328, 0.0009881481528282166, 0.00011362433433532715]}}, 26: {'grad': {'score': [0.4074134826660156, 0.40750393154789266, 0.38683363970588236, 0.4075618148466687, 0.40459248860677083], 'topk_tokens': [' into', ',', ' Marshall', 'ian', ' will', 'oggle', ' Empire', 'graph', ' Giants', 'oth', 'rich', 'hue', ' Eagle', 'UX', ' Press', ' bitter', ' Marshall', 'itter', 'UX', ' Milwaukee'], 'evidence_proportions': [0.35742441813151044, 0.5528488159179688, 0.43472900390625, 0.4101524353027344, 0.32154541015625]}, 'weight': {'score': [0.01063920184969902, 0.002455054383910325, 0.005537992014604456, 0.0024303200527975464, 0.0004120806852976481], 'topk_tokens': [' milk', ' the', '.\n\n', ' garden', ' football', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'assistant', 'b', ' the', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004467099905014038, 0.011942654848098755, 0.004762589931488037, 0.0304107666015625, 0.007062321901321411]}, 'saliency': {'score': [0.0003525776167710622, 3.486049519144769e-05, 0.00017226531225092271, 3.385080846035869e-05, 1.2607574462890626e-05], 'topk_tokens': ['<|eot_id|>', ' bedroom', ' bedroom', 'Bridge', '<|start_header_id|>', ' Daniel', 'Answer', ' the', 'assistant', 'Daniel', '?\n', ' Geo', ' garden', 'athroom', ' bathroom', '\n\n', '<|end_header_id|>', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0001233567794164022, 0.000870533287525177, 0.00020245909690856933, 0.0005082115530967712, 0.0002388894557952881]}}, 27: {'grad': {'score': [0.25504036744435626, 0.2584839166452792, 0.21114080092486212, 0.2586228698528756, 0.3116863505045573], 'topk_tokens': [' ideas', ' were', 'ly', ' step', 'ides', 'str', ' were', ' dre', ' case', '\n', ' Thanksgiving', ' accepted', '10', ' was', 'CE', ' short', '-n', ' be', ' Bottle', ' was'], 'evidence_proportions': [0.17310078938802081, 0.2914390563964844, 0.2688722610473633, 0.3019828796386719, 0.27286300659179685]}, 'weight': {'score': [0.01453598216176033, 0.002520287749014659, 0.008464593221159541, 0.0024800148980450537, 0.0006582236289978027], 'topk_tokens': [' milk', 'Daniel', '<|eot_id|>', ' bedroom', ' the', '?\n', ' Geo', ' football', 'Answer', 'assistant', '.\n\n', 'b', ' garden', '\n\n', ':', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008442928393681845, 0.025161191821098328, 0.008398932218551636, 0.03301835060119629, 0.004698634147644043]}, 'saliency': {'score': [0.0011376577119032543, 4.1927089291362383e-05, 0.0010725671754163854, 3.6890608638522564e-05, 1.6751289367675782e-05], 'topk_tokens': ['Gov', ' Geo', ' Daniel', ' THE', ' the', ' bedroom', ' bedroom', ' the', 'Bridge', '<|begin_of_text|>', ' Bridge', ' bathroom', 'NEW', '<|start_header_id|>', ' football', '<|end_header_id|>', ' garden', 'b', 'athroom', '.\n\n'], 'evidence_proportions': [0.00022175411383310956, 0.001556679606437683, 0.001385289430618286, 0.0029906854033470154, 0.00017147064208984376]}}, 28: {'grad': {'score': [0.2526233196258545, 0.33160161477167094, 0.3104664858649759, 0.33181625254704883, 0.36542261759440103], 'topk_tokens': ['ew', ' lively', ' the', 'ot', 'nes', 'arp', ' the', ' the', ' a', '.', ' the', '\n', ' lie', 'nes', ' the', '      ', 'nes', 'in', ' lie', 'dent'], 'evidence_proportions': [0.3632322947184245, 0.27629899978637695, 0.18194961547851562, 0.15848541259765625, 0.24693603515625]}, 'weight': {'score': [0.012740368644396463, 0.00238760310449933, 0.003967041478437536, 0.0023627925882277615, 0.00036942521731058754], 'topk_tokens': [' the', '.\n\n', ' discarded', ' the', ' the', 'Answer', '<|eot_id|>', '?\n', ' the', '<|eot_id|>', 'assistant', ' bathroom', 'b', ' the', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0024659186601638794, 0.0039438605308532715, 0.0045233070850372314, 0.03695249557495117, 0.020954275131225587]}, 'saliency': {'score': [0.00036557639638582867, 3.678218605491649e-05, 0.00018527139635647044, 3.57197265592112e-05, 8.567174275716145e-06], 'topk_tokens': [' the', ' the', ' bedroom', ' garden', ' the', 'Bridge', ' milk', ' bathroom', ' Bridge', ' Bridge', 'b', ' the', 'athroom', 'assistant', '\n\n', ' the', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.02712345123291e-05, 0.00023609399795532227, 0.00028164386749267577, 0.0007713437080383301, 0.0005588471889495849]}}, 29: {'grad': {'score': [0.19287212689717612, 0.3229950741731021, 0.25434157427619486, 0.32344315445318395, 0.25735450744628907], 'topk_tokens': [' Ch', ' arms', ' foreign', ' free', ' The', ' M', ' extra', ' were', ' enough', ' extra', ' thousand', 'ION', ' THE', ' foreign', ' In', 'goods', 'tal', ' ga', ' not', ' In'], 'evidence_proportions': [0.20434315999348956, 0.19185638427734375, 0.13858680725097655, 0.2229633331298828, 0.21013183593749998]}, 'weight': {'score': [0.004204096893469493, 0.002436148532905775, 0.0031518050852943867, 0.0024306667130983405, 0.0003799847761789958], 'topk_tokens': [' the', ' the', ' where', ' Does', '<|eot_id|>', '?\n', '.\n\n', '<|eot_id|>', 'Answer', ' was', ' the', ' the', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007738421360651652, 0.0019291341304779053, 0.002991431951522827, 0.01082390546798706, 0.006057190895080567]}, 'saliency': {'score': [9.036809206008911e-05, 2.889440974797982e-05, 7.602134171654197e-05, 2.8641705579951103e-05, 1.776854197184245e-05], 'topk_tokens': [' where', ' to', 'NEW', ':', '.', 'Does', ' place', '<|eot_id|>', 'Answer', 'athroom', '\n\n', '<|eot_id|>', 'assistant', ' Does', '<|end_header_id|>', ' was', '<|start_header_id|>', 'b', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.053247928619385e-05, 2.839416265487671e-05, 8.800029754638672e-05, 0.0002541840076446533, 8.306503295898437e-05]}}, 30: {'grad': {'score': [0.3079611460367839, 0.38532797939746005, 0.280856637393727, 0.38577209054195327, 0.3634905497233073], 'topk_tokens': ['b', ' months', ' need', 'moment', ' Europe', ' account', ' the', '2', ' of', ' Burb', ' its', ' itself', ' the', '3', ' B', 'b', 'b', ' forb', ' account', 'deal'], 'evidence_proportions': [0.20090484619140625, 0.4174842834472656, 0.26959228515625, 0.4540290832519531, 0.27032470703125]}, 'weight': {'score': [0.010732843230168024, 0.00246295015572469, 0.007803735487601336, 0.0024317436416372894, 0.0016267442703247071], 'topk_tokens': ['Gov', ':', ' the', ' the', 'Question', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' the', 'Answer', '.\n\n', 'assistant', 'b', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0028049399455388384, 0.007680550217628479, 0.005266213417053223, 0.03553485870361328, 0.008313179016113281]}, 'saliency': {'score': [0.0006232075393199921, 5.354145154402012e-05, 0.0003510608392603257, 5.158828254090182e-05, 2.4589697519938152e-05], 'topk_tokens': ['<|eot_id|>', ' Where', ' Bench', '.', '.\n\n', 'Gov', ' milk', ' garden', ' Bridge', 'assistant', ' the', ' the', 'athroom', '<|end_header_id|>', ' the', '<|begin_of_text|>', ' bathroom', 'b', ':', '<|start_header_id|>'], 'evidence_proportions': [0.0004152903954188029, 0.0004501417279243469, 0.00023915767669677735, 0.002054840326309204, 0.00024990439414978026]}}, 31: {'grad': {'score': [0.3033970793088277, 0.38940152960625546, 0.31674248330733357, 0.38977385199572107, 0.23561100443204244], 'topk_tokens': [' is', ' had', ' the', ' the', ' having', 'If', ' location', ' location', ' the', ' the', ' he', ' the', ' an', ' paper', ' the', ' August', 'membership', ' the', ' that', ' the'], 'evidence_proportions': [0.2328488032023112, 0.32239198684692383, 0.392254638671875, 0.250791072845459, 0.32608633041381835]}, 'weight': {'score': [0.0031478889286518097, 0.002255946015419305, 0.001850056297638837, 0.0022553214666457566, 0.0005796802043914795], 'topk_tokens': [' the', ' the', 'Question', ' was', ':', ' the', '<|eot_id|>', '.\n\n', ' Where', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005591511726379395, 0.0022475868463516235, 0.0008819639682769775, 0.007815837860107422, 0.005506181716918945]}, 'saliency': {'score': [8.479257424672444e-05, 2.283058574096999e-05, 4.4211745262145996e-05, 2.26487935455633e-05, 5.642970403035482e-06], 'topk_tokens': [' a', ' Market', ' B', ' CHIP', '?\n', ' Where', ' the', ' prior', ' Geo', ' discarded', '.\n\n', '<|eot_id|>', 'Answer', ':', ' the', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'b', 'athroom'], 'evidence_proportions': [1.6058484713236492e-05, 7.675588130950928e-05, 3.061890602111817e-05, 0.00015045702457427979, 0.00017534494400024412]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:08:38.701 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:08:38.701 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-2_0-2-4-6-9.pkl | len: 10 |  size: 9.49 KB
Processing depth (0, 2, 4, 6, 9):   3%|▎         | 3/100 [00:56<30:36, 18.93s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.34it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
Processing depth (2, 3, 6, 7, 9):   3%|▎         | 3/100 [01:03<30:36, 18.93s/it]2025-01-22 03:08:45.900 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:08:45.908 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2501, 2507) --> . Daniel journeyed to the
2025-01-22 03:08:45.908 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:08:45.919 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3860, 3864) -->  Daniel grabbed the milk
2025-01-22 03:08:45.919 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:08:45.939 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7228, 7233) --> . Daniel went to the
2025-01-22 03:08:45.939 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:08:45.962 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8443, 8447) -->  Daniel left the milk
2025-01-22 03:08:45.962 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:08:45.992 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10788, 10793) --> . John moved to the
2025-01-22 03:08:45.992 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John journeyed to the office.
2025-01-22 03:08:45.998 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1870, 1876) --> . John journeyed to the
2025-01-22 03:08:45.998 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:08:46.006 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2849, 2855) --> . John went back to the
2025-01-22 03:08:46.006 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:08:46.015 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3130, 3135) --> . Mary moved to the
2025-01-22 03:08:46.015 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:08:46.029 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4784, 4790) --> . Sandra journeyed to the
2025-01-22 03:08:46.029 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary got the football there.
2025-01-22 03:08:46.049 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7031, 7036) --> . Mary got the football
2025-01-22 03:08:46.049 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:08:46.054 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1871, 1877) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:08:48.082 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:08:48.082 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12224])
your chose emoji: ['🧑🏽\u200d🏫', '➰', '❓', '🇵🇪', '👨🏻\u200d🍼', '🕴', '\U0001fac4🏼', '👆', '🧑🏼\u200d❤\u200d💋\u200d🧑🏻', '🇦🇬']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 165292.77it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.02s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.02it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.71it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.58it/s][A100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.18it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.43it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.19it/s][A100%|██████████| 8/8 [00:00<00:00, 17.22it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.74it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.06it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.07it/s][A100%|██████████| 8/8 [00:00<00:00, 17.07it/s]
2025-01-22 03:08:57.500 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.21950324376424155, 0.280819878609775, 0.26559268727022056, 0.28098335331943375, 0.2913204890031081], 'topk_tokens': ['able', ' appearance', ' out', ' absence', ' communication', 'ols', ' whistle', '.', 'remark', 'ings', '�', ' communication', ' comparison', ' Arr', ' compl', ' examined', ' sop', '�', ' Do', 'consider'], 'evidence_proportions': [0.21934636433919272, 0.15394306182861328, 0.2895233154296875, 0.190460205078125, 0.22535400390625]}, 'weight': {'score': [0.02792612835764885, 0.002563882773004608, 0.0026391148567199707, 0.00251365253347155, 0.0008220140750591572], 'topk_tokens': [' Broadway', ' barric', 'Answer', ' milk', 'Third', '<|eot_id|>', ' garden', 'assistant', ' Market', '.', '<|eot_id|>', 'b', ' the', ':', '\n\n', 'Bridge', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001250391205151876, 0.013089656829833984, 0.02182443141937256, 0.11568832397460938, 0.007698130607604981]}, 'saliency': {'score': [0.001075086494286855, 4.19062088894378e-05, 8.635661181281594e-05, 3.9744351666253244e-05, 2.2080769905677208e-05], 'topk_tokens': [' Cedar', ' Miles', ' Hotel', 'assistant', ':', '<|eot_id|>', 'Third', '<|end_header_id|>', ' bathroom', ' bedroom', '<|eot_id|>', 'c', '<|begin_of_text|>', ' milk', ' garden', 'b', 'athroom', '<|start_header_id|>', 'Bridge', ' Bench'], 'evidence_proportions': [2.0325183868408203e-05, 0.0005147233605384827, 0.0006776809692382812, 0.0045088306069374084, 0.0004395008087158203]}}, 25: {'grad': {'score': [0.42405954996744794, 0.4058200633076388, 0.34720465716193705, 0.40594786149394163, 0.3571488600510817], 'topk_tokens': [' set', ' M', ' for', ' ', ' make', ' a', ' free', ' a', ' hate', ' money', 'M', ' at', ' Aw', ' for', ' of', ' inverted', ' a', ' im', ' l', ' no'], 'evidence_proportions': [0.41614786783854163, 0.4220848083496094, 0.3889923095703125, 0.5692214965820312, 0.354071044921875]}, 'weight': {'score': [0.021127539376417797, 0.002488689761116176, 0.0020279481130487777, 0.0024532170702843124, 0.0013978449197915885], 'topk_tokens': [' barric', 'Third', ' garden', 'Answer', 'b', ' Market', ' the', ' milk', '<|eot_id|>', '?\n', ' Bench', '<|eot_id|>', 'assistant', ':', '.', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000595847765604655, 0.01917243003845215, 0.013780200481414795, 0.08278560638427734, 0.005350542068481445]}, 'saliency': {'score': [0.0015192379554112752, 3.414595664312865e-05, 7.256514885846307e-05, 3.110967917532055e-05, 3.6030090772188627e-05], 'topk_tokens': ['.', ' Thomas', ' Anthony', 'Answer', ' garden', '?\n', ' Daniel', ' Ramsey', '<|eot_id|>', ':', 'Third', ' the', ' Market', ' Ramsey', ' milk', '\n\n', ' Bench', '.', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [2.8933087984720864e-05, 0.0014713183045387268, 0.0005405724048614502, 0.006753221154212952, 0.00013741850852966309]}}, 26: {'grad': {'score': [0.513211170832316, 0.5356298752249121, 0.507576213163488, 0.5357524714460897, 0.48917917104867786], 'topk_tokens': [' badly', 'hue', 'graph', 'char', 'graph', ' bonds', 'est', ' far', 'RI', 'b', 'UX', ' began', 'UX', ' bonds', 'graph', 'b', 'b', 'b', 'itter', ' bitter'], 'evidence_proportions': [0.5086345672607422, 0.623565673828125, 0.48373756408691404, 0.453094482421875, 0.5079864501953125]}, 'weight': {'score': [0.014994037648042044, 0.0024511744114757315, 0.001074843546923469, 0.002430282516637798, 0.0011222004890441895], 'topk_tokens': [' the', '.', ' garden', ' the', ' the', '<|eot_id|>', ' Bench', '<|eot_id|>', '?\n', 'Bridge', 'assistant', 'b', ' barric', 'Answer', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00030978520711263024, 0.007457733154296875, 0.009810495376586913, 0.06405830383300781, 0.0045763134956359865]}, 'saliency': {'score': [0.0011023903886477153, 4.513719061014355e-05, 6.40180181054508e-05, 4.299929720166774e-05, 3.8503225033099834e-05], 'topk_tokens': [' Dan', ' barric', ' Ramsey', ' the', ' Daniel', ' Charles', '?\n', 'Answer', '<|start_header_id|>', '<|end_header_id|>', ' Anthony', '.', ' garden', ' Daniel', 'Bridge', 'athroom', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [1.958012580871582e-05, 0.0006915181875228882, 0.0009897112846374513, 0.004369586706161499, 0.0002293825149536133]}}, 27: {'grad': {'score': [0.3064740498860677, 0.4232417841598, 0.3636470121495864, 0.42363858322882103, 0.41013610913203313], 'topk_tokens': [',', ' was', ' was', ' were', ' would', ' was', ' was', ' be', ' was', ' and', ' also', ' were', ' be', ' during', ',', ' was', ' ideas', ' was', ' Bottle', ' business'], 'evidence_proportions': [0.3655357360839844, 0.27832794189453125, 0.37327270507812504, 0.20928573608398438, 0.26906890869140626]}, 'weight': {'score': [0.028189565986394882, 0.0025224971919435312, 0.0019718987100264605, 0.0024734143336411518, 0.0015693361942584697], 'topk_tokens': [' Broadway', '<|eot_id|>', '?\n', ' bathroom', 'Bridge', 'Answer', ' bathroom', '.', 'assistant', 'b', '.\n\n', ' garden', ' barric', ' the', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006203750769297283, 0.017842590808868408, 0.018729084730148317, 0.12000703811645508, 0.005556678771972657]}, 'saliency': {'score': [0.0037014546493689218, 4.404742544003692e-05, 0.0002255580004523782, 3.632705951640204e-05, 4.2676467161912184e-05], 'topk_tokens': [' City', ' Maj', '\n\n', ' left', ' Market', ' Daniel', ':', ' Daniel', 'Bridge', ' Daniel', ' bathroom', '<|start_header_id|>', ' bathroom', 'b', '.', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', '.\n\n', ' the'], 'evidence_proportions': [5.3942203521728516e-05, 0.0018305107951164246, 0.002380049228668213, 0.01703643798828125, 0.00022864341735839844]}}, 28: {'grad': {'score': [0.24551018079121908, 0.31334306088727815, 0.338131119223202, 0.31340758509952926, 0.33195129541250373], 'topk_tokens': [' summer', 'in', ',', ' half', ' inside', '.', ' the', 'nes', '�', ' lively', 'half', '\n', '<|end_header_id|>', ' lie', '600', ' a', 'dent', 'ball', ' half', ' lie'], 'evidence_proportions': [0.2945963541666667, 0.27337646484375, 0.23919677734375, 0.14411211013793945, 0.25174560546875]}, 'weight': {'score': [0.009176275382439295, 0.0023893645069649264, 0.0014289337046006147, 0.002378662624005686, 0.0010732467357928935], 'topk_tokens': ['.\n\n', ' to', 'Bridge', ' Daniel', ' discarded', ' garden', ' the', '<|eot_id|>', '<|eot_id|>', 'Answer', ' the', 'assistant', '?\n', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00034867723782857263, 0.004882931709289551, 0.018152207136154175, 0.01681375503540039, 0.008118152618408203]}, 'saliency': {'score': [0.0005235473314921061, 4.2440934749424504e-05, 5.451721303603228e-05, 4.14583439873595e-05, 2.1413197884192834e-05], 'topk_tokens': [' Dr', '.\n\n', ' the', ' Dr', ' Daniel', '?\n', ' Daniel', 'athroom', ' garden', 'assistant', ' the', ' Bridge', 'b', ' Bridge', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>', '\n\n', '<|start_header_id|>', ':'], 'evidence_proportions': [1.2328227361043295e-05, 0.00022676587104797363, 0.0012288451194763183, 0.0012743771076202393, 6.847381591796875e-05]}}, 29: {'grad': {'score': [0.3394316037495931, 0.3907641324016521, 0.41452520033892465, 0.3907989836119226, 0.3673825043898362], 'topk_tokens': [' ga', 'LY', ' STR', ' LO', 'THE', 're', ' In', ' S', 'y', ' F', 'A', ' THE', 'y', ' The', 'ION', 'ION', 'ER', ' M', 'ION', ' THE'], 'evidence_proportions': [0.37381108601888025, 0.2811136245727539, 0.344549560546875, 0.3295631408691406, 0.347607421875]}, 'weight': {'score': [0.005099943528572719, 0.002461757021893306, 0.001183550147449269, 0.0024601252162865014, 0.0010820113695584812], 'topk_tokens': ['Question', ' to', ' was', ' Does', ' discarded', ' the', '<|eot_id|>', '.\n\n', ' the', '<|eot_id|>', '?\n', 'Answer', 'assistant', 'b', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00028111040592193604, 0.0013984143733978271, 0.0037941575050354, 0.019188880920410156, 0.0038784027099609377]}, 'saliency': {'score': [0.00016927594939867655, 2.9086464607892075e-05, 5.7700802298153145e-05, 2.873003144851598e-05, 4.397768240708571e-05], 'topk_tokens': [' and', ' to', 'THE', ' No', ' the', '<|eot_id|>', 'Does', ' the', 'THE', '<|eot_id|>', 'Answer', 'athroom', 'assistant', ' Does', ':', '<|start_header_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [6.780028343200684e-06, 1.6003847122192383e-05, 0.00011924505233764648, 0.000706598162651062, 0.00010706186294555664]}}, 30: {'grad': {'score': [0.31797172625859577, 0.4013081840202523, 0.3105297088623047, 0.4017261758964665, 0.3837336026705228], 'topk_tokens': [' the', ' of', ' Europe', ' its', ' soon', ' the', ' the', '3', ' months', ' account', ' Burb', 'b', ' B', '2', ' B', 'b', ' forb', ' account', 'b', 'deal'], 'evidence_proportions': [0.328102191289266, 0.3963050842285156, 0.2764404296875, 0.3866138458251953, 0.22976608276367189]}, 'weight': {'score': [0.011863375703493753, 0.00245447502782049, 0.0027001982226091273, 0.002435232016493432, 0.002851143250098595], 'topk_tokens': [' the', ' Anthony', ' Miles', ' Bor', ' Robert', ' discarded', ' barric', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'assistant', 'Answer', 'b', '?\n', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007818043231964111, 0.007284283638000488, 0.0095910906791687, 0.04072093963623047, 0.00801076889038086]}, 'saliency': {'score': [0.000440157949924469, 7.709885175887002e-05, 9.669100537019618e-05, 7.632807753101568e-05, 6.604882387014535e-05], 'topk_tokens': [' Broadway', '<|eot_id|>', 'Answer', ' bathroom', '.', ' the', ' Ramsey', '.\n\n', ' Bridge', ' barric', ' Bridge', ' bathroom', 'Bridge', ' Bench', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [2.5480985641479492e-05, 0.0003019571304321289, 0.0004418313503265381, 0.001363500952720642, 0.00030798316001892087]}}, 31: {'grad': {'score': [0.31730079650878906, 0.3804400759062296, 0.3248813853544347, 0.38071983087247985, 0.2761481871971717], 'topk_tokens': [' the', ' the', ' the', ' is', ' location', ' the', ' an', ' was', ' location', ' he', ' August', ' location', 'If', ' the', ' the', 'did', 'membership', ' the', ' the', ' the'], 'evidence_proportions': [0.34646479288736975, 0.31935787200927734, 0.2995162963867188, 0.26025962829589844, 0.34407577514648435]}, 'weight': {'score': [0.004265030225118001, 0.002269256765263112, 0.0014531402026905732, 0.002267600869141076, 0.0014282061503483698], 'topk_tokens': [' Market', ' was', ' the', 'Question', ' Where', ':', ',', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'assistant', '<|eot_id|>', 'b', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004567950963973999, 0.004234999418258667, 0.003462451696395874, 0.008188486099243164, 0.006522750854492188]}, 'saliency': {'score': [8.684272567431132e-05, 2.653132312147871e-05, 2.390409217161291e-05, 2.6419715938557142e-05, 2.0673183294442985e-05], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' Market', ' Key', ' discarded', ' the', ' the', ' the', ':', '\n\n', '?\n', '<|eot_id|>', 'Answer', ' the', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [9.641051292419434e-06, 0.00010716170072555542, 8.892416954040527e-05, 0.00013012439012527466, 0.0001265227794647217]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:08:57.502 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:08:57.502 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-3_2-3-6-7-9.pkl | len: 10 |  size: 9.45 KB
Processing depth (2, 3, 6, 7, 9):   4%|▍         | 4/100 [01:15<30:13, 18.89s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (0, 1, 6, 7, 8):   4%|▍         | 4/100 [01:22<30:13, 18.89s/it]2025-01-22 03:09:05.272 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:09:05.272 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:09:05.272 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:09:05.277 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1503, 1507) -->  Daniel grabbed the milk
2025-01-22 03:09:05.277 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:09:05.298 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7267, 7272) --> . Daniel went to the
2025-01-22 03:09:05.298 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:09:05.320 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8442, 8446) -->  Daniel left the milk
2025-01-22 03:09:05.321 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:09:05.348 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9727, 9732) --> . John moved to the
2025-01-22 03:09:05.348 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John journeyed to the office.
2025-01-22 03:09:05.353 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1878, 1884) --> . John journeyed to the
2025-01-22 03:09:05.353 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:09:05.362 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2850, 2856) --> . John went back to the
2025-01-22 03:09:05.362 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 03:09:05.370 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3131, 3136) --> . Mary moved to the
2025-01-22 03:09:05.371 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:09:05.388 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4749, 4755) --> . Sandra journeyed to the
2025-01-22 03:09:05.388 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary got the football there.
2025-01-22 03:09:05.407 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7066, 7071) --> . Mary got the football
2025-01-22 03:09:05.407 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:09:05.413 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1879, 1885) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:09:07.438 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:09:07.438 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12221])
your chose emoji: ['💛', '🧑🏻\u200d🎤', '👱\u200d♀️', '🖱', '\U0001faf8🏽', '⏮', '👨🏿\u200d🔧', '🚣🏻\u200d♀️', '🧔🏽\u200d♂️', '🏑']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 226719.14it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:31,  4.50s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.20s/it][A
 75%|███████▌  | 6/8 [00:04<00:00,  2.03it/s][A100%|██████████| 8/8 [00:04<00:00,  1.66it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.12it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.09it/s][A100%|██████████| 8/8 [00:00<00:00, 17.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.50it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.58it/s][A
100%|██████████| 8/8 [00:00<00:00, 16.96it/s][A100%|██████████| 8/8 [00:00<00:00, 17.66it/s]
2025-01-22 03:09:16.107 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2748395999272664, 0.273491614776132, 0.2586776509004481, 0.27353035578617196, 0.3491480427403604], 'topk_tokens': [' du', ' was', '�', '�', 'low', 'sur', '.', ' comparison', '.', '.', ' examined', '�', '�', ' absence', ' St', ' first', '.', ' S', ' first', ' turtle'], 'evidence_proportions': [0.2416202227274577, 0.2537040710449219, 0.2853404998779297, 0.20849990844726562, 0.37418212890625]}, 'weight': {'score': [0.046029962599277496, 0.00256740469583042, 0.004660558174638187, 0.002475815956066965, 0.0008953357896497173], 'topk_tokens': [' the', ' milk', '.', ' Daniel', ' bathroom', 'Answer', '<|eot_id|>', ' Bridge', 'Daniel', 'assistant', '<|eot_id|>', ':', '\n\n', '<|start_header_id|>', ' garden', 'b', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.12574450174967447, 0.03699493408203125, 0.008542144298553466, 0.03694963455200195, 0.002352619171142578]}, 'saliency': {'score': [0.004696615040302277, 3.971149706091556e-05, 0.0001538615016376271, 3.020576097728899e-05, 2.1104851076679844e-05], 'topk_tokens': ['b', ' bend', '<|eot_id|>', '***', '.', ' Daniel', ' milk', ':', ' The', '<|eot_id|>', ' Bridge', ' Bench', '<|start_header_id|>', ' bathroom', '<|begin_of_text|>', ' Daniel', 'Daniel', 'athroom', ' garden', ' bathroom'], 'evidence_proportions': [0.013582850495974222, 0.00572715699672699, 0.00016094446182250978, 0.0018453896045684814, 2.5349855422973633e-05]}}, 25: {'grad': {'score': [0.5116748015085856, 0.5759124356414635, 0.44293829974006205, 0.576410777236058, 0.34213426805311636], 'topk_tokens': [' for', ' with', ' with', ' no', ' containing', ' M', ' set', ' a', ' hate', ' a', ' with', ' little', ' by', ' free', ' money', ' a', ' for', ' of', ' inverted', ' no'], 'evidence_proportions': [0.5921630859375, 0.5290069580078125, 0.47680015563964845, 0.4883232116699219, 0.45477905273437497]}, 'weight': {'score': [0.020049430429935455, 0.002488068417104751, 0.0022552241297329172, 0.002454075651813177, 0.0010716905516962852], 'topk_tokens': [' the', ' the', ' bathroom', ' Bench', 'Answer', '?\n', ' Daniel', 'Daniel', ' garden', '<|eot_id|>', 'b', '<|eot_id|>', ' bathroom', 'assistant', '<|start_header_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.030037959416707356, 0.030059814453125, 0.00337873101234436, 0.03774285316467285, 0.0025708496570587156]}, 'saliency': {'score': [0.0006897946198781332, 2.6753096699870695e-05, 4.753207459169276e-05, 2.5387037041429114e-05, 2.9828759931748913e-05], 'topk_tokens': ['assistant', 'E', 'Answer', ' Dan', ' bend', 'Daniel', '<|eot_id|>', 'b', ' bathroom', '<|start_header_id|>', '<|eot_id|>', ' Geo', ' garden', ' bathroom', 'athroom', ':', ' Bench', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001554777224858602, 0.0009087696671485901, 9.174346923828125e-05, 0.0007510185241699219, 2.5707483291625977e-05]}}, 26: {'grad': {'score': [0.3157246907552083, 0.3205532852891852, 0.31538839901194854, 0.3205772448816759, 0.3588047642861643], 'topk_tokens': ['agle', 'graph', ' Press', 'rich', ' Herald', ' Gutenberg', ' Marshall', ' and', ' Milwaukee', ' galaxy', 'ers', 'ers', ' Marshall', 'hue', 'UX', 'ers', ' Empire', 'UX', ' Eagle', 'ers'], 'evidence_proportions': [0.28299967447916663, 0.41082763671875, 0.3430419921875, 0.2452850341796875, 0.30794677734375]}, 'weight': {'score': [0.026791750142971676, 0.002438717565611395, 0.0018803161733290728, 0.002392236624092486, 0.0008698331732903757], 'topk_tokens': [' prior', ' Bridge', '.\n\n', 'Daniel', '<|eot_id|>', '<|eot_id|>', '?\n', ' bathroom', 'Answer', ' garden', 'assistant', ' the', 'b', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.07649396856625876, 0.012819468975067139, 0.0056101024150848385, 0.02368783950805664, 0.0019916892051696777]}, 'saliency': {'score': [0.00136118878920873, 3.856897636935973e-05, 6.56361089033239e-05, 3.5884186133102974e-05, 3.262296799690493e-05], 'topk_tokens': ['<|end_header_id|>', ' Gray', ' Anthony', '185', ' Jackson', ' Floral', 'assistant', 'Answer', '185', ' Daniel', '?\n', ' bathroom', 'Daniel', '<|begin_of_text|>', '\n\n', ' bathroom', 'athroom', ' garden', 'b', ':'], 'evidence_proportions': [0.004221697648366293, 0.0009440332651138306, 0.0002311885356903076, 0.0005361437797546387, 5.23388385772705e-05]}}, 27: {'grad': {'score': [0.30128729343414307, 0.3276900146644153, 0.29625679464901195, 0.3278299451912976, 0.2719727639229067], 'topk_tokens': [' short', '\n', 'ers', ' conventions', ' convention', ' started', ' Thanksgiving', ' exchanged', ' received', '\n', 'ition', ' other', ' intended', ' Bottle', ' platform', 'roduced', ' designated', ' short', ' step', ' accepted'], 'evidence_proportions': [0.2590818405151367, 0.23813819885253906, 0.2964775085449219, 0.32524704933166504, 0.3880950927734375]}, 'weight': {'score': [0.03322222332159678, 0.0025210137922726376, 0.0026217655223958634, 0.002460167615425032, 0.0012075583780965498], 'topk_tokens': [' Jackson', ' Bridge', 'NEW', '<|eot_id|>', ' Daniel', '?\n', 'Answer', 'assistant', 'Daniel', ' bathroom', '.\n\n', 'b', ' garden', '\n\n', ' bathroom', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.08946055173873901, 0.029898151755332947, 0.0061329066753387455, 0.02517223358154297, 0.0019247949123382567]}, 'saliency': {'score': [0.0016381243864695232, 3.8152002285756365e-05, 0.00026718220290015726, 3.4355654756469826e-05, 2.3669292849879112e-05], 'topk_tokens': [' THE', ' discarded', ' the', ' to', ' Daniel', ' dropped', ' prior', '<|begin_of_text|>', 'THE', ' bathroom', '<|start_header_id|>', 'NEW', 'Daniel', ' Bridge', ' bathroom', ' garden', '<|end_header_id|>', 'b', 'athroom', '.\n\n'], 'evidence_proportions': [0.004541054368019104, 0.0013611316680908203, 0.000485384464263916, 0.0009967684745788574, 4.202723503112793e-05]}}, 28: {'grad': {'score': [0.3014262517293294, 0.35161062310503416, 0.32364312340231505, 0.3517877823934535, 0.3080943399860013], 'topk_tokens': [' the', 'yl', 'arp', ',', 'antic', ' the', 'ien', '\n', ' the', ' a', 'ien', 'ew', 'S', ' lie', 'en', 'ivery', 'nes', ' lively', ' lie', 'dent'], 'evidence_proportions': [0.4279327392578125, 0.3129844665527344, 0.28143310546875, 0.14141273498535156, 0.2883758544921875]}, 'weight': {'score': [0.013602487742900848, 0.002367502653786025, 0.00345614377190085, 0.0023422968803062734, 0.0006334214441237911], 'topk_tokens': [' Daniel', ' Bridge', ' the', 'Answer', ' a', ' the', '<|eot_id|>', ' garden', '?\n', '<|eot_id|>', ' bathroom', 'assistant', ' the', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.01028577486673991, 0.00710827112197876, 0.015830862522125243, 0.033905982971191406, 0.0043067455291748045]}, 'saliency': {'score': [0.0003314452866713206, 3.597183708901181e-05, 0.00010850850273581112, 3.518623710364549e-05, 1.1164815195145145e-05], 'topk_tokens': [' the', ' a', '?\n', 'Bridge', ' the', ' Floral', ' bathroom', ' milk', ' Bridge', ' garden', 'b', 'athroom', '\n\n', 'assistant', ' the', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0003703782955805461, 0.00015150755643844604, 0.0003583371639251709, 0.0007841810584068298, 3.959536552429199e-05]}}, 29: {'grad': {'score': [0.3276201883951823, 0.28896523520584505, 0.3240057720857508, 0.2887910532947436, 0.3275864508844191], 'topk_tokens': ['s', ' not', 'P', ' B', ' S', ' The', 'ION', ' ga', 'Spring', ' PA', ' The', 'b', ' B', 're', ' B', ' In', ' S', ' Ch', ' P', ' M'], 'evidence_proportions': [0.48089599609375, 0.30104827880859375, 0.1256072998046875, 0.2887916564941406, 0.3980224609375]}, 'weight': {'score': [0.006342142820358276, 0.0024239086199805376, 0.0026035089703167185, 0.00241567715255324, 0.001023946269865959], 'topk_tokens': [' the', ' the', '<|eot_id|>', ' to', ' where', ' was', '?\n', ' Does', '<|eot_id|>', ' the', '.\n\n', 'Answer', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006226773063341777, 0.0014140903949737549, 0.002652889490127564, 0.021352052688598633, 0.002104353904724121]}, 'saliency': {'score': [0.00018133223056793213, 3.510382494533249e-05, 8.08572067933924e-05, 3.4687492813343636e-05, 5.293757684769169e-05], 'topk_tokens': ['️', '.', ' place', ' to', '<|eot_id|>', ' where', 'Answer', ' a', '<|eot_id|>', ' was', '<|end_header_id|>', 'NEW', ' Does', 'assistant', 'athroom', '<|start_header_id|>', ':', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00035221874713897705, 1.7076730728149414e-05, 4.45246696472168e-05, 0.0004300624132156372, 4.549622535705566e-05]}}, 30: {'grad': {'score': [0.28094132741292316, 0.2923174927996091, 0.2281936196719899, 0.2925191399848483, 0.25480740301070676], 'topk_tokens': [' of', 'moment', ' an', '2', ' the', ' account', ' of', ' its', ' abroad', ' the', ' Burb', 'b', ' account', '3', ' B', ' forb', 'b', 'b', ' B', 'deal'], 'evidence_proportions': [0.2388483683268229, 0.33896636962890625, 0.25800476074218753, 0.34546852111816406, 0.25634765625]}, 'weight': {'score': [0.012370980034271875, 0.0024412012225046207, 0.004677386844859403, 0.0024153632311646176, 0.003389117217832996], 'topk_tokens': [' Where', ' to', ':', ' the', ' the', '<|eot_id|>', 'Question', ' bathroom', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', '?\n', 'b', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.011394222577412924, 0.01183384656906128, 0.0057605445384979245, 0.034037113189697266, 0.0032503247261047364]}, 'saliency': {'score': [0.001587918649117152, 4.40947423781712e-05, 0.0001512765884399414, 4.074968594813374e-05, 3.5370069165383614e-05], 'topk_tokens': [' the', ' the', '?\n', ' Daniel', ' Bench', '.\n\n', ' Daniel', ' milk', ':', 'assistant', ' Bridge', ' the', ' garden', '<|end_header_id|>', 'athroom', ' bathroom', '<|begin_of_text|>', ' bathroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.004054740071296692, 0.0007942244410514832, 0.00016067624092102052, 0.002349480986595154, 8.068084716796876e-05]}}, 31: {'grad': {'score': [0.24868117769559225, 0.30861641037526555, 0.2519012707121232, 0.3088931455661959, 0.17075571994627675], 'topk_tokens': [' the', ' having', ' the', ' was', ' the', 'did', ' the', ' the', ' they', ' the', ' the', ' he', '7', ' paper', ' the', ' August', 'membership', ' the', ' an', ' the'], 'evidence_proportions': [0.21293052037556964, 0.21979689598083496, 0.2899953842163086, 0.24347352981567383, 0.2775413036346436]}, 'weight': {'score': [0.0028770280381043753, 0.0022556431474485945, 0.001446764258777394, 0.0022566778872841267, 0.000978225661862281], 'topk_tokens': [' where', 'Question', ' was', ',', ':', ' the', '.\n\n', '<|eot_id|>', ' Where', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0012050867080688477, 0.002261422574520111, 0.0016388893127441406, 0.007765412330627441, 0.002703273296356201]}, 'saliency': {'score': [5.412225921948751e-05, 1.8208582656389756e-05, 2.1725016481736128e-05, 1.812790807250219e-05, 8.908971663444273e-06], 'topk_tokens': [' left', ':', ' Where', ' the', ' prior', ' S', ' discarded', '.\n\n', '.', '<|eot_id|>', '?\n', ' the', 'Answer', ':', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'b', 'athroom'], 'evidence_proportions': [2.156694730122884e-05, 5.637854337692261e-05, 5.508661270141601e-05, 0.00012618303298950195, 3.277063369750976e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:09:16.109 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:09:16.110 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-4_0-1-6-7-8.pkl | len: 10 |  size: 9.5 KB
Processing depth (0, 1, 6, 7, 8):   5%|▌         | 5/100 [01:33<29:44, 18.79s/it]Processing depth (0, 1, 6, 7, 8):   5%|▌         | 5/100 [01:34<29:48, 18.82s/it]
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:72 - Selected idx: 8
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the kitchen? 
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:76 - Needle: [' Mary journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Sandra picked up the milk.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Sandra travelled to the hallway.', ' Mary got the football there.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:77 - Real Needle: [' Sandra picked up the milk.', ' Sandra travelled to the hallway.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-22 03:09:16.354 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (3, 4, 5, 7):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:09:23.553 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 03:09:23.564 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3764, 3769) --> . Sandra picked up the
2025-01-22 03:09:23.564 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 03:09:23.577 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4892, 4897) --> . Sandra travelled to the
2025-01-22 03:09:23.577 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:09:23.578 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (149, 154) -->  back to the kitchen.
2025-01-22 03:09:23.578 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:09:23.579 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (146, 152) --> . Daniel went back to the
2025-01-22 03:09:23.579 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:09:23.580 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (206, 212) -->  John journeyed to the office
2025-01-22 03:09:23.580 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 03:09:23.580 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (146, 152) --> . Daniel went back to the
2025-01-22 03:09:23.580 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the bedroom.
2025-01-22 03:09:23.593 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4730, 4735) --> . John moved to the
2025-01-22 03:09:23.593 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:09:23.600 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2290, 2295) -->  Mary moved to the bathroom
2025-01-22 03:09:23.600 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the bedroom.
2025-01-22 03:09:23.604 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1474, 1480) --> . John went back to the
2025-01-22 03:09:23.604 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the office.
2025-01-22 03:09:23.605 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (205, 211) --> . John journeyed to the
2025-01-22 03:09:23.605 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Mary got the football there.
2025-01-22 03:09:23.625 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (7152, 7157) --> . Mary got the football
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:09:25.663 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:09:25.664 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12206])
your chose emoji: ['🧕🏼', '🚢', '✊🏽', '🧑🏼\u200d🎄', '🥔', '\U0001fac5🏾', '📮', '👊🏻', '🙎🏽\u200d♀️', '🖕']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 234646.38it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.30s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.63it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.47it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.54it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.74it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.14it/s][A100%|██████████| 8/8 [00:00<00:00, 17.83it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.41it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 18.21it/s][A100%|██████████| 8/8 [00:00<00:00, 18.99it/s]
2025-01-22 03:09:35.064 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6560403733026414, 0.5365865193261631, 0.5979150625375601, 0.5361831656247267, 1.0624016655815973], 'topk_tokens': [' lin', ' type', ' lin', ' upper', ' lin', ' type', 'hand', ' doctors', ' typ', ' type', ' work', 'dr', 'men', ' project', 'hand', ' hand', ' Project', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.6794326782226563, 0.6153564453125, 0.6466262817382813, 0.6782951354980469]}, 'weight': {'score': [0.009606794232413882, 0.00256003727343854, 0.019767146844130296, 0.002492619447329769, 0.0015226872982802215], 'topk_tokens': [' milk', ' the', ' bedroom', ' office', '<|start_header_id|>', ':', ' bedroom', ' the', ' kitchen', 'assistant', 'Bridge', '\n\n', '<|eot_id|>', ' hallway', ' football', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.037832438945770264, 0.001713573932647705, 0.0006002843379974365, 0.000168532133102417]}, 'saliency': {'score': [0.0004756677718389602, 4.791871209980004e-05, 0.00325662585405203, 3.6878930324291e-05, 7.111165258619521e-05], 'topk_tokens': [' THE', 'Answer', ' the', ' lounge', ' Sandra', 'way', '<|eot_id|>', ' milk', ' hallway', ' office', '\n\n', ' kitchen', ' bedroom', '<|eot_id|>', 'hall', '<|end_header_id|>', 'Bridge', ' bedroom', '<|begin_of_text|>', ' football'], 'evidence_proportions': [0.001910632848739624, 4.715323448181152e-05, 3.6710500717163085e-05, 2.7567148208618164e-06]}}, 25: {'grad': {'score': [0.9409873599097842, 1.2980364926744614, 1.0072682698567708, 1.299587072349986, 0.7540424311602557], 'topk_tokens': [' most', ' two', ' most', ' too', ' of', ' had', ' post', ' two', ' large', ' a', ' a', 'two', ' great', ' per', ' so', 'ome', ' two', ' so', ' two', ' post'], 'evidence_proportions': [0.5970687866210938, 0.8541015624999999, 1.080712890625, 1.1835530598958333]}, 'weight': {'score': [0.006924717199234735, 0.0025529425582991976, 0.0071142613887786865, 0.0025307433071799, 0.0018144840443575824], 'topk_tokens': ['<|start_header_id|>', ' the', '.', ' THE', '.\n\n', ' the', '<|start_header_id|>', ' \n', ' football', 'Answer', ' there', 'hall', ':', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.026537448167800903, 0.001762700080871582, 0.000679624080657959, 8.669992287953694e-05]}, 'saliency': {'score': [0.0003464775426047189, 5.2550277132710284e-05, 0.0006264677414527306, 5.0199857041888574e-05, 7.672497519740352e-05], 'topk_tokens': [':', '<|start_header_id|>', '.\n\n', ' football', ' milk', 'way', ' hallway', 'Answer', ' doctor', 'hall', ' there', 'assistant', ' Dan', ' Mary', '<|eot_id|>', '<|eot_id|>', '.', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013503968715667723, 4.761815071105957e-05, 5.137324333190918e-05, 4.8478444417317706e-06]}}, 26: {'grad': {'score': [0.6695440383184523, 0.6310871782598902, 0.6603043385041065, 0.6309269129449875, 0.5638725845902054], 'topk_tokens': ['s', ' Out', ' Marshall', ' Red', ' a', 'ian', ' true', ' not', ' commander', ' Marshall', ' time', 'could', ' uncommon', ' not', 'ian', 'ucci', ' and', ' true', 'outs', ' it'], 'evidence_proportions': [0.6513916015625, 0.598681640625, 0.6938232421875, 0.7234903971354167]}, 'weight': {'score': [0.0026128235317411878, 0.002498631551953301, 0.007415758493619087, 0.0024826494972738614, 0.002165957181542008], 'topk_tokens': ['?', '.\n\n', ' kitchen', 'Bridge', ' the', ' kitchen', ' football', ' \n', 'Answer', ' hallway', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', 'hall', '<|eot_id|>', 'way', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.009888499975204468, 0.0008078038692474366, 0.0002240002155303955, 4.462897777557373e-05]}, 'saliency': {'score': [0.00011763970057169597, 7.65089179659292e-05, 0.0006451316368885529, 7.461246291014656e-05, 5.506365387528031e-05], 'topk_tokens': [' Father', ' Bridge', '<|start_header_id|>', 'Answer', ' the', ' the', ' Western', ' \n', ' kitchen', '<|eot_id|>', '<|eot_id|>', ':', ' hallway', ' football', 'way', 'Bridge', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00041749477386474607, 5.4436922073364255e-05, 1.816749572753906e-05, 3.3229589462280273e-06]}}, 27: {'grad': {'score': [0.35286095028831843, 0.30907493239490336, 0.389194708604079, 0.3087420508698462, 0.39837589970341436], 'topk_tokens': [' money', ' heart', ' candidates', ' mailing', 'hand', ' was', ' John', ' EAR', ' book', ' earth', ' ear', ' New', ' hand', ' John', ' sco', ' hand', ' business', ' hand', ' hand', 'hand'], 'evidence_proportions': [0.143182373046875, 0.4722991943359375, 0.415576171875, 0.3757985432942708]}, 'weight': {'score': [0.004833190214066278, 0.0025420429099444843, 0.011705945699642867, 0.0025086651585094037, 0.002329230308532715], 'topk_tokens': [' bedroom', ' lounge', ' kitchen', ' \n', ' bedroom', 'Answer', '<|start_header_id|>', ' football', '\n\n', '.\n\n', 'assistant', ' THE', '<|eot_id|>', '<|eot_id|>', 'hall', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.017277318239212035, 0.0014314532279968261, 0.001410388946533203, 0.00015019873778025308]}, 'saliency': {'score': [0.0006682603132157098, 6.0261767798495706e-05, 0.0012167661617963742, 5.549827773185e-05, 8.761220508151584e-05], 'topk_tokens': ['<|start_header_id|>', ' bathroom', '.', ' kitchen', ' \n', ' the', ' office', 'NEW', ' the', ' kitchen', ' there', 'assistant', '<|begin_of_text|>', ' THE', ' football', ' hallway', '<|end_header_id|>', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.0024585962295532227, 0.00016695261001586914, 0.0001599133014678955, 1.769264539082845e-05]}}, 28: {'grad': {'score': [0.565964835030692, 0.5864735735420591, 0.48697476509289866, 0.5868284288419403, 0.4385759212352611], 'topk_tokens': [' hall', ' house', '\n', ' houses', ' locate', 'ched', '\n', 'ial', '\n\n\n\n', ' lengthy', ' York', '\n', ' hall', 'ed', '\n', ' hall', 'ivery', 'ching', ' huge', 'arp'], 'evidence_proportions': [0.577392578125, 0.618682861328125, 0.5793121337890625, 0.5013872782389324]}, 'weight': {'score': [0.0010679889292944046, 0.002448144538248871, 0.0034912580098861302, 0.0024471816476730373, 0.0018916886161874842], 'topk_tokens': [' football', ' the', '.', ' kitchen', '.\n\n', '?', ' before', ' kitchen', 'Answer', ' \n', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'hall', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003286069631576538, 0.0008884608745574951, 0.0002599060535430908, 4.259745279947917e-05]}, 'saliency': {'score': [6.174189703805106e-05, 7.153825723174867e-05, 0.00017975691037300305, 7.120779349741323e-05, 7.28429467589767e-05], 'topk_tokens': ['Answer', ' before', ' hallway', '<|eot_id|>', ' subscribers', ' football', ' hallway', '?', ' kitchen', ' milk', '<|eot_id|>', 'assistant', ' kitchen', '<|end_header_id|>', '<|start_header_id|>', 'way', '\n\n', ':', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0002039790153503418, 4.1365623474121094e-05, 1.0234117507934571e-05, 3.11434268951416e-06]}}, 29: {'grad': {'score': [0.3981030782063802, 0.4234713325434874, 0.3561301598182091, 0.4237313571609345, 0.3512876651905201], 'topk_tokens': [' of', 'ck', 'ond', ' of', ' by', ' of', ' from', ' of', ' in', ' of', ' more', '5', 'ile', ' mild', ' of', ' by', ' Far', 'in', ' for', 'A'], 'evidence_proportions': [0.3308296203613281, 0.26826629638671873, 0.468109130859375, 0.5040232340494791]}, 'weight': {'score': [0.0014342623097555979, 0.0025193475077589485, 0.00218088122514578, 0.002522309642435052, 0.0015011849226774993], 'topk_tokens': [' Where', ' the', '<|start_header_id|>', ' kitchen', ' Does', ' before', '?', ' \n', '.\n\n', 'Answer', 'hall', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'way', ':', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00519755482673645, 0.0005306482315063476, 0.00024090409278869628, 4.5662124951680504e-05]}, 'saliency': {'score': [0.00011588278270903088, 3.1336864935735664e-05, 0.00013364889682867588, 3.086228813788697e-05, 0.00014367478865164298], 'topk_tokens': ['"The', ' the', 'Right', ' THE', ' \n', 'IVE', '�', '?', 'assistant', ' part', ' Does', ':', '<|eot_id|>', 'way', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000467449426651001, 1.1992454528808593e-05, 6.127357482910157e-06, 9.487072626749675e-07]}}, 30: {'grad': {'score': [0.35791451590401785, 0.43478556778362787, 0.3436122796474359, 0.43521112135402734, 0.4716129656191225], 'topk_tokens': [' Fourth', ' boat', ' Paul', ' People', '0', 'doll', ' boat', 'moment', ' Malta', '2', ' hour', ' Star', 'deal', ' preparations', 'AM', ' S', ' Bridge', ' Loan', ' Europe', ' an'], 'evidence_proportions': [0.31043853759765627, 0.31558685302734374, 0.39772338867187496, 0.3995768229166667]}, 'weight': {'score': [0.003647487787973313, 0.0024802374091856465, 0.007013106193297949, 0.002463668626525763, 0.007141419031001904], 'topk_tokens': [' football', ' kitchen', '<|end_header_id|>', ' the', '<|start_header_id|>', ' kitchen', '?', '.\n\n', '\n\n', 'hall', '<|start_header_id|>', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.01304340958595276, 0.0014011502265930177, 0.0005513966083526611, 0.00026957690715789795]}, 'saliency': {'score': [0.0002194572062719436, 6.223224833923784e-05, 0.0007699857919644087, 5.9688490637536596e-05, 0.0003104828022144459], 'topk_tokens': ['Question', ' the', '<|start_header_id|>', '\n\n', '?', ' kitchen', ' hallway', ' Wild', ' \n', ' the', ' office', 'Bridge', ' the', ' football', ' kitchen', 'assistant', 'way', 'hall', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0008006870746612548, 7.501840591430664e-05, 3.354549407958985e-05, 1.03910764058431e-05]}}, 31: {'grad': {'score': [0.36447408085777644, 0.5470043035209281, 0.3236375512220921, 0.5480368525385906, 0.2691288540760676], 'topk_tokens': [' the', ' the', ' able', ' the', ' its', ' United', ' had', 'the', ' F', ' that', ' the', ' the', ' government', ' the', ' would', ' these', ' have', ' Independent', ' the', ' the'], 'evidence_proportions': [0.39150562286376955, 0.30307607650756835, 0.3634397506713867, 0.3939747412999471]}, 'weight': {'score': [0.0007051257860092889, 0.0022831206064335943, 0.001175003938185863, 0.002289405439859438, 0.001978528168466356], 'topk_tokens': [' the', ' before', 'Question', ' Where', ':', ' kitchen', '.\n\n', '?', 'Answer', '<|eot_id|>', ' \n', '<|start_header_id|>', 'assistant', ':', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0017513036727905273, 0.00039806365966796873, 0.0004608035087585449, 0.00029279788335164386]}, 'saliency': {'score': [2.767500423249744e-05, 2.8458942856539846e-05, 5.070292032681979e-05, 2.838889162514335e-05, 3.915197319454617e-05], 'topk_tokens': [' Mess', '<|eot_id|>', ' the', ' kitchen', ':', '.\n\n', ' Where', 'Answer', ' the', ' hallway', 'Question', ' the', '<|begin_of_text|>', ':', ' hallway', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'way', 'hall'], 'evidence_proportions': [5.363225936889649e-05, 5.894899368286133e-06, 4.655718803405762e-05, 8.458892504374186e-06]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:09:35.066 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:09:35.066 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-0_3-4-5-7.pkl | len: 10 |  size: 9.22 KB
Processing depth (3, 4, 5, 7):   1%|          | 1/100 [00:18<30:43, 18.62s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]
Processing depth (0, 3, 4, 9):   1%|          | 1/100 [00:25<30:43, 18.62s/it]2025-01-22 03:09:42.534 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 03:09:42.535 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-22 03:09:42.535 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 03:09:42.546 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3832, 3837) -->  war. Sandra travelled to
2025-01-22 03:09:42.546 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:09:42.547 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (173, 178) -->  back to the kitchen.
2025-01-22 03:09:42.547 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:09:42.548 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (170, 176) --> . Daniel went back to the
2025-01-22 03:09:42.548 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:09:42.549 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (324, 330) -->  John journeyed to the office
2025-01-22 03:09:42.549 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 03:09:42.549 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (170, 176) --> . Daniel went back to the
2025-01-22 03:09:42.550 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the bedroom.
2025-01-22 03:09:42.563 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4752, 4757) --> . John moved to the
2025-01-22 03:09:42.563 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:09:42.570 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2377, 2382) -->  the senate. Mary moved
2025-01-22 03:09:42.570 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the bedroom.
2025-01-22 03:09:42.575 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1569, 1575) -->  ranks. John went back to
2025-01-22 03:09:42.575 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the office.
2025-01-22 03:09:42.576 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (323, 329) --> . John journeyed to the
2025-01-22 03:09:42.576 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Mary got the football there.
2025-01-22 03:09:42.596 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (7108, 7113) --> . Mary got the football
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:09:44.650 | INFO     | test_jbb_retain:begin_test:632 - The bedroom.<|eot_id|>
2025-01-22 03:09:44.651 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12216])
your chose emoji: ['👨🏼\u200d🎨', '🇲🇺', '🧑\u200d🤝\u200d🧑', '🧛🏿', '🏇', '👨🏿\u200d🔧', '🚾', '🦷', '🅿️', '💆🏽\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.53s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.47s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.68it/s][A100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.52it/s][A
 50%|█████     | 4/8 [00:00<00:00, 18.38it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 21.14it/s][A100%|██████████| 8/8 [00:00<00:00, 20.64it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.31it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.07it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.24it/s][A100%|██████████| 8/8 [00:00<00:00, 20.26it/s]
2025-01-22 03:09:53.951 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7890479678199405, 0.5775869675300761, 0.7071405068421975, 0.5768062068557395, 0.9287046402219742], 'topk_tokens': [' writer', 'Frank', ' Aw', ' type', ' work', 'otype', 'itter', ' hand', ' Wide', ' type', ' set', ' type', 'dr', 'hand', 'hand', ' hand', ' hand', ' hand', ' hand', ' Wide'], 'evidence_proportions': [0.7580322265625, 0.9478271484374999, 0.633026123046875, 0.8125966389973958]}, 'weight': {'score': [0.008905968495777674, 0.0025518613884458565, 0.006011133774732932, 0.0025297914918820633, 0.0007343793672228617], 'topk_tokens': ['<|start_header_id|>', ' bedroom', ' milk', '\n\n', ' Bridge', 'Bridge', ' Bridge', 'Answer', ':', '\n\n', '<|start_header_id|>', ' hallway', 'assistant', '<|eot_id|>', '\n\n', 'hall', '<|eot_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.02375450134277344, 0.00628313422203064, 0.0062663197517395025, 0.0009175936381022136]}, 'saliency': {'score': [0.0004889056796119327, 3.996773138404507e-05, 0.0006977793497916979, 3.708243242601529e-05, 2.709078410315135e-05], 'topk_tokens': [' Project', ' ', 'assistant', '\n\n', ' Bridge', 'way', 'andra', ' hallway', ' Bridge', '\n\n', 'E', ' PA', ' boat', 'Bridge', ' bedroom', 'hall', ' football', '<|end_header_id|>', ' Bench', '<|begin_of_text|>'], 'evidence_proportions': [0.0014074981212615968, 0.00037918686866760253, 0.000218123197555542, 4.0496389071146645e-05]}}, 25: {'grad': {'score': [1.326091584705171, 1.523882663526066, 1.2706549228766026, 1.525036499741262, 1.0860298398941282], 'topk_tokens': [' so', ' her', ' so', ' had', 'ad', ' extraordinary', 'ome', ' great', 'po', ' two', ' used', ' post', ' to', '10', 'two', ' too', ' large', ' two', ' post', ' a'], 'evidence_proportions': [1.17864990234375, 1.2031417846679688, 1.4005859374999998, 1.4893391927083335]}, 'weight': {'score': [0.007311040446871803, 0.0025552935701353536, 0.00289955123876914, 0.0025459756382751526, 0.0008155632586706252], 'topk_tokens': [' PA', '<|eot_id|>', '\n\n\n', '<|end_header_id|>', ' the', '.', '.\n\n', '<|start_header_id|>', '<|start_header_id|>', 'hall', 'Answer', ' \n', ':', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.010462760925292969, 0.0034770309925079347, 0.01446514129638672, 0.0019178638855616252]}, 'saliency': {'score': [0.0004800189109075637, 5.99732189426661e-05, 0.0002489915260901818, 5.864147508955186e-05, 2.3743462941003223e-05], 'topk_tokens': [' \n', '.\n\n', '<|end_header_id|>', 'hall', '<|start_header_id|>', ' Mary', ':', ' PA', '.', '.', ' doctor', 'way', '<|start_header_id|>', 'assistant', ' Dan', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0004248261451721192, 0.00019372105598449706, 0.00135001540184021, 3.959735234578451e-05]}}, 26: {'grad': {'score': [0.5901284899030413, 0.5109834209754021, 0.5085310080112555, 0.5108545943990488, 0.46825772240048363], 'topk_tokens': [' Curtis', ' could', 'ors', 'asca', ' and', '3', ' true', 'cert', 'ucci', 'could', 's', ' not', 'asc', ' commander', ' Marshall', ' true', ' not', ' it', 'hall', 'outs'], 'evidence_proportions': [0.6077514648437501, 0.54267578125, 0.6042295455932617, 0.6032357215881348]}, 'weight': {'score': [0.0025317385083153134, 0.002485038391432968, 0.001475360148992294, 0.0024881962785125514, 0.0008191494714646112], 'topk_tokens': ['?', ' hallway', '<|start_header_id|>', '.\n\n', ' kitchen', ' Bridge', ' \n', 'Answer', ' hallway', 'assistant', ' the', '\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002540302276611328, 0.0033961951732635496, 0.003911435604095459, 0.0006544739007949829]}, 'saliency': {'score': [0.00016903593426658992, 9.168914847526547e-05, 9.662371415358323e-05, 9.153973400342796e-05, 3.5432596055288164e-05], 'topk_tokens': ['assistant', ' PA', ' kitchen', ' the', ' hallway', 'Answer', ' Bridge', 'Bridge', ' \n', ' hallway', 'way', '<|eot_id|>', ' Bridge', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>', ':', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00014222264289855956, 0.0002962648868560791, 8.012056350708007e-05, 0.0001594523588816325]}}, 27: {'grad': {'score': [0.6674935477120536, 0.8047611877506342, 0.6905165452223557, 0.8053647046105251, 0.5568853105817523], 'topk_tokens': ['.', '.', '.', '.', '.', '.', '.', '.', '.', ' *\n\n', '.', ' *\n\n', '.', ' *\n\n', '.', ':\n', '.', '.', ' *\n\n', '.'], 'evidence_proportions': [0.73408203125, 0.754119873046875, 0.5555419921875, 0.6331075032552083]}, 'weight': {'score': [0.007266441980997722, 0.0025399973958533137, 0.0033645820923340628, 0.0025291894232033606, 0.0011223133594270736], 'topk_tokens': [' bathroom', ' milk', ' PA', ' \n', ' hallway', 'Answer', ' kitchen', 'assistant', 'NEW', '<|eot_id|>', '.\n\n', '\n\n', '<|eot_id|>', ' hallway', 'hall', '<|start_header_id|>', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.009280633926391602, 0.0033532142639160155, 0.01531611680984497, 0.002140909433364868]}, 'saliency': {'score': [0.002038575354076567, 7.586908356063797e-05, 0.00045137833326290816, 7.12748165634159e-05, 3.83343015398298e-05], 'topk_tokens': [' ST', 'RE', ' bathroom', ' \n', ' Daniel', ' hallway', ' hallway', ' the', '***', ' Bridge', ' Bridge', 'assistant', ' kitchen', '.', '<|begin_of_text|>', 'way', '.\n\n', '<|end_header_id|>', 'NEW', 'hall'], 'evidence_proportions': [0.0019736945629119872, 0.00020930767059326174, 0.0053201794624328615, 0.0008823623259862264]}}, 28: {'grad': {'score': [0.6213462466285342, 0.6556675710599271, 0.5753368475498297, 0.6559845088039812, 0.4182585193997338], 'topk_tokens': [',', ' lodge', '\n', 'ivery', ' type', '\n\n\n\n', '\n', ' line', ' host', ' house', ' line', 'ching', ' houses', ' lengthy', ' hall', '\n\n\n\n\n\n\n', '\n', ' hall', ' huge', 'arp'], 'evidence_proportions': [0.6142898559570312, 0.63502197265625, 0.6617889404296875, 0.5821278889973959]}, 'weight': {'score': [0.002114545731317429, 0.0024552479909498337, 0.001967369745939206, 0.002457401292948986, 0.0008002959546588716], 'topk_tokens': [' Bridge', ' hallway', ' milk', '?', ' \n', '.\n\n', 'Answer', ' the', ' before', 'assistant', ' kitchen', '<|eot_id|>', 'hall', '<|end_header_id|>', '<|eot_id|>', '\n\n', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005061030387878418, 0.006237322092056274, 0.0019013702869415281, 0.0001969138781229655]}, 'saliency': {'score': [8.953752971830822e-05, 8.39282422635879e-05, 7.561384103237055e-05, 8.394522282214275e-05, 1.4758772320217557e-05], 'topk_tokens': ['<|eot_id|>', 'Answer', ' the', ' Bridge', ' hallway', ' hallway', '<|eot_id|>', 'assistant', ' the', ' before', ' milk', ' kitchen', ' Bridge', '<|end_header_id|>', ':', 'way', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'hall'], 'evidence_proportions': [1.3989210128784179e-05, 0.00025815367698669436, 8.96155834197998e-05, 1.1915961901346842e-05]}}, 29: {'grad': {'score': [0.30980773199172246, 0.44600882726504315, 0.3108252745408278, 0.4466776636460765, 0.39544141860235305], 'topk_tokens': [' his', ' by', '.', ' closely', ' from', ' for', ' of', ' of', ' more', ' of', ' of', '5', ' by', ' of', ' George', ' very', '.', ' for', ' of', 'A'], 'evidence_proportions': [0.28731231689453124, 0.22265701293945311, 0.35533447265624996, 0.36324055989583337]}, 'weight': {'score': [0.002304297117959885, 0.002503231490962741, 0.0011543516929333026, 0.0025079016064291615, 0.0005398633934202648], 'topk_tokens': [' part', ' before', '<|start_header_id|>', ' kitchen', ' \n', ' Does', '?', ' the', 'Answer', 'hall', '.\n\n', '<|eot_id|>', 'assistant', ':', 'way', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003540956974029541, 0.002165871858596802, 0.003717374801635742, 0.00021153688430786133]}, 'saliency': {'score': [7.92656626020159e-05, 4.3679848563330625e-05, 6.344379522861578e-05, 4.355499487349112e-05, 2.8719031621539403e-05], 'topk_tokens': [':', '***', 'ot', 'NEW', ' ', ' part', '<|start_header_id|>', '?', ' the', '.\n\n', 'way', '<|eot_id|>', 'hall', '<|eot_id|>', ' Does', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.4286251068115234e-05, 0.00015217065811157227, 0.0001094818115234375, 2.2480885187784832e-05]}}, 30: {'grad': {'score': [0.23758079892113096, 0.37663022160032533, 0.2656397697253105, 0.377226377986491, 0.4151846265035962], 'topk_tokens': [' papers', 'ire', ' wh', ' Paper', 'doll', ' Malta', ' in', ' Press', ' People', ' preparations', ' an', '186', ' Press', ' boat', ' Bridge', 'AM', ' Star', ' boat', ' Europe', ' Loan'], 'evidence_proportions': [0.1871612548828125, 0.25496826171875, 0.2819854736328125, 0.2281036376953125]}, 'weight': {'score': [0.004256397485733032, 0.002441484142378081, 0.003705532887043097, 0.0024342951398900158, 0.002575792490489899], 'topk_tokens': ['<|eot_id|>', ' hallway', '<|end_header_id|>', ' the', '<|start_header_id|>', '.\n\n', '?', ' kitchen', 'Answer', ' \n', 'assistant', 'hall', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.003806447982788086, 0.008616888523101806, 0.004251945018768311, 0.0010013232628504436]}, 'saliency': {'score': [0.0002609732605162121, 6.11383997483772e-05, 0.00016580025355021158, 6.045755885896229e-05, 0.000178080229532151], 'topk_tokens': ['andra', ' Daily', ' Daily', ' \n', '�', ' Bridge', ' Bridge', '?', ' the', ' Daily', ' hallway', '<|end_header_id|>', ' the', 'assistant', ':', ' kitchen', '<|begin_of_text|>', 'hall', 'way', '<|start_header_id|>'], 'evidence_proportions': [9.831786155700684e-05, 0.0005991101264953613, 0.00029537081718444824, 8.607407410939534e-05]}}, 31: {'grad': {'score': [0.2950743834177653, 0.4196605018180524, 0.263086505425282, 0.42037788765123973, 0.2011766783774845], 'topk_tokens': ['f', 'Independent', ' the', ' government', 'F', ' would', ' was', ' intense', ' able', ' the', ' had', ' ever', ' since', ' F', ' have', ' the', ' United', ' these', ' would', ' Independent'], 'evidence_proportions': [0.3084747076034546, 0.36157827377319335, 0.35704312324523924, 0.1768469214439392]}, 'weight': {'score': [0.0010228228001367477, 0.0023192777133376266, 0.0012908211121192346, 0.0023248156161770656, 0.0009368894592164055], 'topk_tokens': [' ', ' before', ':', ' kitchen', ' the', '.\n\n', ' Where', '?', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', ':', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0006278753280639648, 0.0027098536491394045, 0.0005472481250762939, 0.00034239888191223145]}, 'saliency': {'score': [4.694149607703799e-05, 3.3637100062536934e-05, 7.563523757152069e-05, 3.347941277911273e-05, 1.5100316395835271e-05], 'topk_tokens': [' milk', ' Mess', ' moved', 'Print', ' kitchen', '<|eot_id|>', ' hallway', ' ', 'Question', '<|begin_of_text|>', ' Where', '.\n\n', ' hallway', ' the', '<|end_header_id|>', 'assistant', ':', '<|start_header_id|>', 'way', 'hall'], 'evidence_proportions': [7.754564285278322e-06, 0.00012797117233276367, 4.981756210327148e-05, 9.675820668538412e-06]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 03:09:53.953 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:09:53.953 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-1_0-3-4-9.pkl | len: 10 |  size: 9.11 KB
Processing depth (0, 3, 4, 9):   2%|▏         | 2/100 [00:37<30:40, 18.78s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (2, 4, 5, 6):   2%|▏         | 2/100 [00:44<30:40, 18.78s/it]2025-01-22 03:10:01.146 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 03:10:01.153 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2514, 2519) --> . Sandra picked up the
2025-01-22 03:10:01.154 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 03:10:01.168 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4903, 4908) --> . Sandra travelled to the
2025-01-22 03:10:01.168 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:10:01.168 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (166, 171) -->  back to the kitchen.
2025-01-22 03:10:01.169 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:10:01.169 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (163, 169) --> . Daniel went back to the
2025-01-22 03:10:01.169 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:10:01.170 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (317, 323) -->  John journeyed to the office
2025-01-22 03:10:01.170 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 03:10:01.171 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (163, 169) --> . Daniel went back to the
2025-01-22 03:10:01.171 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the bedroom.
2025-01-22 03:10:01.184 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4738, 4743) --> . John moved to the
2025-01-22 03:10:01.185 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:10:01.191 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2347, 2352) --> . Mary moved to the
2025-01-22 03:10:01.191 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the bedroom.
2025-01-22 03:10:01.196 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1547, 1553) --> . John went back to the
2025-01-22 03:10:01.196 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the office.
2025-01-22 03:10:01.197 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (316, 322) --> . John journeyed to the
2025-01-22 03:10:01.197 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Mary got the football there.
2025-01-22 03:10:01.217 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (7149, 7154) --> . Mary got the football
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:10:03.219 | INFO     | test_jbb_retain:begin_test:632 - the football<|eot_id|>
2025-01-22 03:10:03.219 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12218])
your chose emoji: ['🧛🏻\u200d♀️', '🇹🇱', '🚵🏾\u200d♂️', '👳🏽', '🐯', '🐷', '👨🏻\u200d🦽', '🧔\u200d♀', '💂\u200d♀️', '🧑🏽\u200d🍼']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.54s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.47s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.45it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.55it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.62it/s][A100%|██████████| 8/8 [00:00<00:00, 20.76it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.47it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.19it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.35it/s][A100%|██████████| 8/8 [00:00<00:00, 20.38it/s]
2025-01-22 03:10:12.593 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.7147667294456845, 0.5869682005487481, 0.650776594113081, 0.5865428821986252, 1.0175087668678977], 'topk_tokens': ['hand', ' lin', ' hand', ' short', ' typ', ' war', ' type', ' lin', 'sur', ' lin', ' project', ' upper', 'dr', ' work', ' Project', 'hand', ' hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.69722900390625, 0.678497314453125, 0.67445068359375, 0.7932027180989584]}, 'weight': {'score': [0.006192713975906372, 0.002553466610837886, 0.02016793764554537, 0.0024906931082459916, 0.0013568013003378203], 'topk_tokens': ['<|start_header_id|>', 'Bridge', ' bedroom', 'Answer', '<|start_header_id|>', ':', ' the', ' kitchen', ' office', 'assistant', '\n\n', '<|eot_id|>', 'hall', ' hallway', '<|end_header_id|>', '<|eot_id|>', ' bathroom', ' football', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.016598665714263917, 0.008655834197998046, 0.0005398690700531006, 0.00017919143040974933]}, 'saliency': {'score': [0.00045930913516453335, 4.599871528918278e-05, 0.003702425803893652, 3.35589344099001e-05, 5.3059874158917054e-05], 'topk_tokens': [' milk', ' milk', 'assistant', ' kitchen', ' hallway', ' the', ' Sandra', '<|eot_id|>', ' bedroom', '\n\n', 'Bridge', ' bedroom', ' office', 'way', ' hallway', '<|eot_id|>', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ' football'], 'evidence_proportions': [0.001744496822357178, 0.00014470815658569337, 3.4379959106445317e-05, 4.59452470143636e-06]}}, 25: {'grad': {'score': [1.229876926967076, 1.556346002142828, 1.1403397780198317, 1.558243880057431, 0.7903251647949219], 'topk_tokens': [' large', 'been', ' project', ' so', ' a', ' to', ' of', ' had', ' extraordinary', 'ome', ' so', ' a', ' had', ' a', ' two', ' two', ' post', ' great', ' post', ' a'], 'evidence_proportions': [0.8172119140625, 0.9356246948242187, 1.52216796875, 1.5753987630208333]}, 'weight': {'score': [0.003988799594697498, 0.00254794745776513, 0.008148627403454903, 0.0025274981186682237, 0.0017993603691910253], 'topk_tokens': ['?', ' hallway', ' milk', '<|start_header_id|>', '.\n\n', ' the', '<|start_header_id|>', ' \n', ' there', 'Answer', ' football', 'hall', ':', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.010311031341552734, 0.005888241529464722, 0.00041570067405700685, 0.00011498729387919109]}, 'saliency': {'score': [0.00022053150903610956, 4.927897529392915e-05, 0.0007979747576591296, 4.658220375204702e-05, 9.287758307023482e-05], 'topk_tokens': [' Mary', '.\n\n', ' the', ' milk', 'Answer', ' \n', ' Mary', 'way', '.', ' Married', ' hallway', ':', ' there', '<|eot_id|>', 'assistant', ' football', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000569528341293335, 0.00034438967704772947, 3.159046173095703e-06, 7.62939453125e-06]}}, 26: {'grad': {'score': [0.7586001441592262, 0.630938764153404, 0.6896393604767628, 0.6305300632047375, 0.42856872442996863], 'topk_tokens': [' could', ' amendment', ' Marshall', ' commander', 'ian', ' could', ' a', ' Marshall', ' true', ' and', ' uncommon', ' double', ' Red', ' time', ' not', 'ucci', 'could', 'outs', ' true', ' it'], 'evidence_proportions': [0.797216796875, 0.44371337890625, 0.8893798828125, 0.8798421223958333]}, 'weight': {'score': [0.0016563165755498978, 0.0025045290953905628, 0.008036912251741458, 0.002488251611616117, 0.0020013939250599255], 'topk_tokens': [' before', '<|start_header_id|>', '.\n\n', '?', ' kitchen', ' kitchen', ' hallway', 'Answer', ' \n', '<|start_header_id|>', ' football', 'assistant', '\n\n', '<|eot_id|>', 'hall', '<|eot_id|>', 'way', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00286678671836853, 0.0038176417350769044, 0.0002111494541168213, 5.079309145609538e-05]}, 'saliency': {'score': [7.401335807073684e-05, 7.296809294875188e-05, 0.0006114710599948198, 7.12393234164471e-05, 8.604788418972132e-05], 'topk_tokens': [' the', '.', ' Married', ' office', ' hallway', 'Answer', ' kitchen', ' bathroom', 'Bridge', ':', ' \n', '<|eot_id|>', 'way', ' football', ' hallway', '<|eot_id|>', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00010507106781005859, 0.00016638636589050293, 2.5457143783569337e-05, 1.1617938677469889e-05]}}, 27: {'grad': {'score': [0.4326580592564174, 0.36647600417889803, 0.4286339344122471, 0.3661623800989927, 0.42181000565037585], 'topk_tokens': ['print', ' sco', '.', '.', 'cont', ' world', ' New', ' majority', 'bar', ' book', ' majority', ' business', ' hand', ' hand', ' hand', ' ear', ' hand', ' sco', 'hand', ' EAR'], 'evidence_proportions': [0.3751434326171875, 0.4897132873535156, 0.353350830078125, 0.4991302490234375]}, 'weight': {'score': [0.0024667566730862574, 0.002532552808545126, 0.011402473999903752, 0.002504220828640648, 0.0023577231349367084], 'topk_tokens': [' hallway', ' milk', ' THE', ' \n', '<|start_header_id|>', ' kitchen', ' bathroom', 'Answer', '.\n\n', '\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', ' football', 'hall', ':', ' hallway', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004927843809127808, 0.004173135757446289, 0.0008612394332885743, 0.00033179918924967444]}, 'saliency': {'score': [0.0003840696244012742, 5.692866398497871e-05, 0.0011503322002215262, 5.2857230872408396e-05, 8.349075461878921e-05], 'topk_tokens': [' milk', ' there', ' upper', ' the', '<|start_header_id|>', ' the', 'NEW', ' THE', ':', ' \n', 'assistant', ' THE', '<|begin_of_text|>', ' hallway', ' kitchen', '<|end_header_id|>', ' football', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.001043933629989624, 0.00032389163970947266, 0.0001315474510192871, 9.476641813913982e-05]}}, 28: {'grad': {'score': [0.6502627418154762, 0.6523450984569338, 0.5902248284755609, 0.6525479123718045, 0.4524145848823316], 'topk_tokens': [' locate', '\n', '\n\n\n\n\n\n\n', ' of', '\n', ' houses', '\n\n\n\n', ' hundred', '\n', '\n', '\n', 'ivery', ' house', ' hall', ' York', ' hall', 'ching', ' huge', 'arp', ' hall'], 'evidence_proportions': [0.5963867187499999, 0.6849182128906249, 0.702923583984375, 0.6223958333333334]}, 'weight': {'score': [0.001881490151087443, 0.0024489929590021547, 0.005898218124340742, 0.0024389113684683173, 0.0019309281399755766], 'topk_tokens': [' the', 'Question', ' milk', ' football', '.\n\n', '?', ' before', '<|start_header_id|>', ' \n', 'Answer', ' kitchen', 'assistant', '<|eot_id|>', 'hall', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012189209461212157, 0.006511062383651733, 0.00013762712478637695, 2.8873483339945473e-05]}, 'saliency': {'score': [5.9076717921665735e-05, 7.425654530476436e-05, 0.00028668687893794134, 7.360149994363872e-05, 4.0334282499371156e-05], 'topk_tokens': [':', ' kitchen', '.', ' hallway', ' milk', ' before', '<|eot_id|>', ' hallway', '?', ' football', '<|eot_id|>', '<|start_header_id|>', ' kitchen', 'assistant', '<|end_header_id|>', '\n\n', 'way', ':', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [2.9212236404418947e-05, 0.00021464228630065917, 3.1709671020507814e-06, 9.139378865559895e-07]}}, 29: {'grad': {'score': [0.3852448236374628, 0.4504574924822028, 0.3267566974346454, 0.45096680890787455, 0.4046007792154948], 'topk_tokens': [' pur', 'ile', ' rept', 'oused', ' of', ' of', ' of', ' by', ' by', ' Far', ' in', ' of', ' producing', 'ck', ' mild', ' give', ' in', 'A', 'in', ' for'], 'evidence_proportions': [0.3152191162109375, 0.312646484375, 0.5010910034179688, 0.4075597127278646]}, 'weight': {'score': [0.0014347192786988757, 0.0025109251356487746, 0.0023257327385437796, 0.0025133774690492385, 0.001225893244598851], 'topk_tokens': [' Does', ' hallway', ' milk', '<|start_header_id|>', ' before', ' kitchen', '?', '.\n\n', ' \n', '<|start_header_id|>', 'Answer', 'hall', '<|eot_id|>', 'assistant', 'way', ':', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002415645122528076, 0.003374344110488892, 0.0002046823501586914, 2.5957822799682617e-05]}, 'saliency': {'score': [0.00010742885725838798, 2.665379396408477e-05, 0.00013333941117311135, 2.6172171120541276e-05, 7.575389110680783e-05], 'topk_tokens': ['Does', ' the', 'Answer', ' the', ' the', '<|start_header_id|>', '<|start_header_id|>', ' kitchen', '\n\n', '?', 'assistant', ' \n', ' Does', '<|eot_id|>', 'way', 'hall', ':', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00021868348121643067, 0.00022252202033996583, 8.314847946166992e-06, 1.4007091522216797e-06]}}, 30: {'grad': {'score': [0.538528079078311, 0.5808545964681695, 0.4252772208971855, 0.5814266196991913, 0.6603636597142075], 'topk_tokens': [' S', ' J', ' paper', 'moment', ' an', ' hour', ' Bridge', ' description', ' in', '3', ' remains', ' preparations', ' Loan', 'doll', ' account', '2', ' S', ' Europe', '0', 'deal'], 'evidence_proportions': [0.469805908203125, 0.5469482421875, 0.512200927734375, 0.6107190450032552]}, 'weight': {'score': [0.003327969993863787, 0.002474805296428191, 0.008832195630440345, 0.0024529440447488373, 0.004939048579244903], 'topk_tokens': ['<|end_header_id|>', ' kitchen', ' the', ' football', '<|start_header_id|>', '.\n\n', '?', ' kitchen', '<|start_header_id|>', 'hall', '\n\n', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.004347622394561768, 0.008867430686950683, 0.0003868997097015381, 0.00031293431917826336]}, 'saliency': {'score': [0.00016009239923386347, 5.854809014011586e-05, 0.0012462284320440048, 5.456388129008541e-05, 0.000271037220954895], 'topk_tokens': [' Mary', ' the', '<|start_header_id|>', ' \n', ' kitchen', ' the', ' the', ' Wild', ' office', '\n\n', '<|eot_id|>', '?', '<|end_header_id|>', ' hallway', 'assistant', ' kitchen', ' football', 'way', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.00028696656227111816, 0.000307852029800415, 3.9476156234741215e-05, 3.174444039662679e-05]}}, 31: {'grad': {'score': [0.5509702960650126, 0.7175574435724212, 0.42892222068248653, 0.7187707569340167, 0.4057542650085507], 'topk_tokens': [' the', ' the', ' able', ' they', ' the', ' he', ' the', ' these', ' the', ' Independent', ' the', ' have', ' its', ' the', ' would', ' the', ' United', ' the', ' the', ' F'], 'evidence_proportions': [0.5584084510803223, 0.5830133438110351, 0.6148166000843048, 0.46486404041449225]}, 'weight': {'score': [0.0005449468181246803, 0.0022936312893429792, 0.0014036756295424241, 0.0022995050369482586, 0.0010827414014122703], 'topk_tokens': [' Where', ' the', ' before', 'Question', ':', ' kitchen', '.\n\n', '?', 'Answer', '<|eot_id|>', ' \n', '<|start_header_id|>', ':', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0007384419441223145, 0.0011966168880462647, 0.00014695525169372559, 0.00017230212688446045]}, 'saliency': {'score': [9.763808477492559e-06, 3.4695290161343824e-05, 9.471254471020821e-05, 3.454586891210076e-05, 1.4720541058164654e-05], 'topk_tokens': [':', ' milk', '.\n\n', ' the', '<|eot_id|>', ' Where', ' the', ' kitchen', 'Question', 'Answer', ' the', ' hallway', ' hallway', '<|begin_of_text|>', '<|start_header_id|>', ':', 'hall', '<|end_header_id|>', 'assistant', 'way'], 'evidence_proportions': [1.6379356384277344e-05, 1.4013051986694336e-05, 6.109476089477539e-06, 3.7550926208496094e-06]}}, 'pred_res': 'the football<|eot_id|>', 'score': 0}
2025-01-22 03:10:12.594 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:10:12.594 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-2_2-4-5-6.pkl | len: 10 |  size: 9.26 KB
Processing depth (2, 4, 5, 6):   3%|▎         | 3/100 [00:56<30:16, 18.72s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (0, 1, 2, 7):   3%|▎         | 3/100 [01:03<30:16, 18.72s/it]2025-01-22 03:10:20.027 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 03:10:20.027 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-22 03:10:20.028 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 03:10:20.032 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1487, 1492) --> . Sandra travelled to the
2025-01-22 03:10:20.032 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:10:20.033 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (173, 178) -->  back to the kitchen.
2025-01-22 03:10:20.033 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:10:20.034 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (170, 176) --> . Daniel went back to the
2025-01-22 03:10:20.034 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:10:20.035 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (324, 330) -->  John journeyed to the office
2025-01-22 03:10:20.035 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 03:10:20.035 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (170, 176) --> . Daniel went back to the
2025-01-22 03:10:20.035 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the bedroom.
2025-01-22 03:10:20.049 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4727, 4732) --> . John moved to the
2025-01-22 03:10:20.049 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:10:20.055 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2313, 2318) --> . Mary moved to the
2025-01-22 03:10:20.055 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the bedroom.
2025-01-22 03:10:20.060 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1521, 1527) -->  tragedy. John went back to
2025-01-22 03:10:20.060 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the office.
2025-01-22 03:10:20.061 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (323, 329) --> . John journeyed to the
2025-01-22 03:10:20.061 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Mary got the football there.
2025-01-22 03:10:20.081 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (7093, 7098) --> . Mary got the football
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:10:22.147 | INFO     | test_jbb_retain:begin_test:632 - The house.<|eot_id|>
2025-01-22 03:10:22.147 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12202])
your chose emoji: ['🚺', '🉐', '🧙🏻\u200d♂', '🤹🏻\u200d♂️', '💹', '🤥', '💡', '🔢', '🥶', '🧑🏾\u200d⚕️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 70492.50it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.60s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.10s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.87it/s][A100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.20it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.75it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.42it/s][A100%|██████████| 8/8 [00:00<00:00, 17.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.74it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.28it/s][A
100%|██████████| 8/8 [00:00<00:00, 20.44it/s][A100%|██████████| 8/8 [00:00<00:00, 20.00it/s]
2025-01-22 03:10:32.370 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6651495070684523, 0.5134296346912126, 0.7223052978515625, 0.5124965537374724, 0.8675268523547114], 'topk_tokens': [' turned', ' Wide', 'le', ' honor', ' Hunter', 'dr', 'otype', ' high', ' set', ' type', ' hand', ' Hor', ' type', 'hand', 'hand', ' hand', ' hand', ' hand', ' hand', ' Wide'], 'evidence_proportions': [0.7296630859375, 0.6006103515625001, 0.59405517578125, 0.7244160970052084]}, 'weight': {'score': [0.009375184774398804, 0.0025577338881689914, 0.00642192440155225, 0.0025335371077134317, 0.0011711747062449552], 'topk_tokens': ['\n\n', ' PA', '\n\n', 'Bridge', 'andra', 'Answer', ' Bridge', ' Bridge', ':', '<|start_header_id|>', '\n\n', ' hallway', 'assistant', '<|eot_id|>', 'hall', '\n\n', '<|eot_id|>', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.02544898986816406, 0.0054301261901855465, 0.007136797904968262, 0.0011332184076309204]}, 'saliency': {'score': [0.0005241164139338902, 3.888156999090274e-05, 0.0006156632533440223, 3.6190387004194266e-05, 5.0261312601517655e-05], 'topk_tokens': [' the', '<|end_header_id|>', '\n\n', 'assistant', '\n\n', 'Answer', 'E', ' boat', '\n\n', 'way', ' ', ' PA', ' football', ' Bridge', 'andra', 'Bridge', ' Bridge', ' Bench', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0016849935054779053, 0.0002140641212463379, 0.0002483665943145752, 4.488726456960042e-05]}}, 25: {'grad': {'score': [1.3167252313523066, 1.4693044922675134, 1.365127954727564, 1.4699028495703768, 1.2058444898955676], 'topk_tokens': [' too', ' so', ' a', ' so', ' extraordinary', ' g', ' to', ' g', ' no', ' a', ' two', 'two', 'ome', ' her', 'ad', ' o', ' two', ' post', ' large', ' a'], 'evidence_proportions': [1.187078857421875, 1.0930694580078124, 1.43935546875, 1.5089518229166665]}, 'weight': {'score': [0.006089681670779274, 0.0025568722604191337, 0.0034236243137946497, 0.0025479803437703724, 0.0012402577059609549], 'topk_tokens': ['\n\n', '<|end_header_id|>', '?', '.', '\n\n\n', ' the', '<|start_header_id|>', '<|start_header_id|>', '.\n\n', 'hall', ' \n', 'Answer', ':', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.011050891876220702, 0.0017137050628662108, 0.010232877731323243, 0.002149323622385661]}, 'saliency': {'score': [0.00031069063004993255, 5.7252490681636924e-05, 0.0004084874422122271, 5.5686384132733904e-05, 4.6042155246345365e-05], 'topk_tokens': ['.', ':', 'Den', ' ST', ' \n', '<|start_header_id|>', ' Mary', ' doctor', '<|start_header_id|>', 'way', '.', ' PA', ' Mary', ' Dan', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00036194920539855957, 0.00012593865394592283, 0.0005474865436553956, 0.00022460520267486572]}}, 26: {'grad': {'score': [0.7903994605654762, 0.62103191971144, 0.6918839674729568, 0.6205115452181807, 0.5994272037428252], 'topk_tokens': ['ucci', ' commander', ' sm', ' be', 'asca', 'ian', 'could', ' true', 'ible', '3', 'cert', ' not', ' and', 's', ' true', ' Marshall', ' not', ' it', 'hall', 'outs'], 'evidence_proportions': [0.8052490234375, 0.8022216796874999, 0.7681396484375, 0.7867228190104167]}, 'weight': {'score': [0.0015329065777006604, 0.002485218071537104, 0.0014172914700630384, 0.002490294043445548, 0.001128351810027142], 'topk_tokens': [' hallway', '<|start_header_id|>', '?', '.\n\n', ' kitchen', ' \n', ' Bridge', 'Answer', ' hallway', 'assistant', ' the', '\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0020172119140625, 0.0008023321628570556, 0.0028604686260223387, 0.0006318291028340657]}, 'saliency': {'score': [0.00012383432615370977, 8.092884009414791e-05, 0.00012048773276500213, 8.072762049584293e-05, 6.14165043344303e-05], 'topk_tokens': [' the', ' the', '?', 'Answer', 'assistant', 'Bridge', ' Bridge', ' hallway', ' \n', ' the', '<|eot_id|>', ' Bridge', '<|eot_id|>', 'way', '<|start_header_id|>', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', ':', 'hall'], 'evidence_proportions': [8.321404457092286e-05, 6.440281867980957e-05, 0.00019315481185913085, 0.00014944374561309814]}}, 27: {'grad': {'score': [0.4992268880208333, 0.6896618997401167, 0.5789321508163061, 0.6903467573320586, 0.5700577716438138], 'topk_tokens': [' *\n\n', '.', ',', '.', ' duration', '.', '.', '.', ':\n\n', ' *\n\n', '--', '.', '.', ' *\n\n', '.', ' *\n\n', ' *\n\n', '.', ':\n', '.'], 'evidence_proportions': [0.5762451171875, 0.4197265625, 0.43555908203125, 0.554351806640625]}, 'weight': {'score': [0.006218351068950835, 0.0025419855645229367, 0.0031360899790739403, 0.0025337209496394063, 0.0021289307243969975], 'topk_tokens': [' Bridge', ' milk', ' \n', ' hallway', ' bathroom', ' kitchen', '<|eot_id|>', 'Answer', 'assistant', 'NEW', '\n\n', '.\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.009062576293945312, 0.0017971277236938478, 0.012449026107788086, 0.00234028697013855]}, 'saliency': {'score': [0.0013432218914940243, 7.796193038474727e-05, 0.0004611978164085975, 7.454351467966493e-05, 7.703109663359972e-05], 'topk_tokens': [' hallway', ' \n', 'RE', ':', ' Bridge', '***', ' ST', ' the', ' bathroom', '.', ' kitchen', 'assistant', '<|start_header_id|>', ' hallway', '<|begin_of_text|>', 'way', 'NEW', '.\n\n', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.0021209001541137697, 0.0001255154609680176, 0.0025987148284912108, 0.0006636679172515869]}}, 28: {'grad': {'score': [0.6007937476748512, 0.6379824902863837, 0.5287680014585837, 0.6383975029384319, 0.5522999276920241], 'topk_tokens': ['ivery', ' type', '\n', ' host', ' line', ' line', 'ed', ' house', '\n\n\n\n\n\n\n', ' line', ' hall', ' hallway', ' lengthy', ' house', ' houses', '\n', 'ching', ' hall', ' huge', 'arp'], 'evidence_proportions': [0.557550048828125, 0.5545166015625, 0.6805419921875, 0.6089375813802083]}, 'weight': {'score': [0.0009522409666152228, 0.002451569972493423, 0.0019208956987429888, 0.002455866572394593, 0.0006912070877698003], 'topk_tokens': [' Bridge', ' milk', 'ot', ' \n', '.\n\n', '?', 'Answer', ' the', ' before', ' kitchen', 'assistant', '<|eot_id|>', '<|eot_id|>', 'hall', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005478262901306152, 0.0009353399276733399, 0.002232128381729126, 0.0002367645502090454]}, 'saliency': {'score': [4.633551552182152e-05, 8.134955707410963e-05, 7.906785378089317e-05, 8.141742708654549e-05, 1.7245205081239038e-05], 'topk_tokens': [' the', ',', '<|eot_id|>', '<|eot_id|>', ' Bridge', 'Answer', ' hallway', ' hallway', 'assistant', ' milk', ' kitchen', ' before', '<|end_header_id|>', ' Bridge', ':', 'way', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'hall'], 'evidence_proportions': [1.4144182205200194e-05, 3.756880760192871e-05, 0.00012791156768798828, 1.2487173080444336e-05]}}, 29: {'grad': {'score': [0.29666637239002047, 0.39365782466202376, 0.3501435793363131, 0.3939652660836306, 0.3928039706483179], 'topk_tokens': [' by', ' for', ' of', ' from', '\n', ' closely', '\n', ' and', 'in', '5', ' of', ' more', '.', ' very', ' for', '\n', ' George', ' of', '.', 'A'], 'evidence_proportions': [0.250946044921875, 0.36823081970214844, 0.2722076416015625, 0.2955118815104167]}, 'weight': {'score': [0.001958360274632772, 0.0025059045606438913, 0.001443554957707723, 0.0025102627380437052, 0.0007928372645864681], 'topk_tokens': [' ', ' before', ' kitchen', ' Does', 'ot', ' \n', '?', ' the', 'Answer', 'hall', '.\n\n', '<|eot_id|>', 'assistant', 'way', '\n\n', '<|start_header_id|>', ':', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0032625794410705566, 0.000760948657989502, 0.0038088202476501466, 0.00032730400562286377]}, 'saliency': {'score': [0.00011969747997465588, 4.883235177755454e-05, 0.00010084112485249837, 4.8542807945355113e-05, 2.9680072044839665e-05], 'topk_tokens': ['.', '.', ' part', '<|start_header_id|>', '***', 'UL', ' the', 'ot', '.\n\n', ' the', 'way', '<|eot_id|>', 'hall', ' Does', '<|eot_id|>', '<|end_header_id|>', '\n\n', ':', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [9.09566879272461e-05, 9.291768074035645e-05, 0.0002666175365447998, 4.353125890096029e-05]}}, 30: {'grad': {'score': [0.2528891336350214, 0.33723084021181127, 0.33648935953776044, 0.3373790570569657, 0.4151050022670201], 'topk_tokens': [' S', ' J', '186', ' Press', 'ire', ' Paper', ' boat', ' People', 's', ' boat', ' Press', ' in', ' Europe', ' Star', ' Bridge', ' wh', ' Press', ' an', ' Loan', 'AM'], 'evidence_proportions': [0.1719808578491211, 0.25827789306640625, 0.29608306884765623, 0.2798271179199219]}, 'weight': {'score': [0.0032560101577213834, 0.0024429619776230767, 0.003834421053910867, 0.0024370878799897058, 0.0027909960065569195], 'topk_tokens': ['ot', ' milk', '<|end_header_id|>', ' the', '<|start_header_id|>', ' kitchen', '?', '.\n\n', 'Answer', ' \n', '\n\n', 'assistant', 'hall', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004619979858398437, 0.002575540542602539, 0.005246895551681519, 0.0010273555914560952]}, 'saliency': {'score': [0.00020669613565717424, 6.576104517697995e-05, 0.0002153485249250363, 6.503699835851488e-05, 0.0001781528093376938], 'topk_tokens': [' the', 'IR', 'ot', 'andra', ' Daily', ' the', '?', ' Bridge', ' Bridge', ' Daily', '<|end_header_id|>', ' the', ' the', ' kitchen', 'assistant', ':', '<|begin_of_text|>', 'way', '<|start_header_id|>', 'hall'], 'evidence_proportions': [0.00010245442390441895, 0.0002813518047332764, 0.00036454200744628906, 9.981294473012289e-05]}}, 31: {'grad': {'score': [0.1899460894720895, 0.3430125343169603, 0.20350829607401139, 0.34372517825547144, 0.2267499383614988], 'topk_tokens': [',', ' government', 'f', ' delivered', 'Independent', ' an', 'proved', ' was', ' since', ' ever', 'F', ' had', ' have', ' the', ' would', ' these', ' intense', ' F', ' United', ' Independent'], 'evidence_proportions': [0.2364288568496704, 0.13975076675415038, 0.28909640312194823, 0.11041462421417236]}, 'weight': {'score': [0.0005946556727091471, 0.002319203181034324, 0.0012490512468875982, 0.002325621577337869, 0.001210994258218882], 'topk_tokens': ['Question', ':', ' before', ' kitchen', ' the', '.\n\n', ' Where', '?', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', 'assistant', ':', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0008620798587799072, 0.0005917251110076904, 0.0006274640560150147, 0.00034690399964650476]}, 'saliency': {'score': [2.0078250340053012e-05, 3.038560303934575e-05, 6.084335155976124e-05, 3.0305619689357187e-05, 1.8141099384852817e-05], 'topk_tokens': ['Answer', ' PA', ' Geo', ':', ' S', ' moved', 'ot', ' Mess', 'Print', '<|eot_id|>', 'Question', ' Where', '.\n\n', '<|end_header_id|>', ' the', 'assistant', ':', '<|start_header_id|>', 'hall', 'way'], 'evidence_proportions': [1.3738870620727539e-05, 1.5068054199218749e-05, 4.5186281204223634e-05, 8.612871170043945e-06]}}, 'pred_res': 'The house.<|eot_id|>', 'score': 0}
2025-01-22 03:10:32.371 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:10:32.372 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-3_0-1-2-7.pkl | len: 10 |  size: 8.96 KB
Processing depth (0, 1, 2, 7):   4%|▍         | 4/100 [01:15<30:36, 19.13s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (3, 6, 7, 8):   4%|▍         | 4/100 [01:22<30:36, 19.13s/it]2025-01-22 03:10:39.473 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 03:10:39.484 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3763, 3768) --> . Sandra picked up the
2025-01-22 03:10:39.484 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 03:10:39.505 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7225, 7230) --> . Sandra travelled to the
2025-01-22 03:10:39.505 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:10:39.505 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (166, 171) -->  back to the kitchen.
2025-01-22 03:10:39.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:10:39.506 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (163, 169) --> . Daniel went back to the
2025-01-22 03:10:39.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:10:39.507 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (317, 323) -->  John journeyed to the office
2025-01-22 03:10:39.508 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 03:10:39.508 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (163, 169) --> . Daniel went back to the
2025-01-22 03:10:39.508 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the bedroom.
2025-01-22 03:10:39.522 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4729, 4734) --> . John moved to the
2025-01-22 03:10:39.522 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:10:39.529 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2347, 2352) --> . Mary moved to the
2025-01-22 03:10:39.529 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the bedroom.
2025-01-22 03:10:39.534 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1562, 1568) -->  ranks. John went back to
2025-01-22 03:10:39.534 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the office.
2025-01-22 03:10:39.535 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (316, 322) --> . John journeyed to the
2025-01-22 03:10:39.535 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Mary got the football there.
2025-01-22 03:10:39.555 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (7139, 7144) --> . Mary got the football
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:10:41.619 | INFO     | test_jbb_retain:begin_test:632 - the hallway<|eot_id|>
2025-01-22 03:10:41.619 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12215])
your chose emoji: ['🦹🏾\u200d♀️', '🤵\u200d♀️', '🙆\u200d♀', '🏊🏽\u200d♂', '🐬', '🔛', '\U0001faf1🏻\u200d\U0001faf2🏽', '🧙🏻', '🦵🏾', '🧍']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.16s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.02s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.03it/s][A100%|██████████| 8/8 [00:05<00:00,  1.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.79it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.34it/s][A100%|██████████| 8/8 [00:00<00:00, 21.72it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.66it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.57it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.35it/s][A100%|██████████| 8/8 [00:00<00:00, 17.37it/s]
2025-01-22 03:10:50.594 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6737772623697916, 0.5416663802573559, 0.6166610228709686, 0.5411976255619873, 1.0150814964657737], 'topk_tokens': [' typ', ' war', ' Vand', ' Wide', 'sur', ' type', ' type', ' lin', ' upper', ' hand', 'hand', ' project', ' work', 'men', 'hand', ' Project', ' hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.68753662109375, 0.683233642578125, 0.6099365234375, 0.7076314290364584]}, 'weight': {'score': [0.010305997871217273, 0.0025566639340554153, 0.02269554443848439, 0.002478678052137902, 0.0013425548871358235], 'topk_tokens': [' milk', '.', ' bedroom', ' the', 'Answer', '<|start_header_id|>', 'Bridge', ':', 'assistant', ' office', '<|eot_id|>', '\n\n', 'hall', '<|end_header_id|>', ' bathroom', '<|eot_id|>', ' football', 'way', ' hallway', '<|begin_of_text|>'], 'evidence_proportions': [0.025589048862457275, 0.01719258427619934, 0.00034964680671691897, 0.00012825926144917804]}, 'saliency': {'score': [0.0002910267739068894, 4.1599261192862544e-05, 0.0035496682692796756, 2.9915376583356022e-05, 4.8492635999407086e-05], 'topk_tokens': [' milk', ' the', 'Answer', 'assistant', '.', ' milk', ' bedroom', '<|eot_id|>', 'hall', ' office', '\n\n', ' bedroom', 'Bridge', ' bathroom', '<|end_header_id|>', '<|eot_id|>', ' hallway', 'way', '<|begin_of_text|>', ' football'], 'evidence_proportions': [0.0009645342826843261, 0.00024027824401855468, 1.5354156494140623e-05, 1.7881393432617188e-06]}}, 25: {'grad': {'score': [0.9784909202938988, 1.2721780317078286, 0.9101758125500802, 1.273846522897732, 0.6803617931547619], 'topk_tokens': [' great', ' a', ' too', ' of', ' post', ' great', ' a', ' a', ' most', 'ome', ' too', ' two', ' so', ' extraordinary', 'two', ' so', ' two', ' a', ' great', ' post'], 'evidence_proportions': [0.55897216796875, 0.7647033691406251, 1.244775390625, 1.2843424479166667]}, 'weight': {'score': [0.007904437326249621, 0.002552201537536431, 0.01055024755306733, 0.0025173009991116342, 0.0018820412575252473], 'topk_tokens': [' the', ' the', ' milk', ' the', '<|start_header_id|>', '.', ' \n', 'Answer', ' football', '.', ' there', ':', 'hall', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.020769327878952026, 0.01187952160835266, 0.0004269123077392578, 0.00010239581267038982]}, 'saliency': {'score': [0.00032459554218110584, 4.3538531825436685e-05, 0.0007415979336469602, 4.081386379710074e-05, 7.441450679112995e-05], 'topk_tokens': [' Dan', '.', ':', 'hall', '.', 'way', 'Answer', '.', ' Miss', ' hallway', ' Mary', ' milk', 'assistant', ' football', '<|eot_id|>', '<|eot_id|>', ' there', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0009199321269989013, 0.0004288613796234131, 7.4267387390136715e-06, 5.900859832763672e-06]}}, 26: {'grad': {'score': [0.8131016322544643, 0.6691537252286586, 0.71185302734375, 0.6687681207846685, 0.45996868799603174], 'topk_tokens': [' true', ' it', ' single', ' city', ' a', ' could', ' uncommon', ' not', ' City', 'ian', ' Red', ' Marshall', 'could', ' time', 'outs', ' true', 'ucci', ' and', ' double', ' it'], 'evidence_proportions': [0.7284912109374999, 0.52523193359375, 1.006591796875, 0.9622599283854167]}, 'weight': {'score': [0.004724844580604916, 0.0025056450119930237, 0.008833703322288318, 0.0024815129618990634, 0.001599827456095862], 'topk_tokens': ['?', ' the', 'Bridge', ' the', '.\n\n', ' kitchen', ' \n', '<|start_header_id|>', 'Answer', ' football', 'assistant', '\n\n', ' hallway', '<|eot_id|>', 'hall', '<|eot_id|>', 'way', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007456070184707642, 0.012234175205230714, 0.00010815262794494629, 3.82910172144572e-05]}, 'saliency': {'score': [0.0001763190541948591, 7.210922896871787e-05, 0.00045432188572027744, 7.070318357119692e-05, 7.3408323620993e-05], 'topk_tokens': [' office', ' bathroom', ' the', ' the', ' the', ' the', ' the', 'Answer', ' \n', '<|eot_id|>', ' football', '<|eot_id|>', 'way', ':', 'Bridge', '\n\n', ' hallway', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00022685527801513672, 0.0004884183406829834, 1.4466047286987304e-05, 9.000301361083984e-06]}}, 27: {'grad': {'score': [0.3803614718573434, 0.30850163277285564, 0.4115146734775641, 0.30804707073894727, 0.3268032225351485], 'topk_tokens': [' New', '600', ' air', 'bar', ' earth', ' sco', ' money', ' EAR', 'hand', ' was', ' world', ' book', ' hand', ' hand', ' ear', ' hand', ' hand', ' business', ' sco', 'hand'], 'evidence_proportions': [0.17584612369537356, 0.40671997070312504, 0.42586669921875, 0.49090449015299475]}, 'weight': {'score': [0.006402967941193353, 0.002538258753464487, 0.012342857244687203, 0.0025001325621419016, 0.0021057275552598255], 'topk_tokens': [' before', ' THE', ' kitchen', ' milk', ' \n', '<|start_header_id|>', ' bathroom', 'Answer', '\n\n', '.\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', ' football', 'hall', ':', '<|end_header_id|>', ' hallway', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.012168675661087036, 0.013787078857421874, 0.0006226003170013428, 0.00026175876458485925]}, 'saliency': {'score': [0.000763556786945888, 5.813024939326144e-05, 0.0013576623721000475, 5.2743202997952195e-05, 8.864251394120473e-05], 'topk_tokens': ['<|start_header_id|>', ' the', ' office', 'Bridge', ' \n', 'NEW', ' the', ' there', ' the', ' kitchen', 'assistant', ' THE', ':', '<|begin_of_text|>', '<|end_header_id|>', ' football', 'way', ' hallway', '.\n\n', 'hall'], 'evidence_proportions': [0.0020267367362976074, 0.0010209858417510988, 8.338689804077148e-05, 6.319085756937662e-05]}}, 28: {'grad': {'score': [0.579071771530878, 0.5591995056921653, 0.5256398518880209, 0.5592728326304568, 0.3528731361268059], 'topk_tokens': [' house', 'ched', ' lengthy', ' houses', ' to', '\n', ' York', ' locate', '\n\n\n\n\n\n\n', ' hall', 'ivery', 'ed', ' house', ' huge', ' hall', ' hall', '\n', 'ching', 'arp', '\n\n\n\n'], 'evidence_proportions': [0.592352294921875, 0.6149169921875001, 0.5842407226562499, 0.533826192220052]}, 'weight': {'score': [0.00708507498105367, 0.0024507673697222623, 0.005898198256125817, 0.002431704179690374, 0.0016322722510685997], 'topk_tokens': [' the', ' the', ' football', '.\n\n', '?', ' hallway', ' kitchen', ' before', ' \n', '<|start_header_id|>', 'Answer', 'assistant', '<|eot_id|>', 'hall', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003329706192016602, 0.026288759708404538, 0.00011063814163208007, 2.3509065310160317e-05]}, 'saliency': {'score': [0.00016804820015316917, 6.800640437231847e-05, 0.00022256985688820863, 6.733780325704312e-05, 4.4230430845230344e-05], 'topk_tokens': ['.', '.\n\n', ' before', ' hallway', ' football', ' the', ' milk', '?', '<|eot_id|>', ' kitchen', '<|eot_id|>', 'assistant', '<|start_header_id|>', ' hallway', '<|end_header_id|>', ':', 'way', '\n\n', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.00010510683059692383, 0.0005955636501312256, 3.868341445922852e-06, 1.053015391031901e-06]}}, 29: {'grad': {'score': [0.34011913481212797, 0.3659161823551829, 0.27799303103715944, 0.36624277726386917, 0.34933465624612475], 'topk_tokens': [' give', '5', ' of', ' by', ' of', ' of', ' mild', ' Cuba', ' by', ' Far', '\n', 'yl', ' more', '\n', 'in', ' by', ' in', ' of', 'A', ' for'], 'evidence_proportions': [0.2884429931640625, 0.35727844238281253, 0.41287841796875, 0.30825042724609375]}, 'weight': {'score': [0.0019780340648832776, 0.002517562662643855, 0.00258024839254526, 0.002518293486552953, 0.0012774614114609976], 'topk_tokens': [' milk', '<|start_header_id|>', ' kitchen', ' hallway', ' before', ' Does', '?', '.\n\n', ' \n', '<|start_header_id|>', 'Answer', 'hall', 'assistant', '<|eot_id|>', 'way', '\n\n', ':', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0026634931564331055, 0.005499255657196045, 0.00011999607086181641, 2.0831823348999023e-05]}, 'saliency': {'score': [0.00012476671309698197, 2.5245569735389817e-05, 0.0001358489195505778, 2.4718881575052117e-05, 0.00010026352746146066], 'topk_tokens': ['IVE', 'Does', ' the', '�', ' kitchen', '<|start_header_id|>', '?', 'assistant', ' the', ' \n', ':', '<|eot_id|>', 'way', '<|start_header_id|>', ' Does', '<|eot_id|>', 'hall', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00012997984886169433, 0.0003877580165863037, 5.066394805908203e-06, 1.0132789611816406e-06]}}, 30: {'grad': {'score': [0.4513948531377883, 0.5055639504566777, 0.40219722649989986, 0.5059890905519245, 0.5750756642175099], 'topk_tokens': [' months', ' remains', ' in', ' Star', 'doll', ' Republicans', '3', ' account', '�', ' Bridge', ' hour', ' Loan', ' preparations', '2', ' an', '0', '�', ' Europe', 'deal', ' S'], 'evidence_proportions': [0.3867183685302734, 0.472076416015625, 0.45683135986328127, 0.48352686564127606]}, 'weight': {'score': [0.007227293082645961, 0.002477252009184627, 0.00925331772902073, 0.002447311441195131, 0.004562936131916349], 'topk_tokens': [' the', '<|end_header_id|>', ' football', ' the', '<|start_header_id|>', ' kitchen', '.\n\n', '?', '\n\n', '<|start_header_id|>', 'hall', 'assistant', 'Answer', ' \n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.008586055040359496, 0.021288943290710446, 0.00024464130401611326, 0.0001958260933558146]}, 'saliency': {'score': [0.0003344430809929257, 4.702534500364977e-05, 0.0009578642172691149, 4.3607143944747985e-05, 0.00016853071394420805], 'topk_tokens': ['.\n\n', ' the', ' the', ' Wild', ' kitchen', 'Bridge', ' the', '<|eot_id|>', '?', '<|end_header_id|>', '\n\n', ' office', 'hall', ' hallway', ' kitchen', 'assistant', ' football', 'way', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0003684639930725098, 0.0009946227073669434, 1.963973045349121e-05, 1.827875773111979e-05]}}, 31: {'grad': {'score': [0.48640771990730647, 0.6404001966867652, 0.37097217027957624, 0.6415304430300987, 0.327954491925618], 'topk_tokens': [' the', ' the', ' they', ' these', ' the', ' the', ' the', ' its', ' the', ' able', ' government', ' Independent', ' have', ' the', ' would', ' the', ' the', ' F', ' the', ' United'], 'evidence_proportions': [0.4486572265625, 0.5619128227233887, 0.550743293762207, 0.401332567135493]}, 'weight': {'score': [0.0013974436691829137, 0.002284663733180913, 0.0013788350117512238, 0.0022891018760892628, 0.0013797746764289008], 'topk_tokens': [' the', ' before', ' Where', 'Question', ':', ' kitchen', '.\n\n', '?', 'Answer', '<|eot_id|>', ' \n', '<|start_header_id|>', 'assistant', '\n\n', ':', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0014623701572418213, 0.00408092737197876, 0.00015252232551574708, 0.00014453629652659097]}, 'saliency': {'score': [4.5468409856160484e-05, 3.8281724620400096e-05, 7.871633920914088e-05, 3.8139606643848696e-05, 2.0692745844523113e-05], 'topk_tokens': ['.', ':', ' kitchen', ' the', '<|eot_id|>', '<|eot_id|>', ' hallway', ' Where', ' the', 'Question', 'Answer', ' the', '<|begin_of_text|>', '<|start_header_id|>', ':', ' hallway', '<|end_header_id|>', 'way', 'assistant', 'hall'], 'evidence_proportions': [5.6350231170654296e-05, 0.00012458562850952147, 7.045269012451172e-06, 2.4884939193725586e-06]}}, 'pred_res': 'the hallway<|eot_id|>', 'score': 100}
2025-01-22 03:10:50.595 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:10:50.595 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-4_3-6-7-8.pkl | len: 10 |  size: 9.14 KB
Processing depth (3, 6, 7, 8):   5%|▌         | 5/100 [01:34<29:46, 18.80s/it]Processing depth (3, 6, 7, 8):   5%|▌         | 5/100 [01:34<29:56, 18.92s/it]
2025-01-22 03:10:51.026 | INFO     | __main__:<module>:72 - Selected idx: 9
2025-01-22 03:10:51.026 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:10:51.026 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:10:51.026 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:10:51.027 | INFO     | __main__:<module>:76 - Needle: [' Mary got the football there.', ' Sandra travelled to the hallway.', ' Mary moved to the bathroom.', ' Daniel went back to the kitchen.', ' Sandra picked up the milk.', ' John went back to the bedroom.', ' John moved to the bedroom.', ' Sandra went to the kitchen.', ' Mary journeyed to the office.', ' John journeyed to the office.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-22 03:10:51.027 | INFO     | __main__:<module>:77 - Real Needle: [' Sandra travelled to the hallway.', ' Sandra picked up the milk.', ' Sandra went to the kitchen.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-22 03:10:51.027 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (0, 3, 6, 7, 8):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:10:58.366 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 03:10:58.367 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-22 03:10:58.367 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 03:10:58.377 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3814, 3819) -->  war. Sandra picked up
2025-01-22 03:10:58.377 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:10:58.395 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6331, 6336) -->  back to the kitchen.
2025-01-22 03:10:58.395 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra left the milk.
2025-01-22 03:10:58.417 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8388, 8392) -->  Sandra left the milk
2025-01-22 03:10:58.417 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:10:58.435 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (6328, 6334) --> . Daniel went back to the
2025-01-22 03:10:58.435 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:10:58.447 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4483, 4488) --> . Mary got the football
2025-01-22 03:10:58.447 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:10:58.461 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4926, 4931) --> . Mary moved to the
2025-01-22 03:10:58.461 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 03:10:58.479 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6328, 6334) --> . Daniel went back to the
2025-01-22 03:10:58.479 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John went back to the bedroom.
2025-01-22 03:10:58.502 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8157, 8163) --> . John went back to the
2025-01-22 03:10:58.502 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:10:58.514 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4211, 4216) --> . John moved to the
2025-01-22 03:10:58.514 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:10:58.545 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10879, 10885) -->  John journeyed to the office
2025-01-22 03:10:58.545 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  John journeyed to the office.
2025-01-22 03:10:58.577 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (10878, 10884) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:11:00.590 | INFO     | test_jbb_retain:begin_test:632 - kitchen<|eot_id|>
2025-01-22 03:11:00.590 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12225])
your chose emoji: ['🧄', '🐮', '🇩🇿', '↪', '👩🏾\u200d🍳', '🧝🏾', '🐕', '🦅', '🦋', '👨🏾\u200d❤️\u200d💋\u200d👨🏻']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 236298.82it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.95s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.02it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.11it/s][A100%|██████████| 8/8 [00:05<00:00,  1.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.11it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.89it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.70it/s][A100%|██████████| 8/8 [00:00<00:00, 22.06it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.71it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.49it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.36it/s][A100%|██████████| 8/8 [00:00<00:00, 21.60it/s]
2025-01-22 03:11:09.162 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3270917510986328, 0.26098127680196065, 0.2987169119027945, 0.2607244162604982, 0.5188619679418104], 'topk_tokens': ['\n', '�', ' free', ' Sh', ' How', ' way', ' short', '\n', ' Vand', ' hand', 'Sh', ' Project', 'le', 'hand', 'Frank', ' hand', ' hand', ' hand', 'hand', ' hand'], 'evidence_proportions': [0.2382843017578125, 0.41239624023437504, 0.32188262939453127, 0.3322720527648926, 0.3308982849121094]}, 'weight': {'score': [0.021908328533172608, 0.0025534910589485806, 0.005551440593523857, 0.0025041001539252298, 0.0012912755382472072], 'topk_tokens': [' Sandra', '\n\n', ' bedroom', ' the', 'Bridge', ' bathroom', ':', 'Answer', '<|start_header_id|>', '<|eot_id|>', ' hallway', 'assistant', ' hallway', '\n\n', ' kitchen', '<|eot_id|>', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0525853157043457, 0.01378847360610962, 0.010244369506835938, 0.03718852996826172, 0.002643883228302002]}, 'saliency': {'score': [0.0012280356884002686, 3.728530582041767e-05, 0.00035829727466289815, 3.380879921491347e-05, 2.948631500375682e-05], 'topk_tokens': [' bathroom', '<|start_header_id|>', ':', 'assistant', 'Answer', ' Sandra', '<|eot_id|>', ' Sandra', '\n\n', ' Sandra', ' football', ' hallway', 'Bridge', '<|end_header_id|>', ' kitchen', ' bedroom', 'way', '<|begin_of_text|>', ' hallway', 'hall'], 'evidence_proportions': [0.002500790357589722, 0.0013007819652557373, 0.0007596790790557861, 0.001828514039516449, 9.67631737391154e-05]}}, 25: {'grad': {'score': [0.3991446685791016, 0.469068452974219, 0.4719744951297075, 0.46920284618087915, 0.4948931069209658], 'topk_tokens': [' got', '\n', ' o', ' Most', ' most', 'ation', ' perpetrated', ' obtain', 'AY', ' large', ' get', 'ed', ' Wood', 'g', 'led', 'ING', ' getting', ' used', ' g', ' post'], 'evidence_proportions': [0.421630859375, 0.38990058898925783, 0.41119384765625, 0.28839111328125, 0.451904296875]}, 'weight': {'score': [0.009324953556060792, 0.0025423069112551224, 0.0032946497966081668, 0.0025259547624020386, 0.001624126886499339], 'topk_tokens': [' hallway', ' THE', ' the', '.', ' kitchen', ' prior', '?\n', ' to', 'Answer', '.\n\n', 'hall', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0081601619720459, 0.005030882358551025, 0.011780643463134767, 0.023617029190063477, 0.0022995471954345703]}, 'saliency': {'score': [0.0004971075057983398, 2.3906383301488786e-05, 0.00013489066026149652, 2.2578003256778032e-05, 5.287188908149456e-05], 'topk_tokens': ['?\n', 'Answer', ' Merch', ' top', ' Mary', 'way', 'assistant', ':', '<|start_header_id|>', ' hallway', ' Dan', '.', ' prior', ' to', '<|eot_id|>', 'hall', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006620466709136962, 0.0001589953899383545, 0.0009837687015533448, 0.0006017014384269714, 0.0001661380132039388]}}, 26: {'grad': {'score': [0.512657470703125, 0.565371367679455, 0.48036800286708736, 0.5657522439251053, 0.42648769247120827], 'topk_tokens': [' city', ' issued', ' much', ' name', ' not', 'ucci', ' uncommon', ' city', '3', ' city', ' true', ' single', 'ing', ' true', ' red', ' circ', ' a', ' not', 'outs', ' it'], 'evidence_proportions': [0.514471435546875, 0.563623046875, 0.51707763671875, 0.48679351806640625, 0.48223368326822913]}, 'weight': {'score': [0.015397511720657349, 0.002469244134320829, 0.0030456949502993855, 0.0024408251708645996, 0.0014768833744114843], 'topk_tokens': ['Bridge', ' the', ' kitchen', ' the', '.\n\n', '?\n', ' hallway', 'Answer', ' hallway', 'assistant', '\n\n', '<|eot_id|>', ' kitchen', '<|eot_id|>', 'hall', '<|start_header_id|>', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.035674774646759035, 0.003971010446548462, 0.01886301040649414, 0.01930856704711914, 0.0025265912214914956]}, 'saliency': {'score': [0.0007427835464477539, 5.919817502412057e-05, 0.00012546166395529723, 5.758078680035316e-05, 8.40862249505931e-05], 'topk_tokens': [' Bridge', ' the', ' the', ' the', ' the', '?\n', '<|eot_id|>', 'Bridge', 'assistant', ' hallway', '<|eot_id|>', ' hallway', ' kitchen', 'way', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [0.002624917030334473, 0.00010938048362731933, 0.00041810274124145505, 0.00042416900396347046, 0.00018515189488728842]}}, 27: {'grad': {'score': [0.334422607421875, 0.3823343812931898, 0.30496010413536656, 0.3826809277545461, 0.39788859877093086], 'topk_tokens': ['.', '.', '.', ' majority', ' majority', ' *\n\n', '\n', '.', '.', ' *\n\n', ' *\n\n', '.', '\n', ' *\n\n', 'hand', ' *\n\n', ' participate', ':\n', ' duration', '.'], 'evidence_proportions': [0.39378051757812504, 0.438287353515625, 0.30191345214843746, 0.22672271728515625, 0.29729461669921875]}, 'weight': {'score': [0.03424962162971497, 0.00252683973016012, 0.004507668507404816, 0.0024552907438232727, 0.002344516844585024], 'topk_tokens': [' the', ' prior', ' to', ' THE', '?\n', 'Answer', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', ' hallway', '.\n\n', ' kitchen', 'hall', ':', '<|start_header_id|>', ' hallway', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.12671337127685547, 0.0036984562873840334, 0.0208099365234375, 0.020537614822387695, 0.002996876835823059]}, 'saliency': {'score': [0.00122886061668396, 5.274660590430432e-05, 0.00017596513797075322, 4.993434242024621e-05, 0.00010898452380607868], 'topk_tokens': ['<|eot_id|>', ' PA', ' prior', ' bedroom', 'THE', ' Geo', '<|start_header_id|>', ':', 'NEW', ' kitchen', 'assistant', '<|begin_of_text|>', ' hallway', ' hallway', 'way', ' THE', '<|end_header_id|>', '.\n\n', ' kitchen', 'hall'], 'evidence_proportions': [0.003201824426651001, 0.00029754638671875, 0.002169370651245117, 0.0005255639553070068, 4.5925378799438477e-05]}}, 28: {'grad': {'score': [0.2788702392578125, 0.29875103741796494, 0.2275723677415114, 0.2990201091107786, 0.3439944365928913], 'topk_tokens': ['of', ' host', ' huge', 'of', 'ching', 'line', 'hall', ' houses', 'ed', ' hallway', ' house', ' line', 'arp', 'land', ' line', ' hall', ' hallway', ' hall', ' hall', ' hall'], 'evidence_proportions': [0.3797149658203125, 0.28078460693359375, 0.257177734375, 0.21306800842285156, 0.2551829020182292]}, 'weight': {'score': [0.007300224304199219, 0.0024108628568440277, 0.004784907285983746, 0.0023932024023125965, 0.0013007660364282542], 'topk_tokens': [' the', ' to', ' the', ' hallway', ' the', ' the', ' kitchen', '?\n', '.\n\n', 'Answer', '<|eot_id|>', 'assistant', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0063648223876953125, 0.001723027229309082, 0.011551094055175782, 0.0145263671875, 0.004367570082346598]}, 'saliency': {'score': [0.00024050712585449219, 5.601040134252112e-05, 0.0001305074263841678, 5.53923643407599e-05, 2.8200704475928996e-05], 'topk_tokens': ['assistant', ':', ' Bridge', ' hallway', 'Answer', '<|eot_id|>', ' the', ' kitchen', ' milk', '<|eot_id|>', 'Probably', '?\n', '<|end_header_id|>', ' hallway', '<|begin_of_text|>', '<|start_header_id|>', ':', '\n\n', 'way', 'hall'], 'evidence_proportions': [0.0004571318626403809, 5.6403875350952146e-05, 0.0003209531307220459, 0.0002943947911262512, 0.00011044243971506755]}}, 29: {'grad': {'score': [0.340142822265625, 0.3048914995776854, 0.3866704671810835, 0.30455685120513276, 0.36181680087385504], 'topk_tokens': ['\n', ' regular', '\n', '\n', ' THE', '\n', '\n', '\n', 'ance', 'stage', 'br', '\n', '\n', ' a', 'BR', '\n', ' FIRST', '\n', '\n', '\n'], 'evidence_proportions': [0.34021759033203125, 0.202423095703125, 0.41115875244140626, 0.4089469909667969, 0.34979756673177087]}, 'weight': {'score': [0.004147320985794067, 0.0024578837268992813, 0.002598433922498654, 0.0024539608899130315, 0.001489431693636138], 'topk_tokens': [' to', ' Where', 'NEW', '<|start_header_id|>', ' was', '.', ' the', '?\n', 'Answer', '.\n\n', '<|eot_id|>', 'hall', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0027328789234161376, 0.0010283589363098145, 0.004686546325683594, 0.011414289474487305, 0.002631157636642456]}, 'saliency': {'score': [0.00014087200164794922, 3.316719907539749e-05, 0.00013243464323190542, 3.2627569809825515e-05, 6.386380771110798e-05], 'topk_tokens': [' the', ' the', ' place', ':', 'NEW', 'Answer', '.\n\n', 'IVE', '<|start_header_id|>', ' was', '<|eot_id|>', '<|end_header_id|>', 'way', 'assistant', ':', '<|eot_id|>', '<|start_header_id|>', 'hall', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [8.617639541625978e-05, 2.414584159851074e-05, 9.831786155700684e-05, 0.0004961863160133362, 8.230904738108318e-05]}}, 30: {'grad': {'score': [0.24930301666259766, 0.3709865142053586, 0.28449048751439804, 0.3715139256226158, 0.5416330140212486], 'topk_tokens': [' paper', 'D', 'doll', ' W', ',', ' Star', 'deal', ' S', ' ed', '0', ' boat', ' J', ' preparations', ' his', ' N', ' boat', ' in', ' S', ' an', ' M'], 'evidence_proportions': [0.2744342803955078, 0.2292694091796875, 0.31235961914062504, 0.2524583339691162, 0.19040425618489584]}, 'weight': {'score': [0.01648129940032959, 0.002434048966172867, 0.007007584357872987, 0.0023905148375038252, 0.006306218176052488], 'topk_tokens': ['Question', ' hallway', '<|eot_id|>', '<|end_header_id|>', '.', ' kitchen', '<|start_header_id|>', 'Answer', '\n\n', '?\n', 'assistant', 'hall', '<|eot_id|>', '.\n\n', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0069557189941406245, 0.009458911418914796, 0.018228912353515626, 0.051527976989746094, 0.0054504772027333575]}, 'saliency': {'score': [0.0013348245620727539, 6.47891033775946e-05, 0.00028852545298062836, 6.146152987365698e-05, 0.0004039792151286684], 'topk_tokens': ['�', ' kitchen', ' the', '�', '?\n', '<|end_header_id|>', ' milk', ' hallway', ' Sandra', ' Sandra', 'Answer', 'assistant', 'Gov', ' Sandra', '.\n\n', ':', 'way', '<|begin_of_text|>', 'hall', '<|start_header_id|>'], 'evidence_proportions': [0.00042615532875061034, 0.0015338897705078124, 0.000587230920791626, 0.004900306463241577, 0.00017216801643371582]}}, 31: {'grad': {'score': [0.47327275276184083, 0.6650529158127085, 0.4286590463075882, 0.6662049928422196, 0.38892812975521746], 'topk_tokens': [' and', ' D', 'E', ' could', ' to', ' an', ' have', ' the', ' have', ' two', 'F', ' to', ' its', ' his', ' w', ' ever', ' F', 'f', ' its', ' an'], 'evidence_proportions': [0.31553561687469484, 0.5922841787338257, 0.6025240182876587, 0.3605017066001892, 0.4730154871940613]}, 'weight': {'score': [0.003561927080154419, 0.002267692227124934, 0.002958821944701366, 0.002262816369651141, 0.0019997235002188847], 'topk_tokens': [' where', 'Question', ' the', ',', ' was', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '\n\n', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002400708198547363, 0.0022975623607635496, 0.00423583984375, 0.00793600082397461, 0.0021056036154429116]}, 'saliency': {'score': [0.0002770519256591797, 3.7969082428186904e-05, 8.160487199440981e-05, 3.733780432280566e-05, 5.4683150916263974e-05], 'topk_tokens': [' Paul', ' location', '.\n\n', '<|eot_id|>', ':', '.', ' Where', ' the', '<|start_header_id|>', 'Question', '?\n', ' hallway', ':', ' hallway', '<|end_header_id|>', '<|begin_of_text|>', 'Answer', 'hall', 'assistant', 'way'], 'evidence_proportions': [0.0010902881622314453, 2.173185348510742e-05, 0.0001323997974395752, 0.0001399591565132141, 2.406040827433268e-05]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:11:09.163 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:11:09.163 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-0_0-3-6-7-8.pkl | len: 10 |  size: 9.46 KB
Processing depth (0, 3, 6, 7, 8):   1%|          | 1/100 [00:18<29:46, 18.04s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (1, 4, 5, 7, 8):   1%|          | 1/100 [00:25<29:46, 18.04s/it]2025-01-22 03:11:16.247 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 03:11:16.252 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1497, 1502) -->  tragedy. Sandra travelled to
2025-01-22 03:11:16.252 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 03:11:16.266 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4870, 4875) --> . Sandra picked up the
2025-01-22 03:11:16.266 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:11:16.283 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5984, 5989) --> . Sandra went to the
2025-01-22 03:11:16.283 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra left the milk.
2025-01-22 03:11:16.306 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8446, 8450) -->  Sandra left the milk
2025-01-22 03:11:16.306 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:11:16.325 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (6435, 6441) --> . Daniel went back to the
2025-01-22 03:11:16.325 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:11:16.344 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4439, 4444) --> . Mary got the football
2025-01-22 03:11:16.344 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:11:16.358 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4875, 4880) -->  milk. Mary moved to
2025-01-22 03:11:16.359 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 03:11:16.378 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6435, 6441) --> . Daniel went back to the
2025-01-22 03:11:16.378 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John went back to the bedroom.
2025-01-22 03:11:16.402 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8197, 8203) --> . John went back to the
2025-01-22 03:11:16.403 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:11:16.414 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4142, 4147) --> . John moved to the
2025-01-22 03:11:16.414 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:11:16.446 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10884, 10890) -->  journeyed to the office.
2025-01-22 03:11:16.446 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  John journeyed to the office.
2025-01-22 03:11:16.478 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (10883, 10889) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:11:18.494 | INFO     | test_jbb_retain:begin_test:632 - kitchen<|eot_id|>
2025-01-22 03:11:18.494 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12231])
your chose emoji: ['🪝', '🧎🏿\u200d♀️', '🐿', '👨🏾\u200d🤝\u200d👨🏿', '💇\u200d♀', '🤽🏻\u200d♂', '🚴🏾', '🐾', '🚵🏽\u200d♀️', '❗']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 114520.25it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.15s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.01s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.64it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.40it/s][A100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 22.84it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 20.76it/s][A100%|██████████| 8/8 [00:00<00:00, 21.68it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.35it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.92it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.94it/s][A100%|██████████| 8/8 [00:00<00:00, 21.15it/s]
2025-01-22 03:11:27.783 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.31388145446777344, 0.2826887264504276, 0.2710417723044371, 0.2826619732056667, 0.5389164264385516], 'topk_tokens': ['icing', 'u', ' hand', ' Aw', ' procession', ' short', 'ir', 'sur', 'hand', '\n', 'system', ' Vand', 'hand', 'le', ' hand', ' hand', ' hand', ' Project', ' hand', 'Frank'], 'evidence_proportions': [0.362274169921875, 0.35325927734375, 0.30354843139648435, 0.2837948799133301, 0.26940790812174475]}, 'weight': {'score': [0.023511106967926024, 0.0025501831189389903, 0.014238454592533601, 0.0024696682722919186, 0.0016090567295367901], 'topk_tokens': [' the', ' Sandra', '.', ' Bridge', '.', 'Answer', ' bedroom', ':', 'Bridge', ' kitchen', 'assistant', '<|start_header_id|>', '<|eot_id|>', ' hallway', '\n\n', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018450677394866942, 0.017296147346496583, 0.046648687124252325, 0.02942943572998047, 0.02351840337117513]}, 'saliency': {'score': [0.0011211133003234863, 3.553917931442423e-05, 0.0009123362027681791, 3.0499373485342636e-05, 6.899191783024715e-05], 'topk_tokens': [' post', 'assistant', ' Sandra', ' THE', ' office', ' Sandra', ' football', ' West', '<|eot_id|>', ' hallway', ' Bench', '<|start_header_id|>', 'Bridge', ' bedroom', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>', ' bedroom', 'hall'], 'evidence_proportions': [6.780624389648437e-05, 0.001151907444000244, 0.0023560643196105955, 0.0012136250734329224, 0.000882407029469808]}}, 25: {'grad': {'score': [0.327818603515625, 0.42066541231787435, 0.4054388144077399, 0.4209049363555532, 0.3410227115337665], 'topk_tokens': [' g', ' large', 'Williams', ' o', 'yster', 'Ind', ' get', ' getting', ' too', 'ation', ' Aw', '\n', 'adv', ' used', 'st', ' post', ' obtaining', 'g', ' obtain', ' post'], 'evidence_proportions': [0.43609313964843754, 0.3413818359375, 0.32575683593750004, 0.24557113647460938, 0.2828369140625]}, 'weight': {'score': [0.01662089467048645, 0.0025434676428061015, 0.009657247708393978, 0.0024917525155875365, 0.0023432607834155744], 'topk_tokens': [' the', ' top', ' THE', ' hallway', ' Capt', ' post', ' Mary', 'Answer', '?\n', ':', 'hall', '.', '<|start_header_id|>', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005474507808685303, 0.02304553985595703, 0.02299923896789551, 0.027950763702392578, 0.0117930273214976]}, 'saliency': {'score': [0.0004974555969238281, 3.789650793653971e-05, 0.0010380752575703156, 3.374729277956358e-05, 9.014285527742826e-05], 'topk_tokens': ['Spring', ' prior', ' Empire', '.', ' self', 'assistant', ' sentinel', ' Dan', ' Merch', ' Mary', '<|eot_id|>', ' top', 'hall', '<|eot_id|>', ' post', ' Mary', ' post', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [3.361701965332031e-05, 0.0007986307144165039, 0.000798356533050537, 0.0006099119782447815, 0.00030728677908579505]}}, 26: {'grad': {'score': [0.5266693115234375, 0.5585172989018821, 0.48645909627278644, 0.5588136398704109, 0.46697898277869593], 'topk_tokens': [' circ', ' Marshall', ' issued', ' a', ' time', ' Marshall', ' Cl', 'cert', ' single', ' red', 'ian', ' uncommon', ' City', ' and', ' city', '3', ' not', 'ucci', 'outs', ' it'], 'evidence_proportions': [0.606927490234375, 0.56190185546875, 0.573394775390625, 0.5068359375, 0.4047114054361979]}, 'weight': {'score': [0.010063532590866089, 0.0024871720156093547, 0.006372774258638039, 0.0024591566087186776, 0.0020764300456413854], 'topk_tokens': [' CHIP', '<|start_header_id|>', ' the', 'Bridge', '?\n', 'Answer', ' kitchen', ' the', 'assistant', ' kitchen', '\n\n', ' hallway', '<|eot_id|>', 'hall', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0004412531852722168, 0.009595179557800293, 0.015653800964355466, 0.01773238182067871, 0.00870126982529958]}, 'saliency': {'score': [0.0002675580978393555, 7.473413453531631e-05, 0.0002408745961311536, 7.380561546507485e-05, 8.408885735731859e-05], 'topk_tokens': [' Merch', ' the', ' Bridge', '<|eot_id|>', ' CHIP', ' kids', '?\n', ' the', '<|start_header_id|>', '<|eot_id|>', 'Bridge', ' hallway', ' kitchen', 'way', ' kitchen', '\n\n', '<|end_header_id|>', ':', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [1.806020736694336e-05, 0.00035665035247802737, 0.0002957165241241455, 0.00039651989936828613, 0.00029178957144419354]}}, 27: {'grad': {'score': [0.29477409362792967, 0.4148386303108269, 0.2617589510404147, 0.4155758300568103, 0.3745729446411133], 'topk_tokens': ['.', ',', ',', '.', '.', '.', 'hand', '.', ' *\n\n', '.', '--', ' destination', ' candidates', ':\n', '.', ' majority', '\n', ' majority', ' participate', ' duration'], 'evidence_proportions': [0.3001220703125, 0.3165069580078125, 0.3081409454345703, 0.22845840454101562, 0.30527814229329425]}, 'weight': {'score': [0.011160489320755005, 0.0025298600495397526, 0.008796580326862825, 0.002492048396080757, 0.003231725784448477], 'topk_tokens': [' Bridge', ' the', ' bedroom', '?\n', '.', 'Answer', '.\n\n', ' kitchen', ' THE', '\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', 'hall', ':', ' hallway', '<|start_header_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005753695964813233, 0.0067351341247558595, 0.021195054054260254, 0.017396211624145508, 0.011149932940800985]}, 'saliency': {'score': [0.0008274424076080322, 5.4238029451605656e-05, 0.0004816315112969814, 5.1280062709955674e-05, 0.00018247274252084585], 'topk_tokens': ['<|eot_id|>', ' top', '�', ' Sandra', ' bedroom', '.', ' kitchen', ' the', 'assistant', '<|end_header_id|>', ':', ' THE', '<|begin_of_text|>', ' hallway', '.', 'way', ' kitchen', '.\n\n', '<|start_header_id|>', 'hall'], 'evidence_proportions': [4.3964385986328126e-05, 0.0007096469402313232, 0.00198480486869812, 0.0007955804467201233, 0.0006352762381235759]}}, 28: {'grad': {'score': [0.30936187744140625, 0.38074269278368683, 0.2652738522260617, 0.38125935713580716, 0.32560838552621696], 'topk_tokens': ['\n', 'of', '\n', 'line', '\n', ' of', ' huge', 'ivery', '\n', 'ed', ' the', 'ching', ' hall', ' line', ' line', ' hall', 'arp', ' hallway', ' hall', ' hall'], 'evidence_proportions': [0.3093719482421875, 0.26973876953125, 0.38582153320312496, 0.23455810546875, 0.3285255432128906]}, 'weight': {'score': [0.007367353439331054, 0.002433638645188957, 0.007381945848464966, 0.0024076462991921353, 0.0015527307987213135], 'topk_tokens': ['Question', ' Bridge', ' hallway', ' discarded', '.', '.\n\n', ' the', 'Answer', '?\n', ' the', '<|eot_id|>', 'assistant', 'hall', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00027191042900085447, 0.001934760808944702, 0.01087937355041504, 0.012285470962524414, 0.011601954698562622]}, 'saliency': {'score': [0.00017889738082885743, 5.4690672015566946e-05, 0.00018410193614470653, 5.4020811126382994e-05, 2.8912837688739484e-05], 'topk_tokens': [' the', ' hallway', 'Bridge', 'Answer', '.\n\n', 'over', ' milk', 'assistant', ' Bridge', 'Super', '<|eot_id|>', '?\n', '<|end_header_id|>', ' the', '<|begin_of_text|>', ':', '<|start_header_id|>', 'way', '\n\n', 'hall'], 'evidence_proportions': [7.927417755126953e-06, 5.016922950744629e-05, 0.00022125840187072753, 0.00017990916967391968, 0.0003926704327265422]}}, 29: {'grad': {'score': [0.33639175415039063, 0.2922797422111278, 0.35892305618677384, 0.29197556065455166, 0.3102380605844351], 'topk_tokens': ['\n', ' spr', 'ance', '      ', ' a', ' the', ' THE', 'THE', 'br', ' regular', '\n', '\n', 'A', 'stage', '\n', '\n', 'BR', '\n', '\n', ' FIRST'], 'evidence_proportions': [0.34861450195312504, 0.25864410400390625, 0.42111511230468746, 0.383880615234375, 0.2887338002522786]}, 'weight': {'score': [0.004513000249862671, 0.0024652509142102535, 0.00458821883568397, 0.0024542410964346756, 0.0019101225412808931], 'topk_tokens': [' part', ' place', '.', ' Does', ' hallway', '<|start_header_id|>', '?\n', '.\n\n', ' the', 'Answer', 'hall', '<|eot_id|>', 'assistant', ':', 'way', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00013338923454284669, 0.0017764806747436524, 0.005972826480865478, 0.0071926116943359375, 0.0074401795864105225]}, 'saliency': {'score': [0.00016145110130310058, 3.3055857074831714e-05, 0.00013115772834190956, 3.2477726090022946e-05, 0.00011217410747821514], 'topk_tokens': [' Does', ' to', '�', ' the', '<|start_header_id|>', ' part', '.\n\n', '.', 'IVE', ':', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', 'assistant', ' the', 'hall', '<|start_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [1.4019012451171874e-05, 2.042055130004883e-05, 0.00024117231369018554, 0.00020231306552886963, 0.00030816098054250085]}}, 30: {'grad': {'score': [0.26551467895507813, 0.3611148561705595, 0.32213064340444714, 0.3614361699526684, 0.44575333962073693], 'topk_tokens': ['2', ' ed', ' hour', ' boat', ' Star', '3', '0', ' W', ' Paul', 'D', ' Europe', ' his', ' be', ' N', ' J', 'deal', ' S', ' M', ' in', ' an'], 'evidence_proportions': [0.2358489990234375, 0.23266983032226562, 0.2356964111328125, 0.38484954833984375, 0.2628987630208333]}, 'weight': {'score': [0.017566272020339967, 0.002450237130736806, 0.01278467208911211, 0.0023860675468734766, 0.006437868338364822], 'topk_tokens': [' hallway', ' the', ' Miles', '<|eot_id|>', ' the', '<|end_header_id|>', '<|start_header_id|>', '.\n\n', 'Answer', '\n\n', '?\n', 'hall', 'assistant', '<|eot_id|>', '<|end_header_id|>', ':', '<|eot_id|>', '<|start_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0021249353885650635, 0.007414960861206054, 0.020993781089782712, 0.04302692413330078, 0.01906345287958781]}, 'saliency': {'score': [0.0013207364082336425, 6.447437933229125e-05, 0.0005251757609538543, 6.041736169829169e-05, 0.0003402246878697322], 'topk_tokens': ['.', ' Merch', '.', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ' milk', ' Sandra', 'Gov', '<|end_header_id|>', ' the', '<|eot_id|>', ' Sandra', ' Miles', 'assistant', '<|begin_of_text|>', ':', 'way', 'hall', '<|start_header_id|>'], 'evidence_proportions': [0.00024186968803405762, 0.0007927536964416504, 0.0013433516025543213, 0.004370599985122681, 0.0006076892217000326]}}, 31: {'grad': {'score': [0.3768483066558838, 0.6527395112775248, 0.3908137786082732, 0.6541456233309054, 0.38403349289527305], 'topk_tokens': [' could', ' the', ' have', ' and', ' two', ' these', ' United', 'E', 'f', ' its', ' to', 'F', ' V', ' an', ' ever', ' his', ' w', 'f', ' its', ' F'], 'evidence_proportions': [0.3894809722900391, 0.2631290912628174, 0.5175613403320313, 0.2548961639404297, 0.42512766520182294]}, 'weight': {'score': [0.0024441075325012207, 0.0022745558847998295, 0.003363795769520295, 0.0022707170066817825, 0.0015458148259382982], 'topk_tokens': [' milk', ' discarded', 'Question', ' the', ',', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '\n\n', ':', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005144298076629639, 0.0013775825500488281, 0.002779865264892578, 0.004933059215545654, 0.0030018438895543413]}, 'saliency': {'score': [7.091760635375976e-05, 3.841383182010523e-05, 0.0001377853063436655, 3.8028615561291734e-05, 1.7146880810077373e-05], 'topk_tokens': [' Market', ' the', 'CH', ' left', '.', ' Where', ' Mess', 'Question', '.', '<|eot_id|>', '?\n', 'Answer', ' hallway', '<|end_header_id|>', ':', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', 'hall', 'way'], 'evidence_proportions': [1.5515089035034177e-05, 4.74095344543457e-05, 0.00010459423065185547, 0.00011745840311050415, 7.758537928263345e-05]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:11:27.785 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:11:27.785 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-1_1-4-5-7-8.pkl | len: 10 |  size: 9.48 KB
Processing depth (1, 4, 5, 7, 8):   2%|▏         | 2/100 [00:36<30:01, 18.38s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (1, 4, 6, 7, 9):   2%|▏         | 2/100 [00:44<30:01, 18.38s/it]2025-01-22 03:11:35.510 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 03:11:35.515 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1484, 1489) --> . Sandra travelled to the
2025-01-22 03:11:35.515 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 03:11:35.529 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4865, 4870) -->  war. Sandra picked up
2025-01-22 03:11:35.529 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:11:35.547 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6390, 6395) -->  back to the kitchen.
2025-01-22 03:11:35.547 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra left the milk.
2025-01-22 03:11:35.570 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8434, 8438) -->  Sandra left the milk
2025-01-22 03:11:35.570 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:11:35.589 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (6389, 6395) -->  went back to the kitchen.
2025-01-22 03:11:35.589 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:11:35.601 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4469, 4474) --> . Mary got the football
2025-01-22 03:11:35.601 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:11:35.615 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4871, 4876) -->  milk. Mary moved to
2025-01-22 03:11:35.615 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 03:11:35.633 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6388, 6394) -->  Daniel went back to the kitchen
2025-01-22 03:11:35.634 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John went back to the bedroom.
2025-01-22 03:11:35.657 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8185, 8191) --> . John went back to the
2025-01-22 03:11:35.657 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:11:35.674 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4212, 4217) --> . John moved to the
2025-01-22 03:11:35.674 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:11:35.706 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10911, 10917) -->  John journeyed to the office
2025-01-22 03:11:35.706 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  John journeyed to the office.
2025-01-22 03:11:35.738 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (10910, 10916) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:11:37.753 | INFO     | test_jbb_retain:begin_test:632 - kitchen<|eot_id|>
2025-01-22 03:11:37.753 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12251])
your chose emoji: ['👈🏿', '🇲🇿', '👨🏻\u200d❤\u200d💋\u200d👨🏿', '✡', '🧑🏽\u200d🚒', '🚄', '🚶\u200d➡', '🧑🏼\u200d❤\u200d💋\u200d🧑🏻', '👩🏽\u200d🔧', '🇳🇿']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 197379.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.98s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.33s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.85it/s][A100%|██████████| 8/8 [00:05<00:00,  1.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.12it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.61it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.64it/s][A100%|██████████| 8/8 [00:00<00:00, 16.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.27it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.05it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.03it/s][A100%|██████████| 8/8 [00:00<00:00, 17.07it/s]
2025-01-22 03:11:46.756 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.373453369140625, 0.31616006723251083, 0.4021401038536659, 0.31576748692275464, 0.507755885404699], 'topk_tokens': ['ir', ' Vand', ' hand', '.\n', ' Aw', 'ian', ' How', 'system', '\n', 'ra', ' Tie', ' hand', 'hand', ' Project', 'hand', 'le', ' hand', ' hand', ' hand', 'Frank'], 'evidence_proportions': [0.44166412353515627, 0.36802978515624996, 0.3682373046875, 0.34212303161621094, 0.3463643391927083]}, 'weight': {'score': [0.015975888967514038, 0.0025490034223867508, 0.008406360179950029, 0.0025027270440296425, 0.0013101405957165887], 'topk_tokens': ['<|start_header_id|>', ' bedroom', ' Sandra', '\n\n', ' Sandra', 'Bridge', ' bathroom', 'Answer', ' Bridge', ' kitchen', ':', '<|start_header_id|>', 'assistant', '<|eot_id|>', '\n\n', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002355915307998657, 0.021653854846954344, 0.013430786132812499, 0.03504228591918945, 0.012004216512044271]}, 'saliency': {'score': [0.0012761855125427246, 4.4373717260570846e-05, 0.00036085186860500236, 4.083491965724955e-05, 4.412531852722168e-05], 'topk_tokens': [':', ' Sandra', '\n\n', ' Bridge', ' bathroom', '<|eot_id|>', ' Bench', '<|start_header_id|>', ' kitchen', ' hallway', '<|eot_id|>', ' Sandra', ' Sandra', ' bedroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom', 'way', 'hall'], 'evidence_proportions': [0.00018354058265686036, 0.0024400830268859865, 0.0010723650455474853, 0.0019701793789863586, 0.0009239961703618368]}}, 25: {'grad': {'score': [0.47365020751953124, 0.49103340060515954, 0.5301760160006009, 0.49094382044328244, 0.49255325092988855], 'topk_tokens': [' had', ' to', ' o', ' laying', ' for', 'yster', ' prepared', ' two', ' used', ' too', ' getting', 'ING', ' get', ' getting', ' large', ' obtaining', ' obtain', 'AY', ' post', 'g'], 'evidence_proportions': [0.5313232421875, 0.3477371215820313, 0.5527252197265625, 0.34896087646484375, 0.5477472941080729]}, 'weight': {'score': [0.017883957624435426, 0.0025445117984682695, 0.00642653077076643, 0.0025006328086759145, 0.002000393937615787], 'topk_tokens': ['<|eot_id|>', ' Pills', '.', ' Paul', ' Mary', '<|end_header_id|>', 'Answer', '?\n', '<|start_header_id|>', ' Sandra', 'hall', ':', '<|start_header_id|>', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011440932750701905, 0.03366010189056397, 0.015574932098388672, 0.028031110763549805, 0.013846476872762043]}, 'saliency': {'score': [0.0007596242427825927, 3.538647113586496e-05, 0.000639427166718703, 3.196862606458531e-05, 7.301884538987103e-05], 'topk_tokens': [' Ramsey', ' Mary', '<|start_header_id|>', ' Pills', 'Mer', ' Anthony', 'Gov', ' Merch', ' Paul', '\n\n', 'assistant', ' Sandra', 'hall', ' Ramsey', '<|eot_id|>', ' Mary', '<|eot_id|>', ' Dan', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [5.9944391250610354e-05, 0.001970243453979492, 0.0003676354885101318, 0.0012621209025382996, 0.0003255009651184082]}}, 26: {'grad': {'score': [0.531617431640625, 0.5608021527728191, 0.4920429816612831, 0.5610819908123313, 0.519103869269876], 'topk_tokens': [' morning', ' extra', ' city', ' Marshall', 'ers', ' extra', 'and', ' and', ' uncommon', ' single', 'ucci', ' true', ',', ' red', ' Marshall', '3', 'ian', ' not', 'outs', ' it'], 'evidence_proportions': [0.573504638671875, 0.5487060546875, 0.51654052734375, 0.4756622314453125, 0.5323384602864584]}, 'weight': {'score': [0.016465585231781005, 0.0024846023888305444, 0.005660774616094736, 0.0024457676646355434, 0.002175007848178639], 'topk_tokens': [' cap', ' Ramsey', '<|end_header_id|>', ' Bridge', ' hallway', '?\n', '<|start_header_id|>', 'Answer', 'assistant', ' the', '\n\n', ' kitchen', 'hall', '<|eot_id|>', '<|start_header_id|>', 'way', '<|eot_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0011633098125457763, 0.02547547221183777, 0.018596839904785153, 0.022214651107788086, 0.01610048611958822]}, 'saliency': {'score': [0.0007036006450653076, 6.453362884073209e-05, 0.00039020639199476974, 6.2181051878581e-05, 9.855312459609088e-05], 'topk_tokens': [' Ramsey', ' Ramsey', ' Father', ' bathroom', ' kitchen', 'Bridge', ' hallway', ' John', '<|eot_id|>', 'river', '?\n', '<|start_header_id|>', ' Ramsey', ' kitchen', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [4.1979551315307614e-05, 0.000995093584060669, 0.0009780049324035645, 0.0006304606795310974, 0.0008321305116017659]}}, 27: {'grad': {'score': [0.3618438720703125, 0.3855038844140179, 0.3032183524889824, 0.38581566751932295, 0.3644649954403148], 'topk_tokens': [' follow', ' assistance', 'atives', ' ear', ' sco', '.', ' hand', '.', ' EAR', 'prev', ' candidates', ' destination', ' majority', ' majority', '\n', ' New', ' several', ' duration', ' participate', 'hand'], 'evidence_proportions': [0.4685302734375, 0.4827484130859375, 0.277496337890625, 0.30805206298828125, 0.2783355712890625]}, 'weight': {'score': [0.014732168912887573, 0.002530273691522602, 0.007766372118240748, 0.002488497217431041, 0.0019414684351752786], 'topk_tokens': ['<|start_header_id|>', ' the', ' bathroom', ' Bridge', '?\n', 'Answer', ' THE', '<|eot_id|>', '.\n\n', '\n\n', ' hallway', 'assistant', ' kitchen', '<|eot_id|>', 'hall', ':', '<|start_header_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0013906538486480712, 0.011192023754119873, 0.022027206420898435, 0.020020723342895508, 0.0191953182220459]}, 'saliency': {'score': [0.0012874412536621094, 5.047866487573095e-05, 0.0005918962833208916, 4.620964503659881e-05, 9.422512615428251e-05], 'topk_tokens': [' Mary', ' Dan', '.', ' John', ' Sandra', ' hallway', ' Sandra', ':', ' the', 'NEW', '.', 'assistant', '<|end_header_id|>', ' hallway', '<|start_header_id|>', ' THE', 'way', ' kitchen', '.\n\n', 'hall'], 'evidence_proportions': [0.00011143088340759277, 0.0011925876140594482, 0.002150750160217285, 0.0010208860039710999, 0.001804774006207784]}}, 28: {'grad': {'score': [0.37465911865234375, 0.45117860908937896, 0.3193091612595778, 0.45175743564608845, 0.35638644274543313], 'topk_tokens': [' house', ' of', 'of', ' host', ' law', ' hallway', 'ed', ' houses', ' line', 'ching', '\n', 'arp', ' of', ' hundred', 'hall', ' hall', ' hall', ' hall', ' hallway', ' hall'], 'evidence_proportions': [0.33501434326171875, 0.37284088134765625, 0.4487945556640625, 0.24814605712890625, 0.4317741394042969]}, 'weight': {'score': [0.005385640859603882, 0.0024425160423740782, 0.003698510237229176, 0.0024324617442584018, 0.001004939219530891], 'topk_tokens': [' the', ' hallway', ' to', ' discarded', ' Bridge', '.\n\n', ' ', 'Answer', '?\n', ' the', '<|eot_id|>', 'assistant', 'hall', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0004084765911102295, 0.0020535945892333984, 0.007041549682617188, 0.011865615844726562, 0.006610075632731119]}, 'saliency': {'score': [0.00016852378845214845, 5.74399781145395e-05, 0.00011293933941767766, 5.703459908670828e-05, 1.7971151015337775e-05], 'topk_tokens': ['.', 'Right', ' ', 'assistant', '.\n\n', '<|eot_id|>', ' the', ' Bridge', 'Super', '<|eot_id|>', ' milk', ' Ramsey', '?\n', '<|end_header_id|>', ':', '<|begin_of_text|>', '\n\n', 'way', '<|start_header_id|>', 'hall'], 'evidence_proportions': [2.459287643432617e-05, 4.066824913024902e-05, 0.0002321779727935791, 0.0003675222396850586, 0.00020930171012878418]}}, 29: {'grad': {'score': [0.3839987182617188, 0.322461172595428, 0.3726194821871244, 0.32217449394770586, 0.32713176502900965], 'topk_tokens': ['graph', '\n', ' the', '\n', '\n', '\n', ' a', 'ign', '\n', 'br', 'A', '\n', 'ance', '\n', 'BR', '\n', '\n', 'stage', ' FIRST', ' regular'], 'evidence_proportions': [0.41656341552734377, 0.34935760498046875, 0.395001220703125, 0.41839027404785156, 0.35363260904947913]}, 'weight': {'score': [0.0032051467895507814, 0.0024562705413385027, 0.0022108913996280767, 0.0024555197521934167, 0.0010791455998140223], 'topk_tokens': [' was', '<|start_header_id|>', ' the', 'Does', '.', ' ', '.\n\n', ' the', 'Answer', '?\n', 'hall', '<|eot_id|>', 'assistant', ':', 'way', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00013508796691894532, 0.0017425060272216798, 0.004542303085327149, 0.004734396934509277, 0.004848599433898926]}, 'saliency': {'score': [0.00010196447372436524, 3.213666820868555e-05, 7.13207782843174e-05, 3.186809762371085e-05, 5.262494087219238e-05], 'topk_tokens': ['.\n\n', ' Bridge', 'Right', ' Sandra', ' the', 'Answer', ' was', 'IVE', '?\n', ' the', '<|eot_id|>', ':', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'way', 'hall', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.4092197418212893e-05, 9.585022926330567e-05, 8.673071861267089e-05, 0.0002396032214164734, 9.288887182871501e-05]}}, 30: {'grad': {'score': [0.3242474365234375, 0.3511296156006457, 0.3720805828387921, 0.35111771787748264, 0.5121981452493106], 'topk_tokens': [' account', ' J', ' S', ' S', '0', ' boat', ' his', ' N', ' S', ' boat', '3', 'D', ' be', ' S', '0', 'deal', ' in', ' M', ' S', ' an'], 'evidence_proportions': [0.2934745788574219, 0.4027427673339844, 0.23213577270507812, 0.47553253173828125, 0.26038169860839844]}, 'weight': {'score': [0.014370907545089722, 0.002456580987466146, 0.0067256559164096145, 0.002418488117394827, 0.004942111408009249], 'topk_tokens': ['.', '<|eot_id|>', ' the', 'Gov', '<|end_header_id|>', ' Miles', '.\n\n', '<|start_header_id|>', 'Answer', '\n\n', 'hall', '?\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', ':', '<|eot_id|>', '<|start_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002186739444732666, 0.013198548555374145, 0.015112686157226562, 0.031583547592163086, 0.01340810457865397]}, 'saliency': {'score': [0.0008805990219116211, 7.362505419522411e-05, 0.00030794434058360563, 7.122039452647459e-05, 0.00026784293791827035], 'topk_tokens': [' Wild', ' Bridge', ' the', ' Broadway', ' milk', 'Answer', '.', ' Sandra', ' Sandra', ' Gov', '<|eot_id|>', 'assistant', ' Sandra', ' Miles', '<|begin_of_text|>', 'Gov', ':', 'way', '<|start_header_id|>', 'hall'], 'evidence_proportions': [0.0001269400119781494, 0.0015088438987731934, 5.358457565307617e-05, 0.0033014267683029175, 6.040434042612712e-05]}}, 31: {'grad': {'score': [0.40504913330078124, 0.6713278353146167, 0.36116604927258616, 0.6728662501797508, 0.39952618514790256], 'topk_tokens': [' have', ' to', ' have', 'E', ' D', ' the', ' these', 'f', ' to', ' and', ' two', 'F', ' United', ' ever', ' its', ' his', 'f', ' w', ' its', ' F'], 'evidence_proportions': [0.46412353515625004, 0.3903961181640625, 0.47341308593750003, 0.26245689392089844, 0.4061228434244792]}, 'weight': {'score': [0.002138223648071289, 0.002280171551400109, 0.0027793386043646396, 0.002278865659892119, 0.0012938282069037941], 'topk_tokens': [' the', 'Question', ' the', ':', ' milk', ',', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'assistant', ':', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0004242599010467529, 0.0012604296207427979, 0.002327299118041992, 0.004384428262710571, 0.002642989158630371]}, 'saliency': {'score': [0.00010400533676147461, 4.116303869770113e-05, 7.026699873117301e-05, 4.0941044284829947e-05, 1.8248487921322095e-05], 'topk_tokens': ['CH', '.', ' Where', ' hallway', '.', 'Question', '<|eot_id|>', '<|eot_id|>', ' Paul', ' Mess', ' Market', '?\n', 'hall', 'Answer', '<|begin_of_text|>', '<|end_header_id|>', ':', '<|start_header_id|>', 'assistant', 'way'], 'evidence_proportions': [1.0287761688232423e-05, 4.6366453170776366e-05, 0.00017231106758117676, 0.0001329854130744934, 0.0001538942257563273]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:11:46.758 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:11:46.758 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-2_1-4-6-7-9.pkl | len: 10 |  size: 9.58 KB
Processing depth (1, 4, 6, 7, 9):   3%|▎         | 3/100 [00:55<30:09, 18.65s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (2, 3, 6, 7, 9):   3%|▎         | 3/100 [01:02<30:09, 18.65s/it]2025-01-22 03:11:53.947 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 03:11:53.955 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2472, 2477) --> . Sandra travelled to the
2025-01-22 03:11:53.955 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 03:11:53.968 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3752, 3757) --> . Sandra picked up the
2025-01-22 03:11:53.968 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:11:53.986 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6464, 6469) -->  back to the kitchen.
2025-01-22 03:11:53.986 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra left the milk.
2025-01-22 03:11:54.009 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8446, 8450) -->  Sandra left the milk
2025-01-22 03:11:54.010 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:11:54.028 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (6461, 6467) --> . Daniel went back to the
2025-01-22 03:11:54.028 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:11:54.041 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4455, 4460) --> . Mary got the football
2025-01-22 03:11:54.041 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:11:54.055 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4886, 4891) --> . Mary moved to the
2025-01-22 03:11:54.055 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 03:11:54.074 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6461, 6467) --> . Daniel went back to the
2025-01-22 03:11:54.074 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John went back to the bedroom.
2025-01-22 03:11:54.098 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8197, 8203) --> . John went back to the
2025-01-22 03:11:54.098 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:11:54.109 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4158, 4163) --> . John moved to the
2025-01-22 03:11:54.109 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:11:54.141 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10898, 10904) -->  journeyed to the office.
2025-01-22 03:11:54.141 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  John journeyed to the office.
2025-01-22 03:11:54.173 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (10897, 10903) -->  John journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:11:56.300 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:11:56.301 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12253])
your chose emoji: ['🏃🏾\u200d♀\u200d➡️', '🧜🏻\u200d♀', '🤲🏾', '🧑\u200d⚖️', '💆🏼\u200d♀️', '🧝🏼\u200d♂', '🇬🇸', '👨🏾\u200d🚀', '🏊🏼\u200d♂️', '🧎\u200d♀️\u200d➡️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 90687.65it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.59s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.24s/it][A
 75%|███████▌  | 6/8 [00:04<00:01,  1.97it/s][A100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.59it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.52it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.08it/s][A100%|██████████| 8/8 [00:00<00:00, 21.49it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.52it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.18it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.08it/s][A100%|██████████| 8/8 [00:00<00:00, 21.32it/s]
2025-01-22 03:12:04.677 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.251181640625, 0.22011148836220526, 0.26306991088084686, 0.21991036202650996, 0.4052427664570425], 'topk_tokens': [' Aw', 'ra', '\n', ' CONNECT', ' work', 'u', ' hand', '�', ' W', ' Vand', 'hand', '\n', 'le', ' hand', 'Frank', ' Project', 'hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.25146026611328126, 0.23821868896484374, 0.25847015380859373, 0.24621963500976562, 0.2589861551920573]}, 'weight': {'score': [0.020242770910263063, 0.0025489692121510718, 0.007437451527668879, 0.0024970502609734145, 0.0013708407166360438], 'topk_tokens': [' milk', '\n\n', '?\n', ' the', ' the', 'Answer', ' hallway', ':', '<|start_header_id|>', 'Bridge', 'assistant', '<|eot_id|>', ' kitchen', ' hallway', '\n\n', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0022362351417541506, 0.01478642225265503, 0.025467300415039064, 0.06279850006103516, 0.007070913910865784]}, 'saliency': {'score': [0.0008297526836395264, 3.2746721808032644e-05, 0.00031786545729025814, 3.0200398011313962e-05, 4.4050230377021876e-05], 'topk_tokens': ['<|start_header_id|>', ' Bench', ':', ' milk', '<|eot_id|>', 'Answer', ' kitchen', '\n\n', ' office', '<|eot_id|>', ' kitchen', ' bedroom', ' bedroom', ' hallway', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', 'way', ' hallway', 'hall'], 'evidence_proportions': [6.418824195861817e-05, 0.000608980655670166, 0.001696014404296875, 0.001934736967086792, 0.00019315878550211588]}}, 25: {'grad': {'score': [0.34917091369628905, 0.34427453581409106, 0.4275360107421875, 0.3439981572897103, 0.40332990405203284], 'topk_tokens': ['led', 'yster', ' obtain', ' g', ' post', ' get', ' laying', ' most', ' place', ' large', '\n', ' Wood', ' time', ' getting', ' used', 'ING', ' too', 'AY', 'g', ' post'], 'evidence_proportions': [0.41071319580078125, 0.2910797119140625, 0.38489990234375, 0.2501659393310547, 0.38252417246500653]}, 'weight': {'score': [0.014603670835494995, 0.002538986601343977, 0.00396998494099348, 0.002509670160965031, 0.0013578434785207112], 'topk_tokens': [' prior', ' hallway', '<|eot_id|>', ' the', '.\n\n', '<|end_header_id|>', '?\n', ' milk', 'Answer', '<|start_header_id|>', '<|start_header_id|>', 'hall', ':', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009230375289916992, 0.012312781810760499, 0.016251754760742188, 0.047020912170410156, 0.004928375283877054]}, 'saliency': {'score': [0.0005357086658477784, 2.1950609242791607e-05, 0.00019431572694044846, 2.0345770741698e-05, 3.575627831206925e-05], 'topk_tokens': [' Sandra', ' East', '<|start_header_id|>', ' Mary', ' hallway', ' top', ' facts', ' prior', '?\n', '.', 'Mer', 'assistant', ' Mary', ' milk', '\n\n', 'hall', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [5.515813827514648e-05, 0.0005212783813476563, 0.0007703900337219238, 0.0012898892164230347, 0.0002498378356297811]}}, 26: {'grad': {'score': [0.4419818115234375, 0.4550841209471381, 0.41025601900540865, 0.4552543845389459, 0.35274586732360136], 'topk_tokens': [' City', ' city', ' not', ' never', ' extra', ' time', ' much', ' extra', ' issued', ' red', 'ucci', ' a', ' true', 'ian', ' circ', ' uncommon', '3', 'outs', ' not', ' it'], 'evidence_proportions': [0.5299072265625, 0.46563720703125, 0.420257568359375, 0.36225128173828125, 0.4202550252278646]}, 'weight': {'score': [0.01652604103088379, 0.0024721357125215056, 0.0034753542680006763, 0.0024401087147670154, 0.0015917982178172846], 'topk_tokens': ['<|end_header_id|>', '.\n\n', 'Bridge', '<|start_header_id|>', '?\n', ' kitchen', ' the', 'Answer', ' hallway', '\n\n', 'assistant', ' kitchen', '<|start_header_id|>', '<|eot_id|>', 'hall', '<|eot_id|>', 'way', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005190253257751464, 0.008566159009933471, 0.04603443145751953, 0.029166698455810547, 0.0034810254971186323]}, 'saliency': {'score': [0.0004701220989227295, 4.919377877802513e-05, 0.00010766432835505559, 4.81436180630381e-05, 4.761863028866121e-05], 'topk_tokens': ['Question', ' Ramsey', 'Mer', ' Western', ' hallway', 'river', 'Answer', '<|eot_id|>', 'assistant', ' kitchen', 'Bridge', '?\n', 'way', '<|start_header_id|>', '\n\n', ' kitchen', '<|begin_of_text|>', '<|end_header_id|>', ':', 'hall'], 'evidence_proportions': [1.366138458251953e-05, 0.0001737654209136963, 0.0016369819641113281, 0.0004330053925514221, 0.00014983117580413818]}}, 27: {'grad': {'score': [0.22963024139404298, 0.3010787864266737, 0.20215565119034204, 0.30154173064419604, 0.270062369861822], 'topk_tokens': ['.', ' recover', ' victory', ' *\n\n', ' majority', ' *\n\n', ' July', ' and', ' business', '--', ':\n', ',', '\n', ' destination', ' majority', '.', 'hand', ' participate', ' candidates', ' duration'], 'evidence_proportions': [0.2708648681640625, 0.24464263916015627, 0.220880126953125, 0.1726388931274414, 0.22804371515909833]}, 'weight': {'score': [0.015776189565658568, 0.0025291350273799647, 0.0059377665703113265, 0.002491068016747477, 0.0020817857364128375], 'topk_tokens': [' the', ' kitchen', ' lounge', ' THE', ' bathroom', '?\n', 'Answer', '\n\n', '.\n\n', '<|eot_id|>', 'assistant', '<|eot_id|>', ' kitchen', 'hall', '<|start_header_id|>', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0010761260986328126, 0.0061393678188323975, 0.04034996032714844, 0.029950857162475586, 0.006129006544748942]}, 'saliency': {'score': [0.0005715000629425049, 4.0111854166648405e-05, 0.00027831319050911145, 3.826026645858144e-05, 8.752359741035549e-05], 'topk_tokens': [' bedroom', '<|start_header_id|>', ' hallway', ' Mary', '<|eot_id|>', ' the', '<|eot_id|>', 'THE', 'assistant', ' the', ' kitchen', '<|begin_of_text|>', ':', ' THE', '<|end_header_id|>', 'way', ' hallway', ' kitchen', '.\n\n', 'hall'], 'evidence_proportions': [3.5715103149414065e-05, 0.0002859175205230713, 0.0020879685878753664, 0.0002894848585128784, 0.0001802593469619751]}}, 28: {'grad': {'score': [0.248780517578125, 0.3355258911770255, 0.21987484662960738, 0.3360737114753623, 0.31210209857458354], 'topk_tokens': [' house', 'urred', 'ed', ' houses', 'ching', 'land', 'line', ' hallway', 'hall', ' host', ' law', ' hard', ' line', ' line', ' hall', 'arp', ' hallway', ' hall', ' hall', ' hall'], 'evidence_proportions': [0.2387664794921875, 0.254248046875, 0.2850982666015625, 0.18482208251953125, 0.2649434407552083]}, 'weight': {'score': [0.0064638018608093265, 0.0024298855903565728, 0.004842778811087975, 0.0024138955360283377, 0.0010390404997200802], 'topk_tokens': [' kitchen', ' to', 'Question', ' discarded', '.', ' hallway', '.\n\n', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'hall', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0002194821834564209, 0.0014887630939483642, 0.01111736297607422, 0.017006397247314453, 0.004906902710596721]}, 'saliency': {'score': [0.00018090248107910156, 4.426056310028694e-05, 0.00010326504707336426, 4.379163078201099e-05, 2.0966447632888268e-05], 'topk_tokens': [' Most', '<|eot_id|>', '.\n\n', ' hallway', ' milk', '<|eot_id|>', 'Probably', ' the', '.', 'Answer', ' nearly', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'way', ':', 'hall'], 'evidence_proportions': [8.845329284667969e-06, 3.0553340911865236e-05, 0.00040163397789001466, 0.0003528222441673279, 0.00015101830164591473]}}, 29: {'grad': {'score': [0.3243276214599609, 0.252312732427612, 0.35792737129407054, 0.2518272219993311, 0.2616944532284791], 'topk_tokens': [' The', '26', '\n', ' Mr', '\n', ' the', '\n', ' spr', 'ance', ' regular', '\n', ' FIRST', 'A', 'ign', 'BR', '\n', 'br', '\n', '\n', '\n'], 'evidence_proportions': [0.31008262634277345, 0.291973876953125, 0.35009613037109377, 0.3621091842651367, 0.31649843851725257]}, 'weight': {'score': [0.004020472764968872, 0.002456013200799728, 0.0025447607040405273, 0.00245252135026408, 0.0009535335261246253], 'topk_tokens': [' Does', ' the', ' Where', 'Question', '<|start_header_id|>', ' hallway', ' the', '.\n\n', '?\n', 'Answer', 'hall', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'way', '<|end_header_id|>', '<|eot_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00024863481521606443, 0.0009590089321136475, 0.006766223907470703, 0.010042905807495117, 0.00341180960337321]}, 'saliency': {'score': [0.0001293337345123291, 2.4382538081927337e-05, 6.418503247774564e-05, 2.4040012065506983e-05, 3.446004856591937e-05], 'topk_tokens': ['"The', ' Where', 'NEW', ' Does', ' the', 'IVE', '<|start_header_id|>', 'Answer', '?\n', ' the', '.\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'way', 'assistant', 'hall', ':', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [1.3828277587890625e-05, 3.0165910720825193e-05, 0.00010002851486206054, 0.00053444504737854, 6.25749429066976e-05]}}, 30: {'grad': {'score': [0.2625765800476074, 0.33132834471859446, 0.28700136527037007, 0.33161111590586934, 0.4229409009560771], 'topk_tokens': [' Loan', ' S', ' S', ' S', ' his', ' Paul', ' Europe', 'hall', '0', ' N', ' boat', 'deal', ' S', 'D', ' be', ' in', ' M', ' J', ' S', ' an'], 'evidence_proportions': [0.26564741134643555, 0.32074584960937497, 0.2303464889526367, 0.3486013412475586, 0.18105173110961914]}, 'weight': {'score': [0.016272114515304567, 0.002458045594686005, 0.006695447059778066, 0.0024161648220355743, 0.0035708913172798596], 'topk_tokens': [' hallway', 'Question', ' milk', '.', '<|end_header_id|>', ' the', '<|start_header_id|>', '.\n\n', '\n\n', 'Answer', 'hall', '?\n', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0023603737354278564, 0.007303762435913085, 0.023587799072265624, 0.04801654815673828, 0.008079499006271362]}, 'saliency': {'score': [0.00102303147315979, 5.384592713870828e-05, 0.00020003395202832343, 5.1390958993923004e-05, 0.00019042759106076997], 'topk_tokens': [' Sandra', ' hallway', '.', ' Sandra', ' Sandra', '.', 'Bridge', 'Gov', '?\n', 'Answer', '<|eot_id|>', ' milk', '<|eot_id|>', 'assistant', ' the', ':', '<|begin_of_text|>', 'way', 'hall', '<|start_header_id|>'], 'evidence_proportions': [0.0002471566200256348, 0.0008070707321166992, 0.0010739266872406006, 0.0033846572041511536, 0.00023273130257924396]}}, 31: {'grad': {'score': [0.4872289276123047, 0.7155524756827492, 0.4677501091590294, 0.7168133337040898, 0.44929500557910435], 'topk_tokens': [' he', 'F', ' two', ' United', ' have', ' have', ' and', 'E', ' the', ' the', ' the', ' w', ' to', ' ever', ' its', 'f', ' his', ' an', ' F', ' its'], 'evidence_proportions': [0.43355522155761717, 0.4370269775390625, 0.62947998046875, 0.3443412780761719, 0.5505078633626301]}, 'weight': {'score': [0.0027001023292541502, 0.0022719335151403443, 0.002744949780977689, 0.002269542450940828, 0.0013834919737673354], 'topk_tokens': [' was', ' the', ' hallway', ',', ':', ' Where', 'Question', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|start_header_id|>', '\n\n', 'assistant', ':', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0007878184318542481, 0.001387465000152588, 0.003850007057189941, 0.005520224571228027, 0.002549201250076294]}, 'saliency': {'score': [9.240508079528808e-05, 3.678286229272424e-05, 9.882297271337264e-05, 3.6470352469153915e-05, 2.014431460150357e-05], 'topk_tokens': ['.', ' Paul', ' Market', ' Ramsey', '?\n', '.', ' Where', ' Mess', '<|eot_id|>', 'Question', '<|eot_id|>', ':', '<|start_header_id|>', ' hallway', '<|begin_of_text|>', '<|end_header_id|>', 'hall', 'Answer', 'assistant', 'way'], 'evidence_proportions': [4.57763671875e-06, 3.820061683654785e-05, 0.00030710101127624513, 9.223073720932007e-05, 3.196795781453451e-05]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:12:04.679 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:12:04.679 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-3_2-3-6-7-9.pkl | len: 10 |  size: 9.61 KB
Processing depth (2, 3, 6, 7, 9):   4%|▍         | 4/100 [01:13<29:22, 18.36s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]
Processing depth (1, 3, 5, 6, 7):   4%|▍         | 4/100 [01:20<29:22, 18.36s/it]2025-01-22 03:12:12.202 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 03:12:12.206 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1493, 1498) -->  Sandra travelled to the hallway
2025-01-22 03:12:12.206 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 03:12:12.217 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3803, 3808) -->  war. Sandra picked up
2025-01-22 03:12:12.217 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 03:12:12.234 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5983, 5988) --> . Sandra went to the
2025-01-22 03:12:12.235 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra left the milk.
2025-01-22 03:12:12.254 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7208, 7212) -->  Sandra left the milk
2025-01-22 03:12:12.254 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:12:12.273 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (6425, 6431) --> . Daniel went back to the
2025-01-22 03:12:12.273 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary got the football there.
2025-01-22 03:12:12.286 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4462, 4467) --> . Mary got the football
2025-01-22 03:12:12.286 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:12:12.300 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4875, 4880) --> . Mary moved to the
2025-01-22 03:12:12.300 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 03:12:12.318 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6425, 6431) --> . Daniel went back to the
2025-01-22 03:12:12.318 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John went back to the bedroom.
2025-01-22 03:12:12.341 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8144, 8150) --> . John went back to the
2025-01-22 03:12:12.342 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:12:12.353 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4200, 4205) --> . John moved to the
2025-01-22 03:12:12.353 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 03:12:12.385 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10890, 10896) -->  John journeyed to the office
2025-01-22 03:12:12.385 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  John journeyed to the office.
2025-01-22 03:12:12.416 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (10889, 10895) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:12:14.461 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:12:14.461 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12225])
your chose emoji: ['🕣', '🅾', '➰', '🧍🏿\u200d♂', '🚶\u200d➡️', '✌️', '🧗🏿\u200d♀', '🙋🏿\u200d♂', '🚶🏿\u200d♀\u200d➡', '🎗️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 196224.75it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.81s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.28s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.91it/s][A100%|██████████| 8/8 [00:05<00:00,  1.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.82it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.61it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.32it/s][A100%|██████████| 8/8 [00:00<00:00, 21.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.63it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.25it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.13it/s][A100%|██████████| 8/8 [00:00<00:00, 21.38it/s]
2025-01-22 03:12:23.267 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.33309024810791016, 0.2545674542098565, 0.28350767722496617, 0.25431328300424644, 0.4660409183825477], 'topk_tokens': [' CONNECT', 'u', 'system', ' work', 'sur', ' hand', ' Vand', 'hand', '\n', 'le', ' short', 'Frank', 'Sh', ' Project', ' hand', ' hand', ' hand', 'hand', ' hand', ' Wide'], 'evidence_proportions': [0.2728790283203125, 0.47090454101562496, 0.30794677734375, 0.316925048828125, 0.30015071233113605]}, 'weight': {'score': [0.04144661545753479, 0.002552513441385925, 0.005208499156511747, 0.0024640609592013123, 0.003329554856833765], 'topk_tokens': [' Sandra', ' the', ' bedroom', ' THE', 'Bridge', 'Answer', ':', ' the', '<|start_header_id|>', ' hallway', '<|eot_id|>', 'assistant', 'hall', ' kitchen', ' hallway', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.09137697219848633, 0.011174899339675904, 0.044912755489349365, 0.06960105895996094, 0.0034063359101613364]}, 'saliency': {'score': [0.0021623337268829347, 3.4026565175841046e-05, 0.0003520112771254319, 2.8632855638788154e-05, 0.0001797736701318773], 'topk_tokens': [' random', ':', '�', 'Answer', ' bedroom', 'assistant', ' office', '\n\n', ' the', ' football', '<|eot_id|>', ' hallway', 'Bridge', '<|end_header_id|>', ' bedroom', ' THE', ' kitchen', '<|begin_of_text|>', ' hallway', 'hall'], 'evidence_proportions': [0.00692756175994873, 0.00010716915130615234, 0.0015655577182769776, 0.002642996609210968, 8.081893126169841e-05]}}, 25: {'grad': {'score': [0.37208465576171873, 0.4527981414645179, 0.44236520620492786, 0.452997477342329, 0.4902311341237214], 'topk_tokens': ['st', ' perpetrated', ' first', ' of', ' most', ' p', ' obtain', ' get', ' Wood', 'ING', ' too', ' large', ' post', '\n', 'AY', ' g', ' getting', 'g', ' used', ' post'], 'evidence_proportions': [0.5296630859375, 0.29976348876953124, 0.37843017578125, 0.2561798095703125, 0.3730189005533854]}, 'weight': {'score': [0.013964107036590576, 0.0025399287307734265, 0.003241481689306406, 0.002514199939008529, 0.004010785939329762], 'topk_tokens': [' the', ' the', '�', '�', '.', '.\n\n', '?\n', ' the', 'Answer', 'hall', ' THE', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.009067010879516602, 0.007018351554870605, 0.020943552255630493, 0.037540435791015625, 0.0022993932167689004]}, 'saliency': {'score': [0.00039037704467773435, 2.5012123066106964e-05, 0.00011629324692946214, 2.3968544730776354e-05, 0.00016329349097559008], 'topk_tokens': [' East', ' prior', 'Mer', ' Mary', 'Answer', '?\n', ' hallway', ' hallway', 'Probably', ' Daniel', '<|start_header_id|>', 'assistant', '.', ' THE', 'hall', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0006958067417144775, 0.0001676023006439209, 0.0005102396011352539, 0.0006314367055892944, 6.0906012852986656e-05]}}, 26: {'grad': {'score': [0.5024295806884765, 0.5160977279668078, 0.4607715606689453, 0.516303205129466, 0.4576546054775432], 'topk_tokens': [' City', ' single', ' not', ' time', ' City', ' city', ' a', ' much', 'ers', ' uncommon', ' true', 'ing', 'ian', ' true', ' circ', 'ucci', '3', ' not', 'outs', ' it'], 'evidence_proportions': [0.57830810546875, 0.53978271484375, 0.4839935302734375, 0.43178558349609375, 0.4705292383829753]}, 'weight': {'score': [0.019167896509170532, 0.0024812249603870457, 0.0028446645308763552, 0.0024457645088934027, 0.003392671629533929], 'topk_tokens': [' the', 'Bridge', '.\n\n', ' the', ' the', '?\n', ' hallway', ' hallway', 'Answer', 'assistant', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' kitchen', '<|eot_id|>', 'hall', 'way', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.050919842720031736, 0.004792654514312744, 0.012694203853607179, 0.03151988983154297, 0.0018473913272221885]}, 'saliency': {'score': [0.0009451150894165039, 6.241281905281882e-05, 8.888045946756999e-05, 6.0513789561264454e-05, 0.00013371544369196489], 'topk_tokens': [' the', ' Western', 'Answer', ' kitchen', '<|eot_id|>', ' the', 'assistant', ' hallway', '<|eot_id|>', '<|start_header_id|>', '?\n', 'Bridge', 'way', ' hallway', ' kitchen', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', ':', 'hall'], 'evidence_proportions': [0.0037680983543395994, 0.00017891526222229003, 0.0003378510475158692, 0.00047613680362701416, 4.983445008595784e-05]}}, 27: {'grad': {'score': [0.30058372497558594, 0.36550813474645794, 0.26376993228227663, 0.3659677615255088, 0.3472317841093419], 'topk_tokens': ['ided', '.', '.', '.', ',', '.', '.', '.', '\n', ' *\n\n', 'hand', '.', '.', ' majority', ' majority', ' participate', ' *\n\n', ':\n', ' duration', '.'], 'evidence_proportions': [0.2983306884765625, 0.3546337127685547, 0.31034545898437504, 0.19928741455078125, 0.3168156941731771]}, 'weight': {'score': [0.03117790222167969, 0.0025270069429287148, 0.004920889933904012, 0.002460447109106221, 0.003882075770426605], 'topk_tokens': ['Question', ' the', ' the', '?\n', 'Answer', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'assistant', '\n\n', ' hallway', ' THE', ' kitchen', 'hall', '<|start_header_id|>', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.1001622438430786, 0.0042079508304595946, 0.01877157688140869, 0.03618454933166504, 0.0031667500734329224]}, 'saliency': {'score': [0.0016512477397918701, 4.729620520102264e-05, 0.00022536974686842697, 4.342875563757312e-05, 0.00016956460677971274], 'topk_tokens': ['Some', ' the', '<|eot_id|>', ' the', ' upper', ' the', '<|eot_id|>', 'NEW', 'assistant', '<|begin_of_text|>', ' THE', ' hallway', '<|start_header_id|>', ':', '<|end_header_id|>', 'way', ' hallway', ' kitchen', '.\n\n', 'hall'], 'evidence_proportions': [0.006014329195022583, 0.0002519667148590088, 0.0011070191860198975, 0.0009970515966415405, 7.106860478719075e-05]}}, 28: {'grad': {'score': [0.29912437438964845, 0.33403956020506215, 0.24497660612448668, 0.3343968715216133, 0.34774741479905985], 'topk_tokens': ['line', ' the', ' of', '\n', 'ivery', 'of', 'hall', ' huge', '�', ' of', ' line', ' line', 'ching', 'ed', ' hall', 'arp', ' hall', ' hall', ' hallway', ' hall'], 'evidence_proportions': [0.3640968322753906, 0.29775390625, 0.33776016235351564, 0.198486328125, 0.2810182571411133]}, 'weight': {'score': [0.00890550971031189, 0.0024323958682982718, 0.004735792294526711, 0.002411706924967686, 0.0025658476150642005], 'topk_tokens': [' kitchen', 'Question', ' hallway', '.', ' discarded', '.\n\n', ' the', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', 'hall', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004718470573425293, 0.0019018590450286866, 0.015138947963714601, 0.022138118743896484, 0.004214813311894735]}, 'saliency': {'score': [0.0001905691623687744, 5.494668168260414e-05, 8.189143278659918e-05, 5.4581554478542146e-05, 7.120178917707024e-05], 'topk_tokens': ['Right', '<|eot_id|>', ' discarded', ' milk', '.', 'Answer', ' nearly', 'Probably', 'assistant', ' hallway', '?\n', ' the', ' the', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', ':', 'way', '\n\n', 'hall'], 'evidence_proportions': [0.00025010108947753906, 4.864931106567383e-05, 0.00016622543334960939, 0.0004229545593261719, 0.00012458860874176025]}}, 29: {'grad': {'score': [0.3782979965209961, 0.3049280467983317, 0.44316912920047075, 0.3043340274825844, 0.2865942938853118], 'topk_tokens': [' The', 'ance', '\n', '\n', 'ign', ' Mr', ' regular', 'stage', ' the', '\n', '\n', 'br', 'BR', '\n', '\n', ' FIRST', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.394512939453125, 0.24622783660888672, 0.47120361328125004, 0.39612579345703125, 0.38553746541341144]}, 'weight': {'score': [0.0048648881912231445, 0.002485287201182297, 0.002666558210666363, 0.002479815350300934, 0.0026727475352206474], 'topk_tokens': ['Question', 'NEW', ' Does', '<|start_header_id|>', ' hallway', ' the', '.\n\n', '?\n', ' the', 'Answer', '<|eot_id|>', 'hall', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004587882757186889, 0.0012150228023529053, 0.008129137754440307, 0.008970975875854492, 0.002679680784543355]}, 'saliency': {'score': [0.00015765666961669922, 3.095807633760282e-05, 0.00010042465650118314, 3.0474957177922036e-05, 0.00018780817419795668], 'topk_tokens': ['Answer', '�', ' the', ' Does', '\u200d', 'NEW', '.\n\n', 'IVE', '<|end_header_id|>', '<|eot_id|>', ' the', 'assistant', ' the', '<|eot_id|>', 'way', ':', '<|start_header_id|>', 'hall', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0001132965087890625, 3.835558891296387e-05, 0.0003101229667663574, 0.0002876073122024536, 8.035202821095784e-05]}}, 30: {'grad': {'score': [0.2434543228149414, 0.3588220099042668, 0.2651391151623848, 0.35935948319201483, 0.4652916213213387], 'topk_tokens': [' Loan', ' hour', ' Bridge', '2', ' his', 'deal', ' Star', 'doll', ' S', ' preparations', ' J', 'D', ' boat', '0', ' Europe', ' boat', ' in', ' S', ' an', ' M'], 'evidence_proportions': [0.296110725402832, 0.26364746093749997, 0.16944580078125, 0.2514495849609375, 0.2390899658203125]}, 'weight': {'score': [0.01935742735862732, 0.0024587381966253647, 0.007614900668462117, 0.0024074756542501853, 0.010307526184340655], 'topk_tokens': ['<|eot_id|>', 'Question', ' kitchen', '<|end_header_id|>', ' the', '<|start_header_id|>', '�', '.\n\n', '\n\n', 'Answer', 'hall', 'assistant', '?\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.014342808723449708, 0.007649880647659302, 0.02495608329772949, 0.052819252014160156, 0.006319135427474976]}, 'saliency': {'score': [0.0010842347145080566, 5.7756137937590075e-05, 0.0002483251767280774, 5.503547393503412e-05, 0.0005750524795661539], 'topk_tokens': [' upper', ' Sandra', ' Sandra', 'Gov', ' Sandra', ' the', '<|eot_id|>', ' hallway', 'Answer', 'Bridge', '<|end_header_id|>', ' the', '?\n', '�', 'assistant', ':', 'way', '<|begin_of_text|>', '<|start_header_id|>', 'hall'], 'evidence_proportions': [0.0014483571052551268, 0.000717085599899292, 0.001068645715713501, 0.002438656985759735, 0.000196799635887146]}}, 31: {'grad': {'score': [0.4062747013568878, 0.6551029068370017, 0.41062654898716855, 0.6563981454997069, 0.34931352684053324], 'topk_tokens': [' D', ' have', ' have', ' to', ' United', ' and', 'E', ' the', 'F', ' the', ' was', ' its', ' ever', ' to', ' w', 'f', ' his', ' an', ' its', ' F'], 'evidence_proportions': [0.2558582127094269, 0.41830227375030515, 0.5437264442443848, 0.3505384922027588, 0.4442131519317627]}, 'weight': {'score': [0.003021237850189209, 0.0022764540514136435, 0.00314827683644417, 0.0022721281155713567, 0.0017497761774871309], 'topk_tokens': [' was', ' the', '.', ',', 'Question', ':', ' Where', '.\n\n', '<|eot_id|>', '?\n', 'Answer', 'assistant', '\n\n', '<|start_header_id|>', ':', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0025057554244995116, 0.0014278173446655273, 0.0030320048332214355, 0.00722658634185791, 0.001966118812561035]}, 'saliency': {'score': [0.00017609596252441407, 3.565206397464941e-05, 9.814134010901817e-05, 3.516306535306239e-05, 2.5257215661517645e-05], 'topk_tokens': [' Paul', ' the', ' Paul', ':', 'CH', '<|eot_id|>', ' Mess', ' Where', '.', 'Question', ' hallway', ' hallway', ':', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', 'Answer', 'assistant', 'way'], 'evidence_proportions': [0.0006214618682861328, 5.2648782730102536e-05, 6.182789802551269e-05, 0.00010941177606582642, 4.750986893971761e-05]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:12:23.268 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:12:23.268 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-4_1-3-5-6-7.pkl | len: 10 |  size: 9.49 KB
Processing depth (1, 3, 5, 6, 7):   5%|▌         | 5/100 [01:32<29:12, 18.45s/it]Processing depth (1, 3, 5, 6, 7):   5%|▌         | 5/100 [01:32<29:15, 18.48s/it]
2025-01-22 03:12:23.514 | INFO     | __main__:<module>:72 - Selected idx: 10
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:76 - Needle: [' Daniel picked up the apple.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' John went back to the office.', ' Daniel took the football.', ' Sandra moved to the kitchen.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' Mary left the apple.', ' Daniel dropped the football.']
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel dropped the football.']
2025-01-22 03:12:23.515 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (1, 3, 6, 8):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:12:30.812 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:12:30.817 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1498, 1503) -->  tragedy. Mary moved to
2025-01-22 03:12:30.817 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:12:30.828 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3815, 3821) -->  war. Mary journeyed to
2025-01-22 03:12:30.828 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:12:30.848 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7316, 7320) -->  Mary left the apple
2025-01-22 03:12:30.848 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football.
2025-01-22 03:12:30.875 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9690, 9694) -->  dropped the football.
2025-01-22 03:12:30.875 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:12:30.898 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8338, 8343) --> . Daniel picked up the
2025-01-22 03:12:30.898 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John moved to the garden.
2025-01-22 03:12:30.913 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5268, 5273) --> . John moved to the
2025-01-22 03:12:30.913 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:12:30.942 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (10117, 10123) --> . John went back to the
2025-01-22 03:12:30.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:12:30.972 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10921, 10925) -->  Daniel took the football
2025-01-22 03:12:30.972 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:12:30.989 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4700, 4705) --> . Sandra moved to the
2025-01-22 03:12:30.989 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:12:31.008 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (6992, 6996) -->  Daniel left the apple
2025-01-22 03:12:31.008 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 03:12:31.036 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (9837, 9843) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:12:33.234 | INFO     | test_jbb_retain:begin_test:632 - The apple was in the kitchen.<|eot_id|>
2025-01-22 03:12:33.235 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12236])
your chose emoji: ['🏊🏽\u200d♀', '🧑🏿\u200d🦽\u200d➡️', '👨🏻\u200d❤️\u200d👨🏼', '🏃🏿\u200d♀', '🧜\u200d♂', '👷🏿\u200d♀', '🧑🏼\u200d🌾', '👨\u200d👩\u200d👧', '🧜🏾', '🎏']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 71240.83it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.60s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.49s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.66it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.20it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.38it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.33it/s][A100%|██████████| 8/8 [00:00<00:00, 20.49it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.42it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.09it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.12it/s][A100%|██████████| 8/8 [00:00<00:00, 20.19it/s]
2025-01-22 03:12:42.711 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.23217391967773438, 0.31001702261980146, 0.1889197485787528, 0.31048624080177406, 0.2770032034979926], 'topk_tokens': [' proceedings', ' It', '.', ' out', ' It', ' appearance', ' compl', ' agreement', ' absence', ' comparison', ' emb', ' out', 'If', ' item', ' Arr', ' It', ' communication', ' Do', ' communication', 'consider'], 'evidence_proportions': [0.34130096435546875, 0.2017072041829427, 0.17992115020751953, 0.19371795654296875]}, 'weight': {'score': [0.029093505520569652, 0.0025729208914358364, 0.02236381939479283, 0.0024747204355005854, 0.002559450268745422], 'topk_tokens': [' apple', ' Bench', ' bedroom', ' the', ' the', '.', ' kitchen', ':', 'assistant', '<|eot_id|>', 'b', ' the', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'Bridge', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0036029815673828125, 0.01115274926026662, 0.10172462463378906, 0.01523667573928833]}, 'saliency': {'score': [0.001152088767603824, 3.293677233083448e-05, 0.0006641396454402378, 2.9378628016594348e-05, 5.274017651875814e-05], 'topk_tokens': ['position', ' office', ' Fort', ' Bridge', ' apple', ':', '<|eot_id|>', '<|start_header_id|>', ' apple', '<|eot_id|>', '.', ' the', ' Mary', ' kitchen', ' bedroom', '<|begin_of_text|>', ' bathroom', ' Bench', 'Bridge', 'athroom'], 'evidence_proportions': [0.0004988133907318116, 0.0005105932553609213, 0.003614872694015503, 0.0004681423306465149]}}, 25: {'grad': {'score': [0.370928588666414, 0.45094267524665005, 0.27752391270228793, 0.451565566041404, 0.3697012424468994], 'topk_tokens': [' set', ' the', ' locom', ' im', ' old', ' old', ' for', ' York', ' a', ' a', ' for', 'If', ' for', ' a', ' of', ' a', 'ivery', ' l', ' Aw', ' no'], 'evidence_proportions': [0.34836883544921876, 0.3765843709309896, 0.4443477392196655, 0.31722545623779297]}, 'weight': {'score': [0.015855833103782253, 0.002496637699212287, 0.021871209144592285, 0.0024201555725586036, 0.002800710333718194], 'topk_tokens': [' Judge', '�', ' \n', ' Mary', ' Daniel', 'b', 'Answer', '.', ' the', ' apple', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', ' Bench', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.005249226093292236, 0.007659087578455607, 0.05061507225036621, 0.006649971008300781]}, 'saliency': {'score': [0.001484552496357968, 3.6797063083213885e-05, 0.0012867552893502372, 3.094922630425893e-05, 6.501840220557319e-05], 'topk_tokens': [' Mary', ' Ramsey', ' Mary', ' Ramsey', 'assistant', ' Paul', ' Dan', '.', ' Anthony', '<|eot_id|>', '<|eot_id|>', 'athroom', ' Ramsey', ' Mary', ' apple', ' Bench', '<|end_header_id|>', ' apple', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0006108105182647706, 0.0006405313809712728, 0.005156315863132477, 0.00017099827527999878]}}, 26: {'grad': {'score': [0.4181060791015625, 0.4232086174160215, 0.3784881591796875, 0.42334502810672703, 0.4116928524441189], 'topk_tokens': [' Anthony', ' Milwaukee', ' Gutenberg', 'UX', 'graph', ' favor', 'UX', ' executed', 'graph', 'single', 'b', ' Eagle', 'graph', 'ub', 'b', ' Press', ' Marshall', 'issippi', 'itter', ' bitter'], 'evidence_proportions': [0.47969970703125003, 0.44773356119791663, 0.2757301330566406, 0.43904876708984375]}, 'weight': {'score': [0.016134315415432577, 0.002455484069380381, 0.02290269477026803, 0.002375422504332695, 0.0023459285497665406], 'topk_tokens': [' kitchen', '?', ' apple', 'Bridge', ' Bench', '<|eot_id|>', '<|eot_id|>', ' \n', 'assistant', 'Answer', ' apple', 'b', ' bathroom', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0013765692710876465, 0.0025264422098795576, 0.06628656387329102, 0.004841059446334839]}, 'saliency': {'score': [0.0004973693897849635, 4.4264951867696475e-05, 0.0009241606507982527, 4.103103034262487e-05, 5.4499175813463e-05], 'topk_tokens': [' the', ' the', 'Bridge', ' Sandra', ' Anthony', '.', ' Mary', '?', ' \n', '<|end_header_id|>', ' Daniel', ' bathroom', ' Daniel', '<|start_header_id|>', ' the', ':', 'athroom', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0002084672451019287, 0.00015954176584879556, 0.0018227547407150269, 3.985315561294556e-05]}}, 27: {'grad': {'score': [0.2989444983632941, 0.3438765685322024, 0.299754878452846, 0.3440733652893617, 0.28350017335679795], 'topk_tokens': [' Marshall', 'ers', ' other', ' sleep', ' platform', ' state', 'successful', '.', ' started', ' department', ' short', '-n', 'ARCH', ' staff', ' convention', 'ides', ' received', ' designated', ' step', ' accepted'], 'evidence_proportions': [0.365948486328125, 0.24033586184183756, 0.3088722229003906, 0.29317474365234375]}, 'weight': {'score': [0.018010939422406648, 0.00252734065124028, 0.017355606385639735, 0.0024606046908499522, 0.005398125780953301], 'topk_tokens': ['?', ' Bench', ' the', ' the', ' apple', ' \n', ' bedroom', ' apple', 'Answer', 'assistant', 'b', '�', '.\n\n', '<|start_header_id|>', '\n\n', ' bathroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003944277763366699, 0.006554335355758667, 0.06734299659729004, 0.003447115421295166]}, 'saliency': {'score': [0.0015376021987513493, 4.437682441426547e-05, 0.001621671233858381, 3.751785137832385e-05, 0.000169207321272956], 'topk_tokens': [' apple', ' the', ' Daniel', ' the', 'assistant', 'THE', 'Bridge', ' bedroom', 'b', ' Daniel', '�', ' Judge', ' apple', ' the', ' the', '<|begin_of_text|>', '<|end_header_id|>', '.\n\n', ':', 'athroom'], 'evidence_proportions': [0.0001645505428314209, 0.00036795934041341144, 0.006438851356506348, 0.00010713189840316772]}}, 28: {'grad': {'score': [0.354533546849301, 0.319020074706752, 0.30470665863582064, 0.3190058123835492, 0.2558854129579332], 'topk_tokens': ['dent', ' summer', '.', ' probably', ' proof', 'nes', ' became', ' summer', 'na', ' before', 'prev', 'about', ' inside', ' prepared', ' shore', ' half', ' half', ' returns', 'half', 'ball'], 'evidence_proportions': [0.375732421875, 0.36186726888020837, 0.3559093475341797, 0.3156585693359375]}, 'weight': {'score': [0.01531409276159186, 0.002390759746490112, 0.015802208866391863, 0.0023320856351660667, 0.0013710217343436348], 'topk_tokens': [' the', 'Answer', '<|eot_id|>', '<|eot_id|>', ' bedroom', ' bathroom', ' before', 'assistant', ' apple', ' the', ' \n', ' the', '?', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00032178163528442385, 0.0034584601720174155, 0.0612025260925293, 0.005949497222900391]}, 'saliency': {'score': [0.00014826969096535131, 2.915539113473382e-05, 0.00033684543200901575, 2.8085852921571603e-05, 1.8705593215094673e-05], 'topk_tokens': [' to', ' the', ' the', '?', ' apple', ' before', 'b', ' bedroom', ' the', '<|start_header_id|>', 'assistant', ' bathroom', 'Bridge', ' Bridge', '<|begin_of_text|>', '\n\n', ' Bridge', ':', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [2.2661685943603515e-05, 5.2327911059061684e-05, 0.0005485862493515015, 4.887580871582031e-05]}}, 29: {'grad': {'score': [0.37187355443050985, 0.3761724395215857, 0.30539093017578123, 0.376382454428753, 0.37552403344048396], 'topk_tokens': [' LO', ' com', 'y', ' In', ' Pioneer', ' The', 'ION', 're', ' The', ' An', ' a', ' THE', ' In', ' B', 'y', '.', ' ga', 'b', ' The', ' M'], 'evidence_proportions': [0.50567626953125, 0.28780110677083337, 0.40961456298828125, 0.2929878234863281]}, 'weight': {'score': [0.011419586445155897, 0.002450691464387744, 0.009455629757472448, 0.0024165854451105545, 0.0017595072587331137], 'topk_tokens': [' the', ' the', '<|eot_id|>', ' before', ' Does', '<|eot_id|>', '.\n\n', 'Answer', ' \n', '?', ' apple', 'assistant', 'b', ' the', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00014173388481140137, 0.0023777981599171958, 0.049253106117248535, 0.0012460649013519287]}, 'saliency': {'score': [0.0003669042336313348, 3.719638352184107e-05, 0.0002657958439418248, 3.6025646528260455e-05, 0.00010744233926137288], 'topk_tokens': ['THE', ' the', '<|eot_id|>', ' \n', 'Does', ' the', 'Answer', ' the', '�', '<|eot_id|>', 'athroom', 'assistant', ':', ' before', ' Does', '<|end_header_id|>', '\n\n', 'b', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [8.350610733032226e-06, 0.0001554936170578003, 0.0014656484127044678, 3.3468008041381836e-05]}}, 30: {'grad': {'score': [0.4208486456620066, 0.33875145060104994, 0.3843924386160714, 0.3384923384724751, 0.3080190022786458], 'topk_tokens': [' of', ' two', ' the', ' soon', ' its', '2', ' the', ' an', ' an', ' his', '3', ' Burb', ' B', 'b', ' account', ' forb', ' B', 'b', 'deal', 'b'], 'evidence_proportions': [0.3253631591796875, 0.35158793131510413, 0.59832763671875, 0.4666175842285156]}, 'weight': {'score': [0.01733448630885074, 0.0024527700053134844, 0.02207248296056475, 0.002373209680061043, 0.005051207211282518], 'topk_tokens': [' the', ' apple', ' bedroom', ' the', ' the', ' the', '.\n\n', '<|eot_id|>', 'assistant', 'Answer', '<|eot_id|>', '?', 'b', ' \n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002858173847198486, 0.006777435541152954, 0.0625920295715332, 0.0060079097747802734]}, 'saliency': {'score': [0.0015331679268887168, 7.761157370342208e-05, 0.0007374482495444162, 7.344662874117707e-05, 4.279977745480008e-05], 'topk_tokens': [' garden', '.\n\n', ' Bridge', ' Bridge', ' the', ' apple', ' the', ' Mary', 'assistant', 'Bridge', ' Bench', ' the', ' bedroom', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'b', ':', 'athroom'], 'evidence_proportions': [0.00018862485885620117, 0.00027319788932800293, 0.0064634159207344055, 0.00017355382442474365]}}, 31: {'grad': {'score': [0.281361637931121, 0.3505836244949826, 0.2647131987980434, 0.3509382147817373, 0.19294901423984104], 'topk_tokens': [' the', 'did', ' the', ' having', ' the', ' the', ' the', ' that', ' the', 'membership', 'If', ' August', ' they', ' the', ' the', ' population', ' the', ' he', ' the', ' the'], 'evidence_proportions': [0.24124479293823242, 0.2662489016850789, 0.28499555587768555, 0.3505428805947304]}, 'weight': {'score': [0.002627746055000707, 0.0022672930129243127, 0.003891373532158988, 0.002262065977555198, 0.0012265930573145548], 'topk_tokens': [':', ' Where', 'Question', ' the', ' before', '.\n\n', '<|eot_id|>', ' bedroom', '?', 'Answer', ' \n', 'b', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001140838861465454, 0.001395965615908305, 0.005428910255432129, 0.003532886505126953]}, 'saliency': {'score': [5.206152012473658e-05, 1.717705393121936e-05, 7.022619247436524e-05, 1.6970281283973817e-05, 1.0126829147338868e-05], 'topk_tokens': ['Question', ':', '.\n\n', ' Bridge', ' the', ' Emily', 'Answer', ' Market', '?', '<|eot_id|>', ' apple', ' \n', 'b', ' the', ' bedroom', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [1.4919042587280272e-05, 3.282229105631511e-05, 0.00016330182552337646, 1.6108155250549316e-05]}}, 'pred_res': 'The apple was in the kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:12:42.712 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:12:42.713 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-0_1-3-6-8.pkl | len: 10 |  size: 9.08 KB
Processing depth (1, 3, 6, 8):   1%|          | 1/100 [00:19<31:31, 19.11s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 6, 7, 8):   1%|          | 1/100 [00:26<31:31, 19.11s/it]2025-01-22 03:12:49.797 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:12:49.797 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:12:49.797 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:12:49.819 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7224, 7230) --> . Mary journeyed to the
2025-01-22 03:12:49.819 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:12:49.842 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8411, 8415) -->  Mary left the apple
2025-01-22 03:12:49.842 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football.
2025-01-22 03:12:49.868 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9693, 9697) -->  Daniel dropped the football
2025-01-22 03:12:49.868 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:12:49.891 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8295, 8300) --> . Daniel picked up the
2025-01-22 03:12:49.891 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John moved to the garden.
2025-01-22 03:12:49.906 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5263, 5268) --> . John moved to the
2025-01-22 03:12:49.906 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:12:49.935 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (10079, 10085) --> . John went back to the
2025-01-22 03:12:49.935 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:12:49.965 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10886, 10890) -->  took the football.
2025-01-22 03:12:49.965 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:12:49.978 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4681, 4686) --> . Sandra moved to the
2025-01-22 03:12:49.978 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:12:49.997 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (6939, 6943) -->  Daniel left the apple
2025-01-22 03:12:49.997 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 03:12:50.024 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (9800, 9806) -->  Sandra journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:12:52.214 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:12:52.215 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12216])
your chose emoji: ['⛎', '⛔', '🚶🏽\u200d➡️', '🚱', '🇸🇾', '🚶\u200d♂️', '🎰', '🧑🏿\u200d🍳', '🇰🇳', '👩🏻\u200d❤️\u200d💋\u200d👩🏽']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 248551.35it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.54s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.09s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.90it/s][A100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.63it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.73it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.90it/s][A100%|██████████| 8/8 [00:00<00:00, 21.01it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.98it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.43it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.63it/s][A100%|██████████| 8/8 [00:00<00:00, 16.54it/s]
2025-01-22 03:13:01.787 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.18045520782470703, 0.2826600374864453, 0.22241167340959822, 0.28299300785276366, 0.3339040892464774], 'topk_tokens': [' Democracy', ' Kl', 'ings', ' Daily', ' compl', ' whistle', 'MIN', ' emb', 'ob', ' communication', ' comparison', ' whistle', ' Merch', ' communication', ' Do', 'remark', 'deal', ' STE', 'Dou', 'consider'], 'evidence_proportions': [0.18693008422851562, 0.18011474609375, 0.23561477661132812, 0.11771273612976074]}, 'weight': {'score': [0.01720223301335385, 0.002567309132866407, 0.015863473074776785, 0.0025061969839394755, 0.0009144233805792672], 'topk_tokens': [' battle', ' Third', ' Mary', ' Daniel', ' Fort', ' Bench', ' Bridge', '<|eot_id|>', ' Wright', ' Bridge', ':', 'assistant', 'b', '<|eot_id|>', 'Bridge', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003571438789367676, 0.02485513687133789, 0.019008517265319824, 0.02095508575439453]}, 'saliency': {'score': [0.0016114256883922376, 4.877473350164006e-05, 0.001266857555934361, 4.282954098802999e-05, 1.71593257359096e-05], 'topk_tokens': [' Third', '<|eot_id|>', ' battle', ':', ' kitchen', ' Paul', ' Fort', ' Anthony', ' Daniel', '<|begin_of_text|>', ' bedroom', 'b', '<|start_header_id|>', ' Mary', ' Bridge', ' Daniel', ' Bridge', ' Bench', 'athroom', 'Bridge'], 'evidence_proportions': [0.00019504427909851075, 0.004011953870455424, 0.0008344650268554688, 0.0005580708384513855]}}, 25: {'grad': {'score': [0.4416744834498355, 0.43862825376974385, 0.34586900983537944, 0.43889037421148497, 0.4234910147530692], 'topk_tokens': ['\n', ' morning', ' for', ' M', ' a', ' of', ' inverted', ' Aw', ' set', ' the', ' for', ' black', ' a', ' a', 'ivery', '�', ' at', ' l', ' no', ' Aw'], 'evidence_proportions': [0.47506103515625, 0.4032325744628906, 0.6020050048828125, 0.2972736358642578]}, 'weight': {'score': [0.012228099923384817, 0.0024948113092824746, 0.016040500572749545, 0.0024406369066610777, 0.001254894052233015], 'topk_tokens': ['.', ' Mary', ' Paul', ' apple', 'Answer', ',', ' \n', 'b', ' Daniel', '<|eot_id|>', ' Anthony', 'assistant', ' Bench', ':', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007115542888641357, 0.01866176227728526, 0.01788151264190674, 0.011319875717163086]}, 'saliency': {'score': [0.00081331635776319, 4.013362097870904e-05, 0.0011892378330230712, 3.5619924355572356e-05, 3.11319317136492e-05], 'topk_tokens': [' Geo', ' Paul', ',', ' Ramsey', 'athroom', ' Ramsey', '<|eot_id|>', '<|start_header_id|>', ' Third', ' Mary', '<|eot_id|>', ' Bench', ' Paul', ' Daniel', ' Ramsey', '<|end_header_id|>', ' apple', '\n\n', '<|begin_of_text|>', ' Anthony'], 'evidence_proportions': [2.773404121398926e-05, 0.0018084545930226645, 0.0006766170263290405, 0.0004392862319946289]}}, 26: {'grad': {'score': [0.5208820543791118, 0.5644938051139619, 0.5448736327035086, 0.5646183697911775, 0.5836926868983677], 'topk_tokens': ['ian', ' satisf', 'op', 'Johnson', ' executed', ' PA', ' vastly', ' unab', 'RI', 'field', ' I', ' favor', ' favorable', 'b', ' Press', ' compl', 'b', 'issippi', ' bitter', 'itter'], 'evidence_proportions': [0.536474609375, 0.46306355794270837, 0.500946044921875, 0.6080551147460938]}, 'weight': {'score': [0.009701720978084364, 0.002461287147802323, 0.017877726895468574, 0.002405623881551301, 0.0010027859892163958], 'topk_tokens': ['Bridge', ' apple', ' Bench', ' Anthony', ' apple', ' Bridge', '<|eot_id|>', ' Daniel', '<|eot_id|>', ' \n', 'Answer', ' the', 'assistant', 'b', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0023668229579925535, 0.014141778151194256, 0.014909982681274414, 0.007001996040344238]}, 'saliency': {'score': [0.0004729754046389931, 5.083253520416427e-05, 0.001706951005118234, 4.540837893895636e-05, 2.791838986533029e-05], 'topk_tokens': [',', ' Ramsey', ' Merch', ' Daniel', '.', ' Shak', ' Bridge', ' Mary', 'assistant', '<|start_header_id|>', ' apple', ' Daniel', 'athroom', ' Anthony', ':', '<|end_header_id|>', ' Daniel', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [6.93202018737793e-06, 0.0011090983947118125, 0.0003981366753578186, 0.00017618387937545776]}}, 27: {'grad': {'score': [0.2622263055098684, 0.3313270804241857, 0.22692135402134486, 0.33173539239685085, 0.3296104976109096], 'topk_tokens': ['ottle', '-n', ' designated', ' be', ' states', ' beautiful', ' would', ' convention', ' ideas', ' state', ' Bre', ' Marshall', 'est', ' sleep', ' was', 'ly', ' business', ' step', ' accepted', ' Bottle'], 'evidence_proportions': [0.2466094970703125, 0.23551177978515625, 0.2802581787109375, 0.3037872314453125]}, 'weight': {'score': [0.012921858774988275, 0.00253253424457388, 0.014297913653509957, 0.002482457348117601, 0.0012187191418239049], 'topk_tokens': [' Mary', ' Daniel', ' garden', 'CH', ' Anthony', ' apple', ' \n', ' apple', 'Answer', ' bedroom', 'assistant', ' bedroom', 'b', '.\n\n', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005519992113113404, 0.02120267351468404, 0.016166120767593384, 0.0065087080001831055]}, 'saliency': {'score': [0.0008114811621214214, 4.783212011755464e-05, 0.0014353956495012556, 4.264724092918612e-05, 5.172491073608398e-05], 'topk_tokens': ['assistant', '\n\n', 'Bridge', ' \n', '<|end_header_id|>', ' Anthony', ' Daniel', 'THE', ' Bridge', ' the', ' bedroom', ' Daniel', 'THE', ' apple', ':', '<|begin_of_text|>', 'b', '<|start_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [4.332661628723145e-05, 0.001467605431874593, 0.0011818557977676392, 0.0004171133041381836]}}, 28: {'grad': {'score': [0.274355035079153, 0.3185631751780015, 0.2810502188546317, 0.3187401507746473, 0.31238620621817453], 'topk_tokens': ['nes', ' RID', 'ew', ' became', '<|end_header_id|>', 'about', ' in', ' a', ' before', ' returns', 'arp', 'ot', ' balance', ' half', '.', 'half', 'dent', ' inside', ' half', 'ball'], 'evidence_proportions': [0.24812774658203127, 0.3272806803385417, 0.20644378662109375, 0.29566192626953125]}, 'weight': {'score': [0.014352155359167802, 0.0024136565829935065, 0.013194457973752703, 0.002363992832444893, 0.0009748075689588274], 'topk_tokens': ['.\n\n', 'Answer', '<|eot_id|>', '<|eot_id|>', ' \n', ' bedroom', ' Bridge', ' the', '?', 'assistant', ' apple', ' the', ' before', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0012255549430847167, 0.02190883954366048, 0.021290123462677002, 0.012487411499023438]}, 'saliency': {'score': [0.0001902407721469277, 4.5861330269616e-05, 0.0003756880760192871, 4.4686883455238075e-05, 1.5809280531747e-05], 'topk_tokens': ['.\n\n', '?', ' During', ' Third', ' garden', ' the', ' before', 'assistant', ' apple', ' the', ' Bridge', 'Bridge', 'b', '<|end_header_id|>', 'athroom', ' Bridge', '<|begin_of_text|>', '<|start_header_id|>', '\n\n', ':'], 'evidence_proportions': [0.00012109875679016114, 0.00018392999966939289, 0.00023439526557922363, 0.0002419799566268921]}}, 29: {'grad': {'score': [0.29669309917249176, 0.39213821632166096, 0.35799377986363, 0.39238552519974274, 0.4139230183192662], 'topk_tokens': ['Spring', '\n', ' The', '\n', ' Paul', ' THE', ' com', ' M', 'y', 'ION', 's', 'y', ' ga', ',\n', 'b', 're', ' l', 'ION', ' a', ' com'], 'evidence_proportions': [0.2537384033203125, 0.2984631856282552, 0.4221153259277344, 0.22230911254882812]}, 'weight': {'score': [0.012950563117077476, 0.0024631901257205335, 0.00885594061442784, 0.0024284177168474932, 0.0011587245123726982], 'topk_tokens': [' the', ' the', ' Does', ' bedroom', ' apple', 'Answer', ' the', ' \n', '?', '<|eot_id|>', ' before', '.\n\n', 'assistant', 'b', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0002512097358703613, 0.010608300566673279, 0.0421692430973053, 0.0031194686889648438]}, 'saliency': {'score': [0.00048015933287771126, 3.5744123523642536e-05, 0.0002673804759979248, 3.438356772304022e-05, 7.800502436501639e-05], 'topk_tokens': ['�', ' Mary', '      ', ' the', ',', ' the', 'athroom', ' the', ' before', 'Does', '<|eot_id|>', ':', 'Answer', 'assistant', ' Does', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [8.004903793334962e-06, 0.0006984819968541464, 0.0011031031608581543, 0.00011992454528808594]}}, 30: {'grad': {'score': [0.3304242585834704, 0.3408641670015383, 0.3603419712611607, 0.3408244329374904, 0.2973843165806362], 'topk_tokens': [' an', ' an', 'ire', '      ', ' Times', 'SSION', ' soon', ' of', 'CO', ' the', ' Europe', 'b', 'b', '2', ' forb', ' its', ' LINE', ' account', 'deal', 'b'], 'evidence_proportions': [0.3037841796875, 0.33434041341145837, 0.39892578125, 0.2893486022949219]}, 'weight': {'score': [0.016703083326942043, 0.0024592402484961543, 0.016467643635613576, 0.0023966897234620746, 0.0037967119898114888], 'topk_tokens': ['IR', ' the', '<|eot_id|>', ' bedroom', ' Anthony', '.', '<|eot_id|>', '?', ' bedroom', 'assistant', 'Answer', '.\n\n', ' \n', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002186405658721924, 0.02630583941936493, 0.027065634727478027, 0.010082244873046875]}, 'saliency': {'score': [0.0010688524497182745, 8.664634869896514e-05, 0.0006330966949462891, 8.354008663254317e-05, 6.263256072998047e-05], 'topk_tokens': [' the', ' garden', ' apple', 'Bridge', ' the', ' midnight', '<|begin_of_text|>', 'assistant', ' bedroom', ' Anthony', ' Bridge', ' Bench', ' bedroom', '<|end_header_id|>', ' Bridge', ' the', '<|start_header_id|>', ':', 'b', 'athroom'], 'evidence_proportions': [0.00022675395011901857, 0.0017123023668924966, 0.002086549997329712, 0.00013860315084457397]}}, 31: {'grad': {'score': [0.2775088862368935, 0.30353674704026196, 0.2888159956250872, 0.3036197520262706, 0.17915568947792054], 'topk_tokens': [' the', ' the', ' could', ' evening', ' the', ' he', ' the', ' the', ' the', ' August', ' the', ' the', ' department', ' location', ' the', ' population', ' the', ' location', 'membership', ' the'], 'evidence_proportions': [0.30392112731933596, 0.18320337931315103, 0.3156299591064453, 0.34783077239990234]}, 'weight': {'score': [0.005759689368699726, 0.002288507570347426, 0.004116832358496529, 0.0022778257929734914, 0.001481031094278608], 'topk_tokens': [':', ' the', ' Where', ' before', '<|eot_id|>', '?', '.\n\n', ' the', 'Answer', ' bedroom', ' \n', 'b', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004644215106964111, 0.008691559235254925, 0.0046234130859375, 0.0091172456741333]}, 'saliency': {'score': [9.888881131222373e-05, 1.8789426464234097e-05, 0.00015528202056884765, 1.8271618892859394e-05, 2.148364271436419e-05], 'topk_tokens': [' Daniel', ' S', '\n\n', 'Question', ' \n', '<|eot_id|>', ' Market', ' Daniel', 'Answer', ' Daniel', ' apple', 'b', ' the', '<|begin_of_text|>', ':', ' bedroom', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [1.6993284225463865e-05, 0.00012959539890289307, 4.6603381633758545e-05, 0.00020748376846313477]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:13:01.789 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:13:01.789 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-1_0-6-7-8.pkl | len: 10 |  size: 9.13 KB
Processing depth (0, 6, 7, 8):   2%|▏         | 2/100 [00:38<31:10, 19.09s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.14it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (0, 2, 3, 4):   2%|▏         | 2/100 [00:45<31:10, 19.09s/it]2025-01-22 03:13:09.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:13:09.151 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:13:09.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:13:09.159 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2457, 2463) --> . Mary journeyed to the
2025-01-22 03:13:09.159 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:13:09.169 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3794, 3798) -->  Mary left the apple
2025-01-22 03:13:09.169 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football.
2025-01-22 03:13:09.182 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4882, 4886) -->  Daniel dropped the football
2025-01-22 03:13:09.183 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:13:09.205 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8274, 8279) --> . Daniel picked up the
2025-01-22 03:13:09.205 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John moved to the garden.
2025-01-22 03:13:09.220 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5246, 5251) --> . John moved to the
2025-01-22 03:13:09.220 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:13:09.249 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (10032, 10038) --> . John went back to the
2025-01-22 03:13:09.249 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:13:09.279 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10892, 10896) -->  took the football.
2025-01-22 03:13:09.279 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:13:09.292 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4616, 4621) --> . Sandra moved to the
2025-01-22 03:13:09.292 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:13:09.310 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (6840, 6844) -->  Daniel left the apple
2025-01-22 03:13:09.310 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 03:13:09.338 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (9801, 9807) -->  Sandra journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:13:11.417 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:13:11.418 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12222])
your chose emoji: ['👆🏽', '🇹🇹', '🎬', '👈🏼', '👩🏻\u200d🤝\u200d👨🏼', '👚', '🚶🏻\u200d♂️\u200d➡', '🦹🏿\u200d♀', '🧑\u200d🦼\u200d➡️', '🥌']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 189573.06it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.22s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.02s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.65it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.50it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.37it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.94it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.56it/s][A100%|██████████| 8/8 [00:00<00:00, 17.63it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.86it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.42it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.34it/s][A100%|██████████| 8/8 [00:00<00:00, 17.32it/s]
2025-01-22 03:13:20.798 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.30842018127441406, 0.38877481347456544, 0.26812155587332587, 0.38924721525156375, 0.37612335305464895], 'topk_tokens': ['Bridge', ' combined', ' broad', ' compl', ' RID', ' out', ' proceedings', ' Merch', ' Do', 'G', ' whistle', 'vent', ' whistle', 'combination', ' agreement', ' It', ' emb', 'ob', ' Arr', 'consider'], 'evidence_proportions': [0.3165985107421875, 0.39909394582112634, 0.2118377685546875, 0.25876903533935547]}, 'weight': {'score': [0.02841967501138386, 0.002576607078130991, 0.019095316103526525, 0.002488761124091007, 0.0007277621250403555], 'topk_tokens': [' the', 'Answer', ' Bridge', ' the', ' landing', ' kitchen', ' Bridge', '<|eot_id|>', ' bathroom', 'b', ':', '<|start_header_id|>', ' boat', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'Bridge', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03867053985595703, 0.01667636136213938, 0.05923604965209961, 0.0024046897888183594]}, 'saliency': {'score': [0.0028259503213982832, 7.844959299988542e-05, 0.0009606301784515381, 7.162362682369797e-05, 1.9922460380353425e-05], 'topk_tokens': ['Answer', '<|eot_id|>', ' garden', ' Wright', ':', ' Mary', 'b', ' football', ' bedroom', ' office', ' boat', '<|eot_id|>', ' Mary', ' kitchen', ' bathroom', 'athroom', ' Bench', ' Bridge', ' Bridge', 'Bridge'], 'evidence_proportions': [0.004787987470626831, 0.001243611176808675, 0.005429580807685852, 0.00014328211545944214]}}, 25: {'grad': {'score': [0.5707542017886513, 0.4996388316398262, 0.468435423714774, 0.49961754557003313, 0.6667460391395971], 'topk_tokens': [' no', '      ', 'Super', '�', '      ', ' a', '      ', ' the', 'public', ' at', '      ', ' aw', 'ivery', ' set', 'L', ' sund', 'sur', ' of', ' Aw', ' Aw'], 'evidence_proportions': [0.4882568359375, 0.5227762858072917, 0.7887191772460938, 0.5278778076171875]}, 'weight': {'score': [0.018204168269508762, 0.0024989325868571464, 0.010796294042042324, 0.0024505547108484484, 0.0010106492983667475], 'topk_tokens': [' Daniel', ' Mary', ' boat', '.\n\n', 'b', 'Answer', '.', '.', ' apple', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ' Bench', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.010107612609863282, 0.007991239428520203, 0.061342716217041016, 0.0005057081580162048]}, 'saliency': {'score': [0.001955920144131309, 4.436738651954323e-05, 0.0006031726087842668, 3.977633523584519e-05, 3.425227968316329e-05], 'topk_tokens': ['Den', ' PA', 'b', ' Bottle', 'Answer', ':', ' East', ' Dan', 'assistant', ' boat', '.', ' apple', '<|eot_id|>', '<|eot_id|>', 'athroom', ' Bench', '\n\n', ' apple', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000576174259185791, 0.00011645754178365071, 0.008367195725440979, 2.8520822525024414e-05]}}, 26: {'grad': {'score': [0.6300273694490132, 0.6361397622699386, 0.5959785461425782, 0.6362647953837383, 0.6476680354068154], 'topk_tokens': [',', ' loud', ' several', ' unab', ' favorable', ' sop', 'pro', "'t", 'ier', ' Met', ' considerable', 'op', 'b', ' compl', ' Press', ' favor', ' vastly', 'ub', 'itter', ' bitter'], 'evidence_proportions': [0.5398468017578125, 0.7210769653320312, 0.65509033203125, 0.58111572265625]}, 'weight': {'score': [0.026018483074087845, 0.002467893666772511, 0.01280778305871146, 0.0024013948312243344, 0.0005690820123019971], 'topk_tokens': [' the', ' apple', ' bedroom', ' apple', ' barric', '<|eot_id|>', 'Bridge', '<|eot_id|>', 'b', 'Answer', 'assistant', ' \n', ' bathroom', ' kitchen', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.05113934874534607, 0.0065352171659469604, 0.04970264434814453, 0.00015813857316970825]}, 'saliency': {'score': [0.0006671375349948281, 5.234374346664834e-05, 0.0005310816424233573, 5.0007295475314826e-05, 1.3648287246101781e-05], 'topk_tokens': [' lower', ' Sandra', ' apple', ' Merch', 'assistant', ' leve', ' the', ' Bridge', ' Jackson', ' garden', ' Bridge', 'athroom', 'Bridge', ' kitchen', '<|start_header_id|>', '<|end_header_id|>', ':', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00026447772979736327, 0.0006954322258631389, 0.0017892718315124512, 5.885958671569824e-06]}}, 27: {'grad': {'score': [0.28413712350945725, 0.39902453588317993, 0.31692112513950893, 0.39943998902681066, 0.4420236537331029], 'topk_tokens': [' be', 'state', '-n', 'Square', 'ARCH', ' staff', ' platform', 'state', 'ides', 'arch', ' designated', ' sentinel', ' Marshall', ' states', ' state', '\n', ' Bottle', ' business', ' step', ' accepted'], 'evidence_proportions': [0.2930816650390625, 0.27675374348958337, 0.33099365234375, 0.23717498779296875]}, 'weight': {'score': [0.03839107720475448, 0.0025285947834787193, 0.012174166100365775, 0.0024448726437946108, 0.0009587348291748448], 'topk_tokens': [' the', ' barric', ' apple', ' \n', ' garden', ' boat', ' bedroom', ' kitchen', 'Answer', ' apple', 'assistant', 'b', '.\n\n', ' bathroom', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0689165234565735, 0.015356759230295816, 0.07266807556152344, 0.0005087479948997498]}, 'saliency': {'score': [0.003987803270942287, 5.3340947457374e-05, 0.001098825250353132, 4.419241941961499e-05, 3.6594114805522716e-05], 'topk_tokens': [' apple', ' Min', ' Dan', ' bathroom', 'assistant', 'NEW', ' \n', ' bedroom', '<|start_header_id|>', ' the', '<|begin_of_text|>', 'b', ' kitchen', ' apple', ' the', '<|end_header_id|>', ' the', ':', '.\n\n', 'athroom'], 'evidence_proportions': [0.0013088703155517576, 0.005410254001617432, 0.00917430967092514, 1.6286969184875488e-05]}}, 28: {'grad': {'score': [0.44897300318667765, 0.3889841353527607, 0.3908416748046875, 0.3888851457570281, 0.2896933304636102], 'topk_tokens': [' over', ' as', ' summer', ' summer', '600', ' became', ' before', ' half', ' summer', ' summer', ' platform', ' spring', ' shore', ' returns', ' balance', 'half', ' inside', '600', ' half', 'ball'], 'evidence_proportions': [0.5445144653320313, 0.36122767130533856, 0.45721435546875, 0.4529228210449219]}, 'weight': {'score': [0.011576120790682341, 0.0023907835010614376, 0.012835124560764858, 0.002346409715374771, 0.0005527138710021973], 'topk_tokens': [' the', ' the', '.\n\n', 'Answer', '<|eot_id|>', ' the', ' bedroom', '<|eot_id|>', ' \n', '?', ' before', 'assistant', ' apple', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006693041324615479, 0.004917671283086141, 0.038813114166259766, 0.0004306510090827942]}, 'saliency': {'score': [0.0005058050155639648, 4.223313799665018e-05, 0.00040268727711268836, 4.047290789700017e-05, 1.7665326595306396e-05], 'topk_tokens': ['?', ' St', '.\n\n', ' apple', ' the', ' the', 'Bridge', ' kitchen', 'b', ' before', ' Bridge', ' apple', ' Bridge', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '\n\n', ':'], 'evidence_proportions': [0.0007313013076782227, 9.896357854207358e-05, 0.0013098269701004028, 3.0174851417541504e-05]}}, 29: {'grad': {'score': [0.6851063778525904, 0.7418080984790388, 0.6895597321646554, 0.7420468648509808, 0.5168460042853105], 'topk_tokens': ['itter', ' The', ' STR', 'y', ' com', ' The', 'ioneer', ' situation', ' spring', ' Pioneer', ' THE', ' I', 'b', 'ian', 'ing', 'Spring', 'y', 'ION', 'ER', 'y'], 'evidence_proportions': [0.6920684814453124, 0.7294387817382812, 0.8550662994384766, 0.4399452209472656]}, 'weight': {'score': [0.007802180553737439, 0.0024533119279670323, 0.00977504849433899, 0.002423906843445411, 0.0007765724470740871], 'topk_tokens': [' Where', '.', '<|eot_id|>', ' was', ' the', ' before', ' \n', ' Does', '?', '.\n\n', 'Answer', ' the', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00096358060836792, 0.010547439257303875, 0.019869625568389893, 0.0001650974154472351]}, 'saliency': {'score': [0.000602000638058311, 4.8764649100586434e-05, 0.0003967157432011196, 4.69004003056053e-05, 4.437879512184545e-05], 'topk_tokens': ['.', '.', 'us', ':', ' the', 'Answer', '<|eot_id|>', ' was', 'assistant', '      ', ' before', '<|start_header_id|>', '      ', ' the', '<|end_header_id|>', 'athroom', ' Does', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.8318166732788084e-05, 0.000552902619043986, 0.0019903630018234253, 4.388391971588135e-06]}}, 30: {'grad': {'score': [0.3756561279296875, 0.3767733845779013, 0.36541235787527904, 0.3768077994830782, 0.3240341261813515], 'topk_tokens': [' hall', ' his', ' soon', 'ARCH', 'SSION', ' the', ' the', ' Europe', ' LINE', ' of', '2', ' Burb', ' account', ' B', ' B', ' forb', 'deal', 'b', 'b', 'b'], 'evidence_proportions': [0.32334594726562504, 0.2409515380859375, 0.49139404296875, 0.5273628234863281]}, 'weight': {'score': [0.016863369628002767, 0.0024428885992319304, 0.018281154121671403, 0.00237483104990714, 0.0024290386783449272], 'topk_tokens': [' Where', ' the', '.', 'Question', '<|eot_id|>', ' barric', '<|eot_id|>', '?', ' bedroom', '.\n\n', 'assistant', 'Answer', 'b', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.011447799205780028, 0.00983332097530365, 0.05035591125488281, 0.0006853640079498291]}, 'saliency': {'score': [0.0011983090325405723, 9.468434778459233e-05, 0.0009603823934282575, 9.047201514077573e-05, 5.8370201211226614e-05], 'topk_tokens': [' the', ' the', '.\n\n', ' office', ' apple', '.', ' bathroom', 'assistant', ' Bench', 'Bridge', ' garden', ' Bridge', ' bedroom', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', ':', 'athroom', 'b'], 'evidence_proportions': [0.001691657304763794, 0.000560830036799113, 0.0026915743947029114, 4.4576823711395264e-05]}}, 31: {'grad': {'score': [0.2915123136419999, 0.3576096654474613, 0.3291064943586077, 0.3577948154493029, 0.18685351233733327], 'topk_tokens': [' their', '      ', ' they', 'If', ' location', ' location', ' had', ' August', ' THE', 'membership', ' location', ' the', ' the', ' the', ' was', ' the', ' location', ' he', ' the', ' the'], 'evidence_proportions': [0.31490669250488285, 0.2925420602162679, 0.27898311614990234, 0.2732539176940918]}, 'weight': {'score': [0.003038106780303152, 0.0022600824555005033, 0.004754679543631417, 0.0022516942080059808, 0.0010007576722847788], 'topk_tokens': [':', ' before', ' the', '.\n\n', '<|eot_id|>', ' Where', '?', 'Answer', ' the', ' bedroom', ' \n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011251866817474365, 0.0027892738580703735, 0.008432269096374512, 0.00040834397077560425]}, 'saliency': {'score': [8.320337847659462e-05, 2.256229611262222e-05, 7.943851607186453e-05, 2.230407178730064e-05, 1.2444430275967247e-05], 'topk_tokens': ['Bridge', 'light', ' before', ' the', ' Market', 'CH', ' Bridge', '<|eot_id|>', 'Answer', 'Question', ' \n', ' apple', ' the', '<|begin_of_text|>', ' bedroom', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [2.7346611022949216e-05, 4.8259894053141274e-05, 0.00028536468744277954, 3.2782554626464844e-06]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:13:20.801 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:13:20.801 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-2_0-2-3-4.pkl | len: 10 |  size: 9.11 KB
Processing depth (0, 2, 3, 4):   3%|▎         | 3/100 [00:57<30:48, 19.05s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.17it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (2, 5, 6, 9):   3%|▎         | 3/100 [01:05<30:48, 19.05s/it]2025-01-22 03:13:28.702 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:13:28.709 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2487, 2492) --> . Mary moved to the
2025-01-22 03:13:28.709 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:13:28.727 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5965, 5971) --> . Mary journeyed to the
2025-01-22 03:13:28.727 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:13:28.747 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7191, 7195) -->  Mary left the apple
2025-01-22 03:13:28.747 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football.
2025-01-22 03:13:28.776 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10774, 10778) -->  Daniel dropped the football
2025-01-22 03:13:28.776 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:13:28.799 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8283, 8288) --> . Daniel picked up the
2025-01-22 03:13:28.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John moved to the garden.
2025-01-22 03:13:28.814 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5263, 5268) --> . John moved to the
2025-01-22 03:13:28.815 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:13:28.844 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (10077, 10083) --> . John went back to the
2025-01-22 03:13:28.844 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:13:28.874 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10908, 10912) -->  Daniel took the football
2025-01-22 03:13:28.875 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:13:28.888 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4681, 4686) --> . Sandra moved to the
2025-01-22 03:13:28.888 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:13:28.907 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (6951, 6955) -->  Daniel left the apple
2025-01-22 03:13:28.907 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 03:13:28.936 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (9822, 9828) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:13:31.101 | INFO     | test_jbb_retain:begin_test:632 - The apple was in the kitchen.<|eot_id|>
2025-01-22 03:13:31.102 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12212])
your chose emoji: ['Ⓜ️', '\U0001f7f0', '🤿', '👩🏻\u200d💻', '👮🏿\u200d♂️', '🧑🏾\u200d⚕', '🧍🏻', '🎎', '🌗', '👩🏽\u200d❤\u200d👨🏾']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 222214.78it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.52s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.47s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.68it/s][A100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.45it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.50it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.51it/s][A100%|██████████| 8/8 [00:00<00:00, 20.67it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.41it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.00it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.28it/s][A100%|██████████| 8/8 [00:00<00:00, 20.29it/s]
2025-01-22 03:13:40.599 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.22359175431100944, 0.38645152444484243, 0.24596050807407924, 0.38711031247259664, 0.32273710135257605], 'topk_tokens': [' out', ' of', ' Do', ' appearance', ' out', ' the', ' STE', 'remark', ' absence', ' disposed', ' entrance', ' communication', ' appearance', ' emb', ' compl', ' agreement', ' item', ' communication', ' comparison', 'consider'], 'evidence_proportions': [0.2764190673828125, 0.24884637196858725, 0.1743927001953125, 0.16887474060058594]}, 'weight': {'score': [0.0391598735985003, 0.002574392717683896, 0.02043937955583845, 0.0024658162292313904, 0.0017372013041467378], 'topk_tokens': [' office', ' Daniel', ' bedroom', 'Answer', ' apple', ' the', 'Bridge', ':', '<|eot_id|>', 'b', 'assistant', ' bathroom', '<|eot_id|>', ' bedroom', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004216331243515015, 0.024194916089375813, 0.12505149841308594, 0.019395112991333008]}, 'saliency': {'score': [0.0012456994307668584, 3.903571133529657e-05, 0.0004794367722102574, 3.588295680854521e-05, 6.583287860407974e-05], 'topk_tokens': ['<|end_header_id|>', 'assistant', 'position', '\n\n', '.', ' Bench', ' Mary', ':', ' kitchen', '<|eot_id|>', ' apple', ' office', ' apple', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', ' bathroom', 'Bridge', 'athroom', ' bedroom'], 'evidence_proportions': [0.0005502283573150634, 0.0011601199706395466, 0.0031686201691627502, 0.00032048672437667847]}}, 25: {'grad': {'score': [0.3730940065885845, 0.43581647900250714, 0.33473597935267857, 0.43620538982099316, 0.4485604257294626], 'topk_tokens': [' the', ' the', ' a', ' aw', ' old', ' at', ' the', ' for', ' Aw', ' the', ' for', ' a', ' l', ' for', 'ivery', ' of', ' a', ' no', '�', ' Aw'], 'evidence_proportions': [0.2819549560546875, 0.4278450012207031, 0.5182647705078125, 0.2597205638885498]}, 'weight': {'score': [0.0194373115112907, 0.0025037690551857747, 0.01777371253286089, 0.002433364867340645, 0.0021986116965611777], 'topk_tokens': [' Mary', ' the', ' \n', ',', ' Paul', ' apple', ' Anthony', 'b', 'Answer', ' Bench', ':', '<|eot_id|>', ' Daniel', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0034779250621795655, 0.016187687714894615, 0.052297115325927734, 0.011401176452636719]}, 'saliency': {'score': [0.0018537766055056924, 3.6325534737798165e-05, 0.0010491890566689628, 3.057092626709833e-05, 4.442803787462639e-05], 'topk_tokens': [' Geo', ' bathroom', '.', ' May', '<|eot_id|>', ' Ramsey', ' Ramsey', ' Anthony', ' Daniel', ' Mary', ' Mary', '<|eot_id|>', ' Bench', 'athroom', ' Paul', ' apple', ' apple', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.000502091646194458, 0.0014451990524927774, 0.005538605153560638, 0.0004714205861091614]}}, 26: {'grad': {'score': [0.45435273019891037, 0.46970593596500204, 0.43498447963169645, 0.46982985356069495, 0.49607334714947326], 'topk_tokens': ['b', ' considerable', 'CE', ' Eagle', ' far', ' favorable', ' and', 'CE', 'char', 'graph', ' and', ' the', 'ub', ' Met', 'b', ' Press', ' favor', 'issippi', ' bitter', 'itter'], 'evidence_proportions': [0.62274169921875, 0.3525441487630208, 0.32747936248779297, 0.5234527587890625]}, 'weight': {'score': [0.020171047825562328, 0.0024719338216012588, 0.020079981429236275, 0.0023936043394581384, 0.001259846669254881], 'topk_tokens': ['?', ' apple', ' bedroom', ' apple', '<|eot_id|>', ' bedroom', '<|eot_id|>', ' apple', ' \n', 'Answer', ' bathroom', 'assistant', ' the', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007732927799224853, 0.009778082370758057, 0.07008814811706543, 0.01009058952331543]}, 'saliency': {'score': [0.0005080950887579667, 4.401873527273896e-05, 0.0009818204811641148, 4.059463266420204e-05, 3.4334533142321035e-05], 'topk_tokens': [' the', ' Mary', '?', 'assistant', ' kitchen', ' Anthony', ' Daniel', ' apple', ' the', '<|end_header_id|>', ' bathroom', ' \n', ' Daniel', ' bedroom', '<|start_header_id|>', 'athroom', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0001177668571472168, 0.0006902466217676798, 0.0010563358664512634, 0.0001745373010635376]}}, 27: {'grad': {'score': [0.4078855765493293, 0.41964157797789603, 0.4187495504106794, 0.4196625125220943, 0.4009260119813861], 'topk_tokens': [' method', 'state', ' assistance', ' department', ' state', 'ARCH', '-n', ' DAYS', ' started', 'successful', ' convention', 'LES', 'ers', 'ides', ' staff', ' other', ' received', ' designated', ' step', ' accepted'], 'evidence_proportions': [0.5213714599609375, 0.3327153523763021, 0.38024139404296875, 0.4064277410507202]}, 'weight': {'score': [0.026265687064120646, 0.002533043148182866, 0.017183343853269306, 0.002453799602497409, 0.0021764152880870934], 'topk_tokens': [' Mary', ' apple', ' apple', ' apple', ' \n', ' Daniel', 'Answer', ' the', ' bedroom', 'assistant', 'b', ' bedroom', '.\n\n', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003175032138824463, 0.02241039276123047, 0.07940566539764404, 0.007771968841552734]}, 'saliency': {'score': [0.0029072526254151996, 4.928915460627927e-05, 0.0016416166509900774, 4.024115129086093e-05, 8.359415964646773e-05], 'topk_tokens': ['THE', ' the', 'THE', ' \n', ' apple', ' apple', 'assistant', ' the', ' the', ' apple', ' bedroom', '<|start_header_id|>', 'b', ' Daniel', '<|end_header_id|>', ' the', ':', '<|begin_of_text|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.00010718703269958496, 0.0023071070512135825, 0.009813375771045685, 0.00040142983198165894]}}, 28: {'grad': {'score': [0.4055412694027549, 0.37589409633391324, 0.38250432695661274, 0.37582875184249787, 0.29515169606064307], 'topk_tokens': [' platform', ' probably', ' probably', ' summer', ' spring', '.', ' shore', ' balance', ' summer', ' before', ' inside', ' prepared', ' before', ' proof', ' probably', ' half', ' half', ' returns', 'half', 'ball'], 'evidence_proportions': [0.5070343017578125, 0.31141535441080725, 0.48949432373046875, 0.3359107971191406]}, 'weight': {'score': [0.015710474629151195, 0.002403463519014787, 0.012051684515816825, 0.00235490493452497, 0.0010274061650940866], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' the', ' the', 'Answer', ' before', ' the', ' bedroom', '<|eot_id|>', ' \n', 'assistant', ' apple', '?', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00015395283699035645, 0.00533449649810791, 0.05212545394897461, 0.01430511474609375]}, 'saliency': {'score': [0.0003585674260791979, 3.317299066778582e-05, 0.00028263841356549943, 3.194662901379052e-05, 2.1307305856184527e-05], 'topk_tokens': [' Dul', ' the', '.\n\n', ' Far', ' before', '?', ' bedroom', ' the', 'Bridge', 'b', ' apple', ' Bridge', ' Bridge', 'assistant', 'athroom', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [1.1849403381347655e-05, 0.00018107394377390543, 0.0012517645955085754, 0.00016500800848007202]}}, 29: {'grad': {'score': [0.43818102384868424, 0.4516051821371009, 0.3682308741978237, 0.4518661113193519, 0.5027177984064276], 'topk_tokens': ['s', ' l', ' In', 'y', ' The', 'ER', 'ION', ' Pioneer', ' B', 'ION', ' B', ' An', 'y', ' Pioneer', ' The', ' THE', ' a', ' ga', 'b', ' M'], 'evidence_proportions': [0.5733123779296875, 0.42486063639322913, 0.4730949401855469, 0.25433349609375]}, 'weight': {'score': [0.009996798477674785, 0.0024651363839685455, 0.009237679413386754, 0.0024338773932761635, 0.0014106017170530376], 'topk_tokens': [' the', ' bedroom', '<|eot_id|>', ' before', ',', ' the', '<|eot_id|>', '.\n\n', ' the', 'Answer', ' \n', '?', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00023105740547180176, 0.005864103635152181, 0.035185813903808594, 0.0032140016555786133]}, 'saliency': {'score': [0.0005764914186377274, 4.3634027913609245e-05, 0.00040876269340515135, 4.175064712946631e-05, 6.709857420487837e-05], 'topk_tokens': ['.\n\n', 'Does', '<|eot_id|>', ' Mary', ' \n', ' the', ' the', ' Does', ' before', '<|eot_id|>', ':', ' the', 'Answer', 'assistant', ',', '<|end_header_id|>', 'b', '\n\n', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [1.494288444519043e-05, 0.0006591329971949259, 0.0015972703695297241, 0.00013368576765060425]}}, 30: {'grad': {'score': [0.5817333020662007, 0.4392923899278551, 0.5183953421456473, 0.43884218183162527, 0.4091199528087269], 'topk_tokens': [' soon', ' of', ' two', 'itter', 'B', '2', ' it', ' his', ' an', ' Buchanan', ' account', '3', 'b', 'deal', ' Burb', ' B', ' forb', ' B', 'b', 'b'], 'evidence_proportions': [0.39760742187500003, 0.6432647705078125, 0.8156661987304688, 0.4856605529785156]}, 'weight': {'score': [0.02481278933976826, 0.0024651482512216025, 0.02129418339048113, 0.0023760419761984573, 0.0053611614487387915], 'topk_tokens': [' apple', ' the', ' the', '.\n\n', ' the', ' the', ' bedroom', 'assistant', 'Answer', '?', 'b', '<|eot_id|>', '<|eot_id|>', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001997816562652588, 0.021840969721476238, 0.07037115097045898, 0.012230873107910156]}, 'saliency': {'score': [0.0011749251892692165, 8.137366150033508e-05, 0.0012276232242584228, 7.636616098852341e-05, 9.540536186911844e-05], 'topk_tokens': [' the', ' office', ' kitchen', ' apple', ' bathroom', ' Bridge', ' Daniel', ' Bridge', ' bedroom', ' garden', ' apple', ' Bench', ' the', ' bedroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'b', ':', 'athroom'], 'evidence_proportions': [6.1005353927612305e-05, 0.0007558266321818035, 0.00389263778924942, 0.00047826021909713745]}}, 31: {'grad': {'score': [0.3874191359469765, 0.5014965361861776, 0.4002241952078683, 0.5019662346105495, 0.30733030041058856], 'topk_tokens': [' the', ' the', ' the', ' the', ' the', ' had', ' the', ' their', ' the', ' the', ' the', ' August', ' was', ' the', ' the', ' the', ' the', ' the', ' he', ' the'], 'evidence_proportions': [0.4147956848144531, 0.3694098393122355, 0.34648633003234863, 0.4211452007293701]}, 'weight': {'score': [0.004474051688846789, 0.002268475311191854, 0.0031996190547943113, 0.0022623495006333857, 0.0013971500324480462], 'topk_tokens': [':', ' the', ' the', ' before', 'Question', '.\n\n', ' bedroom', '<|eot_id|>', '?', 'Answer', ' \n', 'assistant', 'b', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002300220727920532, 0.004151105880737305, 0.005780816078186035, 0.00636899471282959]}, 'saliency': {'score': [7.3663498225965e-05, 2.5040050339513364e-05, 5.310603550502232e-05, 2.488330706259244e-05, 1.7515186107519902e-05], 'topk_tokens': [' Anthony', ' Do', ' bedroom', ' was', ' the', ' Emily', ' the', ' \n', ' Market', ' apple', '<|eot_id|>', ' the', '<|begin_of_text|>', 'b', ' bedroom', '<|start_header_id|>', ':', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [5.365610122680664e-05, 7.175902525583902e-05, 0.00012107193470001221, 5.412101745605469e-05]}}, 'pred_res': 'The apple was in the kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:13:40.602 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:13:40.602 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-3_2-5-6-9.pkl | len: 10 |  size: 9.17 KB
Processing depth (2, 5, 6, 9):   4%|▍         | 4/100 [01:16<30:57, 19.35s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 4, 5, 6):   4%|▍         | 4/100 [01:24<30:57, 19.35s/it]2025-01-22 03:13:47.881 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:13:47.881 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:13:47.882 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:13:47.896 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4917, 4923) --> . Mary journeyed to the
2025-01-22 03:13:47.896 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:13:47.913 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5999, 6003) -->  Mary left the apple
2025-01-22 03:13:47.913 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football.
2025-01-22 03:13:47.932 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7227, 7231) -->  Daniel dropped the football
2025-01-22 03:13:47.933 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:13:47.955 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8249, 8254) --> . Daniel picked up the
2025-01-22 03:13:47.956 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John moved to the garden.
2025-01-22 03:13:47.970 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5284, 5289) --> . John moved to the
2025-01-22 03:13:47.971 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:13:48.000 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (10045, 10051) --> . John went back to the
2025-01-22 03:13:48.000 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:13:48.029 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10905, 10909) -->  took the football.
2025-01-22 03:13:48.029 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:13:48.043 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4695, 4700) --> . Sandra moved to the
2025-01-22 03:13:48.043 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:13:48.062 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (6957, 6961) -->  Daniel left the apple
2025-01-22 03:13:48.062 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 03:13:48.090 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (9813, 9819) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:13:50.178 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:13:50.179 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12241])
your chose emoji: ['🍢', '👩🏾\u200d🤝\u200d👩🏿', '👨🏿\u200d🍼', '👨🏻\u200d✈', '🇬🇦', '👩🏻\u200d❤️\u200d👨🏿', '🏋🏾\u200d♀', '👳🏼\u200d♀', '👩🏻\u200d🦰', '👱🏾\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 213722.50it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.16s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.02s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.03it/s][A100%|██████████| 8/8 [00:05<00:00,  1.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.85it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.77it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.54it/s][A100%|██████████| 8/8 [00:00<00:00, 21.90it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.60it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.37it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.32it/s][A100%|██████████| 8/8 [00:00<00:00, 21.54it/s]
2025-01-22 03:13:59.102 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.24826335906982422, 0.36006414294437583, 0.2690669468470982, 0.36049967344126027, 0.41212784616570725], 'topk_tokens': [' out', ' STE', ' compromised', ' Bridge', ' Bench', 'ob', ' congreg', 'vent', 'deal', 'Bridge', ' whistle', ' emb', ' out', 'remark', 'G', ' Do', 'combination', ' whistle', ' agreement', 'consider'], 'evidence_proportions': [0.3099395751953125, 0.33067830403645837, 0.1640608310699463, 0.13174819946289062]}, 'weight': {'score': [0.03400269150733948, 0.002572405692682824, 0.013290780782699584, 0.0024926420701537784, 0.0006880822934602436], 'topk_tokens': ['Answer', ' bedroom', ' kitchen', ' \n', ' Mary', ' Bench', '.', 'b', ' barric', '<|eot_id|>', ':', 'assistant', ' Bridge', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.016492557525634763, 0.034164940317471824, 0.047292232513427734, 0.042357444763183594]}, 'saliency': {'score': [0.003354077276430632, 6.890894248181019e-05, 0.00030080931527273996, 6.31226659114484e-05, 1.728158248098273e-05], 'topk_tokens': ['<|begin_of_text|>', ' office', ':', '<|eot_id|>', 'b', ' Wright', 'Mary', '<|start_header_id|>', '<|eot_id|>', ' Bridge', ' kitchen', ' Fort', ' bathroom', ' Mary', ' bedroom', ' Mary', ' Bench', 'athroom', ' Bridge', 'Bridge'], 'evidence_proportions': [0.002065598964691162, 0.005157108108202616, 0.004209265112876892, 0.0014049410820007324]}}, 25: {'grad': {'score': [0.5109208759508634, 0.4684184640499939, 0.45034332275390626, 0.468404114921138, 0.5790857415450247], 'topk_tokens': [' aw', ' at', ' at', ' a', 'ville', ' for', ' a', ' l', ' favored', ' the', ' for', 'sur', 'ivery', ' set', ' the', ' Aw', ' at', ' of', ' no', ' Aw'], 'evidence_proportions': [0.514300537109375, 0.4876441955566406, 0.6871070861816406, 0.36542510986328125]}, 'weight': {'score': [0.023167072158110768, 0.0024868400608624793, 0.009337652581078666, 0.0024349366278800934, 0.0007179715131458484], 'topk_tokens': ['.', ' barric', ' Daniel', '.\n\n', 'b', ' Mary', 'Answer', ' \n', ' apple', '<|eot_id|>', '.', 'assistant', ' Bench', '<|eot_id|>', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004051613807678223, 0.020992303888003033, 0.04967927932739258, 0.02381134033203125]}, 'saliency': {'score': [0.0026423570356870953, 4.045918890558322e-05, 0.00034166829926627023, 3.553889539028603e-05, 2.3735196966873972e-05], 'topk_tokens': ['Print', 'b', ' Min', ' Ramsey', ' Anthony', ' barric', 'assistant', ' apple', ' Dan', '.', ' Geo', ' Mary', '<|eot_id|>', '<|eot_id|>', 'athroom', ' Bench', '<|begin_of_text|>', '<|end_header_id|>', ' apple', '\n\n'], 'evidence_proportions': [0.0002667486667633057, 0.0017292499542236328, 0.008740216493606567, 0.0008836686611175537]}}, 26: {'grad': {'score': [0.5138876061690482, 0.5682449016801596, 0.5510574340820312, 0.5683789746892364, 0.6270203841359991], 'topk_tokens': [' far', ' compl', '�', 'b', ' Jackson', 'ier', ' Anthony', 'graph', ' Marshall', ' Met', 'single', 'graph', 'rich', 'Johnson', ' favor', 'ub', ' Press', 'issippi', ' bitter', 'itter'], 'evidence_proportions': [0.47377929687500003, 0.5050598780314128, 0.5135498046875, 0.5776023864746094]}, 'weight': {'score': [0.022609517762535496, 0.0024657696534200104, 0.009741854667663575, 0.0024134813687955873, 0.0006201053920545076], 'topk_tokens': [' Bench', ' apple', ' apple', '<|eot_id|>', '?', '<|eot_id|>', 'Bridge', ' the', 'Answer', 'b', ' \n', 'assistant', ' kitchen', ' barric', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.015525108575820923, 0.012475470701853434, 0.060944557189941406, 0.008331060409545898]}, 'saliency': {'score': [0.0008690467006281802, 4.3933564290156046e-05, 0.00047444616045270646, 4.141140756693113e-05, 1.773834228515625e-05], 'topk_tokens': [' Jackson', ' kitchen', ' \n', ' the', ' Floral', ' Mary', ' apple', ' garden', 'Bridge', ' the', ' Bridge', '?', ' barric', 'athroom', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [3.787875175476074e-05, 0.0008853822946548462, 0.002539604902267456, 0.00021294504404067993]}}, 27: {'grad': {'score': [0.27592347797594574, 0.38295502835572526, 0.29473348345075334, 0.38337515579862025, 0.40493519431666325], 'topk_tokens': ['\n', ' received', ' Marshall', ' short', ' platform', ' sleep', 'state', ' convention', 'successful', ' exchanged', ' staff', 'ides', ' business', ' designated', ' sentinel', ' states', ' state', ' Bottle', ' step', ' accepted'], 'evidence_proportions': [0.30767517089843754, 0.23680750528971353, 0.277435302734375, 0.29339599609375]}, 'weight': {'score': [0.03461014753893802, 0.0025281216030376328, 0.008021643332072666, 0.0024623437725783764, 0.0010968462417000218], 'topk_tokens': ['CH', ' the', ' Bridge', ' THE', 'THE', ' \n', ' apple', ' bathroom', 'Answer', ' garden', 'assistant', 'b', ' barric', '.\n\n', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02999218702316284, 0.030044967929522198, 0.06966710090637207, 0.01217341423034668]}, 'saliency': {'score': [0.0038092057955892464, 5.5154934598510854e-05, 0.000859978369304112, 4.699285202480125e-05, 4.126868749919691e-05], 'topk_tokens': ['THE', 'Bridge', ' bedroom', ' Bridge', 'NEW', ' \n', ' the', 'assistant', ' kitchen', ' THE', 'THE', ' the', '<|begin_of_text|>', ' the', '<|start_header_id|>', ' apple', ':', '<|end_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.00022507309913635253, 0.003767271836598714, 0.01126580685377121, 0.000895671546459198]}}, 28: {'grad': {'score': [0.4435525191457648, 0.40094842571170064, 0.4238149915422712, 0.40081636602512827, 0.3251537322998047], 'topk_tokens': [' balance', ' returns', ' summer', ' following', ' border', ' spring', ' over', ' prepared', ' before', 'about', ' platform', 'half', ' summer', ' spring', ' summer', ' inside', '600', ' returns', ' half', 'ball'], 'evidence_proportions': [0.5176315307617188, 0.34863026936848956, 0.424346923828125, 0.512542724609375]}, 'weight': {'score': [0.01310276985168457, 0.0023976024246340445, 0.01030786633491516, 0.0023582047693449714, 0.0004941767767855995], 'topk_tokens': ['.', ' was', ' bedroom', '<|eot_id|>', '.\n\n', 'Answer', ' before', ' \n', '<|eot_id|>', ' apple', 'assistant', ' the', 'b', '?', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0036680102348327637, 0.013101865847905476, 0.02614283561706543, 0.01185750961303711]}, 'saliency': {'score': [0.00051235368377284, 4.388294366791684e-05, 0.00025252955300467356, 4.255369220042053e-05, 9.382084796303196e-06], 'topk_tokens': [' garden', ' kitchen', '.\n\n', ' the', ' before', ' Floral', ' the', '?', ' Bridge', 'b', ' apple', 'assistant', '<|begin_of_text|>', ' Bridge', 'athroom', 'Bridge', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.00034630298614501953, 0.00036947925885518396, 0.0010713711380958557, 0.000375211238861084]}}, 29: {'grad': {'score': [0.7337822161222759, 0.7123829026270521, 0.7255196707589285, 0.7123118301216357, 0.45723290694387336], 'topk_tokens': ['s', 're', ' I', 'itter', 'ION', ' THE', 'ioneer', 'ian', 'THE', ' The', ' Pioneer', ' spring', ' THE', ' M', 'b', 'y', 'y', 'Spring', 'ION', 'ER'], 'evidence_proportions': [0.75616455078125, 0.7718197504679362, 0.9056167602539062, 0.4769134521484375]}, 'weight': {'score': [0.009255144156907735, 0.0024550772694659677, 0.007294127770832607, 0.0024305844033126267, 0.0006285162348496286], 'topk_tokens': [' before', ' was', ' the', ' the', '.', ' Does', '<|eot_id|>', ' the', ' \n', 'Answer', '.\n\n', '?', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0003487884998321533, 0.012486229340235392, 0.02063572406768799, 0.004160881042480469]}, 'saliency': {'score': [0.0006471731160816393, 5.143672003928293e-05, 0.00029357501438685826, 4.981294384346916e-05, 3.699032883895071e-05], 'topk_tokens': [' before', '      ', ':', ' the', '?', 'Does', ' was', '.', 'assistant', 'Answer', '<|eot_id|>', '      ', ' Does', 'athroom', ':', '<|end_header_id|>', ' the', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.506209373474121e-05, 0.0008550087610880535, 0.0016178414225578308, 0.00015489012002944946]}}, 30: {'grad': {'score': [0.38278519479851975, 0.36968643203431273, 0.37093353271484375, 0.3696624349041783, 0.34966910512823807], 'topk_tokens': [' months', ' of', ' time', ' itself', ' S', ' soon', '2', ' Europe', ' the', ' the', ' of', ' Burb', ' account', ' B', ' forb', 'deal', 'b', ' B', 'b', 'b'], 'evidence_proportions': [0.31092529296875, 0.31975809733072913, 0.5139312744140625, 0.436004638671875]}, 'weight': {'score': [0.022179934539292987, 0.0024457510605780607, 0.013288070474352156, 0.0023838617524912947, 0.0031239635065982217], 'topk_tokens': [' Where', ' bedroom', 'Question', ' the', ' barric', '<|eot_id|>', '<|eot_id|>', 'Answer', '.', '.\n\n', 'assistant', '?', 'b', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005059933662414551, 0.02901013195514679, 0.0435175895690918, 0.011996984481811523]}, 'saliency': {'score': [0.001048804898011057, 9.297247524551222e-05, 0.0004713484219142369, 9.039626735659092e-05, 5.854336838973196e-05], 'topk_tokens': ['Answer', ' Bridge', '.', ' apple', '.', ' garden', '?', ' bedroom', 'assistant', ' barric', ' Bench', ' Bridge', 'Bridge', '<|end_header_id|>', '<|begin_of_text|>', ' the', ':', '<|start_header_id|>', 'b', 'athroom'], 'evidence_proportions': [0.0006728649139404296, 0.0011991610129674275, 0.0019899457693099976, 0.0003520548343658447]}}, 31: {'grad': {'score': [0.31289322282138626, 0.4340477731250463, 0.379949722971235, 0.43439193786755265, 0.2580838354010331], 'topk_tokens': [' the', ' their', ' had', 'membership', ' the', ' the', ' the', ' the', ' the', ' they', ' the', ' location', ' the', ' the', ' was', ' had', ' he', ' the', ' the', ' the'], 'evidence_proportions': [0.3399452209472656, 0.2988881270090739, 0.3566930294036865, 0.25628606230020523]}, 'weight': {'score': [0.0049953460693359375, 0.0022508993681565254, 0.0044435688427516395, 0.0022403261180389505, 0.0009756747045015034], 'topk_tokens': ['Question', ':', ' before', ' Where', '<|eot_id|>', ' the', '.\n\n', ' bedroom', 'Answer', '?', ' \n', 'b', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008513212203979493, 0.004981776078542073, 0.007894039154052734, 0.007297039031982422]}, 'saliency': {'score': [8.025765419006348e-05, 2.306595089310176e-05, 7.34363283429827e-05, 2.2832185054431693e-05, 1.036775739569413e-05], 'topk_tokens': [' was', ' the', 'Bridge', ' Market', ' the', 'Question', '.\n\n', '<|eot_id|>', ' apple', '?', ' the', 'b', ' \n', '<|start_header_id|>', '<|begin_of_text|>', ':', ' bedroom', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [2.6017427444458008e-05, 3.4143527348836265e-05, 0.00021199136972427368, 8.549541234970093e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:13:59.103 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:13:59.104 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-4_0-4-5-6.pkl | len: 10 |  size: 9.08 KB
Processing depth (0, 4, 5, 6):   5%|▌         | 5/100 [01:35<30:09, 19.04s/it]Processing depth (0, 4, 5, 6):   5%|▌         | 5/100 [01:35<30:19, 19.15s/it]
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:72 - Selected idx: 11
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:76 - Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' John moved to the garden.', ' John went back to the office.', ' Daniel took the football.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-22 03:13:59.372 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 3, 4, 5, 7):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:14:06.235 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:14:06.236 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:14:06.236 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the milk.
2025-01-22 03:14:06.247 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3818, 3823) -->  war. Mary picked up
2025-01-22 03:14:06.247 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:14:06.262 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4943, 4949) --> . Mary journeyed to the
2025-01-22 03:14:06.262 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:14:06.278 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6009, 6013) -->  Mary left the milk
2025-01-22 03:14:06.278 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel dropped the football.
2025-01-22 03:14:06.301 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (8444, 8448) -->  Daniel dropped the football
2025-01-22 03:14:06.302 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:14:06.313 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3902, 3907) -->  city. John moved to
2025-01-22 03:14:06.313 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the office.
2025-01-22 03:14:06.319 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2228, 2234) --> . John went back to the
2025-01-22 03:14:06.320 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel took the football.
2025-01-22 03:14:06.339 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7186, 7190) -->  Daniel took the football
2025-01-22 03:14:06.340 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:14:06.357 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6432, 6436) -->  Daniel left the apple
2025-01-22 03:14:06.357 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:14:06.370 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4552, 4558) --> . Sandra journeyed to the
2025-01-22 03:14:06.371 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 03:14:06.384 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4877, 4882) --> . Daniel picked up the
2025-01-22 03:14:06.384 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 03:14:06.397 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (4557, 4562) -->  the office. Sandra moved
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:14:08.377 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:14:08.378 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12224])
your chose emoji: ['🛅', '🇬🇹', '👁\u200d🗨', '🤾🏻', '🌀', '🚣🏽\u200d♀', '🏊🏼\u200d♀️', '🦹🏼\u200d♀️', '🧛🏾', '🏋\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 187454.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.48s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.07s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.58it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.40it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.05it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.66it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.33it/s][A100%|██████████| 8/8 [00:00<00:00, 18.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.89it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.99it/s][A100%|██████████| 8/8 [00:00<00:00, 20.97it/s]
2025-01-22 03:14:17.690 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.244369904200236, 0.21053619646836919, 0.23721961975097655, 0.2103927111954849, 0.34037303924560547], 'topk_tokens': [' examined', 'est', '202', ' compromised', 'ottle', 'oggle', ' compl', ' latter', ' first', 'itter', ' comparison', ' first', ' first', 'st', ' committee', 'ols', '.', ' absence', ' turtle', 'consider'], 'evidence_proportions': [0.19372406005859377, 0.3142433166503906, 0.27933502197265625, 0.19253301620483398, 0.2197246551513672]}, 'weight': {'score': [0.0397014394402504, 0.0025731795181420424, 0.007497232300894601, 0.0024857847050645493, 0.0006401297591981434], 'topk_tokens': ['\n\n', 'Mary', ' milk', '<|eot_id|>', 'Answer', ' kitchen', 'Bridge', 'assistant', ' the', '<|eot_id|>', ' milk', '<|start_header_id|>', ':', '\n\n', 'b', ' bedroom', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0891326904296875, 0.011032855510711669, 0.005235552787780762, 0.10215544700622559, 0.0029929280281066895]}, 'saliency': {'score': [0.0025028797487417855, 3.474433570088706e-05, 0.00031350084713527136, 2.9074404092308412e-05, 1.5182154519217355e-05], 'topk_tokens': [' Mary', ' location', ' PA', ' Mary', '\n\n', 'Answer', ' Bench', ' office', '<|eot_id|>', '<|eot_id|>', ' milk', ' kitchen', '<|begin_of_text|>', 'b', 'Mary', ' garden', ' bathroom', ' milk', ' bedroom', 'athroom'], 'evidence_proportions': [0.0036473989486694334, 0.0009002625942230224, 0.0004014422496159871, 0.008670210838317871, 6.0327351093292236e-05]}}, 25: {'grad': {'score': [0.5727078119913737, 0.7957213871260836, 0.5417954036167689, 0.7968916480749708, 0.348510189661904], 'topk_tokens': [' for', ' at', ' of', ' the', ' of', ' for', ' a', ' in', ' inverted', ' for', ' in', ' the', ' at', ' no', ' over', ' little', ' a', ' for', ' of', ' no'], 'evidence_proportions': [0.827685546875, 0.50186767578125, 0.6541188557942709, 0.4448394775390625, 0.34828758239746094]}, 'weight': {'score': [0.016327614585558575, 0.002484962966930177, 0.005359091929027012, 0.0024493927703062067, 0.0005139107742006817], 'topk_tokens': ['MIN', ' the', ' Mary', ' bathroom', ' the', 'Mary', 'Answer', ' Bench', '?\n', ' milk', '<|eot_id|>', 'b', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.017966842651367186, 0.013240164518356322, 0.0029698858658472695, 0.050720810890197754, 0.0037812888622283936]}, 'saliency': {'score': [0.0006217919290065765, 3.0002770853264895e-05, 0.00013963069234575545, 2.8520200418689235e-05, 1.1609187201848106e-05], 'topk_tokens': [' the', ' bathroom', '<|start_header_id|>', 'RE', ' top', 'assistant', ' bedroom', ' THE', ' to', ' Min', ' PA', 'Answer', ' Geo', ':', ' Bench', ' milk', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005131065845489502, 0.0003932297229766845, 0.00012422104676564535, 0.002300061285495758, 0.0001114383339881897]}}, 26: {'grad': {'score': [0.22974697748819986, 0.2543159098620113, 0.23875797816685268, 0.25440912008442274, 0.3248340364486452], 'topk_tokens': [' Marshall', ' ice', ' Empire', 'er', 'UX', ' Press', ' and', ' Marshall', ' bitter', 'ers', 'itter', 'rich', 'char', ' Becker', ' Marshall', 'ab', 'ub', ' Eagle', ' commander', 'ers'], 'evidence_proportions': [0.18455810546875, 0.319903564453125, 0.19753964742024738, 0.19272994995117188, 0.2588653564453125]}, 'weight': {'score': [0.021408540507157642, 0.0024500466469430647, 0.00298342193875994, 0.0024111189687824187, 0.0006356121055663578], 'topk_tokens': ['Bridge', ' discarded', ' the', ' the', ' the', ' kitchen', '<|eot_id|>', '<|eot_id|>', ' bedroom', '?\n', 'Answer', 'assistant', ' bathroom', 'b', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0673969030380249, 0.004382705688476563, 0.001828561226526896, 0.034552574157714844, 0.0014313161373138428]}, 'saliency': {'score': [0.00035393362243970233, 4.047025709557028e-05, 0.00011591230119977679, 3.963498488058784e-05, 1.6988269866458953e-05], 'topk_tokens': [' the', ' PA', ' Sandra', '?\n', ' garden', ' Anthony', ' Floral', ' the', ' kitchen', 'assistant', '<|end_header_id|>', ' bathroom', 'Answer', 'Bridge', '<|start_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0008829414844512939, 0.00022149085998535156, 3.72081995010376e-05, 0.0006616413593292236, 2.5607645511627197e-05]}}, 27: {'grad': {'score': [0.36907800038655597, 0.3336477556553887, 0.3142493656703404, 0.33363367098871616, 0.2792201269240606], 'topk_tokens': ['\n', ' successful', ' several', ' excessive', ' execution', 'ition', ' other', ' intended', ' consultations', 'roduced', 'CE', ' received', ' restrictions', ' exchanged', ' assistance', ' designated', ' Thanksgiving', ' step', ' accepted', ' short'], 'evidence_proportions': [0.38870849609375, 0.38680419921875, 0.2148907979329427, 0.48583221435546875, 0.4369087219238281]}, 'weight': {'score': [0.03603388120730718, 0.0025285056443267845, 0.004110491275787354, 0.002457869672054213, 0.0008578149099198599], 'topk_tokens': [' the', 'RE', '<|eot_id|>', ' THE', '?\n', 'NEW', ' kitchen', 'Answer', 'Mary', 'assistant', '.\n\n', 'b', '\n\n', '<|start_header_id|>', ' bedroom', ' bathroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.12526246309280395, 0.008305776119232177, 0.002997209628423055, 0.04107379913330078, 0.003673374652862549]}, 'saliency': {'score': [0.0011017359793186188, 3.706541427599201e-05, 0.0002972407000405448, 3.421710489377781e-05, 5.5258236234150235e-05], 'topk_tokens': [' the', ' dropped', ' to', ' prior', 'RE', ' THE', ' Bridge', 'Bridge', ':', ' the', ' bathroom', '<|begin_of_text|>', 'Mary', 'b', '<|end_header_id|>', 'NEW', ' bedroom', ' kitchen', 'athroom', '.\n\n'], 'evidence_proportions': [0.0024767279624938964, 0.0005391538143157959, 0.0002719461917877197, 0.0022960007190704346, 0.000136643648147583]}}, 28: {'grad': {'score': [0.3451259930928548, 0.43353134208935656, 0.3893515450613839, 0.43383279025437094, 0.47095513722253224], 'topk_tokens': ['ien', ' the', ' an', 'ien', 'ivery', ' a', ' the', ' Cedar', 'ew', 'ot', '      ', "'", 'IO', '.', '.', 'nes', 'nes', 'nes', 'nes', 'dent'], 'evidence_proportions': [0.3040412902832031, 0.17499465942382814, 0.5009148915608723, 0.3364105224609375, 0.38417816162109375]}, 'weight': {'score': [0.013133266319831213, 0.002372190023546881, 0.005324327094214303, 0.002342473502460081, 0.0003459652264912923], 'topk_tokens': [' garden', ' bathroom', ' the', ' milk', ' discarded', ' the', '<|eot_id|>', ' the', '?\n', '<|eot_id|>', 'Answer', ' the', 'assistant', '<|end_header_id|>', 'b', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00987992286682129, 0.004802370071411133, 0.0035084038972854614, 0.05002641677856445, 0.0051577091217041016]}, 'saliency': {'score': [0.00023596485455830893, 2.9274782038986843e-05, 0.00013507774897984095, 2.8562777964085986e-05, 9.94404157002767e-06], 'topk_tokens': ['.\n\n', '<|eot_id|>', ' Bridge', 'Bridge', ' bedroom', ' garden', '?\n', '<|start_header_id|>', ' the', ' the', ' Floral', ' Bridge', 'b', ' milk', '\n\n', 'athroom', 'assistant', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010666251182556152, 0.00011788606643676758, 0.00012271602948506674, 0.0008585229516029358, 9.250640869140625e-05]}}, 29: {'grad': {'score': [0.24268706639607748, 0.27269257857226836, 0.2552980627332415, 0.2728017945770838, 0.25818043663388207], 'topk_tokens': ['ION', ' book', ' a', ' not', 'cret', 'ION', 'ION', ' The', ' B', 'init', '\n', ' In', 'assistant', ' Pioneer', 'adv', 're', 'Spring', ' M', 'b', ' ga'], 'evidence_proportions': [0.326611328125, 0.20467529296874998, 0.23409398396809894, 0.27315807342529297, 0.16771507263183594]}, 'weight': {'score': [0.005417601515849431, 0.0024411674219345027, 0.002731098447527204, 0.0024344627864849875, 0.00039067769807482524], 'topk_tokens': [' the', ' the', '      ', ' in', '<|eot_id|>', ' Does', '<|eot_id|>', '.\n\n', ' the', ' the', '?\n', 'Answer', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.001210552453994751, 0.001689028739929199, 0.0012895266215006511, 0.023364901542663574, 0.0035819411277770996]}, 'saliency': {'score': [0.00021743898590405783, 3.3318270078574666e-05, 9.155017989022391e-05, 3.278761475122265e-05, 1.2187730698358445e-05], 'topk_tokens': [' milk', 'IVE', ' was', 'Answer', ' in', '<|eot_id|>', 'NEW', ' the', ':', ' Does', '<|start_header_id|>', '<|end_header_id|>', ' the', 'athroom', 'assistant', ' the', '      ', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.2809038162231444e-05, 9.729862213134766e-05, 7.753074169158936e-05, 0.0009857341647148132, 6.496906280517578e-05]}}, 30: {'grad': {'score': [0.2773147424062093, 0.28643583680072787, 0.24215087890625, 0.2865812082497561, 0.2440357208251953], 'topk_tokens': [' its', 'moment', ' machines', 'b', 'itter', ' the', ' Times', ' of', ' soon', ' Burb', ' B', ' account', 'B', 'deal', ' forb', 'b', ' B', 'b', ' B', 'b'], 'evidence_proportions': [0.3276458740234375, 0.24309654235839845, 0.23722330729166666, 0.23364639282226562, 0.3609790802001953]}, 'weight': {'score': [0.01681340237458547, 0.0024764680351543732, 0.0077705579144614085, 0.0024329621533396052, 0.0013977260816664923], 'topk_tokens': [' Where', 'Question', '.', ' garden', '<|eot_id|>', ' milk', ' bedroom', '<|eot_id|>', '.\n\n', ' the', 'Answer', 'b', 'assistant', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00837554931640625, 0.005908286571502685, 0.0035652617613474527, 0.0736398696899414, 0.0040378570556640625]}, 'saliency': {'score': [0.0013042750457922618, 4.323734679050123e-05, 0.00027615257671901157, 4.008013625273495e-05, 1.8845474909222316e-05], 'topk_tokens': [' milk', ' office', ' Mary', ' Bench', '.\n\n', ' Wide', ' kitchen', ' Bridge', 'Bridge', 'Mary', 'assistant', ' bedroom', ' garden', ' bathroom', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0031638920307159426, 0.0008003413677215576, 0.00038050611813863117, 0.002011805772781372, 0.0002877935767173767]}}, 31: {'grad': {'score': [0.2564648886521657, 0.3017008201054387, 0.29450810296194896, 0.3018107319607067, 0.22788072625796], 'topk_tokens': ['7', ' the', ' the', ' had', "'clock", ' is', ' they', ' paper', 'If', ' was', ' the', 'did', ' the', ' location', ' having', ' the', ' August', 'membership', ' the', ' he'], 'evidence_proportions': [0.2454922676086426, 0.3308361053466797, 0.21993660926818848, 0.258211612701416, 0.23026233911514282]}, 'weight': {'score': [0.0029671465357144675, 0.0022851431214761645, 0.0018584396157945906, 0.002285025315818467, 0.0007381472322675916], 'topk_tokens': [' was', 'Question', ',', ' milk', ':', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0013166368007659912, 0.002516734600067139, 0.000705207387606303, 0.008481979370117188, 0.00347137451171875]}, 'saliency': {'score': [9.730209906895955e-05, 2.241470032412179e-05, 3.1057425907679964e-05, 2.2242133512377818e-05, 5.620339560130286e-06], 'topk_tokens': [' prior', ' Geo', ' Mary', ' dropped', '.\n\n', '\n\n', ' bedroom', ' Mary', ' milk', '<|eot_id|>', ' the', '<|begin_of_text|>', '?\n', 'Answer', '<|start_header_id|>', ':', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [2.2274255752563476e-05, 0.00018848180770874022, 2.323587735493978e-05, 0.00025375932455062866, 3.1754374504089355e-05]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:14:17.692 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:14:17.692 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-0_0-3-4-5-7.pkl | len: 10 |  size: 9.53 KB
Processing depth (0, 3, 4, 5, 7):   1%|          | 1/100 [00:18<30:04, 18.23s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 5, 6, 8, 9):   1%|          | 1/100 [00:25<30:04, 18.23s/it]2025-01-22 03:14:24.795 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:14:24.800 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1497, 1502) -->  tragedy. Mary moved to
2025-01-22 03:14:24.800 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the milk.
2025-01-22 03:14:24.817 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5966, 5971) --> . Mary picked up the
2025-01-22 03:14:24.817 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:14:24.838 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7194, 7200) --> . Mary journeyed to the
2025-01-22 03:14:24.838 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:14:24.864 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9679, 9683) -->  left the milk.
2025-01-22 03:14:24.865 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel dropped the football.
2025-01-22 03:14:24.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10784, 10788) -->  Daniel dropped the football
2025-01-22 03:14:24.894 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:14:24.904 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3855, 3860) --> . John moved to the
2025-01-22 03:14:24.904 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the office.
2025-01-22 03:14:24.911 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2229, 2235) --> . John went back to the
2025-01-22 03:14:24.911 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel took the football.
2025-01-22 03:14:24.930 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7101, 7105) -->  Daniel took the football
2025-01-22 03:14:24.930 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:14:24.947 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6329, 6333) -->  Daniel left the apple
2025-01-22 03:14:24.947 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:14:24.961 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4497, 4503) --> . Sandra journeyed to the
2025-01-22 03:14:24.961 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 03:14:24.974 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4835, 4840) --> . Daniel picked up the
2025-01-22 03:14:24.975 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 03:14:24.987 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (4502, 4507) -->  the office. Sandra moved
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:14:27.035 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 03:14:27.035 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12226])
your chose emoji: ['🧔🏾\u200d♂', '👨🏽\u200d🦼', '🙈', '🏋🏿\u200d♀', '🌟', '⛹️\u200d♀️', '🚶🏻\u200d♀️', '👦', '🧑\u200d🦽', '🇦🇩']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 264208.13it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.57s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.48s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.31it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.42it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.47it/s][A100%|██████████| 8/8 [00:00<00:00, 20.61it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.40it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.06it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.26it/s][A100%|██████████| 8/8 [00:00<00:00, 20.29it/s]
2025-01-22 03:14:36.485 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.25678714116414386, 0.3437861599237085, 0.262097658429827, 0.3441926573766679, 0.29411034217247595], 'topk_tokens': [' emb', ' considerable', ' agreement', ' committee', ' whistle', ' printed', ' compromised', ' whistle', ' examined', 'ed', ' Do', ' considerable', ' expedition', ' compl', ' EAR', ' STE', ' compromised', 'able', ' exclaimed', 'consider'], 'evidence_proportions': [0.366229248046875, 0.27296600341796873, 0.23389816284179688, 0.24573707580566406, 0.14514446258544922]}, 'weight': {'score': [0.014831083516279856, 0.0025691175823444063, 0.0033716091087886263, 0.002542628314814415, 0.0009723310287182148], 'topk_tokens': [' garden', ' milk', ' the', '?\n', 'Bridge', ' milk', ' kitchen', 'Answer', '<|eot_id|>', ':', 'assistant', '<|start_header_id|>', ' bathroom', '<|eot_id|>', '\n\n', 'b', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007269501686096192, 0.014636015892028809, 0.00901496410369873, 0.05160951614379883, 0.00465083122253418]}, 'saliency': {'score': [0.0010257735848426819, 5.499701905849082e-05, 0.00012255736759730746, 5.288828859196007e-05, 2.2817116517287035e-05], 'topk_tokens': ['Answer', ' kitchen', ' Bridge', ' Mary', ' milk', ':', '\n\n', '<|end_header_id|>', ' milk', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ' milk', 'Bridge', ' garden', '<|begin_of_text|>', 'b', ' bathroom', 'athroom', ' bedroom'], 'evidence_proportions': [7.339715957641601e-05, 0.0008964598178863526, 0.0005216946204503377, 0.00404086709022522, 0.0001189112663269043]}}, 25: {'grad': {'score': [0.47417791684468585, 0.6248315232043197, 0.5392314910888671, 0.6253748007455416, 0.4574089637169471], 'topk_tokens': [' and', ' for', ' be', ' made', ' some', ' was', ' for', ' at', ' was', ' in', ' squ', ' some', ' being', ' of', ' considerable', ' of', ' so', ' set', 'posit', ' Aw'], 'evidence_proportions': [0.7478271484375001, 0.4006805419921875, 0.40410963694254554, 0.552032470703125, 0.2512359619140625]}, 'weight': {'score': [0.012867245823144913, 0.002495338018200405, 0.002859931332724435, 0.0024738354254865997, 0.0014588498152219333], 'topk_tokens': [' the', ' bathroom', ' the', ' Bench', '.\n\n', ' Mary', ' milk', 'b', '?\n', 'Answer', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', ' bedroom', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.002734857797622681, 0.020287430286407473, 0.004031777381896973, 0.03849303722381592, 0.00388491153717041]}, 'saliency': {'score': [0.0004753222068150838, 3.1414679449999926e-05, 5.345770290919713e-05, 3.047587201550248e-05, 4.3148260850172775e-05], 'topk_tokens': [' Geo', ' milk', ' East', ' Seventh', '189', ' top', 'assistant', ' Merch', '<|eot_id|>', '<|eot_id|>', 'Answer', ' Bench', 'b', ' THE', ' milk', ' bedroom', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00013273954391479492, 0.0004966199398040771, 0.00011605024337768555, 0.0018580853939056396, 3.307312726974487e-05]}}, 26: {'grad': {'score': [0.5091037750244141, 0.5222313307187321, 0.5625115530831474, 0.522141376203852, 0.4842145479642428], 'topk_tokens': [' Clean', ' office', ' steam', ' boats', ' will', ' steam', ' steam', ' steam', ' Hill', ' steam', ' readiness', ' steam', ' steam', ' steam', ' week', 'ub', 'AM', ' Eagle', ' commander', ' Becker'], 'evidence_proportions': [0.5428955078125, 0.5815185546875, 0.38142903645833337, 0.5635986328125, 0.5133628845214844]}, 'weight': {'score': [0.006846437851587932, 0.002453907267793298, 0.0012397050857543945, 0.002448736876861522, 0.0008949564053462102], 'topk_tokens': ['Bridge', ' discarded', ' the', ' kitchen', '.\n\n', ' bathroom', '?\n', ' the', '<|eot_id|>', '<|eot_id|>', 'Answer', 'assistant', 'b', '\n\n', '<|start_header_id|>', ' bedroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00023334622383117676, 0.008136886358261108, 0.0027782718340555825, 0.023296475410461426, 0.0031519532203674316]}, 'saliency': {'score': [0.0002048984169960022, 6.16821388609909e-05, 2.8387137821742467e-05, 6.149546132287532e-05, 3.4775642248300405e-05], 'topk_tokens': [' Western', 'river', ' Anthony', '?\n', ' Merch', '<|eot_id|>', 'assistant', 'Bridge', ' the', ' East', 'Answer', ' kitchen', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', ' bedroom', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [1.4340877532958984e-05, 0.0003421306610107422, 7.213155428568521e-05, 0.0006281360983848572, 4.746764898300171e-05]}}, 27: {'grad': {'score': [0.31718095143636066, 0.30731250582950875, 0.33061197825840544, 0.30722603711713603, 0.327196898827186], 'topk_tokens': [' assistance', ' started', 'started', 'arr', 'arr', ' successful', ' several', ' short', 'three', 'ball', ' fifty', 'UG', ' staff', ' designated', ' accepted', ' three', ' STR', ' Thanksgiving', 'str', ' step'], 'evidence_proportions': [0.271942138671875, 0.30228729248046876, 0.210784912109375, 0.442657470703125, 0.4264640808105469]}, 'weight': {'score': [0.007804997265338898, 0.0025239771799987, 0.0025637022086552213, 0.002513448432418491, 0.0012907129067641038], 'topk_tokens': [' kitchen', ' Mary', ' the', 'THE', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', ' THE', 'assistant', 'b', '.\n\n', '\n\n', ' bathroom', ':', '<|start_header_id|>', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010717272758483886, 0.016080856323242188, 0.003844658533732096, 0.014959871768951416, 0.0046623945236206055]}, 'saliency': {'score': [0.0004837041099866231, 4.938517278521066e-05, 0.00016354237283979143, 4.8200361240860276e-05, 6.570311693044809e-05], 'topk_tokens': [' top', ' the', ' dropped', 'NEW', 'THE', ' bathroom', ' Mary', 'Bridge', ' Bridge', ' kitchen', '<|begin_of_text|>', 'THE', 'athroom', '<|start_header_id|>', '<|end_header_id|>', ':', ' THE', '.\n\n', 'b', ' bedroom'], 'evidence_proportions': [8.034110069274903e-05, 0.0012368321418762208, 0.00024333099524180093, 0.0007032379508018494, 0.00018752366304397583]}}, 28: {'grad': {'score': [0.29878973960876465, 0.42903272837939016, 0.32990384783063614, 0.42957466285348234, 0.4045350881723257], 'topk_tokens': ['.', '      ', ' the', ' the', '      ', ' the', "'", ' Cedar', ' its', ' an', 'nes', ' the', ' the', '.', ' the', ' the', 'nes', 'nes', 'nes', 'nes'], 'evidence_proportions': [0.20752182006835937, 0.2702751159667969, 0.36594200134277344, 0.2884197235107422, 0.35815954208374023]}, 'weight': {'score': [0.005420642594496409, 0.0023912580631888403, 0.0015218470777784074, 0.0023877842879824296, 0.0005958447089562049], 'topk_tokens': ['Question', ' Bridge', ' the', ' discarded', '.\n\n', ' bathroom', ' the', '<|eot_id|>', 'Answer', '?\n', ' bedroom', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [9.127259254455566e-05, 0.005136018991470337, 0.003209610780080159, 0.015879809856414795, 0.005295515060424805]}, 'saliency': {'score': [0.00010987867911656697, 3.8058562430427705e-05, 3.544177327837263e-05, 3.792445436303696e-05, 1.2064438599806566e-05], 'topk_tokens': [' milk', '<|eot_id|>', 'Bridge', ' bathroom', '.\n\n', ' Bridge', ' Floral', ' the', ' Nearly', '?\n', ' nearly', 'b', 'athroom', 'assistant', '\n\n', ' bedroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [6.860494613647461e-06, 0.00011797547340393066, 8.759895960489909e-05, 0.0002600252628326416, 0.00011180341243743896]}}, 29: {'grad': {'score': [0.34543100992838544, 0.33267315477681125, 0.40956381389072966, 0.33242686376673525, 0.30438354932344874], 'topk_tokens': [' Ch', ' There', ' S', 'd', 'st', '\n', ' l', '\n', ' pres', '\n', 'UL', 'adv', ' THE', ' The', 'b', ' an', 'ION', 're', '\n', ' M'], 'evidence_proportions': [0.56353759765625, 0.284503173828125, 0.356683095296224, 0.32657432556152344, 0.15093612670898438]}, 'weight': {'score': [0.003391565134127935, 0.0024653428592630415, 0.0008120707103184291, 0.002468270976815737, 0.0010080901476053092], 'topk_tokens': [' discarded', 'Question', ' the', ' Does', ' Where', 'NEW', ' the', '<|eot_id|>', '.\n\n', '<|eot_id|>', '?\n', 'Answer', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [5.739331245422363e-05, 0.004673552513122558, 0.002102831999460856, 0.009078651666641235, 0.0022028088569641113]}, 'saliency': {'score': [0.00014221295714378357, 3.7239297099764245e-05, 2.1886825561523437e-05, 3.7076435034257426e-05, 4.52275459582989e-05], 'topk_tokens': ['nes', ' Where', 'Does', ' the', '<|eot_id|>', 'NEW', 'IVE', ' Does', '<|eot_id|>', '?\n', ' the', 'Answer', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [5.0246715545654295e-06, 0.00018323063850402831, 5.945563316345215e-05, 0.0005087926983833313, 1.9982457160949707e-05]}}, 30: {'grad': {'score': [0.42045410474141437, 0.42494953681758013, 0.4360790252685547, 0.42492639452292474, 0.37097731370192305], 'topk_tokens': [' itself', ' need', ' Sandra', ' of', ' the', ' until', ' of', ' S', ' Buchanan', ' account', ' Burb', 'deal', 'B', ' B', ' forb', 'b', 'b', ' B', 'b', ' B'], 'evidence_proportions': [0.2942962646484375, 0.3729248046875, 0.41513856252034503, 0.5905871391296387, 0.4754033088684082]}, 'weight': {'score': [0.011333840588728586, 0.0024723660011051313, 0.002469818932669503, 0.0024548979450075372, 0.0028751322856316198], 'topk_tokens': [' the', ' Mary', '.', 'Question', ' Broadway', ' the', '.\n\n', ' bedroom', 'b', '<|eot_id|>', '<|eot_id|>', 'assistant', 'Answer', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015801250934600831, 0.01621151566505432, 0.007561047871907552, 0.02909386157989502, 0.005328059196472168]}, 'saliency': {'score': [0.0003270233670870463, 5.5513582086122496e-05, 0.0001349057470049177, 5.4749821970084696e-05, 7.195747815645658e-05], 'topk_tokens': [' the', '<|eot_id|>', '.\n\n', ' Emily', ' Broadway', '<|eot_id|>', '?\n', ' the', ' Miles', ' Bench', ' Bridge', 'Answer', ' bathroom', ':', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', 'b', 'athroom'], 'evidence_proportions': [6.824135780334473e-05, 0.0001458108425140381, 0.0001972516377766927, 0.001136496663093567, 0.00026220083236694336]}}, 31: {'grad': {'score': [0.22599132855733237, 0.3782677076094694, 0.2972372395651681, 0.3788010436389519, 0.2532178085583907], 'topk_tokens': ['ot', ' Van', ' his', ' the', 'E', ' was', 'ible', 'nes', 'nes', ' were', 'cert', ' the', 'nes', ' the', ' OCC', ' had', 'nes', ' was', 'in', ' the'], 'evidence_proportions': [0.1271571159362793, 0.22893905639648438, 0.19784156481424967, 0.2978217601776123, 0.31624364852905273]}, 'weight': {'score': [0.002920589099327723, 0.0022875171074620425, 0.0007584861346653529, 0.002290666027449151, 0.0009755473870497484], 'topk_tokens': [' Market', ' bedroom', ',', ' the', ':', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', ':', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008450627326965332, 0.004359716176986694, 0.0018765528996785483, 0.004174172878265381, 0.0040285587310791016]}, 'saliency': {'score': [4.8920512199401855e-05, 2.9394053235273544e-05, 1.6791479928152902e-05, 2.939178988692598e-05, 9.2162535740779e-06], 'topk_tokens': [' Emily', 'Question', ' location', ' Geo', ' bedroom', '\n\n', 'CH', '.\n\n', ' the', '?\n', '<|eot_id|>', ' Market', '<|begin_of_text|>', 'b', 'Answer', ':', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [3.383159637451172e-05, 5.3822994232177734e-05, 5.247195561726888e-05, 4.000216722488403e-05, 6.524473428726196e-05]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:14:36.486 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:14:36.487 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-1_1-5-6-8-9.pkl | len: 10 |  size: 9.62 KB
Processing depth (1, 5, 6, 8, 9):   2%|▏         | 2/100 [00:37<30:19, 18.56s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 2, 3, 8, 9):   2%|▏         | 2/100 [00:43<30:19, 18.56s/it]2025-01-22 03:14:43.517 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:14:43.518 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:14:43.518 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the milk.
2025-01-22 03:14:43.525 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2470, 2475) --> . Mary picked up the
2025-01-22 03:14:43.525 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:14:43.536 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3773, 3779) --> . Mary journeyed to the
2025-01-22 03:14:43.536 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:14:43.562 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9695, 9699) -->  left the milk.
2025-01-22 03:14:43.562 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel dropped the football.
2025-01-22 03:14:43.591 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10823, 10827) -->  Daniel dropped the football
2025-01-22 03:14:43.591 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:14:43.602 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3886, 3891) --> . John moved to the
2025-01-22 03:14:43.602 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the office.
2025-01-22 03:14:43.609 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2193, 2199) --> . John went back to the
2025-01-22 03:14:43.609 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel took the football.
2025-01-22 03:14:43.628 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7145, 7149) -->  Daniel took the football
2025-01-22 03:14:43.628 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:14:43.645 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6366, 6370) -->  Daniel left the apple
2025-01-22 03:14:43.645 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:14:43.658 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4554, 4560) --> . Sandra journeyed to the
2025-01-22 03:14:43.659 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 03:14:43.672 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4884, 4889) --> . Daniel picked up the
2025-01-22 03:14:43.672 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 03:14:43.685 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (4559, 4564) -->  the office. Sandra moved
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:14:45.701 | INFO     | test_jbb_retain:begin_test:632 - kitchen<|eot_id|>
2025-01-22 03:14:45.701 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12254])
your chose emoji: ['💇🏻\u200d♂️', '🏊\u200d♂️', '🤦🏽\u200d♀️', '\U0001faf1🏾\u200d\U0001faf2🏻', '🇨🇽', '😟', '🧑🏼\u200d🤝\u200d🧑🏼', '👩🏿\u200d❤️\u200d👨🏻', '🧑🏽\u200d⚕️', '🌭']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 177536.68it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.44s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.06s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.58it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.43it/s][A100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.87it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.90it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.05it/s][A100%|██████████| 8/8 [00:00<00:00, 21.18it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.47it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.16it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.53it/s][A100%|██████████| 8/8 [00:00<00:00, 20.50it/s]
2025-01-22 03:14:55.136 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.19568447271982828, 0.23781035756990904, 0.21453123092651366, 0.23796003708039606, 0.2226104941419376], 'topk_tokens': [' material', ' emb', 'ab', 'ob', ' compromised', 'ols', ' out', 'called', ' Ear', 'ad', 'able', 'CE', ' examined', ' comparison', ' committee', ' turtle', ' EAR', ' compl', ' EAR', 'consider'], 'evidence_proportions': [0.25345458984375, 0.2221405029296875, 0.15700419743855795, 0.1853961944580078, 0.15871047973632812]}, 'weight': {'score': [0.046400283773740135, 0.002568939123708447, 0.016984765018735613, 0.002441335731519013, 0.00041031645190331244], 'topk_tokens': [' the', 'Bridge', '\n\n', ' milk', ' milk', 'Answer', '<|eot_id|>', 'assistant', '<|start_header_id|>', ' office', ':', '<|eot_id|>', 'b', ' kitchen', '\n\n', '<|end_header_id|>', ' bedroom', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.14914875030517577, 0.013629710674285887, 0.018983950217564903, 0.0421905517578125, 0.004262149333953857]}, 'saliency': {'score': [0.0027404489616552987, 3.633601655576497e-05, 0.001173370225088937, 2.7753059679141766e-05, 6.5494609135453415e-06], 'topk_tokens': ['NEW', 'el', ' PA', ' milk', ' Bench', '\n\n', 'Answer', '\n\n', ':', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', ' garden', 'b', ' milk', ' kitchen', ' office', ' bathroom', 'athroom', ' bedroom'], 'evidence_proportions': [0.00979810357093811, 0.0002846181392669678, 0.0005047917366027832, 0.0029221922159194946, 0.00015991181135177612]}}, 25: {'grad': {'score': [0.5017043749491373, 0.5786712041461002, 0.5151083809988839, 0.5790050213875233, 0.3183584725984963], 'topk_tokens': [' at', ' the', ' in', ' of', ' for', ' in', ' money', ' was', ' in', ' being', ' for', 'posit', ' for', ' for', ' for', ' for', ' a', ' at', ' no', ' of'], 'evidence_proportions': [0.64603271484375, 0.48628540039062496, 0.6072540283203125, 0.4548530578613281, 0.2290945053100586]}, 'weight': {'score': [0.01834925264120102, 0.0024903718279695166, 0.008361826624189104, 0.0024423218150669717, 0.00036890372153251404], 'topk_tokens': [' office', ' prior', ' milk', '.\n\n', ' the', 'Mary', ' Bench', '?\n', 'Answer', 'b', '<|start_header_id|>', '<|eot_id|>', ' bathroom', '<|eot_id|>', 'assistant', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.032772254943847653, 0.010323786735534668, 0.008671313524246216, 0.039855241775512695, 0.0033632516860961914]}, 'saliency': {'score': [0.0005276935795942942, 2.2242058935023018e-05, 0.00017048886844090054, 2.082219708647996e-05, 7.284264410695722e-06], 'topk_tokens': [' bedroom', ' THE', ' the', 'assistant', '.\n\n', ' Geo', 'RE', 'b', ' top', 'Mary', ' to', 'Answer', ' milk', ' bathroom', ' PA', '<|end_header_id|>', 'athroom', ' Bench', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0011162698268890382, 0.00014737844467163085, 0.00022395451863606772, 0.0011983215808868408, 5.2347779273986816e-05]}}, 26: {'grad': {'score': [0.26033798853556317, 0.26998853811852414, 0.29472416469029017, 0.2699365514207031, 0.33071864548549856], 'topk_tokens': ['outs', ' Marshall', 'UX', ' Press', '.', 'ers', 'ong', ' Marshall', 'els', ' and', 'ers', ' week', ' West', ' Hill', ' readiness', 'is', ' commander', ' readiness', ' Eagle', ' Becker'], 'evidence_proportions': [0.1583892822265625, 0.38266983032226565, 0.24610137939453125, 0.23493576049804688, 0.2816162109375]}, 'weight': {'score': [0.030499812215566635, 0.0024423982829227526, 0.004418365444455828, 0.0023815246327311547, 0.0004607897291901291], 'topk_tokens': [' discarded', ' the', ' the', '.\n\n', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', ' bedroom', 'Answer', 'assistant', ' kitchen', 'b', '<|start_header_id|>', ' bathroom', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.11517353057861328, 0.004308891296386719, 0.004084770878156027, 0.024405241012573242, 0.0031134486198425293]}, 'saliency': {'score': [0.0005894278486569723, 3.7893578756630705e-05, 0.00017380288669041226, 3.641844773061902e-05, 1.209942243432486e-05], 'topk_tokens': [':', ' Bridge', ' the', ' the', '?\n', ' the', ' bedroom', 'Bridge', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', ' bathroom', ' kitchen', 'athroom', '<|begin_of_text|>', '\n\n', 'b', ':'], 'evidence_proportions': [0.002010446786880493, 0.00019417405128479003, 4.2344133059183754e-05, 0.0006605833768844604, 5.669146776199341e-05]}}, 27: {'grad': {'score': [0.2825949986775716, 0.3020851525531329, 0.2677284240722656, 0.3022220806716641, 0.2743776690575384], 'topk_tokens': [' received', 'three', ' successful', ' assistance', 'roduced', ' short', ' extended', ' short', ' short', ' Thanksgiving', ' prolonged', '\n', 'CE', ' several', ' designated', ' accepted', ' exchanged', 'atter', ' short', ' step'], 'evidence_proportions': [0.27896881103515625, 0.2743408203125, 0.1635894775390625, 0.3718109130859375, 0.3867378234863281]}, 'weight': {'score': [0.046168169627587, 0.002522667129092089, 0.010676458903721401, 0.002413397185488556, 0.0004544732391193349], 'topk_tokens': [' the', 'RE', '<|eot_id|>', 'Mary', '?\n', 'NEW', ' office', 'Answer', 'assistant', 'b', '.\n\n', ' kitchen', '\n\n', ' bedroom', '<|start_header_id|>', ':', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.18893874883651735, 0.0084286630153656, 0.00697559118270874, 0.014252245426177979, 0.005584120750427246]}, 'saliency': {'score': [0.0009588003158569336, 3.5292273916913796e-05, 0.0005509010383061001, 3.1995790906569316e-05, 1.8927038356822025e-05], 'topk_tokens': [' to', ' discarded', ' the', ' dropped', ' prior', ' the', ' THE', '<|start_header_id|>', ' office', 'Mary', 'RE', ' bedroom', ':', ' bathroom', '<|end_header_id|>', 'NEW', 'b', 'athroom', '.\n\n', ' kitchen'], 'evidence_proportions': [0.002788490056991577, 0.0004165410995483398, 0.0006420264641443889, 0.000578656792640686, 0.00020481646060943604]}}, 28: {'grad': {'score': [0.2749372124671936, 0.36049073779428337, 0.302341365814209, 0.36082591672748165, 0.38092047168362525], 'topk_tokens': ['ot', ' Cedar', '.', '.', ' the', 'nes', 'nes', ' the', "'", '      ', 'ew', 'IO', 'nes', 'nes', '.', ' an', '      ', 'in', 'nes', 'dent'], 'evidence_proportions': [0.18037261962890624, 0.21219024658203126, 0.39533424377441406, 0.2617000341415405, 0.3042182922363281]}, 'weight': {'score': [0.009799178689718246, 0.002370729389612385, 0.005272006137030465, 0.0023477889510681834, 0.00022602754254494943], 'topk_tokens': [' bedroom', ' to', ' bathroom', '.\n\n', ' the', ' discarded', ' the', '?\n', '<|eot_id|>', 'Answer', '<|eot_id|>', ' the', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.011036211252212524, 0.002973860502243042, 0.00844991703828176, 0.022526979446411133, 0.00608062744140625]}, 'saliency': {'score': [0.00019686048229535422, 2.8222317873405537e-05, 0.00013448170253208705, 2.758562379165602e-05, 6.080314677248719e-06], 'topk_tokens': ['b', ':', '<|eot_id|>', ' Floral', ' Bridge', '.\n\n', ' the', 'Bridge', ' milk', '?\n', ' bedroom', ' the', 'athroom', ' Bridge', '<|start_header_id|>', '\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0003153443336486816, 3.650784492492676e-05, 0.00021825234095255533, 0.0002963542938232422, 0.00011761486530303955]}}, 29: {'grad': {'score': [0.42180609703063965, 0.32805401059088685, 0.4648747035435268, 0.3274769672782211, 0.2902954778363628], 'topk_tokens': ['itter', ' B', 'b', 'UL', 'adv', ' The', ' B', ' Paul', 'b', ' PA', ' to', 'ION', ',', ' The', ' M', '\n', ' book', 'UL', 're', 'b'], 'evidence_proportions': [0.58690185546875, 0.5268341064453126, 0.38407039642333984, 0.38320159912109375, 0.17935943603515625]}, 'weight': {'score': [0.004376581559578578, 0.0024245559289033897, 0.002763150419507708, 0.0024197437119573856, 0.00026309682476905084], 'topk_tokens': [' in', '.', ' Does', ' was', ' the', ' the', '<|eot_id|>', ' the', '<|eot_id|>', '.\n\n', '?\n', 'Answer', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008773565292358398, 0.0028363823890686034, 0.004351591070493063, 0.013444781303405762, 0.0016451478004455566]}, 'saliency': {'score': [0.00014928107460339865, 3.234614037841065e-05, 9.872147015162876e-05, 3.192561447551982e-05, 1.035774907758159e-05], 'topk_tokens': [' milk', '<|eot_id|>', ' the', '.', '.', 'IVE', '      ', '<|eot_id|>', 'Answer', 'NEW', ' Does', '<|start_header_id|>', '<|end_header_id|>', ' was', ':', 'assistant', 'athroom', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [4.83393669128418e-05, 0.0001243770122528076, 0.0002099821964899699, 0.0003181174397468567, 4.6700239181518555e-05]}}, 30: {'grad': {'score': [0.3489061991373698, 0.34515815828853535, 0.34169910975864953, 0.345160709011455, 0.35566694505753055], 'topk_tokens': [' his', 'RI', ' soon', ' Buchanan', ' bouncing', 'b', ' S', '3', ' account', 'deal', 'itter', ' Burb', ' forb', ' B', 'B', 'b', 'b', ' B', 'b', ' B'], 'evidence_proportions': [0.3813385009765625, 0.28355712890625, 0.2653299967447917, 0.50054931640625, 0.3637733459472656]}, 'weight': {'score': [0.012490722040335337, 0.0024551046296298878, 0.007714181286948067, 0.0024202692057191592, 0.0011482777134064705], 'topk_tokens': [':', ' Where', ' milk', ' the', ' the', 'Question', '<|eot_id|>', ' bedroom', '<|eot_id|>', '.\n\n', 'b', 'Answer', '?\n', 'assistant', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010739493370056153, 0.0066359460353851325, 0.011989906430244446, 0.030009984970092773, 0.0052301883697509766]}, 'saliency': {'score': [0.0010771056016286213, 6.162749230010655e-05, 0.0003863845552716936, 5.8697670048271015e-05, 1.872066528566422e-05], 'topk_tokens': ['E', ' location', '<|eot_id|>', ' Wide', ' office', ' Bridge', ' Bench', ' kitchen', '?\n', 'assistant', ' garden', ' the', 'Answer', ' bathroom', '<|end_header_id|>', ':', '<|begin_of_text|>', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0037365078926086424, 0.00014535188674926757, 0.00029144684473673504, 0.0009915530681610107, 0.00018158555030822754]}}, 31: {'grad': {'score': [0.40888450543085736, 0.48101605822038834, 0.4103559766496931, 0.48136072620874076, 0.3769357518483234], 'topk_tokens': ['IT', 'did', ' the', ' its', ' the', 'membership', ' their', ' location', ' the', ' have', ' had', ' the', ' Van', ' office', ' of', ' was', ' had', ' he', ' the', ' the'], 'evidence_proportions': [0.3287467002868652, 0.41870851516723634, 0.38439714908599854, 0.44191408157348633, 0.500478208065033]}, 'weight': {'score': [0.002940855920314789, 0.002258250673861588, 0.0014599689415522984, 0.002259198151703525, 0.00042669042464225525], 'topk_tokens': [' was', ' milk', ',', 'Question', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', 'b', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0012939035892486571, 0.0023688793182373045, 0.0014801472425460815, 0.006774723529815674, 0.004071712493896484]}, 'saliency': {'score': [4.2510529359181724e-05, 2.909213623040028e-05, 3.00424439566476e-05, 2.9063008323734483e-05, 4.256925275248866e-06], 'topk_tokens': ['Mary', ' bedroom', ' prior', ' Do', ' Where', ' left', 'Just', ' milk', ' location', ' the', '<|start_header_id|>', '<|eot_id|>', '?\n', ':', 'Answer', '<|begin_of_text|>', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [2.6518106460571288e-05, 5.089640617370605e-05, 2.47955322265625e-05, 5.822628736495972e-05, 6.287544965744019e-05]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:14:55.138 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:14:55.138 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-2_0-2-3-8-9.pkl | len: 10 |  size: 9.44 KB
Processing depth (0, 2, 3, 8, 9):   3%|▎         | 3/100 [00:55<30:04, 18.60s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (0, 1, 2, 5, 6):   3%|▎         | 3/100 [01:03<30:04, 18.60s/it]2025-01-22 03:15:02.716 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:15:02.717 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:15:02.717 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the milk.
2025-01-22 03:15:02.721 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1536, 1541) --> . Mary picked up the
2025-01-22 03:15:02.722 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:15:02.729 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (2505, 2511) --> . Mary journeyed to the
2025-01-22 03:15:02.729 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:15:02.746 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6002, 6006) -->  Mary left the milk
2025-01-22 03:15:02.746 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel dropped the football.
2025-01-22 03:15:02.766 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7276, 7280) -->  Daniel dropped the football
2025-01-22 03:15:02.766 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:15:02.777 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3943, 3948) --> . John moved to the
2025-01-22 03:15:02.778 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the office.
2025-01-22 03:15:02.784 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2228, 2234) --> . John went back to the
2025-01-22 03:15:02.784 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel took the football.
2025-01-22 03:15:02.804 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7183, 7187) -->  Daniel took the football
2025-01-22 03:15:02.804 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:15:02.821 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6397, 6401) -->  left the apple.
2025-01-22 03:15:02.821 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:15:02.834 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4554, 4560) -->  Sandra journeyed to the office
2025-01-22 03:15:02.834 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 03:15:02.848 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4878, 4883) --> . Daniel picked up the
2025-01-22 03:15:02.848 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 03:15:02.861 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (4558, 4563) -->  the office. Sandra moved
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:15:04.939 | INFO     | test_jbb_retain:begin_test:632 - the bathroom<|eot_id|>
2025-01-22 03:15:04.940 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12239])
your chose emoji: ['👨🏾\u200d🦯\u200d➡️', '💭', '🇹🇨', '👃🏾', '◼️', '👨🏻\u200d🍼', '🧑🏾\u200d🍳', '👉🏻', '🤰🏾', '👨🏽\u200d❤️\u200d👨🏿']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 205855.41it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.04s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.34s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.83it/s][A100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.55it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.36it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.02it/s][A100%|██████████| 8/8 [00:00<00:00, 21.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.39it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.99it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.79it/s][A100%|██████████| 8/8 [00:00<00:00, 21.07it/s]
2025-01-22 03:15:13.880 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2208231290181478, 0.1854578500245058, 0.22104058946881974, 0.18528595783240218, 0.3007700015337039], 'topk_tokens': [' press', ' latter', ' private', ' compromised', ' turtle', ' and', ' fight', ' port', ' bay', 'oub', ' first', ' first', ' or', 'ols', ' minds', '185', ' first', 'ong', 'oggle', '185'], 'evidence_proportions': [0.19843750000000002, 0.213519287109375, 0.2285168965657552, 0.20327091217041016, 0.263946533203125]}, 'weight': {'score': [0.05594481651981672, 0.0025700217192322185, 0.0048531293869018555, 0.0024583165691392642, 0.0008342548822745299], 'topk_tokens': [' Mary', 'From', 'Mary', '.', '\n\n', ' milk', 'Answer', ' the', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'b', ' bedroom', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.1980609893798828, 0.011446350812911987, 0.013893117507298788, 0.051229000091552734, 0.0017160475254058838]}, 'saliency': {'score': [0.0026701626678307853, 2.3204813185051326e-05, 0.000305553844996861, 1.71792690149035e-05, 1.4948921325879219e-05], 'topk_tokens': [':', ' location', ' Bench', ' milk', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ' kitchen', 'From', '\n\n', ' garden', ' office', '<|eot_id|>', ' milk', '<|eot_id|>', '<|begin_of_text|>', ' bedroom', 'b', 'athroom', ' bathroom'], 'evidence_proportions': [0.010646188259124756, 0.0004644453525543213, 0.0002327263355255127, 0.0017665699124336243, 1.7024576663970947e-05]}}, 25: {'grad': {'score': [0.3759339650472005, 0.6761996115447844, 0.3414451599121094, 0.6777528235059668, 0.24757760610335913], 'topk_tokens': [' old', ' old', ' a', ' some', ' of', ' with', ' with', ' the', 'posit', ' with', ' required', 'po', ' over', ' a', ' no', ' under', ' the', ' for', ' of', ' no'], 'evidence_proportions': [0.5227783203125, 0.3898529052734375, 0.38343683878580725, 0.3230762481689453, 0.216583251953125]}, 'weight': {'score': [0.014939850817124048, 0.002491660181203517, 0.0029066545622689383, 0.0024659455478127775, 0.0006594623510654157], 'topk_tokens': [' Mary', 'MIN', ' the', ' prior', '.\n\n', ' Bench', '?\n', 'Answer', 'Mary', ' bathroom', 'b', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.031760025024414065, 0.009313994646072387, 0.008424540360768637, 0.02505093812942505, 0.0006088316440582275]}, 'saliency': {'score': [0.0004178533951441447, 2.658325041300564e-05, 7.048249244689941e-05, 2.5686348422959376e-05, 1.1920546874021873e-05], 'topk_tokens': ['<|start_header_id|>', ' the', 'RE', ' the', 'Answer', '.\n\n', ' milk', ' to', ':', ' PA', 'Mary', 'MIN', ' Bench', ' bathroom', ' THE', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0009802758693695068, 0.00018593072891235352, 0.0001818289359410604, 0.0007717162370681763, 4.902482032775879e-06]}}, 26: {'grad': {'score': [0.24143290519714355, 0.22324059356604695, 0.2450606209891183, 0.2231420696853153, 0.2389544951610076], 'topk_tokens': [' West', '-in', ' printer', 'reader', ' Empire', 'is', 'agle', 'untlet', ' and', 'rich', ' expedition', ' Commander', 'er', ' Eagle', 'er', ' commander', 'ers', ' Moore', ' Marshall', 'ers'], 'evidence_proportions': [0.1765350341796875, 0.2574459075927734, 0.31734339396158856, 0.2578315734863281, 0.17227458953857422]}, 'weight': {'score': [0.027224640051523846, 0.0024355916567165036, 0.0014970132282802037, 0.0023894546693997423, 0.0009046632509965163], 'topk_tokens': [' the', ' kitchen', ' discarded', ' the', ' the', '.\n\n', ' bedroom', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'assistant', 'b', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.1007398247718811, 0.003492635488510132, 0.004933113853136699, 0.0245436429977417, 0.0011139512062072754]}, 'saliency': {'score': [0.0007559085885683695, 3.133808168159788e-05, 3.845606531415667e-05, 2.9890259175448156e-05, 1.8014739721249312e-05], 'topk_tokens': ['MIN', 'Mary', ' Where', ' Floral', '.', ' the', '<|eot_id|>', 'Bridge', 'assistant', ' bedroom', 'Answer', ' kitchen', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0033549189567565916, 2.4759769439697268e-05, 4.0774544080098466e-05, 0.00023301690816879272, 1.6674399375915527e-05]}}, 27: {'grad': {'score': [0.4059538046518962, 0.38648976595684936, 0.3712763581957136, 0.386495128539379, 0.24132804992871407], 'topk_tokens': ['?"', ' execution', 'direction', ' *\n\n', ' *\n\n', ' short', '.', ' installed', ' designated', ' step', '!"', '."', 'roduced', 'ition', ' intended', ' *\n\n', ' consultations', '!"', ' assistance', ' accepted'], 'evidence_proportions': [0.4718547821044922, 0.39774932861328127, 0.33067957560221356, 0.4528656005859375, 0.39983272552490234]}, 'weight': {'score': [0.05068576708436012, 0.002521120008154839, 0.0030423223972320555, 0.0024247403304525797, 0.0010981055406423716], 'topk_tokens': [' kitchen', 'RE', '<|eot_id|>', '<|eot_id|>', '?\n', 'Mary', ' THE', 'Answer', 'NEW', 'assistant', '.\n\n', ' bedroom', 'b', '\n\n', ':', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.20406959056854246, 0.008578819036483765, 0.008019169171651205, 0.02506434917449951, 0.0012109875679016113]}, 'saliency': {'score': [0.0014969346423943837, 3.289860402266003e-05, 0.00020894408226013184, 2.950876107271068e-05, 2.272465290167393e-05], 'topk_tokens': [' Mary', ' prior', ' to', ' THE', '<|end_header_id|>', ' the', 'RE', ' Bridge', ' the', ' kitchen', 'Mary', '<|start_header_id|>', 'NEW', ' bathroom', ' bedroom', '<|begin_of_text|>', 'athroom', ':', '.\n\n', 'b'], 'evidence_proportions': [0.003921246528625489, 0.0006618142127990722, 0.001124615470568339, 0.0015339329838752747, 3.1925737857818604e-05]}}, 28: {'grad': {'score': [0.37111353874206543, 0.38881107125284625, 0.35872650146484375, 0.3889323632763904, 0.3369424495941553], 'topk_tokens': ['.', '\n', '.', ' he', "'", ' the', ',', ' the', '.', ' in', '.', ' Cedar', ' the', 'ien', 'ien', 'nes', ' the', 'nes', 'arp', 'dent'], 'evidence_proportions': [0.42311553955078124, 0.30289154052734374, 0.42994054158528644, 0.3261551856994629, 0.34810638427734375]}, 'weight': {'score': [0.013514394561449686, 0.0023579140189347986, 0.004424468960080828, 0.0023299993053207077, 0.0007060552254701272], 'topk_tokens': [' to', ' the', '.\n\n', ' discarded', ' bedroom', ' bathroom', ' the', '?\n', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.01504833698272705, 0.0016544818878173827, 0.007661024729410807, 0.04640769958496094, 0.0023086071014404297]}, 'saliency': {'score': [0.000290505588054657, 2.033561541157364e-05, 8.70849405016218e-05, 1.9611630701601902e-05, 1.0695977088732598e-05], 'topk_tokens': ['.\n\n', ' Bridge', ' garden', ' Floral', ' the', '<|eot_id|>', '?\n', ' kitchen', ' bathroom', ' milk', '<|start_header_id|>', ' bedroom', ' the', ' Bridge', 'b', 'assistant', 'athroom', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005708217620849609, 4.600882530212402e-05, 9.453793366750081e-05, 0.000806853175163269, 2.333521842956543e-05]}}, 29: {'grad': {'score': [0.26086076100667316, 0.28389793462056856, 0.23961683000837053, 0.28407053010839256, 0.2683185094442123], 'topk_tokens': ['ocity', '<|eot_id|>', ' wholly', ' Good', ' B', ' book', 'd', 'an', ' were', 'assistant', ' not', ' B', ' B', '<|start_header_id|>', ' ga', 'Spring', 'b', ' arms', '\n', 'b'], 'evidence_proportions': [0.391845703125, 0.2604278564453125, 0.2002105712890625, 0.22583389282226562, 0.22367286682128906]}, 'weight': {'score': [0.0046544211606184644, 0.002437217546004015, 0.0015401244163513184, 0.0024354269667367654, 0.000628545880317688], 'topk_tokens': [' was', '.', 'NEW', ' Where', ' the', ' Does', '<|eot_id|>', '?\n', '.\n\n', '<|eot_id|>', ' the', 'Answer', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0025381028652191164, 0.0006567299365997315, 0.0062321871519088745, 0.014086484909057617, 0.0004982203245162964]}, 'saliency': {'score': [0.00023795167605082193, 3.586449015084054e-05, 5.0689492906842914e-05, 3.5423796761851e-05, 1.215208799411089e-05], 'topk_tokens': ['.', '<|eot_id|>', ' the', ' was', ' the', '      ', ' Does', 'IVE', ',', 'NEW', ' in', '<|end_header_id|>', 'assistant', 'athroom', ' the', ':', '<|start_header_id|>', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.777644157409667e-05, 5.112290382385254e-05, 0.00035381813844045, 0.0007349997758865356, 1.3358891010284424e-05]}}, 30: {'grad': {'score': [0.19620310266812643, 0.19287718255775774, 0.18501717703683035, 0.1928932112953908, 0.18020871052375206], 'topk_tokens': [' Fourth', ' the', ' Bridge', ' The', ' Note', ' The', ' account', ' the', ' Times', ' Third', ' Fourth', ' C', ' Roman', '-the', 'deal', ' City', ' City', ' The', ' the', 'ire'], 'evidence_proportions': [0.1891695022583008, 0.25403914451599124, 0.12431844075520834, 0.19159889221191406, 0.245131254196167]}, 'weight': {'score': [0.01490061730146408, 0.0024474818923469074, 0.0063664121287209646, 0.002411691216151233, 0.0025480584456370426], 'topk_tokens': [' the', ':', ' Where', ' kitchen', 'Question', ' bedroom', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', '?\n', 'Answer', 'assistant', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01260049343109131, 0.004159867763519287, 0.01363682746887207, 0.04688453674316406, 0.0011134743690490723]}, 'saliency': {'score': [0.0011059294144312541, 6.73402560300598e-05, 0.00026375991957528253, 6.472999353102742e-05, 3.223541455391126e-05], 'topk_tokens': [' prior', '.', ' office', 'Answer', 'Mary', 'Bridge', ' bedroom', 'assistant', '.\n\n', ' Bridge', ' the', ' garden', ' kitchen', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ':', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0032904744148254395, 0.00029672980308532716, 0.00032751262187957764, 0.0016365870833396912, 2.3715198040008545e-05]}}, 31: {'grad': {'score': [0.19328182439009348, 0.1928828965018649, 0.21277431164469038, 0.19282496546687217, 0.12241612756863618], 'topk_tokens': [' the', ' location', 'membership', 'If', ' office', ' number', ' the', ' the', ' they', ' location', ' having', ' the', ' January', ' instead', ' the', ' the', ' the', ' population', ' that', ' the'], 'evidence_proportions': [0.24049100875854493, 0.23969359397888185, 0.16214968760808307, 0.16977059841156006, 0.14646506309509277]}, 'weight': {'score': [0.0025207102298736572, 0.0022795961838379306, 0.0017418733664921352, 0.0022806659992776618, 0.0010592031937379104], 'topk_tokens': [' where', ' was', ',', 'Question', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015934288501739502, 0.001373147964477539, 0.0014892568190892537, 0.007712960243225098, 0.0014691948890686035]}, 'saliency': {'score': [3.691886862119039e-05, 1.4243219300357175e-05, 3.342458180018834e-05, 1.4143443935406499e-05, 7.611436721606132e-06], 'topk_tokens': ['<|eot_id|>', ' dropped', ' the', ' milk', ' the', 'Question', '<|eot_id|>', ' the', '.\n\n', ' bedroom', ' the', 'Answer', '?\n', '<|begin_of_text|>', 'b', '<|end_header_id|>', ':', 'assistant', 'athroom', '<|start_header_id|>'], 'evidence_proportions': [1.8906593322753906e-05, 1.2320280075073243e-05, 2.969801425933838e-05, 0.00011882185935974121, 1.911073923110962e-05]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-22 03:15:13.881 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:15:13.881 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-3_0-1-2-5-6.pkl | len: 10 |  size: 9.56 KB
Processing depth (0, 1, 2, 5, 6):   4%|▍         | 4/100 [01:14<29:51, 18.66s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (1, 4, 6, 7, 8):   4%|▍         | 4/100 [01:21<29:51, 18.66s/it]2025-01-22 03:15:21.372 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:15:21.377 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1530, 1535) --> . Mary moved to the
2025-01-22 03:15:21.377 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary picked up the milk.
2025-01-22 03:15:21.391 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4884, 4889) --> . Mary picked up the
2025-01-22 03:15:21.391 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:15:21.412 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7153, 7159) -->  Mary journeyed to the bedroom
2025-01-22 03:15:21.412 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:15:21.435 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8393, 8397) -->  Mary left the milk
2025-01-22 03:15:21.435 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel dropped the football.
2025-01-22 03:15:21.461 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9648, 9652) -->  Daniel dropped the football
2025-01-22 03:15:21.461 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:15:21.472 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3819, 3824) --> . John moved to the
2025-01-22 03:15:21.472 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John went back to the office.
2025-01-22 03:15:21.479 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2166, 2172) --> . John went back to the
2025-01-22 03:15:21.479 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel took the football.
2025-01-22 03:15:21.500 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7108, 7112) -->  Daniel took the football
2025-01-22 03:15:21.500 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:15:21.517 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (6246, 6250) -->  Daniel left the apple
2025-01-22 03:15:21.517 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:15:21.530 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (4497, 4503) -->  business. Sandra journeyed to
2025-01-22 03:15:21.530 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 03:15:21.544 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4819, 4824) --> . Daniel picked up the
2025-01-22 03:15:21.544 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 03:15:21.557 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 6 at --> (4503, 4508) -->  the office. Sandra moved
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:15:23.569 | INFO     | test_jbb_retain:begin_test:632 - The bathroom<|eot_id|>
2025-01-22 03:15:23.569 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12237])
your chose emoji: ['👩🏽\u200d🦯\u200d➡', '📓', '🙎🏽\u200d♀️', '🧜🏽', '🚵🏼\u200d♀️', '🎸', '🧑🏿\u200d🦼\u200d➡', '👨\u200d⚖️', '🙇🏽\u200d♂', '🆓']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 222214.78it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.71s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.26s/it][A
 75%|███████▌  | 6/8 [00:04<00:01,  1.95it/s][A100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.67it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.29it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.10it/s][A100%|██████████| 8/8 [00:00<00:00, 21.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.41it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.14it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.79it/s][A100%|██████████| 8/8 [00:00<00:00, 21.09it/s]
2025-01-22 03:15:32.086 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.39223893483479816, 0.26325334536483863, 0.3296868085861206, 0.26280832234866397, 0.40887517834964554], 'topk_tokens': [' absence', ' press', ' S', ' agreement', 'Dr', ' Paul', 'user', 'Penn', ' Dr', ' first', 'Mr', '�', ' first', 'Mal', '�', 'st', 'Mr', ' Paul', ' Hor', ' whistle'], 'evidence_proportions': [0.3546142578125, 0.48032073974609374, 0.3057454427083333, 0.39369964599609375, 0.4574470520019531]}, 'weight': {'score': [0.01595360040664673, 0.0025631557882221695, 0.005076465436390468, 0.0025295512805029283, 0.001119392874993776], 'topk_tokens': [' battle', 'CH', ' old', ' barric', ' milk', 'Answer', '.', ' Bridge', ' bedroom', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', ' bathroom', '\n\n', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004257965087890625, 0.01884880065917969, 0.031865715980529785, 0.016400694847106934, 0.002638876438140869]}, 'saliency': {'score': [0.0012506134808063507, 4.2818803218455095e-05, 0.00041155729975019185, 3.937960121199255e-05, 6.630977517680119e-05], 'topk_tokens': [' battle', ' I', '.', '<|eot_id|>', ' Paul', ' B', '<|end_header_id|>', '<|eot_id|>', ' Dan', 'assistant', ' Bench', ' office', 'Bridge', '<|start_header_id|>', '<|begin_of_text|>', ' Bridge', ' bedroom', 'athroom', 'b', ' bathroom'], 'evidence_proportions': [0.00019649267196655273, 0.0012096762657165527, 0.0032920738061269126, 0.0007251650094985962, 8.269399404525757e-05]}}, 25: {'grad': {'score': [0.5633290608723959, 0.65656147875817, 0.5466748918805804, 0.6570609130057665, 0.4782024684705232], 'topk_tokens': [' of', ' inverted', ' little', ' incorporated', ' with', ' old', ' a', 'M', ' hate', ' a', ' for', ' black', '800', ' if', ' money', ' for', ' no', 'ivery', ' free', ' lie'], 'evidence_proportions': [0.650238037109375, 0.5724609375, 0.5888468424479167, 0.576751708984375, 0.39157867431640625]}, 'weight': {'score': [0.016917376468578976, 0.002488447248546127, 0.004080669369016375, 0.002455443219689937, 0.0010651569617422002], 'topk_tokens': [' discarded', ' Dan', ' barric', ' Bench', ' bathroom', ' Mary', 'Answer', ' Paul', '?\n', 'b', '.', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.007190042734146119, 0.036041641235351564, 0.0176333487033844, 0.0187147855758667, 0.0022998452186584473]}, 'saliency': {'score': [0.0013871739308039348, 3.7275461686982046e-05, 9.150334766932896e-05, 3.445996712428695e-05, 3.005524999216983e-05], 'topk_tokens': [' Paul', ' Mary', '<|eot_id|>', ' bathroom', 'Answer', ' Anthony', ' Mary', ' Mary', ' Miss', ' St', ' Mary', '<|eot_id|>', 'b', '.', ' Dan', ' Bench', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008262217044830323, 0.002588814496994019, 0.0012780825297037761, 0.002069316804409027, 6.780773401260376e-05]}}, 26: {'grad': {'score': [0.43381690979003906, 0.4738168753829657, 0.44416299547467913, 0.4739808918816949, 0.5622609289068925], 'topk_tokens': [' soon', ' when', ' man', '�', ' bitter', 'hue', ' West', 'news', ' and', 'watch', 'UX', 'graph', 'UX', ' week', ',', ' Press', 'graph', 'graph', 'ub', ' Press'], 'evidence_proportions': [0.45037841796875, 0.59598388671875, 0.3021138509114583, 0.2927970886230469, 0.548980712890625]}, 'weight': {'score': [0.011562090367078781, 0.002476823719498379, 0.0020290894167763847, 0.0024602096731190454, 0.0006509987931502493], 'topk_tokens': ['.\n\n', ' Anthony', 'CH', ' Bridge', ' bedroom', '?\n', '<|eot_id|>', ' the', '<|eot_id|>', 'Answer', ' barric', 'assistant', ' bathroom', 'b', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008673727512359619, 0.009910202026367188, 0.02435159683227539, 0.017669200897216797, 0.0017039775848388672]}, 'saliency': {'score': [0.00048464909195899963, 5.879136277180092e-05, 0.00011082461902073451, 5.780279455332908e-05, 3.848734654878315e-05], 'topk_tokens': ['IR', ' AC', '.', ' battle', ' Jackson', ' Merch', ' barric', ' bedroom', 'CH', ' East', ' Bridge', ' Father', '<|end_header_id|>', '<|start_header_id|>', ' bathroom', 'athroom', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [6.517171859741211e-05, 0.0005347490310668946, 0.0010566959778467813, 0.0005476176738739014, 2.5331974029541016e-05]}}, 27: {'grad': {'score': [0.2910722295443217, 0.32552332659952, 0.2893862588065011, 0.3256950385855705, 0.3031587600708008], 'topk_tokens': ['\n', ' Bre', ' Bor', ' Thanksgiving', ' one', '\n', '\n', ' successful', 'IR', '\n', ' several', ' designated', '\n', ' dre', ' accepted', '\n', 'ree', '\n', ' Bottle', ' step'], 'evidence_proportions': [0.20417261123657227, 0.3338340759277344, 0.24737548828125, 0.40961456298828125, 0.2932472229003906]}, 'weight': {'score': [0.012219734489917755, 0.0025260351841745812, 0.0035149071897779193, 0.002504094513988722, 0.0007254226427329214], 'topk_tokens': ['<|eot_id|>', 'THE', 'IR', ' bedroom', '<|eot_id|>', '?\n', ' barric', 'Answer', ' Bridge', 'assistant', 'CH', '.\n\n', 'b', '\n\n', ' bathroom', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0024624228477478026, 0.013143110275268554, 0.023203253746032715, 0.01587587594985962, 0.0031307339668273926]}, 'saliency': {'score': [0.0011007984479268391, 5.2829734445397374e-05, 0.0002428182533809117, 5.0219041785821176e-05, 3.1225775417528655e-05], 'topk_tokens': [' St', ' Mary', ' old', ' Mary', ' Anthony', 'THE', ' NEW', ' AC', ' Mary', '<|begin_of_text|>', 'CH', '\n\n', ':', ' Bridge', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', '.\n\n', 'athroom', 'b'], 'evidence_proportions': [0.0004555463790893555, 0.0009187400341033935, 0.001085887352625529, 0.003104947507381439, 0.0001531541347503662]}}, 28: {'grad': {'score': [0.4428933461507161, 0.5059315002042484, 0.3550255366734096, 0.5064893053451124, 0.43454062311272873], 'topk_tokens': [',', ' the', ' the', ' lie', 'ot', ' the', 'E', '\n', ' lie', 'ien', '.', 'in', 'en', 'nes', 'S', ',', 'ew', ' a', '.', 'dent'], 'evidence_proportions': [0.40205078125, 0.49714202880859376, 0.5251172383626302, 0.41191864013671875, 0.3337745666503906]}, 'weight': {'score': [0.00882718712091446, 0.0024011608042748146, 0.0019975347178322928, 0.0023896594728099216, 0.00035871133992546484], 'topk_tokens': [' was', ' milk', ' Bridge', ' discarded', ' the', ' Bridge', 'Answer', '?\n', '<|eot_id|>', ' bathroom', ' the', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00032967329025268555, 0.002787208557128906, 0.021969318389892578, 0.012395143508911133, 0.0037178993225097656]}, 'saliency': {'score': [0.0002092309296131134, 3.956533919752034e-05, 5.93219484601702e-05, 3.917428300392646e-05, 8.944618074517502e-06], 'topk_tokens': [' St', '?\n', '.\n\n', ' nearly', ' milk', ' St', ' St', ' St', 'athroom', 'b', '.', ' St', ' ST', 'assistant', ' Bridge', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.895427703857422e-05, 0.0003278613090515137, 0.0004012236992518107, 0.00015144795179367065, 6.858259439468384e-05]}}, 29: {'grad': {'score': [0.3470375935236613, 0.31600470449410234, 0.3108863285609654, 0.31595826773365165, 0.2827711481797068], 'topk_tokens': ['super', 'isc', ' M', ' book', 'assistant', ' But', ' body', 'P', 're', ' Pioneer', 'ION', 'ball', 'itter', ' B', ' B', 'Spring', ' B', 'b', ' ga', 'b'], 'evidence_proportions': [0.53817138671875, 0.3347320556640625, 0.3809990882873535, 0.2198333740234375, 0.19976425170898438]}, 'weight': {'score': [0.006425981720288594, 0.00247061260385451, 0.0013793792043413436, 0.0024659548836499733, 0.0007540646352266011], 'topk_tokens': [' milk', ' discarded', ' Does', 'ot', ' the', ' the', '<|eot_id|>', '.\n\n', ' the', '?\n', 'Answer', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00016853809356689455, 0.006017208099365234, 0.012723545233408611, 0.010762214660644531, 0.00097617506980896]}, 'saliency': {'score': [0.0003674427668253581, 4.4510494065440556e-05, 7.534452847072057e-05, 4.3785630281644254e-05, 2.707423348175852e-05], 'topk_tokens': ['.', 'aha', '186', 'am', 'THE', '<|eot_id|>', ' Does', ' the', 'THE', ' the', '<|eot_id|>', 'Answer', 'assistant', ':', '<|end_header_id|>', 'athroom', '<|start_header_id|>', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [6.633996963500976e-06, 0.0003631293773651123, 0.0007921159267425537, 0.0005360767245292664, 1.8201768398284912e-05]}}, 30: {'grad': {'score': [0.31677834192911786, 0.3399667029287301, 0.2727903093610491, 0.34020541029584767, 0.3329222704234876], 'topk_tokens': ['ar', 'b', ' United', ' need', ' W', ' itself', ' an', '186', ' months', ' the', ' account', 'moment', ' Europe', 'ire', ' the', ' of', '2', ' forb', ' account', 'deal'], 'evidence_proportions': [0.294207763671875, 0.41986389160156246, 0.20485242207845053, 0.3397979736328125, 0.3610038757324219]}, 'weight': {'score': [0.013792619109153748, 0.002449979969099456, 0.003461107185908726, 0.00242472647661529, 0.0020086506479664854], 'topk_tokens': [' the', 'CH', ' Miles', ' the', 'IR', ' Anthony', ' NEW', '.\n\n', '<|eot_id|>', 'Answer', 'assistant', '<|eot_id|>', '?\n', 'b', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0021894216537475582, 0.009705924987792968, 0.025671839714050293, 0.024675846099853516, 0.004702925682067871]}, 'saliency': {'score': [0.0009399627645810446, 5.574098307322832e-05, 0.00010977302278791155, 5.384356544362472e-05, 5.489352502320942e-05], 'topk_tokens': [' the', ' Broadway', ' Bridge', 'assistant', 'CH', '.', 'IR', ' St', ' St', ' St', '.\n\n', ' St', ' Mary', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', 'athroom', '<|start_header_id|>', ':', 'b'], 'evidence_proportions': [0.00022449493408203126, 0.0005258738994598389, 0.0015283028284708657, 0.0023217201232910156, 8.764117956161499e-05]}}, 31: {'grad': {'score': [0.35896530747413635, 0.4217588287552977, 0.3838296719959804, 0.4219915325560796, 0.18242694398290232], 'topk_tokens': [' the', ' was', ' the', ' location', 'did', ' the', ' that', ' the', '2', '7', ' location', ' location', ' the', ' location', ' location', 'membership', ' he', ' August', ' the', ' the'], 'evidence_proportions': [0.3672147750854492, 0.22494635581970215, 0.41083335876464844, 0.38561344146728516, 0.4117269515991211]}, 'weight': {'score': [0.003359246999025345, 0.002304592864965302, 0.0010540221418653215, 0.0023061081983608406, 0.0007852002194053249], 'topk_tokens': ['CH', ' was', 'Question', ':', ',', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011766135692596435, 0.001175856590270996, 0.006746570269266764, 0.004463076591491699, 0.002631962299346924]}, 'saliency': {'score': [7.940952976544698e-05, 2.63711693240147e-05, 1.6819579260689873e-05, 2.629411366369305e-05, 8.563069920790823e-06], 'topk_tokens': [' Bre', 'light', ' S', ' B', ':', ' Emily', ' the', 'Answer', ' Market', 'CH', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '?\n', ':', '<|begin_of_text|>', '<|end_header_id|>', 'b', 'athroom', 'assistant'], 'evidence_proportions': [4.7039985656738284e-05, 1.0496377944946289e-05, 0.00018551945686340332, 0.00010944902896881104, 1.6808509826660156e-05]}}, 'pred_res': 'The bathroom<|eot_id|>', 'score': 100}
2025-01-22 03:15:32.087 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:15:32.088 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-4_1-4-6-7-8.pkl | len: 10 |  size: 9.39 KB
Processing depth (1, 4, 6, 7, 8):   5%|▌         | 5/100 [01:32<29:17, 18.50s/it]Processing depth (1, 4, 6, 7, 8):   5%|▌         | 5/100 [01:32<29:23, 18.57s/it]
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:72 - Selected idx: 12
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:76 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John went back to the office.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel journeyed to the kitchen.']
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel journeyed to the kitchen.']
2025-01-22 03:15:32.297 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 4, 8, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:15:39.136 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:15:39.136 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:15:39.137 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:15:39.151 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4868, 4874) -->  war. Mary journeyed to
2025-01-22 03:15:39.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:15:39.177 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9640, 9644) -->  left the apple.
2025-01-22 03:15:39.177 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 03:15:39.208 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10779, 10785) --> . Daniel journeyed to the
2025-01-22 03:15:39.208 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:15:39.232 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8882, 8887) --> . John moved to the
2025-01-22 03:15:39.232 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:15:39.239 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2273, 2279) -->  Sandra journeyed to the office
2025-01-22 03:15:39.239 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:15:39.269 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11176, 11181) --> . Daniel picked up the
2025-01-22 03:15:39.270 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 03:15:39.281 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4088, 4093) --> . Sandra moved to the
2025-01-22 03:15:39.281 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:15:39.310 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10122, 10128) --> . John went back to the
2025-01-22 03:15:39.310 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:15:39.338 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10552, 10556) -->  Daniel left the apple
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:15:41.450 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:15:41.451 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12216])
your chose emoji: ['👱🏻\u200d♀', '🔒', '👨🏼\u200d❤️\u200d👨🏻', '\U0001faf1🏽\u200d\U0001faf2🏻', '🇨🇫', '🧍🏿\u200d♀️', '🦌', '🇦🇹', '👩\u200d🦰', '💟']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 262144.00it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.32s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.05s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.98it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.74it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.96it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.98it/s][A100%|██████████| 8/8 [00:00<00:00, 21.12it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.76it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.63it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.93it/s][A100%|██████████| 8/8 [00:00<00:00, 20.90it/s]
2025-01-22 03:15:51.048 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.26804860432942706, 0.318039698436477, 0.285194335445281, 0.31820966796298106, 0.3508008016298895], 'topk_tokens': ['.', ' black', ' whistle', ' state', ' Arr', '\n', ' communication', ' compl', ' senate', ' out', ' communication', '.', '�', ' out', 'ang', ' agreement', '�', ' whistle', ' turtle', ' Do'], 'evidence_proportions': [0.2664764404296875, 0.3012371063232422, 0.20198440551757812, 0.2802130381266276]}, 'weight': {'score': [0.021394816182908557, 0.002575640807575988, 0.018827848857448946, 0.0025017506840921346, 0.0007743247567790828], 'topk_tokens': [' the', ' the', ' the', ' barric', ' office', ' bathroom', 'Answer', ' bedroom', ' garden', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', 'Bridge', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.031756019592285155, 0.005640789866447449, 0.03383064270019531, 0.020223955313364662]}, 'saliency': {'score': [0.0012955452714647566, 4.625549458064232e-05, 0.0005536685066838418, 4.280642014242702e-05, 1.897591434112967e-05], 'topk_tokens': ['user', 'b', ' top', ' kitchen', '\n\n', ':', '<|start_header_id|>', '<|eot_id|>', ' kitchen', ' Bridge', '<|begin_of_text|>', '<|eot_id|>', ' Bridge', ' Bench', ' bedroom', ' bathroom', ' office', ' garden', 'Bridge', 'athroom'], 'evidence_proportions': [0.0034916460514068607, 0.0004502783219019572, 0.0008844658732414246, 0.0005847811698913574]}}, 25: {'grad': {'score': [0.6816784086681548, 0.8491330819957648, 0.5805193685716198, 0.8501064997039943, 0.6404544360017124], 'topk_tokens': [' for', ' for', ' a', ' o', ' old', ' F', ' a', ' the', ' ', ' M', ' no', ' for', ' a', ' inverted', ' of', ' locom', ' favored', 'ivery', ' l', ' no'], 'evidence_proportions': [0.7418212890625, 0.6283365885416666, 0.74822998046875, 0.640533447265625]}, 'weight': {'score': [0.015207082033157349, 0.0025017467748222224, 0.012357273409443518, 0.0024547069650007135, 0.0006499902842796012], 'topk_tokens': [' garden', '.', ' Daniel', ' apple', ' Daniel', ' apple', '.\n\n', ' Bench', 'b', ' \n', 'Answer', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006715846061706544, 0.006694436073303223, 0.030644893646240234, 0.020503883560498554]}, 'saliency': {'score': [0.0010210076967875163, 4.278616181703039e-05, 0.000581982635682629, 3.972396547247431e-05, 1.953848420757137e-05], 'topk_tokens': ['.', 'Answer', '.', ' Mary', 'b', ' Geo', ' Daniel', '.\n\n', ' Daniel', '<|eot_id|>', ':', 'assistant', ' apple', '<|eot_id|>', ' apple', ' Bench', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000409930944442749, 0.000448534886042277, 0.00314447283744812, 0.0006870677073796589]}}, 26: {'grad': {'score': [0.5772131057012648, 0.6033317452903777, 0.5466493175875756, 0.6035212453060067, 0.5725907887497993], 'topk_tokens': [' unab', ' several', 'rich', 'bec', 'b', 'ub', 'graph', 'graph', 'pro', 'UX', 'issippi', ' Milwaukee', 'UX', 'ier', ' Press', 'b', ' favor', 'b', ' bitter', 'itter'], 'evidence_proportions': [0.5392425537109375, 0.6807149251302084, 0.5836639404296875, 0.5010528564453125]}, 'weight': {'score': [0.021038755064918882, 0.0024779664191103908, 0.012830700605146347, 0.0024195533903170075, 0.00043996678639764657], 'topk_tokens': [' the', ' apple', '?', ' bathroom', ' kitchen', '.\n\n', ' barric', ' garden', '<|eot_id|>', 'Answer', '<|eot_id|>', ' \n', 'assistant', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.022313749790191652, 0.0025671273469924927, 0.045119285583496094, 0.02239420016606649]}, 'saliency': {'score': [0.0007794343289874849, 5.971223947625083e-05, 0.000392461976697368, 5.762220859488397e-05, 1.0788440704345703e-05], 'topk_tokens': [' old', ' the', ' Bridge', ' bathroom', '.', '<|eot_id|>', '?', ' \n', ' the', ' kitchen', ' kitchen', 'Bridge', '<|start_header_id|>', ' garden', 'athroom', '<|end_header_id|>', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0006639540195465089, 0.00018662710984547934, 0.001334547996520996, 0.0010983993609746296]}}, 27: {'grad': {'score': [0.2966188703264509, 0.4253246776600479, 0.3148688654745779, 0.4258282489702932, 0.3471766302030381], 'topk_tokens': ['ly', ' Franklin', 'lin', ' expedition', ' was', ' was', ' received', ' exchanged', '-n', ' Marshall', 'started', ' designated', ' started', ' business', ' Bottle', ' convention', ' sentinel', 'ides', ' step', ' accepted'], 'evidence_proportions': [0.3158477783203125, 0.32492828369140625, 0.35849761962890625, 0.21103286743164062]}, 'weight': {'score': [0.022791683673858643, 0.002535502896370167, 0.012539153137514669, 0.0024750530768745858, 0.0007027391701528471], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' apple', ' kitchen', ' bedroom', ' barric', ' \n', 'Answer', ' bathroom', ' garden', ' bedroom', 'assistant', '.\n\n', 'b', '\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04292716979980469, 0.004444291194279989, 0.03389394283294678, 0.016957998275756836]}, 'saliency': {'score': [0.0008414856025150844, 5.0868474087901296e-05, 0.001103394454525363, 4.6822179595377896e-05, 2.5630405504409582e-05], 'topk_tokens': [' barric', ' apple', ' garden', ' the', 'assistant', ' Bridge', ' bedroom', ' the', 'Bridge', ' the', ' \n', '<|start_header_id|>', ' the', '<|begin_of_text|>', ' kitchen', '<|end_header_id|>', 'b', ':', '.\n\n', 'athroom'], 'evidence_proportions': [0.00039124488830566406, 0.00014650821685791016, 0.002420000731945038, 0.0008593201637268066]}}, 28: {'grad': {'score': [0.32834825061616446, 0.37619888451770805, 0.28760479342552925, 0.3765072004654183, 0.3467092122117134], 'topk_tokens': [' the', ' the', '<|end_header_id|>', ' before', ' the', ' inside', ' as', '.', 'ipp', 'ot', 'nes', 'nes', '\n', ' as', '      ', ' half', ' returns', 'dent', 'in', 'ball'], 'evidence_proportions': [0.37281494140625, 0.36495304107666016, 0.24060440063476562, 0.3131837844848633]}, 'weight': {'score': [0.016177248387109665, 0.0023896676817563916, 0.015916995463832732, 0.0023314046459992793, 0.00039459622069580916], 'topk_tokens': [' the', '.\n\n', 'Answer', ' the', '<|eot_id|>', ' the', '?', ' apple', ' \n', ' garden', '<|eot_id|>', ' before', 'assistant', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.004605436325073242, 0.0027878781159718833, 0.02865886688232422, 0.03088871637980143]}, 'saliency': {'score': [0.0005425796622321719, 4.8506841867154845e-05, 0.00036592541202422113, 4.684533903962674e-05, 1.0199742774440817e-05], 'topk_tokens': [' bedroom', '.\n\n', ' \n', '?', ' bedroom', ' before', ' the', ' apple', 'b', ' Bridge', ' Bridge', 'assistant', 'Bridge', 'athroom', ' garden', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':'], 'evidence_proportions': [0.0003777742385864258, 9.023646513621013e-05, 0.0012640133500099182, 0.0006513049205144247]}}, 29: {'grad': {'score': [0.4805704752604167, 0.41980044406060746, 0.37484039798859625, 0.4198101087908644, 0.49292683274778604], 'topk_tokens': [' The', ' Bre', ' ga', 'ER', ' Paul', ',\n', 'y', 'y', 'UL', ' a', ' Ch', ' THE', 'ION', 're', ' In', ' M', ' The', ' In', ' THE', ' The'], 'evidence_proportions': [0.3582183837890625, 0.5194091796875, 0.6668128967285156, 0.41953023274739587]}, 'weight': {'score': [0.010013578903107416, 0.00245524003000538, 0.008506601856600854, 0.0024267763715061937, 0.0005233132675902484], 'topk_tokens': [' apple', ' Does', '.', ' the', '<|eot_id|>', ' before', ' the', '?', ' \n', '<|eot_id|>', '.\n\n', 'Answer', 'b', 'assistant', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007134079933166504, 0.0037348916133244834, 0.02609395980834961, 0.01332215468088786]}, 'saliency': {'score': [0.00037788635208493187, 3.8247528360268326e-05, 0.000215055481080086, 3.721083387251191e-05, 3.072088711882291e-05], 'topk_tokens': ['Does', '.', ' \n', '<|eot_id|>', ' the', '.', ' Does', 'assistant', '      ', ' ', 'Answer', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ' the', ':', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.9528141021728514e-05, 0.00018334885438283285, 0.0013994649052619934, 0.00018166999022165936]}}, 30: {'grad': {'score': [0.3784183320545015, 0.3863959365121685, 0.44501209259033203, 0.38626035903663536, 0.4279825393467733], 'topk_tokens': [' to', ' had', ' his', ' soon', ' S', '2', 'b', ' Burb', ' the', 'b', ' B', ' its', ' atmosphere', ' B', ' forb', '�', ' account', 'deal', '3', 'b'], 'evidence_proportions': [0.33165893554687503, 0.38307698567708337, 0.6042938232421875, 0.2621421813964844]}, 'weight': {'score': [0.018638064463933308, 0.0024534622418411196, 0.013971164341895811, 0.0023961822704623385, 0.0018255788169495048], 'topk_tokens': [' the', '.', ' the', 'Question', ' barric', ' garden', '?', '<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', 'assistant', 'b', ' \n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0076385498046875, 0.008410091201464335, 0.03715324401855469, 0.025688846906026207]}, 'saliency': {'score': [0.0008469521999359131, 7.682103105453415e-05, 0.0004853615837712442, 7.445088955040601e-05, 3.3197337633942906e-05], 'topk_tokens': ['?', ' kitchen', ' the', ' Bridge', ' bathroom', 'Bridge', '.', ' the', ' Bench', ' Bridge', ' barric', ' bedroom', 'assistant', ' the', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', ':', 'athroom', 'b'], 'evidence_proportions': [0.0011363625526428223, 0.000299726923306783, 0.0013292953372001648, 0.0008314400911331177]}}, 31: {'grad': {'score': [0.30317422889527823, 0.35067538167677353, 0.2926097906404926, 0.35090531161270977, 0.18172571634593077], 'topk_tokens': [' the', ' the', ' their', ' the', ' the', ' they', ' THE', ' the', ' number', 'membership', ' location', '7', ' was', ' had', ' location', ' he', ' the', ' the', ' the', ' the'], 'evidence_proportions': [0.2695268630981445, 0.24544153610865277, 0.32839369773864746, 0.3721334139506022]}, 'weight': {'score': [0.004146852663585118, 0.0022570061911946575, 0.0039000540010390742, 0.0022495580726752708, 0.00072440505027771], 'topk_tokens': ['.', ':', ' before', '.\n\n', ' Where', '<|eot_id|>', ' bedroom', '?', 'Answer', ' the', ' \n', 'b', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001036304235458374, 0.002529819806416829, 0.007150769233703613, 0.006353398164113363]}, 'saliency': {'score': [0.00011898790087018694, 2.0070487304232945e-05, 9.32716554211032e-05, 1.971325036032664e-05, 8.481006099753183e-06], 'topk_tokens': [' Daniel', ':', '?', ' apple', ' Market', ' bedroom', ' the', ' the', ' was', '<|eot_id|>', ' apple', 'b', ' \n', ' the', ' bedroom', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [2.1129846572875977e-05, 7.989505926767984e-05, 0.00023515522480010986, 0.0001621842384338379]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:15:51.049 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:15:51.049 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-0_0-4-8-9.pkl | len: 10 |  size: 9.03 KB
Processing depth (0, 4, 8, 9):   1%|          | 1/100 [00:18<30:46, 18.65s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]
Processing depth (0, 5, 7, 8):   1%|          | 1/100 [00:25<30:46, 18.65s/it]2025-01-22 03:15:58.264 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:15:58.264 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:15:58.264 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:15:58.281 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5944, 5950) --> . Mary journeyed to the
2025-01-22 03:15:58.282 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:15:58.304 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8374, 8378) -->  Mary left the apple
2025-01-22 03:15:58.304 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 03:15:58.332 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9656, 9662) -->  Daniel journeyed to the kitchen
2025-01-22 03:15:58.332 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:15:58.357 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8883, 8888) --> . John moved to the
2025-01-22 03:15:58.357 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:15:58.364 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2329, 2335) --> . Sandra journeyed to the
2025-01-22 03:15:58.364 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:15:58.394 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11172, 11177) --> . Daniel picked up the
2025-01-22 03:15:58.394 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 03:15:58.406 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4127, 4132) -->  St. Sandra moved to
2025-01-22 03:15:58.406 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:15:58.434 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10107, 10113) --> . John went back to the
2025-01-22 03:15:58.434 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:15:58.462 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10554, 10558) -->  Daniel left the apple
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:16:00.580 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:16:00.580 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12218])
your chose emoji: ['⚧', '🧑🏿\u200d❤\u200d💋\u200d🧑🏼', '⚕', '🏂🏿', '💂🏻\u200d♀️', '🧑\u200d🦼', '🏃🏿\u200d♀\u200d➡', '🇸🇮', '👩🏻\u200d🚀', '🫐']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 248551.35it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.57s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.09s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.89it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.43it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.55it/s][A100%|██████████| 8/8 [00:00<00:00, 20.70it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.42it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.99it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.23it/s][A100%|██████████| 8/8 [00:00<00:00, 20.26it/s]
2025-01-22 03:16:09.802 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.20598447890508742, 0.313904552742206, 0.25462538196194556, 0.31424180114756123, 0.36565511067708334], 'topk_tokens': [' communication', '�', ' turtle', '.', ' out', ' absence', 'were', ' communication', ' whistle', ' comparison', ' It', '.', ' agreement', ' Arr', ' compl', 'ob', ' out', ' out', ' Do', 'consider'], 'evidence_proportions': [0.27030029296875, 0.24517949422200522, 0.12067651748657227, 0.17006492614746094]}, 'weight': {'score': [0.020412086021332515, 0.002575852322545265, 0.016080680393403577, 0.0025106694334113065, 0.0008331366380055745], 'topk_tokens': ['\n\n', ' the', ' bathroom', ' office', ' barric', ' Bridge', 'Answer', 'b', '<|eot_id|>', ' bedroom', ' garden', 'Bridge', ':', 'assistant', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0192474365234375, 0.01575988034407298, 0.0334932804107666, 0.01731403668721517]}, 'saliency': {'score': [0.0013703604539235432, 4.387191563013113e-05, 0.0003820340479573896, 4.072134570603657e-05, 2.2504329681396485e-05], 'topk_tokens': ['\n\n', '<|begin_of_text|>', '\n\n', ' floats', ':', ' Wright', '<|eot_id|>', ' Mary', ' kitchen', '<|start_header_id|>', '<|eot_id|>', ' bathroom', ' Bridge', ' Bridge', ' Bench', ' office', ' bedroom', 'Bridge', ' garden', 'athroom'], 'evidence_proportions': [0.0019846677780151365, 0.0015141119559605916, 0.0014971345663070679, 0.0006301701068878174]}}, 25: {'grad': {'score': [0.5404541833060128, 0.595220237577735, 0.4722230972782258, 0.5956280770459733, 0.4411832173665365], 'topk_tokens': [' a', ' no', 'ville', ' old', ' for', ' ', ' set', ' at', ' for', ' for', ' a', ' inverted', ' locom', ' a', ' favored', ' Aw', ' l', 'ivery', ' of', ' no'], 'evidence_proportions': [0.567034912109375, 0.4748560587565104, 0.6869213581085205, 0.48625691731770837]}, 'weight': {'score': [0.013330391475132533, 0.0024991470510465465, 0.014692818926226708, 0.0024493927605513215, 0.0009453940391540527], 'topk_tokens': [' Bottle', ' Mary', '.', ' apple', 'b', '.\n\n', ' apple', ' Bench', ' Daniel', ' \n', 'Answer', '<|eot_id|>', ':', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.003896427154541016, 0.009955078363418579, 0.03709447383880615, 0.008724619944890339]}, 'saliency': {'score': [0.001330528940473284, 3.710326464844749e-05, 0.0006793785479760939, 3.323503611894805e-05, 3.5315752029418945e-05], 'topk_tokens': [' Anthony', '.', ' Mary', ' Daniel', 'Answer', ' Geo', ' Dan', ' Mary', '.', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', ' apple', ' Bench', 'athroom', ' apple', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00017862915992736814, 0.0007860859235127766, 0.005421757698059082, 0.00010740260283152261]}}, 26: {'grad': {'score': [0.5338795979817709, 0.5773984514040893, 0.5430105886151714, 0.5775611533244053, 0.5520164998372395], 'topk_tokens': ['UX', ' PRO', 'UX', 'rich', 'est', '�', 'graph', 'ier', 'graph', ' unab', 'pro', ' favor', ' Milwaukee', 'b', ' Press', 'b', 'b', 'issippi', ' bitter', 'itter'], 'evidence_proportions': [0.5091461181640625, 0.58782958984375, 0.5148773193359375, 0.5132090250651041]}, 'weight': {'score': [0.021092006138392856, 0.002479048253627409, 0.01470688658375894, 0.0024157780503392054, 0.0005383145809173584], 'topk_tokens': [' the', ' apple', ' apple', '.\n\n', ' barric', ' apple', ' bedroom', ' garden', '<|eot_id|>', 'Answer', 'b', ' \n', '<|eot_id|>', 'assistant', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.013527917861938476, 0.009320626656214396, 0.05015921592712402, 0.019788652658462524]}, 'saliency': {'score': [0.0009379855224064418, 5.178121447065694e-05, 0.0005354660172616282, 4.901972878135037e-05, 1.2238423029581705e-05], 'topk_tokens': [' apple', ' Mary', ' the', ' Bridge', ' Daniel', 'Bridge', ' apple', ' Anthony', ' kitchen', ' bedroom', ' \n', ' kitchen', ' garden', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0003283679485321045, 0.0006082753340403239, 0.0018780827522277832, 0.00114897886912028]}}, 27: {'grad': {'score': [0.24603153410412015, 0.3831612808830599, 0.28779380552230344, 0.3836408697086447, 0.3461851501464844], 'topk_tokens': [' be', '\n', ' would', ' Stewart', ' ideas', ' be', ' convention', 'prev', ' exchanged', ' was', ' of', 'ly', '-n', 'ides', ' was', ' was', ' step', ' accepted', ' Bottle', ' business'], 'evidence_proportions': [0.2802864074707031, 0.21898682912190756, 0.21334075927734375, 0.2663243611653646]}, 'weight': {'score': [0.022312886658168975, 0.002535223424381012, 0.012070998068778746, 0.0024768012087605113, 0.0009081693490346273], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' apple', ' apple', ' barric', ' bathroom', ' \n', ' bedroom', 'Answer', ' garden', 'assistant', 'b', '.\n\n', ' bedroom', '\n\n', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02572841644287109, 0.012337908148765564, 0.04939472675323486, 0.011387030283610025]}, 'saliency': {'score': [0.0016791380587078276, 4.719787285435406e-05, 0.0008261049947431011, 4.239740735337007e-05, 4.558245340983073e-05], 'topk_tokens': [' the', ' apple', ' barric', ' apple', ' the', ' the', ' the', 'THE', ' \n', 'THE', ' bedroom', ' kitchen', ' apple', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'b', ':', 'athroom', '.\n\n'], 'evidence_proportions': [0.0002248525619506836, 0.001389622688293457, 0.005368135869503021, 0.0007212261358896892]}}, 28: {'grad': {'score': [0.32302711123511907, 0.35111954688011415, 0.3171017554498488, 0.35125468474541804, 0.3130564880371094], 'topk_tokens': [' Or', 'u', '<|end_header_id|>', ' a', ' returns', '\n', ' the', 'ot', 'ew', ' the', ' the', 'IO', ' the', ' inside', 'nes', 'in', ' half', 'dent', ' in', 'ball'], 'evidence_proportions': [0.327130126953125, 0.37432861328125, 0.2509002685546875, 0.3163909912109375]}, 'weight': {'score': [0.019054976247605823, 0.002391828763759055, 0.013565307663333031, 0.002334609276286989, 0.00043768445650736493], 'topk_tokens': [' kitchen', ' the', ' bedroom', '<|eot_id|>', '.\n\n', '?', ' apple', 'Answer', ' garden', '<|eot_id|>', ' before', ' \n', 'assistant', 'b', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0032070219516754147, 0.011903921763102215, 0.03104543685913086, 0.03141901890436809]}, 'saliency': {'score': [0.0007867259638650077, 4.680482165414098e-05, 0.0002950131893157959, 4.489564231451246e-05, 9.68774159749349e-06], 'topk_tokens': [' the', ' apple', '.\n\n', ' \n', ' the', ' bedroom', ' before', ' apple', 'b', ' Bridge', 'assistant', ' Bridge', 'Bridge', 'athroom', ' garden', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.00023443698883056642, 0.0004237939914067586, 0.0014360547065734863, 0.0011770129203796387]}}, 29: {'grad': {'score': [0.3948364711943127, 0.39590901339778867, 0.38957503534132437, 0.39592699981458734, 0.3851618194580078], 'topk_tokens': ['s', ',\n', ' Ch', ' Pioneer', 'ION', ' a', 'UL', 'y', 'ER', 'y', ' In', 're', ' Paul', ' THE', 'ION', ' THE', ' The', ' M', ' In', ' The'], 'evidence_proportions': [0.307958984375, 0.3496222496032715, 0.5513763427734375, 0.40808868408203125]}, 'weight': {'score': [0.011077530327297393, 0.002459335810969791, 0.00987804320550734, 0.00242556458785585, 0.0006976322333017985], 'topk_tokens': [' the', ' ', ' apple', '<|eot_id|>', '<|eot_id|>', '?', ' the', ' the', 'Answer', ' before', ' \n', '.\n\n', 'b', 'assistant', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0004715800285339355, 0.006059736013412476, 0.029072284698486328, 0.012937113642692566]}, 'saliency': {'score': [0.00042936489695594426, 3.0503107379476017e-05, 0.0002853207049831267, 2.9165656224342565e-05, 4.418651262919108e-05], 'topk_tokens': ['us', 'Does', ' the', ' \n', 'nes', '      ', 'assistant', '.', ' was', 'Answer', '<|eot_id|>', ' Does', '<|end_header_id|>', ' ', ' before', ' the', ':', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.296566963195801e-05, 0.0002979735533396403, 0.0013580173254013062, 0.0002803206443786621]}}, 30: {'grad': {'score': [0.36959039597284227, 0.35959297057191514, 0.3965113239903604, 0.3594816701454716, 0.3633692296346029], 'topk_tokens': [' soon', ' S', ' two', ' of', ' his', ' the', ' its', 'b', '2', ' Burb', ' fifteen', ' the', ' B', ' B', ' account', '3', ' forb', 'b', 'b', 'deal'], 'evidence_proportions': [0.2966278076171875, 0.32988484700520837, 0.5266342163085938, 0.3654022216796875]}, 'weight': {'score': [0.020136535167694092, 0.0024552360492840833, 0.012517988681793213, 0.002399089068176811, 0.002365801731745402], 'topk_tokens': ['.', ' barric', ' bedroom', ' before', ' the', 'Question', '?', '<|eot_id|>', '<|eot_id|>', 'assistant', 'Answer', '.\n\n', 'b', ' \n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0062213301658630375, 0.016839563846588135, 0.03830695152282715, 0.022915899753570557]}, 'saliency': {'score': [0.0008498004504612514, 8.725939781417453e-05, 0.0005037996076768445, 8.488236530440945e-05, 4.0848255157470706e-05], 'topk_tokens': [' apple', ' Wide', 'Answer', ' the', ' bathroom', ' the', 'Bridge', ' Bridge', ' Bench', ' Bridge', ' the', '.', ' bedroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.0008444428443908691, 0.0005707293748855591, 0.0018181279301643372, 0.0004877845446268717]}}, 31: {'grad': {'score': [0.288284474895114, 0.33958685553197765, 0.27402298873470676, 0.33984240897631074, 0.20369693080584209], 'topk_tokens': [' the', ' an', '      ', ' was', ' the', ' the', '7', ' the', ' the', ' the', ' location', ' location', 'membership', ' had', ' they', ' the', ' the', ' the', ' he', ' the'], 'evidence_proportions': [0.25981845855712893, 0.2692757944266001, 0.26125091314315796, 0.3490372101465861]}, 'weight': {'score': [0.005154745919363839, 0.002262630202007239, 0.004009575613083378, 0.002253189020496199, 0.0006765560309092204], 'topk_tokens': ['Question', ':', ' before', ' Where', '.\n\n', '?', '<|eot_id|>', ' bedroom', 'Answer', ' the', ' \n', 'assistant', 'b', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007827103137969971, 0.004967863361040751, 0.010120630264282227, 0.005674401919047038]}, 'saliency': {'score': [0.00013636407398042225, 1.8231456715388946e-05, 9.051638264809885e-05, 1.784345296269935e-05, 7.922649383544921e-06], 'topk_tokens': [' Market', '.\n\n', 'Question', ' Mary', '.', ' was', ' the', ' apple', '<|start_header_id|>', ' bedroom', '<|eot_id|>', ' \n', ' apple', ':', ' the', ' bedroom', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [1.9395351409912108e-05, 0.0001271367073059082, 0.0004013180732727051, 6.642937660217285e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:16:09.803 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:16:09.803 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-1_0-5-7-8.pkl | len: 10 |  size: 9.01 KB
Processing depth (0, 5, 7, 8):   2%|▏         | 2/100 [00:37<30:33, 18.71s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (1, 2, 3, 4):   2%|▏         | 2/100 [00:44<30:33, 18.71s/it]2025-01-22 03:16:17.125 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:16:17.130 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1494, 1499) -->  tragedy. Mary moved to
2025-01-22 03:16:17.130 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:16:17.137 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2453, 2459) --> . Mary journeyed to the
2025-01-22 03:16:17.137 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:16:17.148 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3764, 3768) -->  Mary left the apple
2025-01-22 03:16:17.148 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 03:16:17.162 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4890, 4896) --> . Daniel journeyed to the
2025-01-22 03:16:17.162 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:16:17.187 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8887, 8892) --> . John moved to the
2025-01-22 03:16:17.187 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:16:17.194 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2269, 2275) -->  Sandra journeyed to the office
2025-01-22 03:16:17.194 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:16:17.225 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11158, 11163) --> . Daniel picked up the
2025-01-22 03:16:17.225 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 03:16:17.237 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4110, 4115) --> . Sandra moved to the
2025-01-22 03:16:17.237 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:16:17.266 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10122, 10128) --> . John went back to the
2025-01-22 03:16:17.266 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:16:17.294 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10552, 10556) -->  Daniel left the apple
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:16:19.312 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:16:19.312 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12210])
your chose emoji: ['👯\u200d♂️', '🚴\u200d♀', '🇬🇱', '🔷', '🔘', '🦻🏽', '⛎', '🏌🏻', '👩🏾\u200d❤️\u200d💋\u200d👨🏼', '👳🏻\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 164482.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.29s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.63it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.48it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.51it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.49it/s][A100%|██████████| 8/8 [00:00<00:00, 20.66it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.37it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.05it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.33it/s][A100%|██████████| 8/8 [00:00<00:00, 20.33it/s]
2025-01-22 03:16:28.396 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2547620137532552, 0.3475073028292086, 0.2855020338489163, 0.34782551797672806, 0.34372444295171484], 'topk_tokens': ['ang', ' whistle', 'ols', ' turtle', ' broad', '\n', 'G', ' out', ' communication', ' communication', ' It', ' agreement', ' out', 'consider', ' the', ' whistle', 'vent', ' whistle', ' compl', ' Do'], 'evidence_proportions': [0.3509979248046875, 0.2661310831705729, 0.23691558837890625, 0.1750939687093099]}, 'weight': {'score': [0.009940533410935174, 0.002576883132223668, 0.028200262977230932, 0.002498849958056398, 0.0012039377618191848], 'topk_tokens': [' bathroom', '.', ' office', '.', ' barric', ' bedroom', ' the', ' the', ' the', ':', 'b', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005635440349578858, 0.006986305117607117, 0.029375553131103516, 0.00775223970413208]}, 'saliency': {'score': [0.0003680402324313209, 5.1425555643906106e-05, 0.0007114073922557216, 4.919643426017928e-05, 3.913890069990016e-05], 'topk_tokens': ['\n\n', '<|start_header_id|>', '.', ' hall', ' Bridge', '<|eot_id|>', ' kitchen', ' bedroom', '<|eot_id|>', ' barric', ' Bridge', ' bedroom', ' bathroom', '<|begin_of_text|>', ' garden', ' kitchen', ' office', 'athroom', ' Bench', 'Bridge'], 'evidence_proportions': [1.1450052261352539e-05, 0.00033306082089742023, 0.000892050564289093, 0.0003508379062016805]}}, 25: {'grad': {'score': [0.7076495942615327, 0.7827482274267175, 0.607084704983619, 0.7833256997145396, 0.5465906057784806], 'topk_tokens': ['st', ' F', 'love', ' a', ' at', ' locom', ' a', ' of', ' by', ' for', ' for', ' the', ' set', ' ', ' a', ' a', ' favored', ' no', ' l', 'ivery'], 'evidence_proportions': [0.685638427734375, 0.7052841186523438, 0.7227630615234375, 0.7182820638020833]}, 'weight': {'score': [0.004069511379514422, 0.0025025184779400265, 0.015165274181673604, 0.002467533503205317, 0.0011477710595771448], 'topk_tokens': [' bedroom', ' barric', ' the', '.\n\n', 'b', 'Answer', ' \n', ' apple', '.', ' Bench', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', '.', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00041337013244628905, 0.0018998831510543823, 0.013802766799926758, 0.002797087033589681]}, 'saliency': {'score': [0.0002798069091070266, 4.347519988579509e-05, 0.0009823287686994, 4.067383268516458e-05, 5.478689919656782e-05], 'topk_tokens': [' bedroom', '.', '.\n\n', ' apple', 'b', 'athroom', ' Daniel', 'river', 'assistant', ' barric', ':', '<|eot_id|>', '<|eot_id|>', '.', '.', ' apple', ' Bench', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.9943714141845704e-05, 7.38998254140218e-05, 0.0011517107486724854, 0.00012099742889404297]}}, 26: {'grad': {'score': [0.7376788911365327, 0.6982676750376136, 0.6104281025548135, 0.6984235332079031, 0.655058504930183], 'topk_tokens': [' the', ' a', ' OCC', 'bec', 'aining', ' barric', ' next', ' unab', 'b', 'RI', 'CE', 'char', 'occ', "'t", 'ier', ' favor', 'b', 'b', 'itter', ' bitter'], 'evidence_proportions': [0.78907470703125, 0.8155008951822916, 0.6167373657226562, 0.6976547241210938]}, 'weight': {'score': [0.002660047440301804, 0.0024704292464156727, 0.018707118688091155, 0.0024287123354080624, 0.0010131593070813079], 'topk_tokens': ['.\n\n', ' bathroom', ' garden', '<|eot_id|>', 'Bridge', '<|eot_id|>', ' bedroom', 'assistant', 'Answer', ' \n', ' the', ' the', 'b', ' barric', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [5.8770179748535156e-05, 0.0015870332717895508, 0.009367227554321289, 0.0014293392499287925]}, 'saliency': {'score': [7.710144633338565e-05, 6.925958454057429e-05, 0.0006853861193503103, 6.767545480808923e-05, 3.0111021070338007e-05], 'topk_tokens': [' bedroom', ' kitchen', ' back', ' bathroom', ' Bridge', 'Answer', ' hall', ' \n', ' barric', ' garden', ' the', ' the', '<|start_header_id|>', 'athroom', 'Bridge', '<|end_header_id|>', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [2.9683113098144534e-06, 7.422268390655518e-05, 0.00023032724857330322, 3.960728645324707e-05]}}, 27: {'grad': {'score': [0.314525876726423, 0.437293540950217, 0.35872409420628704, 0.43770582396952146, 0.4361149802136777], 'topk_tokens': [' Thanksgiving', ' prev', ' conventions', ' designated', ' was', 'event', ' conventions', 'started', ' started', ' was', ' mon', 'lin', ' Franklin', ' convention', ' accepted', ' sentinel', ' business', ' was', ' step', 'ides'], 'evidence_proportions': [0.3270725250244141, 0.26245625813802087, 0.22243881225585938, 0.4175313313802083]}, 'weight': {'score': [0.005310336748758952, 0.0025339777050113576, 0.01244056801642141, 0.002503930189217228, 0.0011851622987149367], 'topk_tokens': ['<|eot_id|>', ' garden', ' lounge', '.', ' \n', 'Answer', ' bedroom', 'assistant', '.', 'Bridge', 'b', '.\n\n', ' bathroom', ' barric', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00024152398109436035, 0.0031023969252904258, 0.015593767166137695, 0.004886666933695475]}, 'saliency': {'score': [0.0011046372708820161, 5.6542243241463404e-05, 0.0014294905047262869, 5.1232532552664694e-05, 5.21469472059563e-05], 'topk_tokens': ['.', '.', ' bedroom', ' kitchen', '\n\n', ' the', ' bathroom', ' kitchen', '.', ' the', ' \n', ' the', '<|start_header_id|>', 'Bridge', ' barric', '<|begin_of_text|>', '<|end_header_id|>', '.\n\n', 'athroom', ':'], 'evidence_proportions': [1.5723705291748044e-05, 0.0006780525048573811, 0.0032077282667160034, 0.0010365893443425496]}}, 28: {'grad': {'score': [0.5481999715169271, 0.4146935414459746, 0.451691166047127, 0.4143686864674263, 0.3462587328099493], 'topk_tokens': [' summer', ' after', ' After', ' as', ' before', ' summer', 'nes', ' speakers', 'nes', ' summer', ' summer', ' about', 'half', ' in', ' of', ' as', ' half', ' inside', ' returns', 'ball'], 'evidence_proportions': [0.5502380371093749, 0.547472635904948, 0.6844635009765625, 0.4563865661621094]}, 'weight': {'score': [0.002273511318933396, 0.0023979094713099084, 0.017936925734243086, 0.0023585132750307356, 0.0006051690720800144], 'topk_tokens': ['<|eot_id|>', 'Answer', ' the', ' apple', '<|eot_id|>', ' the', ' bedroom', '?', ' \n', ' the', 'assistant', ' the', ' before', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [6.608963012695312e-05, 0.0016443034013112385, 0.008257389068603516, 0.0007529854774475098]}, 'saliency': {'score': [7.985887073335194e-05, 4.649314073734257e-05, 0.000690441939138597, 4.479401294519101e-05, 1.4866910763640902e-05], 'topk_tokens': [' the', ' kitchen', ' back', ' the', ' the', ' apple', ' before', ' bedroom', ' the', 'assistant', '<|begin_of_text|>', ' garden', ' Bridge', 'b', 'Bridge', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', ':', '\n\n'], 'evidence_proportions': [3.087520599365234e-06, 7.883707682291666e-05, 0.00023728609085083008, 3.9905309677124023e-05]}}, 29: {'grad': {'score': [0.5733918689546131, 0.46750576837949315, 0.39514947706653225, 0.467507366678862, 0.384804284394677], 'topk_tokens': ['y', ' An', ' com', ' Paul', ' Bre', 'b', ' LO', ' Ch', ' B', ' a', ' In', 're', 'y', ' THE', 'y', ' ga', ' M', ' The', ' The', ' In'], 'evidence_proportions': [0.43953857421875, 0.6778564453125, 0.7259368896484375, 0.4787750244140625]}, 'weight': {'score': [0.0017606701169695174, 0.002465803461364531, 0.01132847128375884, 0.002444428993618299, 0.0011337252695169022], 'topk_tokens': [' the', ' apple', ' bedroom', ' the', '<|eot_id|>', '?', ' before', '.\n\n', 'Answer', ' the', ' \n', 'b', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.65085220336914e-05, 0.0017188886801401773, 0.004628896713256836, 0.0012937684853871663]}, 'saliency': {'score': [0.00011921922365824382, 4.172800487785992e-05, 0.0004017603012823289, 4.0676420568763085e-05, 8.477381805875408e-05], 'topk_tokens': ['nes', ' the', 'Does', ' the', '<|eot_id|>', '?', ' \n', ' the', ' before', 'assistant', '<|eot_id|>', 'athroom', 'Answer', ' Does', '<|start_header_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', ':', '\n\n'], 'evidence_proportions': [7.069110870361328e-06, 8.226931095123291e-05, 0.00035706907510757446, 9.106099605560303e-05]}}, 30: {'grad': {'score': [0.5237877255394345, 0.4432805035649769, 0.5283892231602823, 0.4429245277431762, 0.40783220974367057], 'topk_tokens': [' soon', ' Buchanan', ' to', ' the', ' months', ' his', ' S', '2', ' the', ' B', 'b', ' atmosphere', ' Burb', 'b', ' account', ' B', '3', ' forb', 'deal', 'b'], 'evidence_proportions': [0.3745147705078125, 0.5114644368489584, 0.8435821533203125, 0.44730885823567706]}, 'weight': {'score': [0.005051784572147187, 0.00245080759062049, 0.01923181164649225, 0.002403539138820138, 0.003579697947003948], 'topk_tokens': [' before', ' the', '<|eot_id|>', '.', ' the', '?', 'Answer', ' bedroom', '<|eot_id|>', '.\n\n', 'assistant', ' barric', 'b', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00048086643218994143, 0.003841305772463481, 0.016372203826904297, 0.0025244156519571943]}, 'saliency': {'score': [0.00027053696768624444, 9.540598456712981e-05, 0.0005241267142757293, 9.401069690439911e-05, 5.383989704189016e-05], 'topk_tokens': [' the', ' the', '.', '.\n\n', ' garden', 'assistant', ' bedroom', ' Bridge', 'Bridge', ' Bridge', ' Bench', ' bathroom', ' barric', '<|end_header_id|>', ' the', '<|start_header_id|>', '<|begin_of_text|>', 'athroom', ':', 'b'], 'evidence_proportions': [7.760524749755859e-06, 0.0001957466204961141, 0.0008727014064788818, 0.0001628647247950236]}}, 31: {'grad': {'score': [0.2714730231534867, 0.4084900140977068, 0.3326027124158798, 0.4089200661626657, 0.2163967516884875], 'topk_tokens': ['2', ' the', ' the', ' location', ' office', ' location', 'membership', ' the', 'If', ' the', ' the', ' was', ' August', ' the', ' had', ' the', ' the', ' he', ' the', ' the'], 'evidence_proportions': [0.25436286926269536, 0.2300144036610921, 0.2900944948196411, 0.31477578977743786]}, 'weight': {'score': [0.0010994388943626767, 0.002260464020387591, 0.0029845612664376538, 0.0022606230955474443, 0.0011245336995195988], 'topk_tokens': [':', ' the', ' the', '.\n\n', ' before', ' Where', '<|eot_id|>', '?', ' bedroom', 'Answer', ' \n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00029508471488952636, 0.0010248174269994101, 0.0030134916305541992, 0.0005683203538258871]}, 'saliency': {'score': [1.9398473557971773e-05, 2.1263104169147954e-05, 6.632554915643507e-05, 2.1151453930535075e-05, 1.3999529738924397e-05], 'topk_tokens': [' the', ' Bridge', '.', ' was', ' Market', ' the', 'Answer', '.\n\n', ' the', '<|eot_id|>', ' apple', ' \n', ' the', ':', ' bedroom', '<|begin_of_text|>', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [1.5860795974731447e-05, 2.210835615793864e-05, 3.9614737033843994e-05, 6.159146626790365e-06]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:16:28.397 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:16:28.397 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-2_1-2-3-4.pkl | len: 10 |  size: 9.01 KB
Processing depth (1, 2, 3, 4):   3%|▎         | 3/100 [00:55<30:09, 18.66s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (3, 4, 5, 8):   3%|▎         | 3/100 [01:03<30:09, 18.66s/it]2025-01-22 03:16:35.749 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:16:35.760 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3706, 3711) --> . Mary moved to the
2025-01-22 03:16:35.760 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:16:35.774 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4865, 4871) --> . Mary journeyed to the
2025-01-22 03:16:35.774 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:16:35.790 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5933, 5937) -->  Mary left the apple
2025-01-22 03:16:35.791 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 03:16:35.819 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9646, 9652) -->  Daniel journeyed to the kitchen
2025-01-22 03:16:35.819 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:16:35.844 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8889, 8894) --> . John moved to the
2025-01-22 03:16:35.844 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:16:35.850 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2273, 2279) -->  Sandra journeyed to the office
2025-01-22 03:16:35.851 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:16:35.882 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11167, 11172) --> . Daniel picked up the
2025-01-22 03:16:35.882 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 03:16:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4063, 4068) --> . Sandra moved to the
2025-01-22 03:16:35.893 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:16:35.923 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10131, 10137) --> . John went back to the
2025-01-22 03:16:35.923 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:16:35.951 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10561, 10565) -->  Daniel left the apple
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:16:37.987 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:16:37.987 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12219])
your chose emoji: ['👨🏿\u200d🤝\u200d👨🏾', '⛪', '🇺🇿', '6️⃣', '🙋🏽\u200d♀', '🇮🇸', '👨🏻\u200d🦽\u200d➡', '👫🏾', '🦟', '🚵🏽\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 246723.76it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.08s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.01it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.69it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.57it/s][A100%|██████████| 8/8 [00:05<00:00,  1.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 23.02it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 19.05it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.22it/s][A100%|██████████| 8/8 [00:00<00:00, 18.06it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.68it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.07it/s][A100%|██████████| 8/8 [00:00<00:00, 17.78it/s]
2025-01-22 03:16:47.553 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3093170892624628, 0.3541375873249187, 0.33549327235068044, 0.3542624192249608, 0.3755819107356824], 'topk_tokens': [':\n\n', ' proceedings', ' the', ' expedition', 'If', ' S', ' compl', ' were', '.', 'ols', ' of', ' and', ' out', ' whistle', ' agreement', ' communication', ' communication', ' out', ' Do', ' turtle'], 'evidence_proportions': [0.27169647216796877, 0.4145329793294271, 0.3302574157714844, 0.22149149576822919]}, 'weight': {'score': [0.02083829612958999, 0.0025793565429577635, 0.02830025649839832, 0.0024823322512619594, 0.0013628053037743819], 'topk_tokens': ['.', ' office', 'Answer', ' the', ' bedroom', ' kitchen', ':', ' garden', ' the', '<|eot_id|>', 'b', 'assistant', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'Bridge', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.022406512498855592, 0.0033306082089742026, 0.04639720916748047, 0.01999986171722412]}, 'saliency': {'score': [0.0010288130669366745, 4.323203353791322e-05, 0.0007985416919954362, 3.9607399099658726e-05, 7.539948350504826e-05], 'topk_tokens': ['\n\n', ':', '.', ' apple', 'Answer', ' back', ' apple', '<|start_header_id|>', '<|begin_of_text|>', ' Bench', '.', '<|eot_id|>', ' bedroom', '<|eot_id|>', ' kitchen', ' office', ' garden', 'Bridge', ' bathroom', 'athroom'], 'evidence_proportions': [0.0006848275661468506, 0.0002582371234893799, 0.0028590038418769836, 0.0008659164110819499]}}, 25: {'grad': {'score': [0.7516541253952753, 0.8949803393087261, 0.7013904202368951, 0.8957207779269191, 0.5805580741480777], 'topk_tokens': [' a', ' the', ' o', ' for', ' inverted', ' favored', ' the', ' a', ' for', ' a', ' set', ' for', ' a', ' old', ' locom', ' for', ' of', 'ivery', ' l', ' no'], 'evidence_proportions': [0.5988784790039062, 0.9000040690104167, 0.7799835205078125, 0.71173095703125]}, 'weight': {'score': [0.015629481701623826, 0.0024986323270952226, 0.01901842701819635, 0.0024338943260854254, 0.0014260662229437578], 'topk_tokens': ['.', ' Bench', '.\n\n', 'b', ' apple', ' Daniel', ' \n', 'Answer', ' the', ' apple', ' bathroom', ':', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.011339819431304932, 0.0009755492210388184, 0.04974007606506348, 0.01111773649851481]}, 'saliency': {'score': [0.0017617529346829368, 4.666375316041832e-05, 0.0011044558017484604, 4.1009815089900475e-05, 5.672636785005268e-05], 'topk_tokens': [' THE', '<|eot_id|>', ' Mary', ' \n', 'athroom', ' Dan', '.\n\n', 'Answer', '.', ':', '<|eot_id|>', ' Geo', ' Mary', ' Bench', ' bathroom', ' apple', ' apple', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0009054243564605713, 5.521873633066813e-05, 0.007676839828491211, 0.0002385030190149943]}}, 26: {'grad': {'score': [0.6099755423409599, 0.6240638679225986, 0.5472132775091356, 0.6242839355594131, 0.5872253116808439], 'topk_tokens': [' in', 'op', ' compl', 'hue', 'present', 'char', 'CE', 'issippi', ' PRO', 'occ', ' Met', ' favor', 'UX', 'ub', 'pro', 'b', 'b', ' favor', ' bitter', 'itter'], 'evidence_proportions': [0.698291015625, 0.6913197835286458, 0.4808082580566406, 0.5411465962727864]}, 'weight': {'score': [0.01584482476824806, 0.0024891098632962316, 0.01882889097736728, 0.0024244423836298234, 0.0008357371154584383], 'topk_tokens': ['Bridge', '?', ' kitchen', ' the', ' apple', ' garden', '<|eot_id|>', 'Answer', '<|eot_id|>', ' \n', 'assistant', ' the', 'b', '\n\n', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0038533091545104982, 0.000549683968226115, 0.03946352005004883, 0.025387098391850788]}, 'saliency': {'score': [0.0006980129650660924, 5.92953902066505e-05, 0.0006354576156985375, 5.6725620439822494e-05, 5.4293949353067496e-05], 'topk_tokens': ['assistant', '�', ' the', ' the', ' kitchen', ' kitchen', '?', ' apple', ' \n', 'Bridge', ' the', '<|start_header_id|>', ':', ' garden', '<|end_header_id|>', 'athroom', ' bathroom', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.00014156699180603027, 1.1126200358072916e-05, 0.0018191337585449219, 0.0011011908451716104]}}, 27: {'grad': {'score': [0.37493642171223956, 0.47933414130256913, 0.47162002132784936, 0.47953393504378633, 0.41635021410490336], 'topk_tokens': ['ARCH', ' DAYS', ' Marshall', ' effect', ' states', ' Four', ' four', ' Franklin', ' conventions', 'lin', ' received', 'started', ' sentinel', 'ides', '-n', ' designated', ' started', ' convention', ' step', ' accepted'], 'evidence_proportions': [0.46149291992187497, 0.3612111409505208, 0.35498809814453125, 0.3298301696777344]}, 'weight': {'score': [0.01736824001584734, 0.0025314432648858663, 0.014530254948523736, 0.0024752776203531657, 0.001328702427838978], 'topk_tokens': ['.', ' kitchen', '<|eot_id|>', 'Bridge', ' lounge', ' apple', ' bedroom', ' garden', ' \n', 'Answer', 'assistant', 'b', '.\n\n', '\n\n', '<|start_header_id|>', ' bathroom', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008067739009857178, 0.0014827648798624675, 0.056604743003845215, 0.014846463998158772]}, 'saliency': {'score': [0.001957327127456665, 5.898112956952042e-05, 0.001264564452632781, 5.263451091951293e-05, 7.692567611995496e-05], 'topk_tokens': [' Daniel', 'THE', ' the', 'THE', ' THE', 'Answer', ' \n', 'Bridge', ' bedroom', ' the', ' kitchen', ' bathroom', ' the', '<|start_header_id|>', '<|begin_of_text|>', 'b', '<|end_header_id|>', ':', 'athroom', '.\n\n'], 'evidence_proportions': [0.0014974296092987059, 0.0002720355987548828, 0.005932331085205078, 0.0013758639494578044]}}, 28: {'grad': {'score': [0.4198030744280134, 0.43750850956523074, 0.31685398470970894, 0.43784639817726057, 0.37719562179163885], 'topk_tokens': ['ig', ' half', ' RID', 'arp', ' inside', ' platform', ' before', ' probably', 'nes', 'ipp', ' as', ' an', '\n', 'half', 'nes', 'dent', ' half', '.', 'nes', ' returns'], 'evidence_proportions': [0.45927734374999996, 0.523413340250651, 0.3702392578125, 0.31634012858072913]}, 'weight': {'score': [0.012305955092112223, 0.0024039454671603492, 0.019190379688816685, 0.0023440998087383823, 0.0006463849230816489], 'topk_tokens': ['.\n\n', ' the', ' bathroom', ' apple', '<|eot_id|>', ' the', ' garden', ' before', 'Answer', '?', ' \n', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0025570929050445553, 0.0004376421372095744, 0.017765045166015625, 0.028658926486968994]}, 'saliency': {'score': [0.000520511752083188, 4.60633980312981e-05, 0.0005551393954984603, 4.3947968996247016e-05, 2.3406195013146652e-05], 'topk_tokens': [' \n', ' bathroom', ' nearly', ' the', ' before', '?', ' Bridge', ' the', ' bedroom', ' Bridge', 'athroom', ' apple', 'b', 'assistant', 'Bridge', ' garden', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', ':'], 'evidence_proportions': [0.00015304088592529296, 3.5643577575683594e-05, 0.0007830709218978882, 0.0011365662018458047]}}, 29: {'grad': {'score': [0.5123924981980097, 0.4719475390497157, 0.4700505656580771, 0.471882581123095, 0.48526648471229955], 'topk_tokens': [' Pioneer', ' Paul', 'ION', ' LO', ' Paul', ' The', 'y', 'ER', ' Paul', ' THE', ' An', 're', ' In', ' a', ' ga', ' In', ' THE', ' The', ' The', ' M'], 'evidence_proportions': [0.35385208129882817, 0.5985132853190104, 0.6811447143554688, 0.4458872477213542]}, 'weight': {'score': [0.007439146439234416, 0.002461534536639791, 0.013394141389477638, 0.0024250973417020375, 0.0009815994846193415], 'topk_tokens': [' garden', ' the', '<|eot_id|>', ' before', '?', '<|eot_id|>', ' the', ' apple', '.\n\n', ' \n', 'Answer', ' the', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0017872095108032227, 0.0005445381005605061, 0.013641893863677979, 0.01490853726863861]}, 'saliency': {'score': [0.0005213646661667596, 4.884173184989264e-05, 0.0003708485634096207, 4.7206136665093476e-05, 6.425263066040843e-05], 'topk_tokens': ['IVE', ' bedroom', '<|eot_id|>', 'Does', ' \n', ' the', ' ', ' before', '<|eot_id|>', 'assistant', ' Does', 'Answer', '<|end_header_id|>', ' the', 'athroom', ':', '\n\n', 'b', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.00014609694480895996, 3.6040941874186196e-05, 0.0014384984970092773, 0.0007079889376958212]}}, 30: {'grad': {'score': [0.5402112688337054, 0.4217108662673483, 0.5287169179608745, 0.42123381646813773, 0.43048228715595444], 'topk_tokens': [' S', ' to', ' abroad', ' the', ' soon', ' Buchanan', ' his', ' of', ' its', 'B', ' account', '3', ' Burb', 'deal', 'b', ' forb', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.511798095703125, 0.5308202107747396, 0.7859268188476562, 0.4094696044921875]}, 'weight': {'score': [0.015723158915837605, 0.002461953476044535, 0.018913918925869848, 0.002397163316374836, 0.00356942964227576], 'topk_tokens': [' the', ' bedroom', ' the', 'Question', ' the', ' bathroom', '<|eot_id|>', '?', '.\n\n', '<|eot_id|>', 'Answer', 'assistant', 'b', ' \n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00627564787864685, 0.0015479326248168945, 0.03598833084106445, 0.024261196454366047]}, 'saliency': {'score': [0.0004017622697921026, 7.567713331267167e-05, 0.0007599765254605201, 7.337137579232234e-05, 6.891120421259027e-05], 'topk_tokens': [' Bridge', ' Wide', ' the', '.', '<|eot_id|>', 'Answer', ' the', ' Bench', ' Bridge', 'Bridge', ' the', 'assistant', ' bedroom', '<|end_header_id|>', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>', 'athroom', 'b', ':'], 'evidence_proportions': [0.000417172908782959, 4.86224889755249e-05, 0.0010628998279571533, 0.0003013014793395996]}}, 31: {'grad': {'score': [0.330028908593314, 0.41613796966357974, 0.3279429712603169, 0.4165112100278342, 0.2028839097995507], 'topk_tokens': [' office', ' the', ' the', ' the', ' the', ' location', 'membership', 'If', ' August', ' the', ' their', ' was', ' the', ' the', ' had', ' the', ' he', ' the', ' the', ' the'], 'evidence_proportions': [0.3244805812835694, 0.31918446222941077, 0.28427547216415405, 0.37599925200144446]}, 'weight': {'score': [0.002872467041015625, 0.0022554632451252977, 0.0037422237857695547, 0.002250611424544059, 0.0008718516481550116], 'topk_tokens': [':', ' the', ' Where', ' the', ' before', '.\n\n', ' bedroom', '<|eot_id|>', '?', 'Answer', ' \n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0023970782756805417, 0.0005054175853729248, 0.005769014358520508, 0.003704642256100972]}, 'saliency': {'score': [5.8104594548543297e-05, 2.1875970220433145e-05, 9.805733157742409e-05, 2.1619402980255906e-05, 9.591642178987203e-06], 'topk_tokens': [' Where', ' was', '.', ' Market', ' the', '.\n\n', ' the', '?', ' apple', ' \n', '<|eot_id|>', ' the', 'b', ' bedroom', '<|start_header_id|>', ':', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [8.259415626525879e-05, 2.2749106089274087e-06, 0.00012224167585372925, 5.0768256187438965e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:16:47.554 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:16:47.555 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-3_3-4-5-8.pkl | len: 10 |  size: 9.06 KB
Processing depth (3, 4, 5, 8):   4%|▍         | 4/100 [01:15<30:10, 18.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.15it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.17it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.19it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]
Processing depth (0, 2, 7, 9):   4%|▍         | 4/100 [01:22<30:10, 18.86s/it]2025-01-22 03:16:55.434 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:16:55.435 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:16:55.435 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:16:55.442 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2501, 2507) --> . Mary journeyed to the
2025-01-22 03:16:55.443 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:16:55.465 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8365, 8369) -->  Mary left the apple
2025-01-22 03:16:55.465 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 03:16:55.496 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10758, 10764) --> . Daniel journeyed to the
2025-01-22 03:16:55.497 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:16:55.521 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (8878, 8883) --> . John moved to the
2025-01-22 03:16:55.521 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:16:55.528 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2283, 2289) --> . Sandra journeyed to the
2025-01-22 03:16:55.528 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:16:55.559 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11162, 11167) --> . Daniel picked up the
2025-01-22 03:16:55.559 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 03:16:55.571 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4099, 4104) --> . Sandra moved to the
2025-01-22 03:16:55.571 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:16:55.601 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10113, 10119) --> . John went back to the
2025-01-22 03:16:55.601 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Daniel left the apple.
2025-01-22 03:16:55.629 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10537, 10541) -->  Daniel left the apple
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:16:57.859 | INFO     | test_jbb_retain:begin_test:632 - The apple was in the kitchen.<|eot_id|>
2025-01-22 03:16:57.859 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12214])
your chose emoji: ['🎗️', '⏮️', '🏉', '🤞🏿', '💁🏾\u200d♂', '🧔🏽\u200d♂', '🧙🏻\u200d♀️', '🧑🏾\u200d🦽', '👳🏿\u200d♀️', '🦹🏾\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 162098.71it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.40s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.06s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.94it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.58it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.63it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.44it/s][A100%|██████████| 8/8 [00:00<00:00, 20.67it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 14.75it/s][A
 50%|█████     | 4/8 [00:00<00:00, 17.25it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 20.42it/s][A100%|██████████| 8/8 [00:00<00:00, 19.78it/s]
2025-01-22 03:17:07.237 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2322679247174944, 0.32330709449435213, 0.2609203707787298, 0.3236232318556755, 0.3760834344675843], 'topk_tokens': [' expedition', ' out', ' agreement', ' communication', 'If', 'adj', ' It', ' communication', ' out', 'remark', ' out', 'ols', 'vent', ' emb', ' It', 'ob', ' compl', ' Arr', 'consider', ' Do'], 'evidence_proportions': [0.31617431640625, 0.30876668294270837, 0.1456470489501953, 0.14359442392985025]}, 'weight': {'score': [0.026129318135125295, 0.002575715552026593, 0.022472255652950655, 0.002484353579369485, 0.00071588074657279], 'topk_tokens': ['.', ' office', ' Bench', ' the', ' Bridge', 'Answer', ' the', '<|eot_id|>', 'b', ' bathroom', '<|start_header_id|>', 'Bridge', 'assistant', ':', ' bedroom', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04588737487792969, 0.016180331508318584, 0.03378725051879883, 0.01450796922047933]}, 'saliency': {'score': [0.0015295885858081636, 4.196113812959535e-05, 0.0006536226118764569, 3.783440717296545e-05, 1.4944815299880337e-05], 'topk_tokens': [' Mary', ' Sandra', '<|start_header_id|>', ' *', ':', ' kitchen', ' Bridge', '<|begin_of_text|>', '<|eot_id|>', ' office', 'b', ' Bench', '<|eot_id|>', ' kitchen', ' Bridge', ' bathroom', ' garden', ' bedroom', 'Bridge', 'athroom'], 'evidence_proportions': [0.002937966585159302, 0.0017069826523462932, 0.0012224316596984863, 0.0003833174705505371]}}, 25: {'grad': {'score': [0.5702703566778273, 0.6089702905019645, 0.5148961467127646, 0.6092768254027267, 0.5623854516257702], 'topk_tokens': [' locom', ' o', ' set', ' the', ' the', ' a', ' old', ' for', ' inverted', ' favored', ' a', ' Aw', ' for', ' a', ' at', ' for', ' l', 'ivery', ' no', ' of'], 'evidence_proportions': [0.59189453125, 0.5078595479329427, 0.6244792938232422, 0.578521728515625]}, 'weight': {'score': [0.013389272349221366, 0.0025023955062337894, 0.015475515396364273, 0.002450542556764263, 0.0006839884838587801], 'topk_tokens': [' Bottle', ' bathroom', ' Daniel', ' apple', 'b', '.\n\n', 'Answer', ' \n', '<|start_header_id|>', ' apple', ' Bench', '<|eot_id|>', '.', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.01246039867401123, 0.005997608105341594, 0.03580367565155029, 0.006612062454223633]}, 'saliency': {'score': [0.0011799094222840808, 3.696997973418737e-05, 0.0010498329516380064, 3.2415891742196734e-05, 1.807699740772516e-05], 'topk_tokens': [' Bottle', '.', ' PA', ' bathroom', ' Dan', 'Answer', '.', ' Geo', ':', '.', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', ' apple', ' Bench', '\n\n', ' apple', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000580984354019165, 0.0001965165138244629, 0.0050356388092041016, 9.19202963511149e-05]}}, 26: {'grad': {'score': [0.5877947126116071, 0.5826953476712777, 0.5452197905509703, 0.5827820435699199, 0.564075080441757], 'topk_tokens': [' unab', 'ree', ' Press', ' brav', 'b', 'bec', ' vastly', ' favorable', 'b', 'CE', 'est', 'ier', 'RI', 'pro', ' PA', 'b', ' favor', 'b', ' bitter', 'itter'], 'evidence_proportions': [0.520538330078125, 0.6681162516276041, 0.589202880859375, 0.5625813802083333]}, 'weight': {'score': [0.024349506412233626, 0.0024745960209054878, 0.021362778640562487, 0.0023887015055394987, 0.0004799504515150903], 'topk_tokens': [' apple', ' kitchen', ' apple', '.\n\n', ' garden', ' the', '<|eot_id|>', '<|eot_id|>', ' bathroom', 'Answer', 'b', ' apple', ' \n', 'assistant', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.042245048284530635, 0.004060864448547363, 0.051656246185302734, 0.011520703633626303]}, 'saliency': {'score': [0.0007590296722593761, 4.5806301238250546e-05, 0.000680094765078637, 4.295873583171576e-05, 1.1642214278100243e-05], 'topk_tokens': [' the', ' kitchen', ' kitchen', ' bathroom', ' apple', ' apple', ' Bridge', 'Bridge', '.', ' bedroom', ' the', '<|start_header_id|>', ' \n', ' garden', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0007527649402618408, 0.00031919777393341064, 0.0018417835235595703, 0.0004822462797164917]}}, 27: {'grad': {'score': [0.25178673153831843, 0.3720468316753806, 0.2512386075911983, 0.3725622872487047, 0.3608511803855359], 'topk_tokens': [' Franklin', ' Bottle', ' were', 'prev', ' expedition', ' was', ' designated', ' sentinel', ' one', ' be', 'ly', ' Stewart', 'lin', ' Marshall', 'ides', ' convention', ' was', '-n', ' accepted', ' step'], 'evidence_proportions': [0.2648040771484375, 0.2548039754231771, 0.191131591796875, 0.27835845947265625]}, 'weight': {'score': [0.02824853431610834, 0.0025351254963646487, 0.016585973962660757, 0.0024549316708266464, 0.0008255310461554729], 'topk_tokens': [' kitchen', '<|eot_id|>', ' apple', ' apple', ' \n', ' bedroom', ' garden', 'Answer', ' bedroom', ' apple', 'assistant', 'b', '.\n\n', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.06519362926483153, 0.005551288525263469, 0.04888463020324707, 0.006400803724924723]}, 'saliency': {'score': [0.0012182494004567463, 4.429696497496035e-05, 0.0015014429246225664, 3.855717657428683e-05, 2.809267648508851e-05], 'topk_tokens': [' apple', 'b', ' kitchen', ' the', ' the', ' bedroom', '<|end_header_id|>', ' the', ' \n', ' the', 'NEW', ' bedroom', ' apple', '<|start_header_id|>', ' apple', ' kitchen', '<|begin_of_text|>', ':', 'athroom', '.\n\n'], 'evidence_proportions': [0.00019599199295043944, 0.0007272710402806599, 0.0046178922057151794, 0.00029468039671579993]}}, 28: {'grad': {'score': [0.3304374331519717, 0.34145165691889373, 0.28428453014742944, 0.3416163490380077, 0.25191986728721943], 'topk_tokens': ['      ', ' summer', ' half', ' the', ' balance', ' as', ' the', ' spring', ' summer', '.', ' the', 'ball', 'ipp', ' inside', 'dent', 'half', 'nes', 'ot', ' half', ' returns'], 'evidence_proportions': [0.3495269775390625, 0.41738382975260413, 0.27008056640625, 0.26782099405924475]}, 'weight': {'score': [0.012412030072439285, 0.002398678565011669, 0.017650117797236287, 0.0023425277226068234, 0.00044683041706891126], 'topk_tokens': [' the', ' kitchen', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' bedroom', '?', ' garden', 'Answer', ' \n', ' apple', ' before', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0047236204147338865, 0.003106748064359029, 0.028821468353271484, 0.017184694608052574]}, 'saliency': {'score': [0.00044840858096168156, 4.363407858013082e-05, 0.00048194393034904236, 4.181838848930889e-05, 1.0056394926259216e-05], 'topk_tokens': [' \n', ' the', ' apple', ' apple', ' before', ' the', ' bedroom', 'b', ' Bridge', 'Bridge', 'athroom', ' apple', 'assistant', ' Bridge', ' garden', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.00032908916473388676, 7.990499337514241e-05, 0.0015197619795799255, 0.00020210941632588705]}}, 29: {'grad': {'score': [0.29463958740234375, 0.3604158300165753, 0.25410172247117563, 0.36080029680069414, 0.4424402747355716], 'topk_tokens': ['y', ' a', ' Paul', ' LO', 'UL', ' In', 'y', ' Ch', 'ION', 'tal', ',\n', ' THE', 're', ' The', 'ION', ' The', ' M', ' In', ' THE', ' ga'], 'evidence_proportions': [0.246875, 0.28997802734375, 0.3157005310058594, 0.32506434122721356]}, 'weight': {'score': [0.009865584827604749, 0.0024615919530474634, 0.012674312437734297, 0.0024227856903766046, 0.0006509687699062723], 'topk_tokens': ['nes', ' the', '<|eot_id|>', '<|eot_id|>', ' before', ' the', '?', ' the', ' \n', 'Answer', '.\n\n', ' apple', 'b', 'assistant', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008855342864990235, 0.002752790848414103, 0.033725738525390625, 0.008554985125859579]}, 'saliency': {'score': [0.00041876094681876045, 3.020105926321329e-05, 0.00023845414961538007, 2.8999612206938432e-05, 3.775702395909269e-05], 'topk_tokens': ['Does', ' before', ' a', ' the', '      ', ' apple', ' ', 'athroom', 'Answer', ' Does', '<|eot_id|>', 'nes', 'assistant', '<|end_header_id|>', ' the', ':', 'b', '<|start_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [4.976391792297363e-05, 0.00011215110619862874, 0.0016806870698928833, 0.00019158422946929932]}}, 30: {'grad': {'score': [0.33845738002232145, 0.3886149864574594, 0.4262083730389995, 0.38860577270909197, 0.3628399388890871], 'topk_tokens': [' months', ' two', ' an', ' Burb', ' soon', ' the', ' of', ' its', ' B', 'b', ' the', '2', 'b', '3', ' it', ' forb', ' B', ' account', 'b', 'deal'], 'evidence_proportions': [0.343157958984375, 0.1433842976888021, 0.5145187377929688, 0.41223907470703125]}, 'weight': {'score': [0.014958177294049944, 0.0024568690911753823, 0.01828263267394035, 0.002394959831551373, 0.0026007508727866157], 'topk_tokens': [' barric', ':', '.', ' the', 'Question', ' bedroom', '?', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', 'b', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010088217258453368, 0.00636791189511617, 0.041385650634765625, 0.00998842716217041]}, 'saliency': {'score': [0.0009023859387352353, 7.606949481868877e-05, 0.0006553771034363777, 7.316680832552018e-05, 3.5165481164421837e-05], 'topk_tokens': [' garden', ' the', ' apple', 'Bridge', '.', ' apple', ' bathroom', ' Bridge', ' Bench', 'assistant', ' Bridge', '.', ' the', ' bedroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'athroom', ':', 'b'], 'evidence_proportions': [0.0014297604560852052, 0.00045100847880045575, 0.0017247498035430908, 0.0003660420576731364]}}, 31: {'grad': {'score': [0.31631505489349365, 0.36014674117153966, 0.2936777885883085, 0.3603917886801232, 0.18611072570505277], 'topk_tokens': [' the', ' the', ' paper', ' the', ' August', ' the', ' was', ' population', ' number', ' the', ' location', 'membership', ' had', ' location', ' the', ' the', ' he', ' the', ' the', ' the'], 'evidence_proportions': [0.321394681930542, 0.32903385162353516, 0.2961730360984802, 0.3127912481625875]}, 'weight': {'score': [0.0038925125485374814, 0.0022756052434839613, 0.0039029554013283022, 0.002268667067709255, 0.0012113628253130845], 'topk_tokens': [' the', ':', ' before', ' Where', '.\n\n', '?', '<|eot_id|>', 'Answer', ' the', ' bedroom', ' \n', 'b', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010065793991088866, 0.0018867055575052896, 0.01232445240020752, 0.0026819705963134766]}, 'saliency': {'score': [0.00011127903347923642, 2.0871700451730598e-05, 9.542126809397052e-05, 2.0525659301670012e-05, 1.2647517969910528e-05], 'topk_tokens': [' Bridge', ' the', ' the', '.', ' was', ' apple', ' apple', ':', 'Answer', '<|eot_id|>', ' apple', 'b', ' \n', '<|begin_of_text|>', ' the', '<|start_header_id|>', ' bedroom', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [1.9615888595581053e-05, 4.154940446217855e-05, 0.00040496140718460083, 6.160636742909749e-05]}}, 'pred_res': 'The apple was in the kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:17:07.238 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:17:07.238 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-4_0-2-7-9.pkl | len: 10 |  size: 9.06 KB
Processing depth (0, 2, 7, 9):   5%|▌         | 5/100 [01:34<30:19, 19.15s/it]Processing depth (0, 2, 7, 9):   5%|▌         | 5/100 [01:35<30:05, 19.01s/it]
2025-01-22 03:17:07.439 | INFO     | __main__:<module>:72 - Selected idx: 13
2025-01-22 03:17:07.439 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 03:17:07.439 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:17:07.439 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:17:07.439 | INFO     | __main__:<module>:76 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Mary got the football.', ' Sandra journeyed to the office.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-22 03:17:07.440 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the football.', ' Mary journeyed to the bedroom.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-22 03:17:07.440 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (4, 5, 6, 7, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:17:14.567 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:17:14.580 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4895, 4900) --> . Mary moved to the
2025-01-22 03:17:14.581 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football.
2025-01-22 03:17:14.597 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5999, 6003) -->  Mary got the football
2025-01-22 03:17:14.597 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:17:14.618 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7271, 7277) --> . Mary journeyed to the
2025-01-22 03:17:14.618 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the football.
2025-01-22 03:17:14.641 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8452, 8456) -->  Mary left the football
2025-01-22 03:17:14.641 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 03:17:14.672 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10789, 10795) --> . Daniel journeyed to the
2025-01-22 03:17:14.672 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:17:14.675 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (932, 937) --> . Daniel picked up the
2025-01-22 03:17:14.675 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:17:14.692 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5953, 5958) --> . Sandra moved to the
2025-01-22 03:17:14.692 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:17:14.697 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1938, 1943) --> . John moved to the
2025-01-22 03:17:14.698 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:17:14.708 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3661, 3667) --> . Sandra journeyed to the
2025-01-22 03:17:14.708 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:17:14.712 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1262, 1266) -->  Daniel left the apple
2025-01-22 03:17:14.712 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:17:14.726 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4933, 4939) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:17:16.774 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 03:17:16.775 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12221])
your chose emoji: ['🤹🏿\u200d♀', '*⃣', '🏃🏾\u200d♂', '👨🏾\u200d❤️\u200d💋\u200d👨🏼', '🇭🇹', '🚳', '✒️', '💆\u200d♂️', '🐖', '🎖️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.62s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.24s/it][A
 75%|███████▌  | 6/8 [00:04<00:01,  1.98it/s][A100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.88it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.67it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.30it/s][A100%|██████████| 8/8 [00:00<00:00, 21.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.72it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.30it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.08it/s][A100%|██████████| 8/8 [00:00<00:00, 21.37it/s]
2025-01-22 03:17:25.132 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.24543930053710938, 0.3753824483661751, 0.2632099274666079, 0.37593520369520317, 0.3133835719181941], 'topk_tokens': [' agreement', 'remark', ' appearance', ' appearance', 'BR', ' appear', ' appearance', ' compromised', ' absence', ' expedition', ' EX', ' committee', ' considerable', 'able', ' exclaimed', ' appearance', ' compromised', ' examined', ' STE', 'consider'], 'evidence_proportions': [0.2503654479980469, 0.3486442565917969, 0.25826009114583337, 0.22793197631835938, 0.17138163248697916]}, 'weight': {'score': [0.023869898319244385, 0.0025714480596063026, 0.0066068999228938935, 0.0025174079326953174, 0.0021638278777782734], 'topk_tokens': [' THE', ' Mary', ' Sandra', ' football', ' kitchen', ':', 'Answer', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'Bridge', ' football', 'b', '<|eot_id|>', '\n\n', '<|end_header_id|>', ' bathroom', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010132867097854614, 0.04635930061340332, 0.01740194360415141, 0.059070467948913574, 0.0033253977696100874]}, 'saliency': {'score': [0.0020376348495483396, 5.2292321696487394e-05, 0.0005474753918186311, 4.6951736689084144e-05, 7.331783954913799e-05], 'topk_tokens': [' Market', ' April', ' the', ':', '<|eot_id|>', '.', '.', ' kitchen', 'Bridge', '<|start_header_id|>', '<|begin_of_text|>', ' football', '<|eot_id|>', ' Sandra', ' Mary', ' football', 'b', ' bathroom', 'athroom', ' bedroom'], 'evidence_proportions': [0.0006880998611450195, 0.005567893385887146, 0.0012205392122268677, 0.004404082894325256, 4.820525646209717e-05]}}, 25: {'grad': {'score': [0.454913330078125, 0.46742040823891523, 0.4680414507465978, 0.4674445226896287, 0.42546210655799277], 'topk_tokens': ['      ', ' William', ' Packet', ' Williams', ' in', 'ings', 'arr', ' passage', ' compos', 'Williams', 'arr', ' compos', ' the', ' partnership', ' Aw', ' set', 'L', ' squ', 'posit', ' Aw'], 'evidence_proportions': [0.61910400390625, 0.330841064453125, 0.4199930826822917, 0.5164337158203125, 0.3947092692057292]}, 'weight': {'score': [0.017452036142349244, 0.0025023555100276208, 0.0036227058979772754, 0.002468786075623078, 0.0024329744852506197], 'topk_tokens': ['�', ' Mary', '.', 'b', ' Bench', '?\n', ' THE', ' Mary', 'Answer', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', ' bedroom', '<|eot_id|>', ' bathroom', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.021817576885223386, 0.04291200637817383, 0.009979421893755596, 0.012752354145050049, 0.007446507612864176]}, 'saliency': {'score': [0.0002711617946624756, 3.139723180086201e-05, 8.755826180981052e-05, 3.076153932865477e-05, 8.701865489666279e-05], 'topk_tokens': [' top', ' Judge', ' Paul', ' April', 'assistant', '<|eot_id|>', ' Seventh', 'b', ' Geo', 'Answer', ' Merch', '189', ' bedroom', ' Bench', 'athroom', ' bathroom', ' THE', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000459754467010498, 0.0006173178553581238, 0.00012693305810292563, 0.00015494227409362793, 0.00010493894418080647]}}, 26: {'grad': {'score': [0.403115234375, 0.4848978931367085, 0.6226393176663306, 0.4847150021364311, 0.49264561579777644], 'topk_tokens': ['ist', ' galaxy', ' steam', ' readiness', ' Becker', ' God', ' printer', 'AM', ' commander', ' printer', ' Hill', ' Marshall', 'printer', ' and', 'ers', ' Run', ' proprietor', 'ub', 'is', ' Eagle'], 'evidence_proportions': [0.4499267578125, 0.5416107177734375, 0.3106689453125, 0.44207763671875, 0.33824666341145837]}, 'weight': {'score': [0.009016839265823364, 0.0024650182711516374, 0.002016926004040626, 0.0024526986899068683, 0.0019598942536574143], 'topk_tokens': [' Bench', ' barric', '�', 'Bridge', '?\n', ' kitchen', '<|eot_id|>', '<|eot_id|>', 'Answer', 'assistant', ' the', 'b', '\n\n', ' bedroom', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0037047803401947022, 0.01135706901550293, 0.009724155068397522, 0.01496189832687378, 0.007212713360786438]}, 'saliency': {'score': [0.0003334939479827881, 6.463820924153503e-05, 0.0001477108847710394, 6.387418504610883e-05, 0.0001053768854874831], 'topk_tokens': [' Merch', ' Anthony', ' James', 'assistant', ' Merch', 'Answer', ' East', ' the', '<|start_header_id|>', 'Bridge', '�', ' kitchen', ' bathroom', 'athroom', ' bedroom', '<|end_header_id|>', '\n\n', 'b', '<|begin_of_text|>', ':'], 'evidence_proportions': [9.496808052062988e-05, 0.00037203729152679443, 0.00041871269543965656, 0.0005156248807907104, 0.0002999305725097656]}}, 27: {'grad': {'score': [0.40149200439453125, 0.44247118964869314, 0.4767739542068974, 0.4424679922399201, 0.3855208763709435], 'topk_tokens': ['\n', ' back', ' other', 'direction', ' STR', ' staff', 'ers', ' intended', '."', ':\n\n', ' expected', 'roduced', ' *\n\n', 'ackers', 'str', ' designated', ' assistance', ' step', ' Thanksgiving', ' accepted'], 'evidence_proportions': [0.43914031982421875, 0.3916778564453125, 0.37657419840494794, 0.5078048706054688, 0.3307037353515625]}, 'weight': {'score': [0.012526427507400512, 0.00253060023197953, 0.002127760841000465, 0.0025110894117325562, 0.002245775552896353], 'topk_tokens': ['Bridge', ' Mary', 'THE', '<|eot_id|>', '?\n', ' the', '<|eot_id|>', 'Answer', 'assistant', '.\n\n', ' THE', 'b', '\n\n', '<|start_header_id|>', ':', ' bedroom', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01127229332923889, 0.029298484325408936, 0.009497602780659992, 0.010735273361206055, 0.006613095601399739]}, 'saliency': {'score': [0.0007036161422729492, 5.707603328281048e-05, 0.00019498313626935406, 5.539633054469934e-05, 0.00012389696561373198], 'topk_tokens': ['\n\n', ' Dan', '.', ' EAR', ' kitchen', ' Mary', '<|end_header_id|>', 'THE', 'NEW', ' the', '<|start_header_id|>', '<|begin_of_text|>', ' THE', ' Bridge', 'Bridge', '.\n\n', 'athroom', ' bathroom', ' bedroom', 'b'], 'evidence_proportions': [0.0011707842350006104, 0.0011113658547401428, 0.0003151843945185343, 0.0004464760422706604, 0.00060233473777771]}}, 28: {'grad': {'score': [0.30686431884765625, 0.3814137993058609, 0.30009657336819556, 0.38177413551686706, 0.4049232189471905], 'topk_tokens': ['E', ' the', ' in', 'nes', ' the', ' ', ' ', ',', '.', 'ot', '.', ' the', "'", 'S', '.', '.', 'nes', 'nes', 'nes', '.'], 'evidence_proportions': [0.293060302734375, 0.3440399169921875, 0.3415501912434896, 0.1562671661376953, 0.35929616292317706]}, 'weight': {'score': [0.013235212564468383, 0.0023898875526108666, 0.0017460700004331528, 0.0023692453122115464, 0.001212448340195876], 'topk_tokens': [' kitchen', '.\n\n', ' Bridge', ' the', ' bedroom', ' bathroom', ' the', '<|eot_id|>', 'Answer', '?\n', ' the', 'assistant', '<|eot_id|>', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0028688251972198487, 0.008136749267578125, 0.024966061115264893, 0.00828137993812561, 0.01684455076853434]}, 'saliency': {'score': [0.0004264509677886963, 2.8839298275284744e-05, 8.452323175245715e-05, 2.7880513456446178e-05, 3.540745148291955e-05], 'topk_tokens': ['�', 'Question', '.\n\n', ' nearly', ' Floral', 'Bridge', ' kitchen', ' Nearly', ' the', ' Bridge', 'b', ' the', 'athroom', '\n\n', ' bedroom', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [6.090998649597168e-05, 0.00022108852863311768, 0.0013773292303085327, 0.00010882318019866943, 0.00012885034084320068]}}, 29: {'grad': {'score': [0.45056488037109377, 0.41433574017429853, 0.5831167159541961, 0.4138313073378343, 0.37509026160606973], 'topk_tokens': ['\n', '\n', '\n', 'd', '\n', 're', 'b', ' an', ' S', '\n', '\n', ' l', '\n', ' book', ' on', 'd', 'ION', ' M', '\n', 'b'], 'evidence_proportions': [0.5505859375, 0.5394992828369141, 0.4321441650390625, 0.4428443908691406, 0.33149210611979163]}, 'weight': {'score': [0.004547692537307739, 0.0024477700600449327, 0.0010673451808191116, 0.0024469724852030877, 0.0016962166015918438], 'topk_tokens': ['IVE', 'Question', ' Where', 'NEW', '�', ' Does', '.\n\n', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', ' the', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004388868808746338, 0.0036286115646362305, 0.006264860431353251, 0.004550039768218994, 0.0035740335782368975]}, 'saliency': {'score': [0.0002155625820159912, 5.2663080725838375e-05, 5.764153695875599e-05, 5.2315708957637005e-05, 6.47265177506667e-05], 'topk_tokens': [' number', 't', '<|eot_id|>', 'Question', 'Does', 'nes', '<|eot_id|>', 'NEW', '<|end_header_id|>', 'Answer', ' the', 'IVE', ' Does', 'assistant', 'athroom', ':', '\n\n', 'b', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00020758509635925292, 0.0001629963517189026, 0.0003587851921717326, 0.0002191886305809021, 0.00011161466439565022]}}, 30: {'grad': {'score': [0.24091194152832032, 0.3778286030155202, 0.33399717269405244, 0.37822157563855974, 0.366768165735098], 'topk_tokens': ['ire', ' S', ' soon', 'AM', ' Times', 'b', ' Burb', ' Times', ' itself', ' United', 'B', ' B', 'deal', ' account', ' forb', ' B', 'b', ' B', 'b', 'b'], 'evidence_proportions': [0.2282867431640625, 0.27518463134765625, 0.17716471354166669, 0.287994384765625, 0.2609434127807617]}, 'weight': {'score': [0.013692551851272583, 0.0024660200816798584, 0.0031462321358342324, 0.0024412214403321756, 0.003516504856256338], 'topk_tokens': [':', '�', ' the', 'Question', ' the', ' bedroom', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'b', 'assistant', 'Answer', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008519041538238525, 0.012865781784057617, 0.021835342049598694, 0.010624915361404419, 0.01245729128519694]}, 'saliency': {'score': [0.0009117674827575684, 7.249566321675691e-05, 0.00018940241106094852, 7.047348170198945e-05, 9.743983928973858e-05], 'topk_tokens': ['Question', ' Ramsey', ' Mary', '.', ' the', ' Mary', 'Answer', ' the', ' Bench', ' Broadway', ' bedroom', 'Bridge', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom', 'athroom', ':', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0007930517196655274, 0.0011902227997779846, 0.0015206138292948404, 0.00034890323877334595, 0.0005914568901062012]}}, 31: {'grad': {'score': [0.20488283157348633, 0.2104318251160427, 0.16593990210563905, 0.21055657646810436, 0.1796197827045734], 'topk_tokens': [' the', ' having', ' the', ' of', ' were', ' the', 'user', ' he', ' Van', ' had', ' was', 'nes', ' the', ' had', 'nes', 'ible', 'nes', ' was', 'in', 'nes'], 'evidence_proportions': [0.2589419841766357, 0.16462528705596924, 0.15209444363911948, 0.2226090431213379, 0.22764281431833905]}, 'weight': {'score': [0.0029876375198364257, 0.0022929571373924533, 0.0007132272566518475, 0.0022955544924830075, 0.0011194806832533616], 'topk_tokens': [' bathroom', ' football', ',', ':', ' the', ' Where', 'Question', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', '<|start_header_id|>', 'assistant', ':', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0019545018672943114, 0.004017472267150879, 0.0019810845454533896, 0.003804922103881836, 0.0036237239837646484]}, 'saliency': {'score': [0.00011120080947875977, 2.3069501371751904e-05, 2.4382145174088017e-05, 2.28850853082618e-05, 1.0570654502281775e-05], 'topk_tokens': [' Mary', ' Bridge', ' Mary', ' the', '<|eot_id|>', 'Bridge', ' the', 'CH', ' bedroom', ' Market', 'Question', '<|eot_id|>', 'Answer', '<|start_header_id|>', 'b', '<|end_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'athroom'], 'evidence_proportions': [0.00018848776817321778, 0.000209711492061615, 0.00010211269060770671, 2.378225326538086e-05, 4.8488378524780273e-05]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:17:25.134 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:17:25.134 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-0_4-5-6-7-9.pkl | len: 10 |  size: 9.57 KB
Processing depth (4, 5, 6, 7, 9):   1%|          | 1/100 [00:17<29:02, 17.60s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.26it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.54it/s]
Processing depth (1, 3, 4, 6, 9):   1%|          | 1/100 [00:25<29:02, 17.60s/it]2025-01-22 03:17:32.705 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:17:32.710 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1502, 1507) -->  tragedy. Mary moved to
2025-01-22 03:17:32.710 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football.
2025-01-22 03:17:32.720 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3677, 3681) -->  Mary got the football
2025-01-22 03:17:32.720 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:17:32.734 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4849, 4855) --> . Mary journeyed to the
2025-01-22 03:17:32.734 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the football.
2025-01-22 03:17:32.753 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7190, 7194) -->  Mary left the football
2025-01-22 03:17:32.754 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 03:17:32.785 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10770, 10776) --> . Daniel journeyed to the
2025-01-22 03:17:32.785 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:17:32.788 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (932, 937) --> . Daniel picked up the
2025-01-22 03:17:32.788 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:17:32.804 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5863, 5868) --> . Sandra moved to the
2025-01-22 03:17:32.804 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:17:32.810 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1921, 1926) --> . John moved to the
2025-01-22 03:17:32.810 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:17:32.820 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3586, 3592) --> . Sandra journeyed to the
2025-01-22 03:17:32.820 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:17:32.824 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1251, 1255) -->  Daniel left the apple
2025-01-22 03:17:32.824 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:17:32.838 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4874, 4880) -->  war. John went back to
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:17:34.940 | INFO     | test_jbb_retain:begin_test:632 - the bathroom<|eot_id|>
2025-01-22 03:17:34.940 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12208])
your chose emoji: ['🍎', '🙎🏽', '🈵', '💍', '🇳🇴', '🐐', '👨🏽\u200d❤️\u200d👨🏽', '🌶️', '👫🏾', '🟤']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 140395.11it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.15s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.01s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.04it/s][A100%|██████████| 8/8 [00:05<00:00,  1.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.92it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.06it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.24it/s][A100%|██████████| 8/8 [00:00<00:00, 21.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.88it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.69it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.12it/s][A100%|██████████| 8/8 [00:00<00:00, 21.06it/s]
2025-01-22 03:17:44.157 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3593328857421875, 0.3301854700999611, 0.340514644499748, 0.3300991772330381, 0.39871924531226066], 'topk_tokens': [' first', 'ols', ' and', ' congreg', ' appearance', ' absence', ' whistle', ' first', ' competitor', 'st', ' compl', ' in', 'able', ' conventions', '202', ' agreement', ' committee', ' compromised', ' examined', 'consider'], 'evidence_proportions': [0.43579864501953125, 0.3358917236328125, 0.4349937438964844, 0.34234619140625, 0.2469024658203125]}, 'weight': {'score': [0.024829078912734986, 0.0025703351138977697, 0.01469364954579261, 0.0024936348800549337, 0.0034848790542752134], 'topk_tokens': [' Mary', ' the', ' garden', ' the', 'Answer', '<|eot_id|>', '<|start_header_id|>', ' kitchen', ' bedroom', 'assistant', ':', ' football', '<|eot_id|>', '\n\n', 'b', 'Bridge', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.013682967424392699, 0.04575371742248535, 0.004209955533345541, 0.08445024490356445, 0.0010394255320231118]}, 'saliency': {'score': [0.0014130711555480958, 4.9711401305769965e-05, 0.0004630088806152344, 4.585321819473323e-05, 0.0001195669174194336], 'topk_tokens': ['\n\n', '.', ' Mary', ':', ' football', '<|start_header_id|>', 'Answer', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', ' kitchen', ' office', ' Bench', ' bedroom', ' football', ' garden', 'b', ' bathroom', 'athroom', 'Bridge'], 'evidence_proportions': [0.0010953903198242187, 0.0024666935205459595, 0.00011929372946421306, 0.004784829914569855, 2.1328528722127278e-05]}}, 25: {'grad': {'score': [0.7287525177001953, 0.7494531377958398, 0.7264778383316532, 0.7495543101352706, 0.4819938902761422], 'topk_tokens': [' not', ' the', ' at', ' old', ' in', ' old', ' a', ' the', ' for', ' money', ' the', ' the', ' in', ' set', ' for', ' a', ' a', ' for', ' no', ' of'], 'evidence_proportions': [0.7667430877685546, 0.7845954895019531, 0.8190733591715494, 0.6228485107421875, 0.6401468912760416]}, 'weight': {'score': [0.015325907468795776, 0.002496340967749565, 0.0051654990642301495, 0.0024631461455762167, 0.003181549848294726], 'topk_tokens': [' bedroom', '.\n\n', ' the', '.', ' Mary', ' Mary', '?\n', 'b', 'Answer', '<|start_header_id|>', '<|eot_id|>', 'assistant', ' bathroom', ' Bench', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.024204981327056885, 0.042113304138183594, 0.0022035539150238037, 0.01803576946258545, 0.0013841936985651653]}, 'saliency': {'score': [0.0004366695880889893, 3.308193727164799e-05, 0.00018955911359479352, 3.18527736569667e-05, 0.0001324619732651056], 'topk_tokens': ['<|eot_id|>', ' THE', ' Paul', ' Ramsey', '?\n', '.\n\n', ' Mary', '<|start_header_id|>', ' Mary', 'assistant', '<|end_header_id|>', ' THE', ':', 'Answer', 'b', 'athroom', ' bathroom', ' Bench', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000709831714630127, 0.0009109973907470703, 7.23103682200114e-05, 0.0007611215114593506, 4.087388515472412e-05]}}, 26: {'grad': {'score': [0.4255840301513672, 0.4435058873658996, 0.509322997062437, 0.44337488909603295, 0.4842856538062002], 'topk_tokens': [' Becker', 'is', ' Fletcher', '-in', ' ice', 'UL', ',', 'ers', 'agle', ' Peter', ' Hill', 'UX', ' George', ' Empire', ' Marshall', ' and', 'UX', 'ine', 'ub', ' Eagle'], 'evidence_proportions': [0.45721435546875, 0.4926910400390625, 0.5363566080729166, 0.34564208984375, 0.2970094680786133]}, 'weight': {'score': [0.007806634902954102, 0.002448904983886639, 0.0029478707621174473, 0.002436612825342678, 0.001911641335954853], 'topk_tokens': [' bedroom', '.\n\n', ' Bench', '<|eot_id|>', 'Bridge', ' the', ' the', '<|eot_id|>', '?\n', 'assistant', 'Answer', ' kitchen', 'b', '<|start_header_id|>', '\n\n', ' bathroom', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.007382786273956299, 0.009748399257659912, 0.0013066728909810386, 0.025600790977478027, 0.0015025238196055095]}, 'saliency': {'score': [0.00030683040618896486, 6.313142783518223e-05, 0.000288143273322813, 6.205632773901101e-05, 6.54301222632913e-05], 'topk_tokens': [' Sandra', ' the', ' bedroom', ' Anthony', '?\n', 'athroom', ' Sandra', ' garden', ' Mary', ' Bench', 'Answer', 'Bridge', ' bathroom', ' the', ' kitchen', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0007091820240020752, 0.0005209147930145264, 3.397961457570394e-05, 0.0003664270043373108, 6.193419297536215e-05]}}, 27: {'grad': {'score': [0.2894401550292969, 0.3980790965241381, 0.3887181435861895, 0.3983264155762522, 0.4022728134604061], 'topk_tokens': [' conventions', 'ition', ' speculation', ' platform', ' one', ' four', ' drafted', '129', 'ors', 'roduced', 'CE', ' started', ' expected', ' excessive', ' intended', ' short', ' Thanksgiving', ' designated', ' step', ' accepted'], 'evidence_proportions': [0.38632049560546877, 0.2727537155151367, 0.22692362467447919, 0.3596315383911133, 0.23555310567220053]}, 'weight': {'score': [0.016407694816589356, 0.0025284356112640657, 0.004988324257635301, 0.002493615534902845, 0.004337016273947323], 'topk_tokens': ['<|eot_id|>', ' the', ' Mary', ' Mary', '<|eot_id|>', ' kitchen', '?\n', ' THE', 'Answer', ' bedroom', 'assistant', '.\n\n', 'b', '<|start_header_id|>', '\n\n', ':', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.020595848560333252, 0.035091668367385864, 0.003386040528615316, 0.03309619426727295, 0.002357572317123413]}, 'saliency': {'score': [0.0017199015617370605, 5.1288207990318186e-05, 0.0008232247444891161, 4.5887519674799364e-05, 0.0002442008140040379], 'topk_tokens': ['<|begin_of_text|>', ' dropped', ' the', '<|start_header_id|>', ' the', ' Mary', ' the', ' Mary', ' THE', ' the', ' bedroom', 'NEW', 'Bridge', ':', ' kitchen', '<|end_header_id|>', 'athroom', ' bathroom', 'b', '.\n\n'], 'evidence_proportions': [0.0018389046192169188, 0.0028724446892738342, 0.0009159594774246216, 0.004012830555438995, 0.00012769301732381183]}}, 28: {'grad': {'score': [0.48884105682373047, 0.542454797096368, 0.3945622521062051, 0.5429422518393965, 0.5373017180199716], 'topk_tokens': [',', ',', '.', 'ot', 'yl', 'nes', ' an', 'ien', 'antic', ' the', ',', 'nes', 'ien', '.', ' the', 'nes', 'in', 'dent', 'nes', 'nes'], 'evidence_proportions': [0.258929443359375, 0.2864227294921875, 0.7714767456054688, 0.28278231620788574, 0.6701164245605469]}, 'weight': {'score': [0.007944332361221313, 0.0023819372251943447, 0.004962629848910916, 0.0023639149010696316, 0.0012950260265200747], 'topk_tokens': ['Question', ' garden', ' the', '.\n\n', ' discarded', ' the', '<|eot_id|>', ' the', '<|eot_id|>', 'Answer', '?\n', 'assistant', ' bathroom', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0016819775104522706, 0.011155664920806885, 0.0011980533599853516, 0.0290219783782959, 0.003716588020324707]}, 'saliency': {'score': [0.0005165445804595947, 4.346004019360725e-05, 0.000203845001036121, 4.207797130896985e-05, 3.546476364135742e-05], 'topk_tokens': ['Question', ' the', '?\n', ' Floral', '<|eot_id|>', ' the', ' nearly', ' Mary', 'Bridge', ' Bridge', ' bathroom', 'athroom', 'b', ' the', '\n\n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.00019270777702331545, 0.0016966983675956726, 6.557007630666098e-05, 0.0010919421911239624, 6.701548894246419e-05]}}, 29: {'grad': {'score': [0.5600635528564453, 0.4079103801771968, 0.5153742144184728, 0.4073233618161552, 0.4188999662212297], 'topk_tokens': [' MILL', ' Pioneer', ' The', ' a', 'ION', ' Bench', ' The', ' The', ' B', ' B', ' book', 're', ' Pioneer', ' B', ' ga', 'Spring', ' M', 'b', '\n', 'b'], 'evidence_proportions': [0.644122314453125, 0.5917205810546875, 0.6733194986979166, 0.5405354499816895, 0.36867268880208337]}, 'weight': {'score': [0.0022233259677886964, 0.0024358876535810676, 0.0026771964565400156, 0.002435709411643847, 0.0020281427046831917], 'topk_tokens': [' discarded', 't', 'Question', ' Where', ' Does', '.\n\n', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', ' the', 'assistant', ' the', 'b', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008355438709259032, 0.0033946633338928223, 0.0010586778322855632, 0.0065912604331970215, 0.0008516112963358561]}, 'saliency': {'score': [0.00012991786003112792, 5.752706340270901e-05, 0.00015116891553325037, 5.713934910145379e-05, 9.171051137587604e-05], 'topk_tokens': ['IVE', ' was', 'NEW', 'Question', '      ', ':', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', 't', ' Does', '<|end_header_id|>', 'assistant', 'athroom', ' the', '<|start_header_id|>', '<|begin_of_text|>', 'b', '\n\n'], 'evidence_proportions': [6.769895553588867e-05, 0.00019256025552749634, 6.56197468439738e-05, 0.00040844082832336426, 1.8621484438578286e-05]}}, 30: {'grad': {'score': [0.45114300727844237, 0.39550950195072576, 0.48362042826990925, 0.39517035951147544, 0.4838060210732853], 'topk_tokens': [' of', 'b', ' his', 'arp', ' bouncing', ' S', '3', 'itter', ' account', ' Buchanan', 'deal', ' B', 'B', ' Burb', ' forb', 'b', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.466729736328125, 0.5233917236328125, 0.5042292277018229, 0.5245752334594727, 0.2879472176233927]}, 'weight': {'score': [0.007998415231704713, 0.00245911104069345, 0.006571362095494424, 0.0024372301367465874, 0.007366276254840926], 'topk_tokens': [',', ':', ' the', ' bathroom', ' bedroom', ' the', 'Question', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', 'b', '?\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00487866997718811, 0.009983539581298828, 0.00266339381535848, 0.025148868560791016, 0.0031761725743611655]}, 'saliency': {'score': [0.0003264927864074707, 7.423341894177115e-05, 0.00046860883312840613, 7.270876883675855e-05, 0.00014943585676305434], 'topk_tokens': ['.', '<|eot_id|>', ' Ramsey', 'Question', ' garden', ' Broadway', '<|eot_id|>', ' Sandra', 'Answer', ' Bridge', 'Bridge', ' the', ' Bench', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.0003369450569152832, 0.0006968379020690918, 0.0001402050256729126, 0.00041157007217407227, 0.00020045538743336994]}}, 31: {'grad': {'score': [0.3921392822265625, 0.5278789472466712, 0.3712692145378359, 0.5285575480973068, 0.27240844158565297], 'topk_tokens': [' the', 'did', ' the', ' the', ' had', ' the', ' the', ' the', ' the', ' the', ' their', 'membership', ' of', ' the', ' the', ' was', ' August', ' he', ' the', ' the'], 'evidence_proportions': [0.35754928588867185, 0.28309595584869385, 0.3525279362996419, 0.3969740867614746, 0.5300479729970297]}, 'weight': {'score': [0.0025038039684295655, 0.002290306625509914, 0.0010758676836567541, 0.0022929648051581626, 0.001685850176156736], 'topk_tokens': [' was', ' football', ',', ' the', ':', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|start_header_id|>', 'b', ':', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0013576507568359376, 0.0027596354484558105, 0.0008261352777481079, 0.007613182067871094, 0.0015597939491271973]}, 'saliency': {'score': [7.866859436035156e-05, 3.5948900672319667e-05, 3.208844892440304e-05, 3.587088188679804e-05, 1.186833662145278e-05], 'topk_tokens': [' football', ' Paul', ' was', ' dropped', ' prior', ' the', '\n\n', ' Market', ',', '?\n', ' the', '<|eot_id|>', '<|begin_of_text|>', 'Answer', '<|start_header_id|>', ':', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [0.0001077115535736084, 0.00010649114847183228, 7.4158112208048505e-06, 0.0001997053623199463, 2.6479363441467285e-05]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-22 03:17:44.158 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:17:44.159 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-1_1-3-4-6-9.pkl | len: 10 |  size: 9.55 KB
Processing depth (1, 3, 4, 6, 9):   2%|▏         | 2/100 [00:36<30:07, 18.44s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (2, 3, 6, 8, 9):   2%|▏         | 2/100 [00:44<30:07, 18.44s/it]2025-01-22 03:17:51.710 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:17:51.718 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2511, 2516) --> . Mary moved to the
2025-01-22 03:17:51.718 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football.
2025-01-22 03:17:51.729 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3856, 3860) -->  Mary got the football
2025-01-22 03:17:51.729 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:17:51.750 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7267, 7273) --> . Mary journeyed to the
2025-01-22 03:17:51.750 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the football.
2025-01-22 03:17:51.776 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9725, 9729) -->  Mary left the football
2025-01-22 03:17:51.776 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 03:17:51.807 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10792, 10798) --> . Daniel journeyed to the
2025-01-22 03:17:51.807 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:17:51.810 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (932, 937) --> . Daniel picked up the
2025-01-22 03:17:51.810 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:17:51.826 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5967, 5972) --> . Sandra moved to the
2025-01-22 03:17:51.827 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:17:51.832 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1965, 1970) -->  the ground. John moved
2025-01-22 03:17:51.832 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:17:51.843 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3742, 3748) --> . Sandra journeyed to the
2025-01-22 03:17:51.843 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:17:51.846 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1262, 1266) -->  Daniel left the apple
2025-01-22 03:17:51.847 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:17:51.861 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4947, 4953) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:17:53.828 | INFO     | test_jbb_retain:begin_test:632 - bedroom<|eot_id|>
2025-01-22 03:17:53.828 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12234])
your chose emoji: ['👳\u200d♂️', '🧑🏾\u200d🎤', '🧎\u200d♂️\u200d➡️', '👩🏻\u200d🦼', '🤷🏾\u200d♀', '👩🏼\u200d✈', '👨🏽\u200d🦽', '🤱🏿', '🔟', '\U0001f979']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 212369.82it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.62s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.10s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.88it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.83it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.10it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.39it/s][A100%|██████████| 8/8 [00:00<00:00, 21.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.70it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.64it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.07it/s][A100%|██████████| 8/8 [00:00<00:00, 20.99it/s]
2025-01-22 03:18:03.151 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.27370735168457033, 0.36438244153074434, 0.2997383609894783, 0.36473305673006573, 0.30821465826653816], 'topk_tokens': ['ad', ' absence', ' considerable', ' EX', ' emb', ' turtle', 'remark', ' committee', 'ed', ' exclaimed', ' compl', ' compromised', ' appearance', 'able', ' expedition', ' STE', 'ols', ' examined', ' compromised', 'consider'], 'evidence_proportions': [0.3214752197265625, 0.3503761291503906, 0.25931580861409503, 0.30631446838378906, 0.17544174194335938]}, 'weight': {'score': [0.028681271076202393, 0.002570229748782549, 0.005977936329380158, 0.0025079676243934166, 0.002012117342515425], 'topk_tokens': [' Mary', ' kitchen', '.', ' the', '?\n', 'Bridge', '<|start_header_id|>', ':', 'Answer', '<|eot_id|>', 'assistant', 'b', ' football', '<|eot_id|>', '\n\n', '<|end_header_id|>', ' bathroom', 'athroom', ' bedroom', '<|begin_of_text|>'], 'evidence_proportions': [0.022812461853027342, 0.01984429359436035, 0.021333863337834675, 0.09407901763916016, 0.003212173779805501]}, 'saliency': {'score': [0.0016622292995452882, 5.850973760145517e-05, 0.0004850549082602224, 5.4132766142706496e-05, 7.151009200455306e-05], 'topk_tokens': [' the', '.', '\n\n', ' Mary', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '.', ':', ' Sandra', '<|start_header_id|>', '.', '<|eot_id|>', 'Bridge', 'b', ' garden', ' football', 'athroom', ' bathroom', ' bedroom'], 'evidence_proportions': [0.0012234866619110106, 0.0014306232333183289, 0.001285096009572347, 0.00540480762720108, 6.433327992757162e-05]}}, 25: {'grad': {'score': [0.543271484375, 0.5328249933283791, 0.5110969543457031, 0.5328588499027406, 0.47447917987773947], 'topk_tokens': [' To', 'Mr', ' so', ' at', ' Bre', ' at', ' and', ' for', ' being', ' they', ' in', ' Packet', '�', ' Aw', ' of', ' set', 'L', 'posit', ' squ', ' Aw'], 'evidence_proportions': [0.6922607421875, 0.5167236328125, 0.5195210774739584, 0.53302001953125, 0.46739705403645837]}, 'weight': {'score': [0.016622095108032225, 0.0025021309185098216, 0.002900392778458134, 0.0024721378783327873, 0.0026210565845687667], 'topk_tokens': [' Mary', ' Mary', ' Mary', ' Mary', '.\n\n', 'b', '?\n', '<|start_header_id|>', ' Bench', ' bathroom', 'Answer', ':', 'assistant', '<|eot_id|>', ' bedroom', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.01736820936203003, 0.020843982696533203, 0.01224823792775472, 0.033233642578125, 0.0064852337042490635]}, 'saliency': {'score': [0.0010940861701965333, 3.1419682270893914e-05, 7.542294840658865e-05, 2.912670439975464e-05, 8.48692732971984e-05], 'topk_tokens': [' Az', ' Merch', '<|eot_id|>', '\n\n', ' Geo', ' Mary', ' Mary', ' THE', 'b', '<|eot_id|>', 'assistant', ' Mary', 'Answer', ' bedroom', ' Mary', ' bathroom', 'athroom', ' Bench', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.001652073860168457, 0.0017353370785713196, 0.0007392366727193197, 0.0018807798624038696, 3.1982858975728355e-05]}}, 26: {'grad': {'score': [0.5912080383300782, 0.6572019828440385, 0.7989866195186492, 0.6569765928822896, 0.5491575265859628], 'topk_tokens': [' Clean', 'itol', ' ice', ' printer', 'itol', ' steam', ' readiness', 'AM', ' Run', ' Press', ' Becker', ' Marshall', ' God', 'fect', '-in', ' galaxy', ' Hill', ' commander', 'ub', ' Eagle'], 'evidence_proportions': [0.7738037109375, 0.799072265625, 0.4306310017903646, 0.571990966796875, 0.4738572438557943]}, 'weight': {'score': [0.008851644992828369, 0.0024550797902866545, 0.002050425737134872, 0.0024429814523492244, 0.002020873806693337], 'topk_tokens': [' discarded', ' Bench', ' kitchen', 'Bridge', '.\n\n', ' the', '?\n', '<|eot_id|>', '<|eot_id|>', 'assistant', 'Answer', 'b', '<|start_header_id|>', '\n\n', ' bathroom', ' bedroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0061860203742980955, 0.003665417432785034, 0.006997779011726379, 0.027862071990966797, 0.003710731863975525]}, 'saliency': {'score': [0.000290297269821167, 7.2303132565863e-05, 0.00013708587615720688, 7.169085783614341e-05, 0.00011154312592048149], 'topk_tokens': [' East', ' Floral', '�', ' Az', '?\n', '<|start_header_id|>', ' Anthony', ' kitchen', 'assistant', 'Bridge', ' kitchen', 'Answer', 'athroom', ' bathroom', '<|end_header_id|>', '\n\n', ' bedroom', 'b', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.000463569164276123, 0.00014745444059371948, 0.00019415716330210367, 0.0005681142210960388, 0.00015206138292948404]}}, 27: {'grad': {'score': [0.33302261352539064, 0.348399684837736, 0.4008855512065272, 0.3482976706291633, 0.3816029808738015], 'topk_tokens': [' the', '�', ' short', ' both', ' *\n\n', 'end', ' decision', ' started', ' intended', ' successful', 'ers', ' STR', ' assistance', ' staff', 'ackers', 'str', ' designated', ' Thanksgiving', ' accepted', ' step'], 'evidence_proportions': [0.42688598632812497, 0.28128814697265625, 0.2745825449625651, 0.44936370849609375, 0.270172119140625]}, 'weight': {'score': [0.012619991302490235, 0.0025238444078998297, 0.0024471523300293955, 0.002503318489013796, 0.00235628385048408], 'topk_tokens': [' discarded', ' Mary', ' football', 'THE', ' THE', '?\n', '<|eot_id|>', '<|eot_id|>', 'Answer', 'assistant', '.\n\n', 'b', '<|start_header_id|>', '\n\n', ':', ' bathroom', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01560584306716919, 0.011390119791030884, 0.011370927095413208, 0.020792245864868164, 0.006752590338389078]}, 'saliency': {'score': [0.0009311401844024659, 5.3488235024734876e-05, 0.00023757738451803885, 5.121846551740906e-05, 5.541412861316235e-05], 'topk_tokens': [' Daniel', ' kitchen', ' THE', 'athroom', ' the', 'NEW', ' Bridge', 'Answer', ':', '<|start_header_id|>', '<|begin_of_text|>', 'THE', '<|end_header_id|>', 'Bridge', 'THE', ' THE', ' bathroom', '.\n\n', ' bedroom', 'b'], 'evidence_proportions': [0.0008354663848876953, 0.00033761560916900635, 0.0011016776164372761, 0.0014831796288490295, 0.0008679876724878948]}}, 28: {'grad': {'score': [0.3594157028198242, 0.4414987828282054, 0.3814906458700857, 0.4418199657562008, 0.3275460206068955], 'topk_tokens': [' the', '\n', '      ', ' the', ' an', 'ot', 'nes', ',', '.', ',', '      ', '.', "'", 'nes', '      ', ' the', '.', 'nes', '.', 'nes'], 'evidence_proportions': [0.2318960189819336, 0.28572940826416016, 0.4750080108642578, 0.15822601318359375, 0.5333404541015625]}, 'weight': {'score': [0.008446476459503173, 0.0023803551952813764, 0.0022698506232230894, 0.0023681864579140227, 0.0011876535106014896], 'topk_tokens': [' the', ' kitchen', 'Question', ' bathroom', '.\n\n', ' discarded', ' the', '<|eot_id|>', ' bedroom', 'Answer', '?\n', 'assistant', '<|eot_id|>', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002646028995513916, 0.0022729039192199707, 0.012572089831034342, 0.01850128173828125, 0.006567080815633139]}, 'saliency': {'score': [0.00021041274070739745, 3.7423434031110014e-05, 0.00012563697753414032, 3.684389601982185e-05, 2.4048538951130656e-05], 'topk_tokens': [' Sandra', '<|eot_id|>', 'Bridge', ' bathroom', ' Nearly', '.\n\n', ' the', ' kitchen', ' Bridge', ' Floral', ' nearly', 'athroom', 'b', 'assistant', ' bedroom', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [9.77635383605957e-05, 0.00013049691915512085, 0.0002920826276143392, 0.00043120235204696655, 0.0001287013292312622]}}, 29: {'grad': {'score': [0.37722625732421877, 0.37107082627865284, 0.4735068044354839, 0.3707974988754018, 0.34420846963857676], 'topk_tokens': ['\n', 'b', ' L', ' THE', 'st', ' pres', ' S', ' Ch', 'UL', 'd', ' an', ' ST', 'd', 'd', ' l', 're', 'ION', ' M', '\n', 'b'], 'evidence_proportions': [0.4985107421875, 0.4084930419921875, 0.3715718587239583, 0.34433555603027344, 0.2828928629557292]}, 'weight': {'score': [0.003870701789855957, 0.0024493156602065666, 0.0013018548488616943, 0.0024493186675056764, 0.0013481289535373837], 'topk_tokens': [' the', ' was', 'Question', ' the', ' Where', ' Does', ' the', '.\n\n', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0027592122554779054, 0.0015107393264770508, 0.004374926288922628, 0.010107994079589844, 0.0017078320185343423]}, 'saliency': {'score': [0.0001773512363433838, 4.5318326656263307e-05, 6.312420291285361e-05, 4.5002030382875876e-05, 4.618585883796989e-05], 'topk_tokens': [':', 'IVE', '      ', ' the', 'am', '<|eot_id|>', '<|end_header_id|>', 'nes', '?\n', 't', '<|eot_id|>', 'Answer', ' Does', 'assistant', 'athroom', ':', '<|start_header_id|>', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00012121200561523437, 5.034357309341431e-05, 0.000204165776570638, 0.0005405843257904053, 3.983577092488607e-05]}}, 30: {'grad': {'score': [0.3464718532562256, 0.43047717631306437, 0.3956488332440776, 0.43073822316566757, 0.36771369290042233], 'topk_tokens': [' an', ' the', ' the', ' itself', ' until', ' of', ' his', 'B', ' Buchanan', ' S', ' B', ' Burb', 'b', ' account', 'deal', ' forb', ' B', 'b', ' B', 'b'], 'evidence_proportions': [0.2091705322265625, 0.5308609008789062, 0.2957133849461874, 0.41550445556640625, 0.3427003224690755]}, 'weight': {'score': [0.01244969606399536, 0.0024745380351422043, 0.004560485962898501, 0.0024487566266796993, 0.0038323201142348253], 'topk_tokens': [' Where', ' bathroom', ':', ',', ' the', 'Question', ' bedroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'b', 'assistant', 'Answer', '?\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009356087446212769, 0.005059659481048584, 0.016577675938606262, 0.02551579475402832, 0.007115681966145833]}, 'saliency': {'score': [0.0005734992027282714, 5.848505698242462e-05, 0.00022958651665718326, 5.699260981933754e-05, 9.269760800646497e-05], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' Broadway', ' the', 'Bridge', ' Emily', '?\n', ' Bridge', ' kitchen', '.\n\n', 'Answer', ' Bench', ' bedroom', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'b', ':', 'athroom'], 'evidence_proportions': [0.0005728602409362793, 0.00016526877880096436, 0.0005668103694915771, 0.000756576657295227, 0.000730822483698527]}}, 31: {'grad': {'score': [0.2690701866149902, 0.3498944825495615, 0.26531272934329125, 0.35027562053065986, 0.24252700031577767], 'topk_tokens': [' was', 'cert', ' his', 'user', ' the', ' Van', ' he', 'nes', 'ible', 'nes', ' OCC', ' the', ' the', ' adher', ' the', ' had', 'nes', ' was', 'in', ' the'], 'evidence_proportions': [0.29180278778076174, 0.1705012321472168, 0.21021588643391925, 0.2879319190979004, 0.36211880048116046]}, 'weight': {'score': [0.0035410642623901365, 0.0022863827763288016, 0.0008802894623048844, 0.002287386130370605, 0.001374109224839644], 'topk_tokens': [' bedroom', ' was', ',', ':', ' the', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|start_header_id|>', ':', 'b', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0031909167766571043, 0.0026174187660217285, 0.0026065558195114136, 0.007582366466522217, 0.002688924471537272]}, 'saliency': {'score': [5.460381507873535e-05, 2.722415969273794e-05, 2.6851892471313477e-05, 2.716891372764593e-05, 1.3016648106760793e-05], 'topk_tokens': [' Geo', ' Bridge', ' the', ' Emily', 'Question', 'CH', ' bedroom', '\n\n', ' the', ' Market', '<|eot_id|>', '?\n', '<|start_header_id|>', 'Answer', '<|begin_of_text|>', 'b', ':', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [7.746219635009766e-05, 2.2776424884796143e-05, 2.401570479075114e-05, 4.6566128730773926e-05, 9.27199920018514e-05]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:18:03.153 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:18:03.153 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-2_2-3-6-8-9.pkl | len: 10 |  size: 9.54 KB
Processing depth (2, 3, 6, 8, 9):   3%|▎         | 3/100 [00:55<30:13, 18.69s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (1, 2, 3, 4, 7):   3%|▎         | 3/100 [01:03<30:13, 18.69s/it]2025-01-22 03:18:10.670 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:18:10.675 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1541, 1546) --> . Mary moved to the
2025-01-22 03:18:10.675 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football.
2025-01-22 03:18:10.682 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2514, 2518) -->  Mary got the football
2025-01-22 03:18:10.682 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:18:10.696 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3856, 3862) --> . Mary journeyed to the
2025-01-22 03:18:10.697 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the football.
2025-01-22 03:18:10.710 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4937, 4941) -->  Mary left the football
2025-01-22 03:18:10.711 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 03:18:10.735 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (8456, 8462) --> . Daniel journeyed to the
2025-01-22 03:18:10.735 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:18:10.738 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (932, 937) --> . Daniel picked up the
2025-01-22 03:18:10.738 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:18:10.755 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5976, 5981) --> . Sandra moved to the
2025-01-22 03:18:10.755 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:18:10.761 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1948, 1953) --> . John moved to the
2025-01-22 03:18:10.761 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:18:10.772 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3743, 3749) --> . Sandra journeyed to the
2025-01-22 03:18:10.772 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:18:10.775 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1262, 1266) -->  Daniel left the apple
2025-01-22 03:18:10.775 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:18:10.790 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4982, 4988) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:18:12.945 | INFO     | test_jbb_retain:begin_test:632 - the bedroom<|eot_id|>
2025-01-22 03:18:12.946 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12239])
your chose emoji: ['🔶', '😶\u200d🌫', '🦌', '🇹🇿', '🙎🏾\u200d♀', '💆🏻\u200d♀️', '🤹🏿\u200d♂', '🧑🏼\u200d🎓', '👨🏿\u200d❤\u200d👨🏼', '🧑🏿\u200d🦯\u200d➡️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:34,  4.99s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.33s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.84it/s][A100%|██████████| 8/8 [00:05<00:00,  1.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.38it/s][A
 50%|█████     | 4/8 [00:00<00:00, 19.20it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 19.63it/s][A100%|██████████| 8/8 [00:00<00:00, 18.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.88it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.09it/s][A
100%|██████████| 8/8 [00:00<00:00, 16.48it/s][A100%|██████████| 8/8 [00:00<00:00, 17.25it/s]
2025-01-22 03:18:21.937 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.28512451171875, 0.32593650425245824, 0.3148764025780462, 0.32604836728916015, 0.38171557100807746], 'topk_tokens': [' advantage', ':\n\n', 'able', ' conventions', 'adj', ' S', ' competitor', ' It', 'announcement', ' compl', ' absence', ' appearance', ' agreement', 'ols', ' turtle', ' appearance', ' compromised', ' committee', ' examined', 'consider'], 'evidence_proportions': [0.2826507568359375, 0.2831268310546875, 0.36021169026692706, 0.25212669372558594, 0.2354291280110677]}, 'weight': {'score': [0.021593517065048216, 0.00256643418372992, 0.013953947251842867, 0.0024984307390274367, 0.0009990312704225865], 'topk_tokens': [' office', ' the', ' Mary', ' barric', '<|start_header_id|>', 'Answer', ' garden', ' football', '<|eot_id|>', ':', 'assistant', 'Bridge', '<|eot_id|>', '\n\n', 'b', ' bedroom', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010822230577468872, 0.04600238800048828, 0.007421945532162985, 0.059807777404785156, 0.002992406487464905]}, 'saliency': {'score': [0.0014743304252624512, 5.244145380128269e-05, 0.0004491325347654281, 4.8515255885935764e-05, 3.215815962814703e-05], 'topk_tokens': [' office', ' Bridge', '<|start_header_id|>', ':', '<|eot_id|>', ' the', ' office', '.', ' Mary', ' Bench', '<|eot_id|>', '.', ' football', '<|begin_of_text|>', 'b', 'Bridge', ' bathroom', ' garden', 'athroom', ' bedroom'], 'evidence_proportions': [0.0007827401161193847, 0.003741323947906494, 0.00028945008913675946, 0.003946833312511444, 7.587174574534099e-05]}}, 25: {'grad': {'score': [0.6981178665161133, 0.7156034138365872, 0.6876662162042433, 0.7157103555573827, 0.39843634861271554], 'topk_tokens': [' for', ' make', ' the', ' made', ' the', ' money', ' over', ' at', ' some', ' as', ' in', ' set', 'posit', ' for', ' for', ' a', ' a', ' no', ' for', ' of'], 'evidence_proportions': [0.8194656372070312, 0.613494873046875, 0.7375483512878418, 0.6744308471679688, 0.6297709147135417]}, 'weight': {'score': [0.012106038331985473, 0.0024908502357805736, 0.005415766469893917, 0.0024636836425044667, 0.0015946786578108624], 'topk_tokens': [' Mary', ' bedroom', ' discarded', '.', '.', ' the', '?\n', 'b', '<|start_header_id|>', 'Answer', ' bathroom', 'assistant', ' Bench', '<|eot_id|>', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.013992726802825928, 0.021573305130004883, 0.005282307664553325, 0.020405888557434082, 0.005512783924738566]}, 'saliency': {'score': [0.0003923022747039795, 3.5592994937669254e-05, 0.0001747367843504875, 3.450722524573137e-05, 5.305031450783334e-05], 'topk_tokens': [' top', ' Geo', '<|start_header_id|>', '<|eot_id|>', ' barric', '<|eot_id|>', ' THE', ' Merch', ' bedroom', 'assistant', 'b', ':', ' THE', 'athroom', 'Answer', ' bathroom', '<|end_header_id|>', ' Bench', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00024904608726501466, 0.000577777624130249, 0.00032264987627665204, 0.0009580180048942566, 8.054077625274658e-05]}}, 26: {'grad': {'score': [0.435, 0.44571247421785654, 0.5440585228704637, 0.4454842684364037, 0.4208648030350848], 'topk_tokens': [' Gutenberg', 'agle', ' Empire', 'ente', ' Malta', ' boats', 'UX', ' galaxy', 'issippi', 'UX', ' Marshall', ' Press', ' Becker', 'ers', ' into', ' and', '-in', ' Marshall', ' Eagle', 'ub'], 'evidence_proportions': [0.42135009765625, 0.545562744140625, 0.45860799153645837, 0.379913330078125, 0.38578287760416663]}, 'weight': {'score': [0.005119602680206298, 0.0024597148181682823, 0.0038832089593333584, 0.0024506367355384556, 0.0011490241783421215], 'topk_tokens': [' Bench', 'Bridge', ' kitchen', ' bedroom', '<|eot_id|>', '?\n', ' barric', '<|eot_id|>', 'assistant', 'Answer', ' the', 'b', ' the', '<|start_header_id|>', '\n\n', ' bathroom', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.005278939008712768, 0.00864112377166748, 0.0009913047154744465, 0.011416912078857422, 0.002569233377774556]}, 'saliency': {'score': [0.0001871979236602783, 5.988542004516557e-05, 0.00022795123438681326, 5.919669094332994e-05, 3.951460849948046e-05], 'topk_tokens': [' Anthony', 'river', ' Merch', ' barric', ' Bench', ' garden', 'athroom', 'Bridge', ' the', ' kitchen', ' the', 'Answer', '<|start_header_id|>', ' bedroom', ' bathroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.00025001764297485354, 0.0003174692392349243, 3.0810634295145675e-05, 0.00020736455917358398, 0.00019094347953796387]}}, 27: {'grad': {'score': [0.3659219837188721, 0.4538624026069959, 0.4676094670449534, 0.4540078442182405, 0.37359139977431877], 'topk_tokens': ['ackers', ' follow', ' consultations', ' ground', 'event', 'ers', ' other', ' one', 'ors', ' started', '!"', ' assistance', '\n', 'roduced', ' intended', ' short', ' designated', ' Thanksgiving', ' accepted', ' step'], 'evidence_proportions': [0.35811562538146974, 0.4183769226074219, 0.3574059804280599, 0.47730255126953125, 0.2717196146647135]}, 'weight': {'score': [0.012667047977447509, 0.0025246629059032335, 0.006408676985771426, 0.002493974898085695, 0.0014976975394458305], 'topk_tokens': [' Mary', ' garden', '<|eot_id|>', ' the', '<|eot_id|>', '?\n', ' THE', 'Answer', ' barric', 'assistant', '.\n\n', ' bedroom', 'b', '<|start_header_id|>', '\n\n', ':', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.014119613170623779, 0.028506487607955933, 0.0031085411707560224, 0.025064289569854736, 0.002190629641215007]}, 'saliency': {'score': [0.0008899652957916259, 5.135679484543273e-05, 0.0007990839019898445, 4.7734215422723755e-05, 4.7783299190242114e-05], 'topk_tokens': [' discarded', ' the', ' the', ' football', 'THE', ' dropped', ' the', '.', ' kitchen', '<|begin_of_text|>', ' THE', ':', ' bedroom', 'Bridge', '<|end_header_id|>', ' Bridge', 'athroom', ' bathroom', '.\n\n', 'b'], 'evidence_proportions': [0.0011007368564605712, 0.0010281503200531006, 0.00019871691862742105, 0.0026519596576690674, 0.00013878444830576578]}}, 28: {'grad': {'score': [0.3790754699707031, 0.5127935111960873, 0.41121301343364103, 0.5133262492940086, 0.4801670051202541], 'topk_tokens': ['IO', 'ot', 'ien', ',', ' an', ' ', 'nes', ' a', ' the', ' the', '.', '.', 'in', ' the', 'dent', "'", 'nes', 'nes', ' the', 'nes'], 'evidence_proportions': [0.365985107421875, 0.2840147018432617, 0.44113032023111975, 0.19541549682617188, 0.5137430826822917]}, 'weight': {'score': [0.004731636047363281, 0.0023819179126706473, 0.003973491730228547, 0.002373048575586156, 0.000870955789961466], 'topk_tokens': [' was', '.\n\n', ' garden', ' Bridge', ' discarded', '?\n', '<|eot_id|>', 'Answer', ' the', ' bathroom', ' the', '<|eot_id|>', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0019858479499816895, 0.004660487174987793, 0.003312408924102783, 0.01087796688079834, 0.004388898611068726]}, 'saliency': {'score': [0.00023330211639404296, 3.748134769386101e-05, 0.00017810252404982044, 3.672188801188667e-05, 2.124578487582323e-05], 'topk_tokens': [' the', 'Question', ' kitchen', ' nearly', '<|eot_id|>', ' bedroom', ' Bridge', ' bathroom', 'b', 'Bridge', '<|start_header_id|>', ' the', 'athroom', '\n\n', ' the', 'assistant', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [9.467005729675293e-05, 0.00036482512950897217, 0.0002302726109822591, 0.0005769878625869751, 3.5052498181660966e-05]}}, 29: {'grad': {'score': [0.5812408447265625, 0.41806427635139276, 0.5757883133426789, 0.4173282793584411, 0.39350885879702685], 'topk_tokens': [' Bench', 'b', ' The', ' a', ' Paul', 's', ' B', ' Pioneer', ' B', ' The', ' The', 'Spring', 're', ' book', ' B', ' ga', ' M', '\n', 'b', 'b'], 'evidence_proportions': [0.7290771484375, 0.6391067504882812, 0.6148834228515625, 0.5740089416503906, 0.39064534505208337]}, 'weight': {'score': [0.003667978048324585, 0.0024461773481464057, 0.0023547507101489653, 0.0024439033622834045, 0.0008875451436856898], 'topk_tokens': [' in', ' was', ' the', ' Where', ' Does', '.\n\n', '<|eot_id|>', '?\n', '<|eot_id|>', ' the', 'Answer', ' the', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013373613357543944, 0.0036077499389648438, 0.0013068914413452148, 0.010199308395385742, 0.0036571770906448364]}, 'saliency': {'score': [0.00030517220497131345, 5.5435525788685946e-05, 0.00015954817495038434, 5.465832989966754e-05, 3.125536732557343e-05], 'topk_tokens': [':', '      ', 'IVE', ' was', ' part', ' the', '      ', ' the', '<|eot_id|>', '<|eot_id|>', 'Answer', '<|end_header_id|>', ' Does', 'assistant', ':', ' the', 'athroom', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [4.14729118347168e-05, 0.00030381232500076294, 8.654097716013591e-05, 0.0011418014764785767, 0.00018670658270517987]}}, 30: {'grad': {'score': [0.34480499267578124, 0.3577373811883526, 0.33803853681010587, 0.3578140243763175, 0.3471824744852578], 'topk_tokens': [' Times', 'itter', ' Europe', ' the', ' soon', ' its', 'b', ' of', ' a', ' Burb', ' account', ' B', 'B', 'deal', ' forb', ' B', 'b', 'b', ' B', 'b'], 'evidence_proportions': [0.32581787109375, 0.25282859802246094, 0.4041951497395833, 0.38002777099609375, 0.33907318115234375]}, 'weight': {'score': [0.007920585870742798, 0.002458003470563086, 0.006410729500555223, 0.0024367414430779186, 0.002859755260188405], 'topk_tokens': [':', ',', ' bathroom', 'Question', ' the', ' barric', ' the', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'Answer', 'assistant', 'b', '?\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006280702352523804, 0.008546590805053711, 0.0031269838412602744, 0.021269679069519043, 0.0047640254100163775]}, 'saliency': {'score': [0.0007689535617828369, 6.830073215080776e-05, 0.00037668885723237067, 6.607880923776581e-05, 6.622766576162198e-05], 'topk_tokens': [' garden', ' Broadway', ' Bridge', ' Mary', '.', ' the', ' the', ' the', '.\n\n', ' barric', ' Bench', ':', 'Bridge', ' Bridge', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ' bathroom', 'athroom', 'b'], 'evidence_proportions': [0.000644916296005249, 0.0010918155312538147, 0.00032207369804382324, 0.002215184271335602, 0.0001398026943206787]}}, 31: {'grad': {'score': [0.2656104052066803, 0.4127240912467509, 0.300046117075028, 0.4133125418745472, 0.27710663908865396], 'topk_tokens': [' is', ' the', ' the', ' that', ' the', ' the', ' the', ' the', 'did', ' they', 'membership', ' August', ' the', 'If', ' the', ' had', ' was', ' the', ' the', ' he'], 'evidence_proportions': [0.3606519222259521, 0.2880987524986267, 0.22375435133775073, 0.2573206424713135, 0.21879947185516357]}, 'weight': {'score': [0.001555246114730835, 0.002291895170419011, 0.0012731234873494795, 0.0022959980875835737, 0.0009387193656549221], 'topk_tokens': [' the', ' was', ',', ':', 'Question', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|start_header_id|>', 'b', 'assistant', ':', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0018407464027404784, 0.002256542444229126, 0.0009130090475082397, 0.0023450255393981934, 0.0009655157725016276]}, 'saliency': {'score': [7.824897766113281e-05, 2.6282298566858435e-05, 3.677991128736927e-05, 2.6148982222554092e-05, 1.0724111301142994e-05], 'topk_tokens': [' Bridge', ' football', '<|start_header_id|>', ' dropped', ',', ' Mary', '\n\n', ' Emily', ' the', 'CH', ' Market', ' the', '<|eot_id|>', 'Answer', '<|begin_of_text|>', ':', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [0.00015514492988586426, 0.00012718886137008667, 5.940596262613932e-05, 5.758553743362427e-05, 1.4161070187886557e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:18:21.939 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:18:21.940 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-3_1-2-3-4-7.pkl | len: 10 |  size: 9.54 KB
Processing depth (1, 2, 3, 4, 7):   4%|▍         | 4/100 [01:14<29:58, 18.73s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 4, 6, 8, 9):   4%|▍         | 4/100 [01:22<29:58, 18.73s/it]2025-01-22 03:18:29.789 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:18:29.790 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:18:29.790 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the football.
2025-01-22 03:18:29.803 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4879, 4883) -->  Mary got the football
2025-01-22 03:18:29.803 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:18:29.825 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7220, 7226) --> . Mary journeyed to the
2025-01-22 03:18:29.825 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the football.
2025-01-22 03:18:29.851 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9658, 9662) -->  left the football.
2025-01-22 03:18:29.851 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 03:18:29.882 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10785, 10791) --> . Daniel journeyed to the
2025-01-22 03:18:29.882 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:18:29.885 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (938, 943) --> . Daniel picked up the
2025-01-22 03:18:29.885 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:18:29.902 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5936, 5941) --> . Sandra moved to the
2025-01-22 03:18:29.902 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:18:29.908 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (1971, 1976) -->  the ground. John moved
2025-01-22 03:18:29.908 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:18:29.918 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (3695, 3701) --> . Sandra journeyed to the
2025-01-22 03:18:29.918 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:18:29.922 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (1268, 1272) -->  Daniel left the apple
2025-01-22 03:18:29.922 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:18:29.936 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4911, 4917) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:18:31.881 | INFO     | test_jbb_retain:begin_test:632 - Mary<|eot_id|>
2025-01-22 03:18:31.881 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12237])
your chose emoji: ['\U0001fae9', '👵🏻', '👈🏽', '🙎🏻\u200d♂', '🚵', '✍🏾', '👩🏽\u200d🎤', '👩🏻\u200d❤️\u200d💋\u200d👩🏽', '👩🏼\u200d❤\u200d👨🏼', '🅾️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 225197.53it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.84s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.05it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.77it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.66it/s][A100%|██████████| 8/8 [00:05<00:00,  1.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 22.88it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.91it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.08it/s][A100%|██████████| 8/8 [00:00<00:00, 17.91it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.57it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 17.00it/s][A100%|██████████| 8/8 [00:00<00:00, 17.71it/s]
2025-01-22 03:18:41.035 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.269075927734375, 0.33734532773884296, 0.3015758760513798, 0.33757641670818167, 0.3970236361026764], 'topk_tokens': [' Key', 'ols', ' hopes', 'en', 'st', ' appearance', ' honor', ' told', ' latter', ' appearance', ' committee', ' hopes', ' appearance', ' absence', ' first', ' compl', ' STE', ' first', ' or', 'consider'], 'evidence_proportions': [0.26576385498046873, 0.298095703125, 0.2718238830566406, 0.2564849853515625, 0.2581354777018229]}, 'weight': {'score': [0.020463823080062865, 0.0025719056721606286, 0.004665598753959902, 0.002529866734149029, 0.0005197662860155106], 'topk_tokens': ['.', 'ORT', ' football', ' bathroom', ' football', 'Answer', '<|eot_id|>', ' football', ':', ' Bridge', '<|start_header_id|>', 'assistant', '<|eot_id|>', ' bedroom', 'b', ' barric', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02370448112487793, 0.035114288330078125, 0.011222496628761292, 0.04265451431274414, 0.002443830172220866]}, 'saliency': {'score': [0.004565635919570923, 6.31177907480913e-05, 0.0002138854995850594, 5.349560163166675e-05, 1.7347931861877442e-05], 'topk_tokens': ['<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '.', ' football', ' composing', 'ORT', ' football', ' barric', ' Bull', ' football', 'Mary', ' Bench', '.', ' bedroom', ' garden', '<|begin_of_text|>', ' Mary', ' bathroom', 'athroom'], 'evidence_proportions': [0.009327751398086549, 0.011228583753108978, 0.000805219014485677, 0.004354633390903473, 5.632638931274414e-05]}}, 25: {'grad': {'score': [0.7075762939453125, 0.5327738643471711, 0.5973842990013861, 0.532250802609299, 0.43874549865722656], 'topk_tokens': [' considered', 'E', ' for', 'posit', ' at', ' at', ' incorporated', ' free', ' the', ' money', ' set', ' make', ' no', ' Aw', ' of', ' at', ' for', ' in', ' for', 'M'], 'evidence_proportions': [0.7927734375000001, 0.705108642578125, 0.6761220296223958, 0.8646354675292969, 0.564971923828125]}, 'weight': {'score': [0.011333200931549072, 0.0025018181676178977, 0.002667697206620247, 0.0024832752573004852, 0.0007844597101211547], 'topk_tokens': [' bedroom', ' to', '.', 'b', ' prior', '?\n', ' Mary', ' discarded', ' Bench', 'Answer', '<|start_header_id|>', ' barric', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004226183891296387, 0.035904645919799805, 0.005894273519515991, 0.012555837631225586, 0.0054985880851745605]}, 'saliency': {'score': [0.0014110445976257325, 4.283392838403291e-05, 7.952413251323085e-05, 3.99331845348005e-05, 2.0385906100273133e-05], 'topk_tokens': [' Met', ' Anthony', 'assistant', ' to', ':', '<|eot_id|>', ' Geo', ' Dan', ' Ot', ' barric', 'b', ' Merch', 'Mary', 'Answer', 'athroom', ' Bench', '\n\n', '<|end_header_id|>', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.0001414179801940918, 0.007329337298870087, 0.0005119591951370239, 0.0004223659634590149, 8.174280325571696e-05]}}, 26: {'grad': {'score': [0.399620361328125, 0.4722476934295854, 0.48169216032951107, 0.47237268561840995, 0.5206996440887451], 'topk_tokens': ['pro', 'ire', ' satisf', 'ente', ' expedition', 'field', ' Malta', ' Peter', 'char', ' medicine', ' Press', ' Empire', 'UX', ' Eagle', ' vastly', 'UX', 'issippi', 'is', 'pro', ' prof'], 'evidence_proportions': [0.41958007812500003, 0.482452392578125, 0.33426411946614587, 0.339599609375, 0.433135986328125]}, 'weight': {'score': [0.010167689323425292, 0.002484412988026937, 0.0028162223677481372, 0.002467803664392965, 0.0005966365337371826], 'topk_tokens': [' bathroom', ' Bridge', ' the', ' the', ' discarded', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'b', 'assistant', ' bedroom', '\n\n', ' barric', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.012880879640579225, 0.011583924293518066, 0.005041137337684631, 0.021915733814239502, 0.0042570630709330235]}, 'saliency': {'score': [0.00021432638168334962, 6.980899161373089e-05, 0.0001791561803510112, 6.923424624253947e-05, 2.5356560945510866e-05], 'topk_tokens': ['<|eot_id|>', ' discarded', ' AC', ' Floral', ' Father', 'IR', 'ORT', 'CH', ' kitchen', ' Merch', ' Anthony', ' Bridge', ' barric', ' bedroom', 'athroom', '<|end_header_id|>', '\n\n', 'b', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.00023161768913269044, 0.00017189979553222656, 0.00015956660111745197, 0.0004431530833244324, 0.0001304099957148234]}}, 27: {'grad': {'score': [0.35115315437316896, 0.39761730518216404, 0.3810591543874433, 0.39775477288118416, 0.41213661432266235], 'topk_tokens': [' platform', ' Franklin', ' drafted', ' several', 'am', ' speculation', '\n', 'lin', 'po', ' Bre', ' accepted', 'Republicans', 'ree', ' business', ' West', ' restrictions', ' designated', 'CE', ' step', ' Thanksgiving'], 'evidence_proportions': [0.41648712158203127, 0.3526134490966797, 0.2878570556640625, 0.3715019226074219, 0.34546490510304767]}, 'weight': {'score': [0.015562121868133544, 0.0025316694982690747, 0.002383269609943513, 0.002505310263805145, 0.0007145002484321595], 'topk_tokens': [' THE', '<|eot_id|>', ' Bridge', 'THE', '?\n', ' football', 'CH', '.\n\n', 'Answer', ' bathroom', 'assistant', 'b', '\n\n', ' bedroom', '<|start_header_id|>', ' barric', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03135577440261841, 0.027789145708084106, 0.005103468894958496, 0.016347527503967285, 0.004184444745381673]}, 'saliency': {'score': [0.0008887839317321777, 6.905508294604184e-05, 0.0002464238674409928, 6.692182182169868e-05, 3.603845834732056e-05], 'topk_tokens': ['ORT', ' Mary', 'NEW', ' bathroom', 'Bridge', 'Mary', ' to', 'CH', ' prior', ' THE', '<|begin_of_text|>', 'THE', '<|start_header_id|>', ':', '.\n\n', 'athroom', 'b', ' bedroom', ' Bridge', '<|end_header_id|>'], 'evidence_proportions': [0.0017066597938537598, 0.0014055371284484863, 0.0003724594910939535, 0.0011643916368484497, 0.00019530455271402994]}}, 28: {'grad': {'score': [0.5774244880676269, 0.6289873609355852, 0.47879332880819997, 0.6294753030578478, 0.5730570912361145], 'topk_tokens': ['ien', ',', ' the', '\n', ' the', '.', '      ', ' the', 'antic', ' the', 'nes', '.', ',', ' the', 'dent', ' a', "'", 'nes', ',', 'S'], 'evidence_proportions': [0.604180908203125, 0.26949405670166016, 0.6503259340922037, 0.4610624313354492, 0.7650876839955648]}, 'weight': {'score': [0.009711875915527343, 0.0023904261246226193, 0.003202022083343998, 0.002373338491702972, 0.0003684930503368378], 'topk_tokens': [' where', ' was', ' the', ' Bridge', ' garden', ' to', ' the', '?\n', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.005283367633819581, 0.00823146104812622, 0.01140323281288147, 0.011083602905273438, 0.011783401171366373]}, 'saliency': {'score': [0.0006579530239105225, 4.948503976943446e-05, 0.00010258343911940051, 4.8101442430024073e-05, 1.1614710092544556e-05], 'topk_tokens': [' kitchen', 'Question', ' Mary', 'Bridge', '?\n', ' bedroom', ' Bridge', ' Mary', ' the', ' the', ' garden', '<|start_header_id|>', 'b', 'athroom', 'assistant', ' Bridge', '\n\n', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00018519163131713867, 0.001718573272228241, 0.0009085337320963541, 0.0003577619791030884, 0.0002943873405456543]}}, 29: {'grad': {'score': [0.37260124206542966, 0.4033808190838184, 0.41148228799143144, 0.4034233620819573, 0.404431426525116], 'topk_tokens': [' book', 't', ' extra', ' graphic', ' Dr', 'Dr', 'adv', ' extra', ' not', 'Spring', ' B', 're', 'tal', 'b', 'b', 'init', 'cret', ' M', 'b', ' ga'], 'evidence_proportions': [0.552197265625, 0.34178686141967773, 0.369293212890625, 0.2502155303955078, 0.32837931315104163]}, 'weight': {'score': [0.0034053683280944823, 0.0024651761148490156, 0.0015287976111135176, 0.002465629408372051, 0.0005055367946624756], 'topk_tokens': [' Where', ' Does', '.\n\n', ' where', ' was', '<|eot_id|>', '?\n', ' the', ' the', '<|eot_id|>', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '\n\n', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.000363922119140625, 0.003889799118041992, 0.003573367993036906, 0.006678104400634766, 0.003267129262288411]}, 'saliency': {'score': [0.000160292387008667, 7.972109551523246e-05, 8.159683596703314e-05, 7.955100110934428e-05, 2.321191132068634e-05], 'topk_tokens': [' place', '      ', ':', 'IVE', ' to', '<|eot_id|>', 'Answer', ' where', ' Does', ' was', ' the', 'assistant', 'athroom', '<|end_header_id|>', ' the', ':', '<|start_header_id|>', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [2.6613473892211914e-05, 6.843358278274536e-05, 0.00026676058769226074, 0.00041410326957702637, 5.725522836049398e-05]}}, 30: {'grad': {'score': [0.23332588195800782, 0.26172600041807087, 0.2176764703566028, 0.26189634992508065, 0.3287617951631546], 'topk_tokens': [' Project', ' celebrations', ' Times', ' town', ' City', ' New', ' Union', ' Times', ' term', ' of', ' Million', ' Roman', ' a', ' B', ' United', 'ire', 'b', ' forb', 'b', 'b'], 'evidence_proportions': [0.23081398010253906, 0.21100807189941406, 0.21284357706705728, 0.36374950408935547, 0.18383089701334637]}, 'weight': {'score': [0.00981683611869812, 0.0024636003706190322, 0.0038801018268831314, 0.002444908443596202, 0.0015817318111658097], 'topk_tokens': [' the', ' Where', ' the', 'Question', 'ORT', ' bedroom', '<|eot_id|>', '.\n\n', '<|eot_id|>', ' barric', '?\n', 'Answer', 'assistant', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004349267482757569, 0.011204838752746582, 0.011869435509045918, 0.015449047088623047, 0.007640401522318522]}, 'saliency': {'score': [0.0009190607070922852, 5.648895805957271e-05, 0.0002055398879512664, 5.433983851324471e-05, 2.54753977060318e-05], 'topk_tokens': [' garden', 'athroom', ' Anthony', ' Where', 'CH', ' bathroom', ' the', ' Mary', ' barric', 'IR', 'assistant', ' the', ' bedroom', ' the', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0009485960006713868, 0.001722082495689392, 0.0012675672769546509, 0.0003145933151245117, 0.00041357179482777917]}}, 31: {'grad': {'score': [0.20039230346679687, 0.2399168731340396, 0.19464471263270225, 0.24011315934687794, 0.1604224804788828], 'topk_tokens': [' having', 'ased', 'nes', 'nes', ' St', ' St', ' majority', 'n', ' July', 'h', ' the', ' he', ' August', 'nes', ' forb', ' majority', ' May', ' January', ' Mr', 'membership'], 'evidence_proportions': [0.26952056884765624, 0.1986183524131775, 0.14763220151265463, 0.22948837280273438, 0.1773307720820109]}, 'weight': {'score': [0.0028438234329223634, 0.0022970967043459026, 0.0011716369659669937, 0.0022988384216534644, 0.0006213042885065079], 'topk_tokens': [' where', ' the', 'Question', ',', ':', '.\n\n', ' Where', ' the', '<|eot_id|>', 'Answer', '?\n', 'b', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000716698169708252, 0.00400368869304657, 0.0033285021781921387, 0.004209935665130615, 0.0024477640787760415]}, 'saliency': {'score': [0.00021612167358398438, 3.06737262244318e-05, 4.2148174778107674e-05, 3.0264016228605082e-05, 4.7281384468078615e-06], 'topk_tokens': [' Paul', 'ot', ' Miles', ' Mary', 'Question', '.', '\n\n', 'Mary', 'Answer', '<|eot_id|>', ' the', ' Mary', '?\n', ':', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'b', 'athroom'], 'evidence_proportions': [2.541542053222656e-05, 0.0007900968194007874, 0.0002776682376861572, 3.155320882797241e-05, 5.389253298441569e-05]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-22 03:18:41.037 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:18:41.038 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-4_0-4-6-8-9.pkl | len: 10 |  size: 9.53 KB
Processing depth (0, 4, 6, 8, 9):   5%|▌         | 5/100 [01:33<29:51, 18.86s/it]Processing depth (0, 4, 6, 8, 9):   5%|▌         | 5/100 [01:33<29:41, 18.75s/it]
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:72 - Selected idx: 14
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:76 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Daniel took the football.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' John went back to the office.', ' Mary left the apple.', ' Daniel left the apple.']
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel left the apple.']
2025-01-22 03:18:41.273 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
Processing depth (1, 2, 8, 9):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:18:48.752 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:18:48.757 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1494, 1499) -->  tragedy. Mary moved to
2025-01-22 03:18:48.757 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:18:48.764 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2496, 2502) --> . Mary journeyed to the
2025-01-22 03:18:48.765 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:18:48.791 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (9777, 9781) -->  Mary left the apple
2025-01-22 03:18:48.791 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:18:48.820 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10789, 10793) -->  Daniel left the apple
2025-01-22 03:18:48.821 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:18:48.833 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4458, 4463) -->  business. Daniel picked up
2025-01-22 03:18:48.833 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:18:48.844 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3817, 3822) -->  war. Sandra moved to
2025-01-22 03:18:48.844 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:18:48.869 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7615, 7620) -->  attempt. John moved to
2025-01-22 03:18:48.869 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:18:48.891 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8409, 8413) -->  Daniel took the football
2025-01-22 03:18:48.892 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:18:48.902 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (3645, 3651) --> . Sandra journeyed to the
2025-01-22 03:18:48.902 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:18:48.926 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (8275, 8281) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:18:51.157 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:18:51.157 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12215])
your chose emoji: ['🧝', '🙌', '👨🏽\u200d🤝\u200d👨🏾', '🏃🏼', '🇷🇪', '🚵🏽\u200d♀️', '👨🏽\u200d⚖️', '🍍', '👨🏻\u200d🎨', '👧🏻']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 172074.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.80s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.05it/s][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.17it/s][A100%|██████████| 8/8 [00:05<00:00,  1.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.16it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.78it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.64it/s][A100%|██████████| 8/8 [00:00<00:00, 22.00it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.18it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.61it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.66it/s][A100%|██████████| 8/8 [00:00<00:00, 21.88it/s]
2025-01-22 03:18:59.803 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.31266965364155014, 0.34589027683284906, 0.3072467927009829, 0.34604060062054814, 0.38547066095713023], 'topk_tokens': ['ed', '.', 'were', ' as', '�', ' absence', ' communication', '.', ' out', ' out', ' agreement', ' the', '�', 'ols', ' It', ' and', '.', '.', ' comparison', ' turtle'], 'evidence_proportions': [0.3606094360351562, 0.32078679402669275, 0.29341697692871094, 0.25982189178466797]}, 'weight': {'score': [0.015646893727151973, 0.002575337701430003, 0.025395515464967298, 0.0024967886321369086, 0.00117594567505089], 'topk_tokens': [' the', ' garden', '.', ' office', '<|eot_id|>', ' top', ' lounge', 'Bridge', 'assistant', ' bedroom', '<|start_header_id|>', '<|eot_id|>', ':', ' bathroom', 'b', '\n\n', ' football', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006596922874450684, 0.017806212107340496, 0.01973879337310791, 0.027050018310546875]}, 'saliency': {'score': [0.0005519515589663857, 3.596642224600163e-05, 0.0006275292365781722, 3.365362105911927e-05, 3.7358016581148716e-05], 'topk_tokens': [':', ' kitchen', ' bedroom', ' composing', 'Bridge', ' football', '<|eot_id|>', '<|end_header_id|>', ' Bench', ' lounge', '<|eot_id|>', 'b', ' bedroom', 'c', ' office', ' office', '<|begin_of_text|>', ' garden', 'athroom', ' bathroom'], 'evidence_proportions': [1.8930435180664064e-05, 0.0007323523362477621, 0.0007253959774971008, 0.0007741823792457581]}}, 25: {'grad': {'score': [0.5709592919600638, 0.8327457679627394, 0.49856678132087956, 0.8340059168475149, 0.39181531442178263], 'topk_tokens': [' o', ' York', 'love', 'If', ' inverted', ' a', ' a', ' the', ' ', ' a', ' a', ' for', 'a', ' favored', ' a', ' no', ' im', 'ivery', ' locom', ' l'], 'evidence_proportions': [0.477947998046875, 0.524200439453125, 0.6734390258789062, 0.6548819541931152]}, 'weight': {'score': [0.012470182619596782, 0.0025040506633609303, 0.010229241463445848, 0.0024688076142180054, 0.0018908578801799464], 'topk_tokens': [' garden', '.', ' Anthony', ' \n', ' Daniel', ' Geo', ' Bench', 'Answer', '.\n\n', 'b', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', '.', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.001291579008102417, 0.010279759764671326, 0.01747274398803711, 0.024726510047912598]}, 'saliency': {'score': [0.0007925974695306076, 4.021305317066172e-05, 0.0002667817377275036, 3.846100409668892e-05, 4.3293511545335924e-05], 'topk_tokens': [' Mary', 'b', '<|eot_id|>', 'Answer', ' apple', '.\n\n', 'athroom', ' apple', '.', ' Anthony', ' apple', ' top', ' Dan', '<|end_header_id|>', ':', ' Bench', ' Geo', '.', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00010136365890502929, 0.0005427102247873943, 0.0014318004250526428, 0.001392267644405365]}}, 26: {'grad': {'score': [0.4325668937281558, 0.4702783866528892, 0.40701441611013106, 0.4704984475875042, 0.40393380861024597], 'topk_tokens': ['CE', ' Press', ' God', 'graph', ' Empire', ' compl', ' PRO', ' Milwaukee', 'issippi', 'RI', 'b', ' Gutenberg', 'hue', 'pro', ' Press', 'bec', ' favor', 'ub', 'itter', ' bitter'], 'evidence_proportions': [0.5075725555419921, 0.431365966796875, 0.360504150390625, 0.4126739501953125]}, 'weight': {'score': [0.015301084832141274, 0.0024854615855127216, 0.009542410412142354, 0.002447471590829005, 0.001252057181822287], 'topk_tokens': ['.', ' bedroom', ' the', '<|eot_id|>', ' garden', 'Answer', ' \n', ' barric', ' bathroom', ' the', ' the', '<|eot_id|>', 'assistant', 'b', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00022433996200561522, 0.005357816815376282, 0.023064136505126953, 0.041298866271972656]}, 'saliency': {'score': [0.0005125481831400018, 5.534940342365339e-05, 0.0007118457748043922, 5.296296651311645e-05, 2.8537737356649864e-05], 'topk_tokens': [' Sandra', ' Mary', ' Daniel', ' Dan', ' the', ' bedroom', ' Geo', ':', '<|start_header_id|>', ' \n', ' bathroom', ' Anthony', '.', ' Daniel', ' garden', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [3.322362899780273e-05, 0.00048294663429260254, 0.0005445778369903564, 0.0011240765452384949]}}, 27: {'grad': {'score': [0.2698460127177991, 0.3334798683295138, 0.2648382494526525, 0.3337541067780514, 0.3121815758782464], 'topk_tokens': [' would', ' platform', ' beautiful', ' designated', ' was', ' conventions', 'str', 'lin', ' started', 'ly', ' be', 'ides', ' business', ' convention', ' was', '-n', ' was', ' Bottle', ' accepted', ' step'], 'evidence_proportions': [0.2678009033203125, 0.2413457234700521, 0.33751678466796875, 0.2474820613861084]}, 'weight': {'score': [0.011654802058872423, 0.002534311440986587, 0.009529920354966194, 0.002502247568692603, 0.0014512683894183185], 'topk_tokens': [' Geo', '<|eot_id|>', ' \n', ' bedroom', ' bedroom', 'Answer', ' barric', 'assistant', '<|start_header_id|>', ' garden', 'b', '.', '.\n\n', ' lounge', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007330060005187989, 0.010074203213055929, 0.01703333854675293, 0.02229940891265869]}, 'saliency': {'score': [0.0007903764122410825, 4.6625079666071505e-05, 0.0010972763261487407, 4.278702378508137e-05, 4.614325794013771e-05], 'topk_tokens': [' football', ' barric', '.', ' bedroom', ' kitchen', ' THE', ' bedroom', 'Bridge', ' lounge', ' the', '<|begin_of_text|>', ' Daniel', '<|end_header_id|>', ' top', ' Bridge', ' bathroom', '.', ':', 'athroom', '.\n\n'], 'evidence_proportions': [2.0885467529296877e-05, 0.0009690821170806885, 0.0008886605501174927, 0.0013858973979949951]}}, 28: {'grad': {'score': [0.2944616016588713, 0.354866830860462, 0.29276275634765625, 0.3551193724173923, 0.3729768186002164], 'topk_tokens': ['nes', 'in', 'ot', '.', ' lie', 'yl', ' inside', ' lie', 'arp', ' lively', 'nes', ' RID', ' returns', '\n', 'nes', ' half', ' a', 'dent', ' half', 'half'], 'evidence_proportions': [0.29074630737304685, 0.4175516764322917, 0.2224278450012207, 0.18650436401367188]}, 'weight': {'score': [0.015497704869822451, 0.002415249675126918, 0.010668507506770472, 0.0023737952338481402, 0.0009578885258855047], 'topk_tokens': [' the', '?', 'Answer', ' bedroom', ' Daniel', '<|eot_id|>', ' garden', ' the', ' the', ' before', '<|eot_id|>', ' \n', 'b', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [7.37607479095459e-05, 0.0029979447523752847, 0.014918267726898193, 0.054106712341308594]}, 'saliency': {'score': [0.00024363398551940918, 3.35068500925933e-05, 0.0002952852556782384, 3.2511818358021926e-05, 1.8130283097963075e-05], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' \n', 'athroom', ' Far', ' bedroom', ' apple', ' Bridge', 'b', '<|start_header_id|>', ' the', ' the', ' garden', '\n\n', 'assistant', 'Bridge', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.045797348022461e-06, 4.918376604715983e-05, 0.0001936778426170349, 0.0008860006928443909]}}, 29: {'grad': {'score': [0.2984920300935444, 0.36583950125774983, 0.2819058818201865, 0.36615849732568906, 0.4164565962714118], 'topk_tokens': [' The', ' enough', ' B', 'y', ' Ch', ' thousand', ' extra', ' THE', ',\n', ' Paul', 'tal', ' In', 're', ' a', ' The', ' THE', ' In', ' The', ' M', ' ga'], 'evidence_proportions': [0.284185791015625, 0.3682581583658854, 0.3293914794921875, 0.18082618713378906]}, 'weight': {'score': [0.010892191999836973, 0.002480459283506213, 0.005432396165786251, 0.0024598040431248053, 0.0009937221939499315], 'topk_tokens': ['<|eot_id|>', ' bedroom', '?', ' the', ' the', '<|eot_id|>', 'Answer', 'ith', ' before', '.\n\n', ' \n', 'b', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.06791877746582e-05, 0.003736789027849833, 0.021125346422195435, 0.02491903305053711]}, 'saliency': {'score': [0.0003920087688847592, 3.533136788045234e-05, 0.00016657190938149728, 3.4440068784001466e-05, 4.2910914163331724e-05], 'topk_tokens': ['Does', 'c', ' the', ' part', '<|eot_id|>', 'athroom', ' Does', '      ', 'Answer', '<|eot_id|>', ' before', 'assistant', 'ith', '<|end_header_id|>', '<|start_header_id|>', ':', 'b', ' the', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.6697597503662104e-06, 0.0003054191668828329, 0.0006767883896827698, 0.0007187873125076294]}}, 30: {'grad': {'score': [0.38120134253250926, 0.3784472424458279, 0.38263037896925406, 0.3784322847589547, 0.3327465701747585], 'topk_tokens': [' itself', ' B', ' of', ' it', ' soon', ' B', ' to', ' the', '2', ' two', ' the', ' its', ' Burb', 'b', 'b', ' forb', ' an', '3', ' account', 'deal'], 'evidence_proportions': [0.287884521484375, 0.2856394449869792, 0.6458721160888672, 0.37651944160461426]}, 'weight': {'score': [0.015995634229559647, 0.0024700548044299313, 0.012690090364025484, 0.0024228977439906705, 0.003883342887904193], 'topk_tokens': [' Anthony', ' bedroom', ' Miles', ' before', ' barric', '?', '<|eot_id|>', ' the', '.\n\n', '<|eot_id|>', 'Answer', 'b', 'assistant', ' \n', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001212310791015625, 0.01069102684656779, 0.029924869537353516, 0.028502464294433594]}, 'saliency': {'score': [0.0007936170226649234, 5.9681494666465e-05, 0.0004620379017245385, 5.751040462284477e-05, 3.666370301633268e-05], 'topk_tokens': [' apple', ' to', ' the', '.', 'Bridge', ' garden', ' the', ' Bench', ' the', ' Bridge', 'assistant', '<|start_header_id|>', ' bedroom', ' the', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', 'b', 'athroom', ':'], 'evidence_proportions': [2.58326530456543e-05, 0.0005180488030115764, 0.001218266785144806, 0.001742050051689148]}}, 31: {'grad': {'score': [0.26516015906082957, 0.32611096020434877, 0.25531621325400566, 0.3263864945877468, 0.19677073891098434], 'topk_tokens': [' August', 'membership', 'If', ' they', ' the', ' the', ' the', ' the', ' the', ' he', 'did', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' population', ' the'], 'evidence_proportions': [0.2346670150756836, 0.23378833134969074, 0.3047640323638916, 0.3107304573059082]}, 'weight': {'score': [0.0030418006997359427, 0.002309373717277603, 0.0025031643529092114, 0.00230773633871323, 0.0014154133764473168], 'topk_tokens': [' Where', ':', ' the', ' before', '?', ' the', '.\n\n', '<|eot_id|>', ' bedroom', 'Answer', ' \n', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000699901580810547, 0.0032507777214050293, 0.0038552284240722656, 0.004842281341552734]}, 'saliency': {'score': [7.144400947972347e-05, 1.8963228955677088e-05, 6.631112867786038e-05, 1.876065501408072e-05, 1.597323933163205e-05], 'topk_tokens': [' the', ' was', 'ith', ' Market', ' the', '?', ' the', '.\n\n', ':', '<|eot_id|>', ' apple', 'Answer', ' \n', '<|start_header_id|>', ' bedroom', ' the', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [3.550052642822266e-05, 0.0001056740681330363, 7.06017017364502e-05, 6.587058305740356e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:18:59.805 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:18:59.805 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-0_1-2-8-9.pkl | len: 10 |  size: 8.98 KB
Processing depth (1, 2, 8, 9):   1%|          | 1/100 [00:18<30:25, 18.44s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
Processing depth (0, 2, 4, 8):   1%|          | 1/100 [00:26<30:25, 18.44s/it]2025-01-22 03:19:07.588 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:19:07.589 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:19:07.589 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:19:07.596 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2447, 2453) --> . Mary journeyed to the
2025-01-22 03:19:07.597 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:19:07.610 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4896, 4900) -->  Mary left the apple
2025-01-22 03:19:07.610 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:19:07.636 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9666, 9670) -->  left the apple.
2025-01-22 03:19:07.637 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:19:07.649 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4373, 4378) --> . Daniel picked up the
2025-01-22 03:19:07.649 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:19:07.659 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3696, 3701) --> . Sandra moved to the
2025-01-22 03:19:07.659 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:19:07.680 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7473, 7478) --> . John moved to the
2025-01-22 03:19:07.681 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:19:07.703 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8363, 8367) -->  Daniel took the football
2025-01-22 03:19:07.703 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:19:07.713 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (3539, 3545) --> . Sandra journeyed to the
2025-01-22 03:19:07.714 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:19:07.738 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (8209, 8215) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:19:09.855 | INFO     | test_jbb_retain:begin_test:632 - The office.<|eot_id|>
2025-01-22 03:19:09.855 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12228])
your chose emoji: ['👰🏿\u200d♀️', '👩🏼\u200d❤\u200d👨🏾', '👨🏾\u200d🦼\u200d➡️', '🚵🏻\u200d♂️', '🧎\u200d➡', '🏊🏾\u200d♂️', '🙎🏻\u200d♂', '🙃', '🪂', '🏄🏽\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 192841.56it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.41s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.44s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.71it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.97it/s][A
 50%|█████     | 4/8 [00:00<00:00, 17.41it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 20.99it/s][A100%|██████████| 8/8 [00:00<00:00, 20.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.16it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.74it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.98it/s][A100%|██████████| 8/8 [00:00<00:00, 21.01it/s]
2025-01-22 03:19:19.007 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.25851816880075557, 0.33037594023383204, 0.3027983634702621, 0.33055820790782425, 0.4430388527354975], 'topk_tokens': [' It', ' Merch', '�', 'vent', '�', '�', ' agreement', ' and', ' THE', 'remark', ' comparison', 'combination', 'ings', ' Arr', ' Do', ' compl', 'ob', 'deal', '�', 'consider'], 'evidence_proportions': [0.26317958831787114, 0.32148488362630206, 0.22562408447265625, 0.19113540649414062]}, 'weight': {'score': [0.024064512629258007, 0.0025747931702393718, 0.012107116560782156, 0.002517014113115311, 0.0011001914397053335], 'topk_tokens': [' bathroom', ' boat', '\n\n', ' building', ' barric', ' composing', 'Answer', 'b', ' bedroom', ':', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'Bridge', '<|eot_id|>', '\n\n', ' Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.019937562942504882, 0.01971706748008728, 0.054003238677978516, 0.005805641412734985]}, 'saliency': {'score': [0.0031153998876872814, 5.3292760941785586e-05, 0.0006171349556215348, 4.708151856076296e-05, 4.734321572314734e-05], 'topk_tokens': [' composing', ' office', '<|eot_id|>', ' Dan', ' Miles', '<|eot_id|>', ' Sandra', 'b', ' garden', 'c', 'Mary', ' bedroom', ' bathroom', ' Mary', '<|begin_of_text|>', ' Mary', ' Bench', 'athroom', 'Bridge', ' Bridge'], 'evidence_proportions': [0.002818185091018677, 0.003026386102040609, 0.006489977240562439, 0.0002458617091178894]}}, 25: {'grad': {'score': [0.6094069229929071, 0.5086514926212085, 0.4800879878382529, 0.5085670262778219, 0.5715977022017555], 'topk_tokens': [' $', 'ville', 'ivery', ' l', ' for', ' not', ' im', ' inverted', ' a', ' a', ' a', ' of', ' ', ' at', ' set', ' the', ' Aw', ' of', ' a', ' no'], 'evidence_proportions': [0.6092086791992187, 0.4892050425211588, 0.6954765319824219, 0.703887939453125]}, 'weight': {'score': [0.014768928289413452, 0.0025031548491406155, 0.006101329480448077, 0.0024748654550895755, 0.0012450074327403102], 'topk_tokens': ['Mary', ' Mary', ' Mary', 'b', ' apple', '.\n\n', 'Answer', ' \n', ' Bench', ' Dan', '<|start_header_id|>', '.', '<|eot_id|>', ':', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.005173301696777344, 0.010053083300590515, 0.04258167743682861, 0.006024479866027832]}, 'saliency': {'score': [0.0011671765854484157, 4.640620227106048e-05, 0.00014869724550554828, 4.439769232765362e-05, 3.803838258502127e-05], 'topk_tokens': ['Answer', ' barric', ' Ear', ':', ' boat', 'Print', ' Mary', 'assistant', '.', ' Geo', '.', '<|eot_id|>', ' apple', 'athroom', '<|eot_id|>', ' Bench', ' Dan', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0002827763557434082, 0.0003858009974161784, 0.004341810941696167, 0.00027010589838027954]}}, 26: {'grad': {'score': [0.42061213443153783, 0.4586710901410862, 0.4151560875677293, 0.45884119811565766, 0.5060454620712105], 'topk_tokens': [' barric', 'RI', ' vastly', 'hue', ' Gutenberg', ' Press', ' unab', 'bec', ' compl', ' Paul', ' Anthony', 'b', 'single', 'ub', 'issippi', ' favor', ' brav', ' Press', 'itter', ' bitter'], 'evidence_proportions': [0.4064544677734375, 0.5006612141927084, 0.37105560302734375, 0.36779212951660156]}, 'weight': {'score': [0.019059694127032633, 0.0024858212278027845, 0.005886806595710016, 0.0024513138694996495, 0.000551410105036593], 'topk_tokens': [' garden', ' bedroom', ' apple', ' bathroom', '.\n\n', ' the', 'b', ' Bridge', '<|eot_id|>', ' \n', 'Answer', '<|eot_id|>', ' barric', 'assistant', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.023403334617614745, 0.011209929982821148, 0.03601527214050293, 0.008449211716651917]}, 'saliency': {'score': [0.0006591128675561203, 3.996985579110081e-05, 0.00028050426513917987, 3.839196531303449e-05, 2.3505811033577756e-05], 'topk_tokens': [' old', ' Merch', ' \n', ' Jackson', 'assistant', ' Father', ' garden', ' the', ' barric', ' the', ' kitchen', ' Bridge', ' Dan', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0002485334873199463, 0.001126651962598165, 0.0009609982371330261, 0.00016914308071136475]}}, 27: {'grad': {'score': [0.23216066862407483, 0.3755319055927306, 0.2307101834204889, 0.37612410055946105, 0.3520951983572423], 'topk_tokens': [' ideas', 'ly', '.', ' was', '\n', ' step', ' were', ' accepted', 'ottle', ' were', ' be', ' was', ' beautiful', ' be', ' would', ' was', ' would', ' was', ' business', ' Bottle'], 'evidence_proportions': [0.24865112304687498, 0.22588602701822919, 0.1914844512939453, 0.26163578033447266]}, 'weight': {'score': [0.026228760418139006, 0.0025326573890634262, 0.005215836148108205, 0.0024888675114603702, 0.0007603281530840643], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' apple', ' bedroom', ' \n', ' bedroom', 'Answer', ' Bridge', ' bathroom', ' garden', 'b', 'assistant', ' barric', '.\n\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03945586085319519, 0.01401752730210622, 0.050220489501953125, 0.004020005464553833]}, 'saliency': {'score': [0.0026288126644335294, 4.736014461587509e-05, 0.00048599416209805396, 4.221727847857248e-05, 3.422813853998294e-05], 'topk_tokens': [' the', ' barric', ' St', ' THE', ' Mary', 'Mary', ' \n', 'assistant', ' bedroom', ' Bridge', '<|end_header_id|>', 'b', ' the', ' Dan', '<|begin_of_text|>', '<|start_header_id|>', ' the', 'athroom', '.\n\n', ':'], 'evidence_proportions': [0.0005033910274505615, 0.0036037713289260864, 0.006372459232807159, 7.950514554977417e-05]}}, 28: {'grad': {'score': [0.4123380560623972, 0.3588486435114923, 0.3600957932010774, 0.3587620364612629, 0.3237657218143858], 'topk_tokens': ['about', ' received', ' prepared', ' almost', ' returns', ' over', ' balance', '600', ' shore', ' summer', ' summer', ' summer', ' following', ' returns', 'ball', '600', ' half', ' inside', 'half', ' half'], 'evidence_proportions': [0.41612777709960935, 0.36276499430338544, 0.5137290954589844, 0.3805694580078125]}, 'weight': {'score': [0.00945615297869632, 0.002412442417938529, 0.0075939980245405625, 0.0023882688094943078, 0.00041844961286961345], 'topk_tokens': [' Bridge', 'Answer', ' garden', '.\n\n', ' bedroom', '?', ' the', '<|eot_id|>', ' apple', ' the', ' before', '<|eot_id|>', 'b', 'assistant', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003379225730895996, 0.005223497748374939, 0.020302772521972656, 0.012554675340652466]}, 'saliency': {'score': [0.00019271593344838997, 3.497239058195702e-05, 0.00018485611484896753, 3.4344895075287646e-05, 1.1260139531102674e-05], 'topk_tokens': ['<|eot_id|>', '.', ' the', '.\n\n', ' ST', ' kitchen', 'Bridge', ' the', ' garden', 'athroom', 'b', ' apple', ':', ' Bridge', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', ' Bridge', '\n\n', '<|start_header_id|>'], 'evidence_proportions': [5.8585405349731444e-05, 0.00013172129789988202, 0.0004490762948989868, 0.00019551068544387817]}}, 29: {'grad': {'score': [0.6256681743421053, 0.640154204416544, 0.6003426274945659, 0.6402781181720646, 0.40948621157942144], 'topk_tokens': ['ian', ' steam', 'ages', ' Ch', 'y', ' Pioneer', ' spring', ' I', 'ION', ' marched', ' M', ' Paul', 'y', 'ing', ' l', ' com', 'Spring', 'y', 'ER', 'ION'], 'evidence_proportions': [0.601373291015625, 0.6282552083333334, 0.68438720703125, 0.5934371948242188]}, 'weight': {'score': [0.01737728714942932, 0.0024691679112978085, 0.005426473194553006, 0.0024383879482976108, 0.0006655906808787379], 'topk_tokens': [' the', ' apple', '.', ' the', ' Does', ' \n', 'Answer', '<|eot_id|>', ' the', '.\n\n', ' before', 'b', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000957942008972168, 0.013437246282895407, 0.04877758026123047, 0.012411236763000488]}, 'saliency': {'score': [0.001092076301574707, 4.923499114924114e-05, 0.0002461344965042606, 4.7107261934555204e-05, 3.441174825032552e-05], 'topk_tokens': ['      ', ' the', ' Mary', ' St', ' in', '      ', 'assistant', ' before', 'athroom', '<|eot_id|>', 'Answer', ' the', ' Does', 'b', '<|start_header_id|>', ':', '<|end_header_id|>', ' the', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.4317035675048829e-05, 0.0006582885980606079, 0.003670342266559601, 0.0005116909742355347]}}, 30: {'grad': {'score': [0.3428340711091694, 0.32683795713440744, 0.34800627923780875, 0.326759134036901, 0.28731204860511866], 'topk_tokens': [' Loan', 'ARCH', 'moment', ' Times', ' United', ' the', 'SSION', ' the', ' its', 'ire', ' of', ' Europe', ' account', ' LINE', '2', ' forb', 'b', 'b', 'deal', 'b'], 'evidence_proportions': [0.27255859374999997, 0.2840983072916667, 0.36126708984375, 0.5003490447998047]}, 'weight': {'score': [0.019194813151108592, 0.0024485300747276706, 0.009584381695716612, 0.002404248753103675, 0.0024440754419085622], 'topk_tokens': [':', ' the', 'Question', ' the', '?', ' bedroom', ' barric', '<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', 'b', 'assistant', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.007999300956726074, 0.011989086866378784, 0.0526123046875, 0.010580301284790039]}, 'saliency': {'score': [0.001404889320072375, 9.120420485250051e-05, 0.00033267659525717456, 8.854057614453544e-05, 6.239715663866065e-05], 'topk_tokens': [' the', ' apple', ' Bench', 'Bridge', '.', ' the', ' Bridge', ' bedroom', ' bathroom', ' garden', ' bedroom', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', ' the', ' Bridge', '<|start_header_id|>', ':', 'b', 'athroom'], 'evidence_proportions': [0.0013744592666625978, 0.0009065419435501099, 0.003296680748462677, 0.00029865652322769165]}}, 31: {'grad': {'score': [0.29180627120168584, 0.2952049566634921, 0.2387106610882667, 0.29535403294512796, 0.13025995266848597], 'topk_tokens': [' the', ' that', ' location', ' Press', ' department', ' the', ' the', ' January', ' is', ' the', ' August', ' apple', ' Press', ' the', ' he', ' the', ' population', 'membership', ' the', ' the'], 'evidence_proportions': [0.28551254272460935, 0.25476710001627606, 0.30298948287963867, 0.34404897689819336]}, 'weight': {'score': [0.002978147644745676, 0.0022841401233100783, 0.0027040685376813335, 0.002281988910457867, 0.0008312648740308039], 'topk_tokens': [':', ' Where', ' the', ' before', '.\n\n', '?', '<|eot_id|>', 'Answer', ' bedroom', ' the', ' \n', 'b', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009960055351257322, 0.002979561686515808, 0.005226731300354004, 0.00320512056350708]}, 'saliency': {'score': [0.00012712259041635613, 1.8082068738577927e-05, 4.609842454233477e-05, 1.7840686508728634e-05, 8.78825955007268e-06], 'topk_tokens': [' the', ':', '\n\n', ' Mary', ' before', '<|eot_id|>', ' \n', '.\n\n', 'Question', 'Answer', '<|begin_of_text|>', ' apple', ':', '<|start_header_id|>', ' the', ' bedroom', '<|end_header_id|>', 'b', 'athroom', 'assistant'], 'evidence_proportions': [2.9784440994262695e-05, 0.00020558138688405353, 0.0002204105257987976, 3.781914710998535e-05]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 03:19:19.011 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:19:19.011 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-1_0-2-4-8.pkl | len: 10 |  size: 9.09 KB
Processing depth (0, 2, 4, 8):   2%|▏         | 2/100 [00:37<30:51, 18.89s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (3, 4, 5, 9):   2%|▏         | 2/100 [00:45<30:51, 18.89s/it]2025-01-22 03:19:26.847 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:19:26.859 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3803, 3808) --> . Mary moved to the
2025-01-22 03:19:26.859 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:19:26.874 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4914, 4920) --> . Mary journeyed to the
2025-01-22 03:19:26.874 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:19:26.890 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (6014, 6018) -->  Mary left the apple
2025-01-22 03:19:26.891 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:19:26.920 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10810, 10814) -->  Daniel left the apple
2025-01-22 03:19:26.920 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:19:26.932 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4437, 4442) --> . Daniel picked up the
2025-01-22 03:19:26.932 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:19:26.943 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3752, 3757) --> . Sandra moved to the
2025-01-22 03:19:26.943 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:19:26.964 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7577, 7582) -->  bonds. John moved to
2025-01-22 03:19:26.964 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:19:26.986 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8386, 8390) -->  Daniel took the football
2025-01-22 03:19:26.986 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:19:26.997 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (3584, 3590) --> . Sandra journeyed to the
2025-01-22 03:19:26.997 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:19:27.020 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (8232, 8238) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:19:29.116 | INFO     | test_jbb_retain:begin_test:632 - The garden.<|eot_id|>
2025-01-22 03:19:29.116 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12242])
your chose emoji: ['🇧🇳', '🙎🏾\u200d♂', '👨\u200d🦼\u200d➡️', '🧑🏻\u200d❤️\u200d💋\u200d🧑🏼', '✡️', '🦸🏽\u200d♀', '🤸🏾\u200d♂', '👩🏻\u200d❤\u200d💋\u200d👩🏻', '👱🏼\u200d♀️', '🤷🏾\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 208412.62it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.32s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.05s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.97it/s][A100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.61it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.71it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.75it/s][A100%|██████████| 8/8 [00:00<00:00, 20.90it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.64it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.46it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.73it/s][A100%|██████████| 8/8 [00:00<00:00, 20.72it/s]
2025-01-22 03:19:38.365 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3822459170692845, 0.3610894772898377, 0.35661875817083544, 0.361067879908688, 0.43126856454528206], 'topk_tokens': [' the', 'the', '.', ' compl', ' of', 'oggle', ' first', ':\n\n', '.', ' Do', 'ols', ' the', ' *\n\n', '.', 'ottle', 'itter', ' and', 'ounding', 'est', ' turtle'], 'evidence_proportions': [0.612457275390625, 0.37695948282877606, 0.23616790771484375, 0.2484893798828125]}, 'weight': {'score': [0.030685401276538248, 0.0025711216893474537, 0.015998411563134963, 0.0024931866915414644, 0.001096122630751959], 'topk_tokens': [' the', ' I', ' the', ' a', 'Bridge', '<|eot_id|>', 'b', '<|start_header_id|>', 'assistant', ' football', ' garden', ':', '<|eot_id|>', ' lounge', '\n\n', ' bedroom', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03129423856735229, 0.011933897932370504, 0.065155029296875, 0.023581981658935547]}, 'saliency': {'score': [0.0012639133553755911, 4.58371692796102e-05, 0.00048239481064581103, 4.28296469820968e-05, 5.9603169412896186e-05], 'topk_tokens': ['<|start_header_id|>', ' apple', ' THE', ' the', ' top', ':', ' the', '<|eot_id|>', ' football', '<|begin_of_text|>', ' kitchen', '<|eot_id|>', ' I', 'Bridge', ' lounge', ' office', ' bathroom', ' garden', 'athroom', ' bedroom'], 'evidence_proportions': [0.001585733890533447, 0.0005228718121846517, 0.0025901198387145996, 0.0006469935178756714]}}, 25: {'grad': {'score': [0.6856747677451686, 0.8860306702033993, 0.6817191339308216, 0.8868621929398616, 0.5211302124627746], 'topk_tokens': [' for', ' a', ' inverted', ' l', ' old', ' the', ' favored', ' old', ' for', 'itable', ' im', ' with', 'ivery', ' locom', 'If', 'ville', ' no', ' of', ' the', ' for'], 'evidence_proportions': [0.6614929199218751, 0.628339131673177, 0.7235450744628906, 0.7640352249145508]}, 'weight': {'score': [0.02764429857856349, 0.002485855962364661, 0.006882519491257206, 0.002435482286341418, 0.001215048650703808], 'topk_tokens': [' bedroom', ' THE', ' the', '.\n\n', ' apple', ' a', 'b', ' slept', 'Answer', '<|start_header_id|>', '.', '<|eot_id|>', 'assistant', ':', ' bathroom', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.022747087478637695, 0.010044187307357788, 0.06076216697692871, 0.027048110961914062]}, 'saliency': {'score': [0.0020996175314250743, 3.917211170048556e-05, 0.00031313300132751465, 3.526549008890663e-05, 5.324081619187157e-05], 'topk_tokens': ['<|eot_id|>', ' Daniel', 'Answer', ' I', ' Geo', ' Mary', ' Mary', 'athroom', '.', ':', '<|eot_id|>', ' Mary', ' top', ' apple', ' THE', ' slept', ' bathroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0017437458038330079, 0.0011390050252278647, 0.0045594945549964905, 0.0015254989266395569]}}, 26: {'grad': {'score': [0.4251299406352796, 0.5399777930724275, 0.46263590166645663, 0.5403533328698765, 0.5264184592974068], 'topk_tokens': ['BR', 'RI', 'CE', 'CE', 'issippi', ' galaxy', 'present', 'pro', ' Met', ' far', 'bec', ' favor', 'b', ' PRO', 'pro', 'ub', 'b', '�', 'itter', ' bitter'], 'evidence_proportions': [0.46367797851562503, 0.44481404622395837, 0.3174095153808594, 0.45513916015625]}, 'weight': {'score': [0.0207417042631852, 0.002478484593395702, 0.0051818345823595605, 0.002443158228214572, 0.0008166355661826559], 'topk_tokens': [' apple', 'Bridge', ' apple', '?', ' bedroom', '<|eot_id|>', ' \n', ' garden', 'Answer', '<|eot_id|>', 'b', 'assistant', '<|start_header_id|>', ' the', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008636152744293212, 0.0040272871653238935, 0.04041099548339844, 0.041275978088378906]}, 'saliency': {'score': [0.0007952436020499782, 5.055926390305594e-05, 0.00018026463447078583, 4.9069319752798357e-05, 3.0426695795342475e-05], 'topk_tokens': ['assistant', ' top', ' apple', '.\n\n', ' apple', '?', ' apple', ' kitchen', ' \n', ' bedroom', ' the', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '\n\n', ' garden', ' bathroom', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.0005775988101959229, 0.0002599656581878662, 0.0017285272479057312, 0.0009369328618049622]}}, 27: {'grad': {'score': [0.5019830528058504, 0.5392976485938138, 0.5118906267227665, 0.5394254546617083, 0.42010512210354944], 'topk_tokens': ['event', ' *\n\n', ' conventions', ' states', 'ides', '\n\n\n\n\n\n\n', ' assistance', 'ARCH', 'ided', ' expected', ' sentinel', ' DAYS', '-n', ' convention', ' received', ' designated', 'started', ' step', ' started', ' accepted'], 'evidence_proportions': [0.605877685546875, 0.29418373107910156, 0.6032829880714417, 0.5825138092041016]}, 'weight': {'score': [0.023240409399333754, 0.0025223779892522297, 0.0058545922079393945, 0.00248162840027553, 0.0013584816219783065], 'topk_tokens': [' apple', ' the', ' \n', '<|eot_id|>', ' bedroom', 'Answer', ' THE', ' garden', 'b', 'assistant', '.\n\n', ' bedroom', '<|start_header_id|>', '\n\n', ' lounge', '<|end_header_id|>', ' bathroom', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01682276725769043, 0.008919974168141684, 0.04742741584777832, 0.028556108474731445]}, 'saliency': {'score': [0.0016096391175922594, 6.0564948861187164e-05, 0.0005960983614767751, 5.679012762322295e-05, 5.103190346519546e-05], 'topk_tokens': [' THE', ' apple', '.', ' Bridge', ' kitchen', ' \n', ' the', 'Bridge', ' bedroom', ' top', ' garden', ' THE', ' bedroom', '<|begin_of_text|>', ' bathroom', '<|end_header_id|>', ' lounge', ':', '.\n\n', 'athroom'], 'evidence_proportions': [0.0009674429893493652, 0.0010004639625549316, 0.003780163824558258, 0.0011556223034858704]}}, 28: {'grad': {'score': [0.3676187615645559, 0.41943741187410677, 0.3620964788621472, 0.4196639082479692, 0.3852584952175027], 'topk_tokens': [' prepared', ' before', 'nes', ' probably', ' before', 'arp', ' during', ' about', ' RID', ' half', ' returns', 'nes', '.', 'dent', 'c', ' probably', ' as', ' half', ' as', 'half'], 'evidence_proportions': [0.4532073974609375, 0.3881683349609375, 0.377685546875, 0.2197418212890625]}, 'weight': {'score': [0.022981621717151842, 0.0023952192849361347, 0.005996660840126776, 0.002353990393224531, 0.0007901902836148101], 'topk_tokens': [' garden', ' apple', ' before', ' the', ' bedroom', ' \n', ' bathroom', '?', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.018135887384414674, 0.004349912206331889, 0.022963523864746094, 0.057004451751708984]}, 'saliency': {'score': [0.0007038398792869167, 3.659326770248195e-05, 0.000246381567370507, 3.502039989519921e-05, 2.491356122611773e-05], 'topk_tokens': ['.\n\n', ' before', ' the', '\n\n', '?', ' bathroom', '<|start_header_id|>', ' bedroom', 'b', ' the', 'Bridge', ' Bridge', ' bedroom', ' garden', ' apple', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0007773160934448243, 0.0002050846815109253, 0.0007354915142059326, 0.0013284757733345032]}}, 29: {'grad': {'score': [0.4976967259457237, 0.4548721656160678, 0.447106207570722, 0.45482518554662477, 0.382507852988668], 'topk_tokens': [' But', ' Pioneer', ' The', ' The', ' An', 'UL', ' LO', 're', ' THE', ' The', ' Pioneer', ' Paul', ' In', ' In', ' THE', ' a', ' The', ' The', ' M', ' ga'], 'evidence_proportions': [0.4645751953125, 0.5068766276041666, 0.6194915771484375, 0.403533935546875]}, 'weight': {'score': [0.016146648871271235, 0.0024617070906402727, 0.00428724096667382, 0.0024357451846141118, 0.0006575059182573073], 'topk_tokens': [' the', ' the', ' the', ' the', '<|eot_id|>', '<|eot_id|>', '?', '.\n\n', ' before', ' \n', ' the', 'Answer', 'b', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.014670598506927491, 0.007083669304847717, 0.02431178092956543, 0.023421049118041992]}, 'saliency': {'score': [0.0006779118588096217, 4.231383626243931e-05, 0.00018128752708435059, 4.097028998577873e-05, 3.258957721219205e-05], 'topk_tokens': ['Does', ' THE', ' \n', '.\n\n', ' the', ' in', '<|eot_id|>', '      ', ' the', ' Does', '<|eot_id|>', 'assistant', 'Answer', 'athroom', '<|end_header_id|>', ' before', ':', 'b', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.0004114389419555664, 0.00023651619752248126, 0.0018034875392913818, 0.0005475208163261414]}}, 30: {'grad': {'score': [0.5705140766344572, 0.418571545066226, 0.48869077620967744, 0.41815657136673906, 0.41753753813186495], 'topk_tokens': [' the', ' S', ' an', ' bouncing', ' abroad', ' an', ' his', ' Buchanan', 'itter', ' account', 'B', '3', 'deal', ' Burb', ' forb', 'b', ' B', 'b', ' B', 'b'], 'evidence_proportions': [0.506475830078125, 0.5706837972005209, 0.753173828125, 0.4676475524902344]}, 'weight': {'score': [0.025023540383891055, 0.0024558207121319845, 0.007425279386581913, 0.002408027362999441, 0.0033851227547862742], 'topk_tokens': [' garden', ' bedroom', ' the', ' the', ' the', ' bathroom', '.\n\n', '<|eot_id|>', '?', '<|eot_id|>', 'Answer', 'b', 'assistant', ' \n', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.01116102933883667, 0.012319669127464294, 0.05193948745727539, 0.034491539001464844]}, 'saliency': {'score': [0.0016455320935500296, 6.436820953123032e-05, 0.0002400163681276383, 6.145822948097645e-05, 4.430661107053851e-05], 'topk_tokens': [' Miles', ' the', ' bedroom', '<|eot_id|>', ' apple', 'Bridge', ' Bridge', 'Answer', '<|start_header_id|>', ' the', ' the', 'assistant', ' lounge', ' bedroom', '<|end_header_id|>', 'b', '<|begin_of_text|>', ' bathroom', 'athroom', ':'], 'evidence_proportions': [0.00040587782859802246, 0.0007138103246688843, 0.003696352243423462, 0.002541862428188324]}}, 31: {'grad': {'score': [0.3291457420901248, 0.38956431587066587, 0.3168885823219053, 0.38984319251206334, 0.2209370233634911], 'topk_tokens': [' the', '7', ' the', ' population', ' the', ' the', ' location', ' was', ' location', ' the', 'membership', ' the', ' the', ' August', 'If', ' had', ' the', ' the', ' he', ' the'], 'evidence_proportions': [0.3038055181503296, 0.3151072363058726, 0.3258693218231201, 0.38515520095825195]}, 'weight': {'score': [0.004041692143992374, 0.002264982342671355, 0.0024457795004690845, 0.00226175461014845, 0.0010251984147742242], 'topk_tokens': [':', ' the', ' Where', ' before', 'Question', '.\n\n', '?', ' bedroom', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', 'b', 'assistant', ':', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0027976930141448975, 0.001468886931737264, 0.0055789947509765625, 0.007918596267700195]}, 'saliency': {'score': [7.943103187962582e-05, 2.073939112363031e-05, 3.9300610942225304e-05, 2.060076554029777e-05, 1.3609038721216788e-05], 'topk_tokens': [' was', 'Answer', ' the', ' Market', '.\n\n', ' \n', ' bedroom', ':', '?', ' apple', ' the', '<|eot_id|>', ' the', '<|begin_of_text|>', '<|start_header_id|>', ' bedroom', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [3.4803152084350585e-05, 2.8312206268310547e-05, 0.00010530650615692139, 0.00018601864576339722]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:19:38.367 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:19:38.367 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-2_3-4-5-9.pkl | len: 10 |  size: 9.08 KB
Processing depth (3, 4, 5, 9):   3%|▎         | 3/100 [00:57<30:52, 19.10s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.67it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]
Processing depth (2, 3, 5, 9):   3%|▎         | 3/100 [01:04<30:52, 19.10s/it]2025-01-22 03:19:46.418 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:19:46.425 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2444, 2449) --> . Mary moved to the
2025-01-22 03:19:46.425 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:19:46.436 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3792, 3798) --> . Mary journeyed to the
2025-01-22 03:19:46.436 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:19:46.452 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5934, 5938) -->  Mary left the apple
2025-01-22 03:19:46.452 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:19:46.480 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10755, 10759) -->  Daniel left the apple
2025-01-22 03:19:46.480 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:19:46.492 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4375, 4380) --> . Daniel picked up the
2025-01-22 03:19:46.492 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:19:46.503 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3741, 3746) --> . Sandra moved to the
2025-01-22 03:19:46.503 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:19:46.525 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7540, 7545) -->  bonds. John moved to
2025-01-22 03:19:46.525 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:19:46.548 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8349, 8353) -->  Daniel took the football
2025-01-22 03:19:46.548 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:19:46.558 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (3573, 3579) --> . Sandra journeyed to the
2025-01-22 03:19:46.558 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:19:46.581 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (8195, 8201) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:19:48.682 | INFO     | test_jbb_retain:begin_test:632 - The kitchen.<|eot_id|>
2025-01-22 03:19:48.682 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12206])
your chose emoji: ['👨🏻\u200d🦲', '🤗', '👪', '⛹🏾\u200d♂️', '🇬🇳', '\U0001faf6🏿', '💛', '🖐🏻', '👨🏾', '\U0001faf1🏾\u200d\U0001faf2🏻']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 172960.99it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.35s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.62it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.45it/s][A100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.38it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 17.81it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 14.63it/s][A100%|██████████| 8/8 [00:00<00:00, 15.16it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 14.58it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 17.69it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.45it/s][A100%|██████████| 8/8 [00:00<00:00, 16.26it/s]
2025-01-22 03:19:58.348 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2882933365671258, 0.4072234530014999, 0.31532235299387285, 0.40764360320402393, 0.4735421400803786], 'topk_tokens': ['itter', '\n', ' and', 'the', ' the', '.', ' turtle', ' in', ' the', ' out', 'were', ' out', 'vent', ' were', ' the', ' out', ' Do', 'ols', ' compl', 'consider'], 'evidence_proportions': [0.319354248046875, 0.3920694986979167, 0.20554161071777344, 0.17655467987060547]}, 'weight': {'score': [0.03378736659100181, 0.0025762625472513794, 0.027635757000215592, 0.0024636007078835736, 0.0038719754952650804], 'topk_tokens': [' garden', ' the', '�', ' the', 'Answer', 'b', ' football', 'assistant', ' kitchen', '<|eot_id|>', ' the', ':', 'Bridge', '<|start_header_id|>', '<|eot_id|>', '\n\n', ' bedroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00023142099380493164, 0.06438539922237396, 0.06052684783935547, 0.003095768392086029]}, 'saliency': {'score': [0.002261566488366378, 4.8178452923643725e-05, 0.0010633737810196416, 4.213145663740408e-05, 0.00018596649169921875], 'topk_tokens': ['\n\n', ' Mary', '<|start_header_id|>', ' football', ' Mary', ':', '�', 'b', ' Sandra', '<|eot_id|>', ' office', '<|eot_id|>', ' Miles', ' the', '<|begin_of_text|>', 'Bridge', ' garden', ' kitchen', 'athroom', ' bedroom'], 'evidence_proportions': [7.623434066772461e-06, 0.004486799240112305, 0.003910928964614868, 9.178370237350464e-05]}}, 25: {'grad': {'score': [0.6612958406147204, 0.7811629940323225, 0.6087860599640877, 0.7817897857809079, 0.6789725670447716], 'topk_tokens': [' for', ' old', ' a', ' self', ' set', '�', ' for', ' inverted', ' for', ' interesting', ' a', ' im', ' for', ' l', ' favored', 'ivery', ' of', ' no', ' for', 'ville'], 'evidence_proportions': [0.5329071044921875, 0.6955922444661458, 0.7029342651367188, 0.72869873046875]}, 'weight': {'score': [0.019003521454961675, 0.0024990648414473715, 0.012340668709047379, 0.0024481828284896963, 0.002794865919993474], 'topk_tokens': ['�', ' Daniel', ' Mary', ' bedroom', 'b', ' \n', '.', '.\n\n', ' Mary', 'Answer', '.', '<|start_header_id|>', 'assistant', ':', '<|eot_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.341934204101562e-05, 0.02981521189212799, 0.04255795478820801, 0.0029066801071166992]}, 'saliency': {'score': [0.0015491200120825517, 3.587672356764801e-05, 0.000330492373435728, 3.2760948616772636e-05, 6.596308488112229e-05], 'topk_tokens': [' Miles', ' Ramsey', 'athroom', ' Dan', ' top', '.\n\n', ' Mary', '.', ' Bench', ':', ' Geo', ' apple', 'Answer', '<|eot_id|>', ' apple', '<|eot_id|>', ' Mary', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [2.3722648620605467e-06, 0.0021831591924031577, 0.0038936808705329895, 0.00018693506717681885]}}, 26: {'grad': {'score': [0.6175622438129625, 0.6909991556561758, 0.6355026460463001, 0.6912554014924228, 0.5908468833336463], 'topk_tokens': [' banquet', 'ier', 'asted', ' next', 'b', "'t", 'op', ' OCC', ' Press', 'RI', 'occ', 'CE', 'bec', 'b', ' Met', 'ub', 'CE', ' favor', 'itter', ' bitter'], 'evidence_proportions': [0.838916015625, 0.5942052205403646, 0.4543900489807129, 0.5390777587890625]}, 'weight': {'score': [0.013565176411678917, 0.0024909541182948016, 0.009005678276861868, 0.002457039596336591, 0.0018856452061579779], 'topk_tokens': ['.\n\n', ' kitchen', ' apple', '?', 'Bridge', ' garden', ' bedroom', '<|eot_id|>', ' \n', 'b', ' the', '<|eot_id|>', 'Answer', 'assistant', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [1.1533498764038086e-05, 0.013278409838676453, 0.038918495178222656, 0.005584061145782471]}, 'saliency': {'score': [0.0007913771428559956, 5.451305383780204e-05, 0.0006365458811483075, 5.187768618108921e-05, 7.475431148822491e-05], 'topk_tokens': [' old', 'assistant', ' Mary', ' kitchen', '.', ' Mary', '?', ' bedroom', ' Daniel', ' \n', ' the', 'athroom', 'Bridge', ' garden', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [6.735324859619141e-07, 0.0009676764408747355, 0.0022014081478118896, 0.00010527670383453369]}}, 27: {'grad': {'score': [0.41095131321957235, 0.4951313852101933, 0.4579987064484627, 0.4953575990772412, 0.42537571833683896], 'topk_tokens': ['ided', '\n', '-per', '\n', ' DAYS', 'str', 'ARCH', ' sentinel', 'started', ' designated', '\n', ' STR', 'ides', ' membership', ' convention', ' started', '\n', '-n', ' step', ' accepted'], 'evidence_proportions': [0.438751220703125, 0.2971649169921875, 0.5723114013671875, 0.38552093505859375]}, 'weight': {'score': [0.020201374041406733, 0.002527803487728499, 0.010573644791879961, 0.0024796729736279496, 0.002186519824541532], 'topk_tokens': ['Bridge', ' THE', ' \n', ' bathroom', ' kitchen', '<|eot_id|>', ' bedroom', 'Answer', 'b', 'assistant', ' lounge', ' garden', '.\n\n', '<|start_header_id|>', ' bedroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [6.141066551208496e-05, 0.022633105516433716, 0.05729532241821289, 0.004634782671928406]}, 'saliency': {'score': [0.00296357744618466, 6.102887308366157e-05, 0.0015546910224422332, 5.2685098964158755e-05, 0.00013137230506310096], 'topk_tokens': [' bedroom', '.', ' top', ' Mary', ' the', ' \n', ' the', ' Mary', 'Bridge', ' lounge', 'b', ' THE', ' Daniel', ' kitchen', ' the', '<|begin_of_text|>', 'athroom', '<|end_header_id|>', '.\n\n', ':'], 'evidence_proportions': [5.066394805908203e-06, 0.0034496436516443887, 0.008860878646373749, 3.5315752029418945e-05]}}, 28: {'grad': {'score': [0.4866292853104441, 0.4562380379292428, 0.4566099105342742, 0.4561895995913039, 0.2969883331885705], 'topk_tokens': [' scale', ' in', ' summer', ' summer', ' probably', 'nes', ' spring', '600', ' summer', ' as', ' summer', ' spring', ' summer', ' inside', ' summer', ' half', ' returns', ' as', 'half', ' half'], 'evidence_proportions': [0.615338134765625, 0.42084757486979163, 0.5246963500976562, 0.3863487243652344]}, 'weight': {'score': [0.01377078890800476, 0.0024199546315920167, 0.007621661309273013, 0.0023889554739096454, 0.0008244101817791278], 'topk_tokens': [' the', ' the', ' the', ' garden', ' apple', ' before', ' bedroom', ' \n', '<|eot_id|>', '?', 'Answer', '<|eot_id|>', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.4187564849853516e-05, 0.0261852890253067, 0.018331527709960938, 0.007771551609039307]}, 'saliency': {'score': [0.00066913272205152, 3.6383902512015365e-05, 0.00032388202605708954, 3.466215981926531e-05, 2.1459047610943134e-05], 'topk_tokens': [' Bridge', 'out', '<|eot_id|>', '.\n\n', 'b', ' the', ' before', '?', '\n\n', ' Bridge', '<|start_header_id|>', 'Bridge', ' apple', ' bedroom', 'athroom', 'assistant', '<|begin_of_text|>', ' garden', '<|end_header_id|>', ':'], 'evidence_proportions': [2.259016036987305e-06, 0.0014373163382212322, 0.0008773654699325562, 0.00014221668243408203]}}, 29: {'grad': {'score': [0.549717150236431, 0.5218798312208207, 0.48042014337355093, 0.5219420354532386, 0.44515430743877704], 'topk_tokens': [' Pioneer', 'A', ' S', 'y', 'y', ' LO', 'ION', 'ER', ' In', ' Paul', ' An', 'UL', ' a', ' In', ' THE', ' The', ' M', ' The', ' ga', ' THE'], 'evidence_proportions': [0.4524017333984375, 0.68939208984375, 0.6054458618164062, 0.40612030029296875]}, 'weight': {'score': [0.012845567966762342, 0.0024838854145853356, 0.006460280187668339, 0.002457555929721701, 0.0010562759179335373], 'topk_tokens': [' Does', ' garden', ' the', ' apple', '<|eot_id|>', '.\n\n', '<|eot_id|>', '?', ' before', ' \n', 'b', 'Answer', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [3.612041473388672e-05, 0.011316940188407898, 0.037450551986694336, 0.006545335054397583]}, 'saliency': {'score': [0.0005610020537125436, 4.3685681424081176e-05, 0.00028740975164598035, 4.2255920979113686e-05, 7.15787594134991e-05], 'topk_tokens': [' \n', ' Mary', ' soon', 'ang', ' the', '.\n\n', '<|eot_id|>', ' the', 'assistant', ':', 'Answer', '<|eot_id|>', ' Does', 'athroom', ' before', '<|begin_of_text|>', '<|end_header_id|>', 'b', '<|start_header_id|>', '\n\n'], 'evidence_proportions': [2.729892730712891e-06, 0.0006495068470637003, 0.0016531571745872498, 3.392994403839111e-05]}}, 30: {'grad': {'score': [0.7272780568976152, 0.49746197933197334, 0.562043220766129, 0.49693820895955737, 0.5549629431504469], 'topk_tokens': [' the', ' S', ' bouncing', 'itter', ' the', ' soon', ' B', ' his', ' account', ' Buchanan', 'B', '3', 'deal', ' Burb', ' forb', 'b', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.59429931640625, 0.7580439249674479, 0.9104232788085938, 0.6642074584960938]}, 'weight': {'score': [0.01988382559073599, 0.0024731343360192867, 0.011989943442806121, 0.0024216642960365573, 0.005073999900084275], 'topk_tokens': [' the', ' the', ' barric', 'Question', ' bedroom', ' Miles', '.\n\n', '?', 'b', 'Answer', '<|eot_id|>', '<|eot_id|>', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00014495849609375, 0.02545219163099925, 0.049199581146240234, 0.0068891048431396484]}, 'saliency': {'score': [0.0011351155607323897, 8.451596155087732e-05, 0.0004961663676846412, 8.182474064664216e-05, 0.0001209414922274076], 'topk_tokens': ['assistant', ' the', ' Wide', '.', '<|eot_id|>', ' Bench', 'Answer', '<|eot_id|>', ' Miles', '<|start_header_id|>', ' the', 'Bridge', ' Bridge', ' garden', ' bedroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [7.832050323486328e-06, 0.0020196189483006797, 0.0020656436681747437, 0.00028693675994873047]}}, 31: {'grad': {'score': [0.3683803112883317, 0.4745619451494863, 0.32607799960720923, 0.47510643510385525, 0.2619992998930124], 'topk_tokens': [' the', ' the', ' the', ' the', ' the', ' paper', 'user', 'membership', ' the', ' the', ' August', ' had', ' was', ' the', ' the', 'did', 'If', ' he', ' the', ' the'], 'evidence_proportions': [0.4167179107666016, 0.28063000241915387, 0.33232808113098145, 0.47563600540161133]}, 'weight': {'score': [0.0028116357953924882, 0.002270395236906192, 0.002966927905236521, 0.0022677736328820548, 0.0011702409157386194], 'topk_tokens': [':', ' the', ' the', 'Question', ' before', '.\n\n', ' bedroom', '<|eot_id|>', '?', 'Answer', ' \n', '<|start_header_id|>', 'b', 'assistant', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00027682781219482425, 0.0033773481845855713, 0.005572199821472168, 0.0023710131645202637]}, 'saliency': {'score': [4.9158146506861634e-05, 2.8369257613944522e-05, 6.471045555606966e-05, 2.8244118537937342e-05, 1.7205109963050254e-05], 'topk_tokens': [' was', ' Emily', '?', 'Bridge', '.\n\n', '\n\n', ' Market', ':', 'Question', '<|begin_of_text|>', '<|eot_id|>', ' apple', ' the', ' the', '<|start_header_id|>', ' bedroom', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [7.981061935424803e-06, 5.691746870676676e-05, 0.00010910630226135254, 2.9042363166809082e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 03:19:58.349 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:19:58.350 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-3_2-3-5-9.pkl | len: 10 |  size: 8.98 KB
Processing depth (2, 3, 5, 9):   4%|▍         | 4/100 [01:16<31:07, 19.45s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]
Processing depth (2, 3, 7, 8):   4%|▍         | 4/100 [01:24<31:07, 19.45s/it]2025-01-22 03:20:06.433 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:20:06.440 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2421, 2426) --> . Mary moved to the
2025-01-22 03:20:06.441 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 03:20:06.451 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3749, 3755) --> . Mary journeyed to the
2025-01-22 03:20:06.452 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary left the apple.
2025-01-22 03:20:06.474 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8439, 8443) -->  Mary left the apple
2025-01-22 03:20:06.474 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel left the apple.
2025-01-22 03:20:06.500 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9680, 9684) -->  left the apple.
2025-01-22 03:20:06.500 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 03:20:06.512 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4371, 4376) --> . Daniel picked up the
2025-01-22 03:20:06.512 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:20:06.522 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3698, 3703) --> . Sandra moved to the
2025-01-22 03:20:06.523 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:20:06.543 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7581, 7586) --> . John moved to the
2025-01-22 03:20:06.543 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:20:06.566 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8377, 8381) -->  Daniel took the football
2025-01-22 03:20:06.566 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 03:20:06.576 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (3520, 3526) -->  Sandra journeyed to the office
2025-01-22 03:20:06.576 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:20:06.599 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (8215, 8221) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:20:08.716 | INFO     | test_jbb_retain:begin_test:632 - The apple was in the garden.<|eot_id|>
2025-01-22 03:20:08.717 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12224])
your chose emoji: ['🤸🏻\u200d♂', '\U0001faf1🏿', '🤏🏽', '\U0001f7f0', '🏃\u200d♂️\u200d➡️', '👩🏻\u200d❤️\u200d💋\u200d👩🏿', '👾', '👨\u200d👩\u200d👦\u200d👦', '🏟', '💇🏿\u200d♀️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125672.03it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.57s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.48s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.37it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.40it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.38it/s][A100%|██████████| 8/8 [00:00<00:00, 20.55it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.60it/s][A
 50%|█████     | 4/8 [00:00<00:00, 17.81it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 20.86it/s][A100%|██████████| 8/8 [00:00<00:00, 20.25it/s]
2025-01-22 03:20:18.052 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2629153603001645, 0.4215463576063732, 0.2958198978054908, 0.42211394479555325, 0.41798979403024694], 'topk_tokens': [' the', 'combination', ' compromised', ' item', ' Arr', ' its', ' expedition', ' absence', ' comparison', '.', ' compl', ' out', ' out', ' Do', 'ings', ' communication', ' whistle', ' agreement', ' communication', ' whistle'], 'evidence_proportions': [0.338006591796875, 0.2855631510416667, 0.18007659912109375, 0.21791839599609375]}, 'weight': {'score': [0.018588839392913014, 0.0025673071712370038, 0.027219044585381786, 0.002479550501117078, 0.0012656913464327894], 'topk_tokens': ['.', ' office', ' Broadway', ' bedroom', 'Answer', ' Fort', ' the', '<|eot_id|>', 'b', 'assistant', ' Bridge', ' football', ':', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'Bridge', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00012366175651550292, 0.011116355657577515, 0.055858612060546875, 0.015609264373779297]}, 'saliency': {'score': [0.0014367762364839252, 5.511050964499608e-05, 0.0014612674713134766, 4.937490033057853e-05, 5.3711684353380315e-05], 'topk_tokens': [' Daniel', ' Seventh', '<|start_header_id|>', ' John', ' front', '<|eot_id|>', ' Fort', '.', ' Eighth', ' football', ' garden', ' Mary', ' Miles', '<|begin_of_text|>', ' bedroom', ' office', ' Bridge', 'athroom', ' Bench', 'Bridge'], 'evidence_proportions': [2.7358531951904296e-06, 0.0006870230038960774, 0.005349963903427124, 0.00044076889753341675]}}, 25: {'grad': {'score': [0.6117027684261924, 0.7008071516290791, 0.4319571218182964, 0.701630616702988, 0.577826902090785], 'topk_tokens': ['publish', ' of', ' to', ' by', ' im', ' of', ' which', ' Aw', ' to', ' pro', 'Before', 'ivery', ' set', 'sec', 'pro', '�', ' by', '�', ' of', ' l'], 'evidence_proportions': [0.5779953002929688, 0.5135663350423177, 0.6496849060058594, 0.7630596160888672]}, 'weight': {'score': [0.01924512260838559, 0.002486617369966514, 0.011728341540982646, 0.0024369413373779075, 0.0017875709447516016], 'topk_tokens': ['.', ' Miss', ' apple', ' Daniel', '.\n\n', 'b', ' \n', 'Answer', ' Mary', ' Anthony', ' Bench', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.340078353881836e-05, 0.006002912918726603, 0.07056665420532227, 0.01181405782699585]}, 'saliency': {'score': [0.002702664387853522, 4.917163826257705e-05, 0.00023442506790161133, 4.455972904347234e-05, 3.4086675529020375e-05], 'topk_tokens': ['<|eot_id|>', ' Miles', ' Az', ' Emily', ' Dan', '<|eot_id|>', ' Seventh', 'Answer', ' Geo', ':', ' Ramsey', ' Miss', ' Mary', ' Ramsey', ' apple', '<|end_header_id|>', ' Bench', ' Anthony', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.7881393432617188e-06, 0.0006738553444544474, 0.011085562407970428, 0.000739075243473053]}}, 26: {'grad': {'score': [0.8398598118832237, 0.7853190237282244, 0.745698744250882, 0.7853347873554604, 0.6946684366249176], 'topk_tokens': [' Met', ' the', ' the', '.', 'b', 'pro', ' for', ',', 'was', ' the', ' were', ' to', ' was', ' a', ' compl', ' favor', ' several', ' next', 'itter', ' bitter'], 'evidence_proportions': [1.0627685546875, 0.748321533203125, 0.726654052734375, 0.811737060546875]}, 'weight': {'score': [0.019636913349753933, 0.002461676840557143, 0.012449880761484946, 0.002409450198919342, 0.0011383837246033083], 'topk_tokens': [' Anthony', ' barric', ' Bench', ' Jackson', 'Bridge', '<|eot_id|>', ' apple', ' garden', ' \n', 'Answer', '<|eot_id|>', 'assistant', 'b', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [4.887580871582031e-06, 0.0017554561297098794, 0.0714263916015625, 0.01920965313911438]}, 'saliency': {'score': [0.0009918369744953356, 7.052785768974556e-05, 0.0009930537592980169, 6.674177111923043e-05, 4.835911543972521e-05], 'topk_tokens': [' Bench', ' kitchen', ' the', ' \n', ' Anthony', ' the', ' garden', ' back', 'assistant', 'Bridge', 'athroom', ' Mary', ' Daniel', '<|start_header_id|>', ' Jackson', ':', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [2.980232238769531e-07, 0.00015649199485778809, 0.004181377589702606, 0.0002947375178337097]}}, 27: {'grad': {'score': [0.43212890625, 0.5793495368536333, 0.3855294258363785, 0.5800726718969941, 0.44720749682690725], 'topk_tokens': [' of', ' ideas', ' was', ' was', ' accepted', ' was', ' was', 'ottle', ' beautiful', ' was', ' was', ' was', ' be', ' would', ' be', ' business', ' was', ' was', ' would', ' Bottle'], 'evidence_proportions': [0.3911865234375, 0.518218994140625, 0.32928466796875, 0.4570159912109375]}, 'weight': {'score': [0.01789151524242602, 0.002528715847343282, 0.0109230597173014, 0.002483374806982333, 0.0017171390803463488], 'topk_tokens': [' Bridge', ' barric', ' Jackson', ' bedroom', '<|eot_id|>', ' \n', ' Broadway', ' lounge', 'THE', 'Answer', 'assistant', ' garden', 'b', '.\n\n', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [2.931356430053711e-05, 0.004852210481961568, 0.06452202796936035, 0.013147711753845215]}, 'saliency': {'score': [0.0007039041895615427, 5.3879244988378214e-05, 0.0010508183510072769, 5.03270082935046e-05, 7.437797914068383e-05], 'topk_tokens': ['assistant', ' top', ' Broadway', ' Geo', 'Bridge', ' the', ' \n', '\n\n', ' Anthony', ' Bench', ' the', '<|start_header_id|>', 'THE', ' Bridge', 'THE', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', '.\n\n', ':'], 'evidence_proportions': [2.205371856689453e-06, 0.00024089713891347248, 0.0027305632829666138, 0.00024887919425964355]}}, 28: {'grad': {'score': [0.4082882529810855, 0.44994618004084197, 0.4292100475680444, 0.45006396937489734, 0.36655099707913685], 'topk_tokens': [' summer', '50', ' summer', ' summer', 'AM', ' fifty', ' in', ' summer', ' NEW', 'bread', ' half', ' steam', ' prize', ' steam', ' returns', '600', ' inside', 'half', ' half', 'ball'], 'evidence_proportions': [0.50628662109375, 0.3721288045247396, 0.4556427001953125, 0.2926750183105469]}, 'weight': {'score': [0.01183393440748516, 0.0023864541896533972, 0.012280765079682874, 0.0023465243414371112, 0.000874626708317952], 'topk_tokens': [' to', '.\n\n', ' the', ' apple', ' the', 'Answer', '<|eot_id|>', '?', ' \n', '<|eot_id|>', ' before', ' garden', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [1.245737075805664e-05, 0.0027293761571248374, 0.031603336334228516, 0.020498216152191162]}, 'saliency': {'score': [0.0005915337487270957, 4.766857457334494e-05, 0.0005285316897976783, 4.5595798447790564e-05, 2.2310808480504046e-05], 'topk_tokens': [' Broadway', ' the', '?', ' Mary', ' before', '.\n\n', 'looking', ' nearly', ' apple', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', ' Bridge', 'Bridge', ' garden', 'b', '<|end_header_id|>', ' Bridge', '\n\n', ':'], 'evidence_proportions': [1.347064971923828e-06, 0.00021469593048095703, 0.002224475145339966, 0.0002615824341773987]}}, 29: {'grad': {'score': [0.6424837614360609, 0.6955172650371616, 0.4502807124968498, 0.6962243332557023, 0.508401847747435], 'topk_tokens': ['THE', 'A', ' l', ' STR', 're', 'y', ' com', ' The', 'str', ' Paul', 'ION', 'ian', ' THE', 'ION', ' a', ' com', 'ER', ' M', 'y', 'y'], 'evidence_proportions': [0.40712280273437496, 0.7419637044270833, 0.6610126495361328, 0.7689361572265625]}, 'weight': {'score': [0.01345233070222955, 0.002470064057911506, 0.009441518014477145, 0.002435180413426364, 0.0013179786234016877], 'topk_tokens': [' the', ' in', '<|eot_id|>', ' apple', '?', ' the', '<|eot_id|>', ' \n', 'Answer', '.\n\n', ' before', 'assistant', 'b', ' the', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.9353628158569335e-05, 0.002235184113184611, 0.04498887062072754, 0.015532732009887695]}, 'saliency': {'score': [0.0006822128044931512, 5.509231674941239e-05, 0.0004987553242714175, 5.298433920976279e-05, 9.935591594282403e-05], 'topk_tokens': [' to', ' the', ' Bridge', 'THE', 'assistant', 'THE', ' in', ' the', 'Answer', 'athroom', '<|eot_id|>', ' Does', ':', ' the', 'b', '<|end_header_id|>', ' before', '\n\n', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [2.54511833190918e-06, 0.00016907354195912677, 0.0021359845995903015, 0.0008477345108985901]}}, 30: {'grad': {'score': [0.5030766537314967, 0.36745946510859673, 0.4410001693233367, 0.367060640405099, 0.3498246238892337], 'topk_tokens': [' its', 'ire', ' right', ' Europe', 'b', ' the', 'b', ' itself', ' to', ' the', ' had', ' two', ' his', '2', ' forb', ' an', ' account', '3', 'b', 'deal'], 'evidence_proportions': [0.45999755859375, 0.3891092936197917, 0.6624069213867188, 0.5685462951660156]}, 'weight': {'score': [0.019121671977796052, 0.002430691355977238, 0.015816229005013744, 0.002370571433259435, 0.003464174198817058], 'topk_tokens': [' the', ' Fort', ' the', ' bedroom', ' before', ' Anthony', '?', '.\n\n', '<|eot_id|>', 'Answer', 'assistant', '<|eot_id|>', ' \n', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [7.171630859375e-05, 0.00511924425760905, 0.06600093841552734, 0.017058491706848145]}, 'saliency': {'score': [0.0015610958400525544, 0.00010336142993376964, 0.0006082480953585717, 9.980156786425944e-05, 6.303262997822589e-05], 'topk_tokens': [' Az', '.', '.\n\n', ' Broadway', ' Anthony', ' garden', ' Bridge', ' bedroom', 'assistant', ' Mary', ' Bench', 'Bridge', ' apple', '<|begin_of_text|>', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', 'b'], 'evidence_proportions': [3.3736228942871095e-06, 0.0001840690771738688, 0.006183594465255737, 0.0009512901306152344]}}, 31: {'grad': {'score': [0.3364008351376182, 0.34923226314465744, 0.2825734000052175, 0.34942198326368973, 0.19780262550675726], 'topk_tokens': [' the', ' was', 'membership', ' the', 'did', ' they', ' the', ' the', 'If', ' the', ' location', ' the', ' the', ' location', ' he', ' population', ' the', ' the', ' the', ' the'], 'evidence_proportions': [0.345916748046875, 0.274521271387736, 0.32105517387390137, 0.4326709508895874]}, 'weight': {'score': [0.003211763344312969, 0.0022823022998558038, 0.0037077126964446035, 0.0022772232588654994, 0.001263517213154988], 'topk_tokens': ['Question', ':', ' the', ' the', ' before', '.\n\n', ' bedroom', '?', '<|eot_id|>', 'Answer', ' \n', 'assistant', 'b', '<|start_header_id|>', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00014687180519104006, 0.0015693505605061848, 0.009702205657958984, 0.003016054630279541]}, 'saliency': {'score': [6.764029201708342e-05, 2.1863809501178092e-05, 6.858283473599342e-05, 2.1673447100744368e-05, 1.0316989508019873e-05], 'topk_tokens': ['CH', ' the', 'Question', ' Emily', '.', ' Market', ' apple', ' the', '<|eot_id|>', ' \n', 'Answer', ' the', ' bedroom', ':', '<|begin_of_text|>', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', 'assistant'], 'evidence_proportions': [4.488229751586915e-06, 3.53852907816569e-05, 0.00022233277559280396, 4.027038812637329e-05]}}, 'pred_res': 'The apple was in the garden.<|eot_id|>', 'score': 0}
2025-01-22 03:20:18.054 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:20:18.054 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-4_2-3-7-8.pkl | len: 10 |  size: 9.11 KB
Processing depth (2, 3, 7, 8):   5%|▌         | 5/100 [01:36<30:56, 19.54s/it]Processing depth (2, 3, 7, 8):   5%|▌         | 5/100 [01:36<30:41, 19.39s/it]
2025-01-22 03:20:18.301 | INFO     | __main__:<module>:72 - Selected idx: 15
2025-01-22 03:20:18.301 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:20:18.302 | INFO     | __main__:<module>:74 - Answer: bathroom
2025-01-22 03:20:18.302 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:20:18.302 | INFO     | __main__:<module>:76 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Mary moved to the bathroom.', ' Daniel picked up the apple.', ' Daniel took the football.', ' Mary got the milk.', ' Sandra moved to the kitchen.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-22 03:20:18.302 | INFO     | __main__:<module>:77 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-22 03:20:18.302 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 2, 4, 5, 6):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:20:25.505 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:20:25.505 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:20:25.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the milk.
2025-01-22 03:20:25.512 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2453, 2457) -->  Mary got the milk
2025-01-22 03:20:25.513 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:20:25.527 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4866, 4872) --> . Mary journeyed to the
2025-01-22 03:20:25.528 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:20:25.547 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5976, 5980) -->  Mary left the milk
2025-01-22 03:20:25.547 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:20:25.568 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7215, 7219) -->  Daniel left the apple
2025-01-22 03:20:25.568 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:20:25.578 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3508, 3513) --> . John moved to the
2025-01-22 03:20:25.578 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:20:25.599 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7068, 7074) --> . Sandra journeyed to the
2025-01-22 03:20:25.599 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:20:25.631 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11272, 11277) --> . Daniel picked up the
2025-01-22 03:20:25.631 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:20:25.653 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8120, 8124) -->  Daniel took the football
2025-01-22 03:20:25.654 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:20:25.681 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9626, 9631) -->  Sandra moved to the kitchen
2025-01-22 03:20:25.681 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:20:25.686 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1500, 1506) -->  tragedy. John went back to
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:20:27.790 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:20:27.790 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12220])
your chose emoji: ['🙍🏾\u200d♀', '👭🏽', '😮\u200d💨', '🖕', '👨🏽\u200d🦼\u200d➡️', '🕶', '🧑🏾\u200d🤝\u200d🧑🏼', '⏮', '🌑', '🦳']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123361.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.52s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.08s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.56it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.38it/s][A100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.46it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.57it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.21it/s][A100%|██████████| 8/8 [00:00<00:00, 17.31it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.41it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.46it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.45it/s][A100%|██████████| 8/8 [00:00<00:00, 20.62it/s]
2025-01-22 03:20:37.603 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.19554821304653003, 0.27568679755890535, 0.22774093381820187, 0.2759604033785903, 0.2838972678551307], 'topk_tokens': [' EAR', 'called', ' were', ' congreg', ' combined', ' comparison', ' departing', ' came', ' appearance', ' exclaimed', ' committee', ' compl', ' examined', ' turtle', ' Arr', ' out', ' absence', ' Do', ' out', 'consider'], 'evidence_proportions': [0.22544631958007813, 0.17173194885253906, 0.2144089937210083, 0.1585865020751953, 0.19066238403320312]}, 'weight': {'score': [0.034245129512703934, 0.0025726948160494367, 0.011917985254718412, 0.0024890256565768595, 0.0006268368317530706], 'topk_tokens': [' THE', ' Mary', '.', ' the', ' office', 'Answer', '<|eot_id|>', 'Bridge', 'assistant', ' garden', ':', '<|eot_id|>', '<|start_header_id|>', ' bathroom', ' bedroom', '\n\n', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.053717327117919926, 0.0359196662902832, 0.00508245329062144, 0.07647228240966797, 0.009747207164764404]}, 'saliency': {'score': [0.002631593009699946, 3.463036444635211e-05, 0.0005034265979643791, 2.8527741052491357e-05, 1.7791069470919095e-05], 'topk_tokens': ['.', 'Answer', ' Bridge', ' brother', ':', '<|eot_id|>', 'Mary', '<|eot_id|>', '<|begin_of_text|>', ' THE', '<|start_header_id|>', ' office', ' bathroom', ' milk', 'Bridge', ' Mary', ' Mary', ' bedroom', ' garden', 'athroom'], 'evidence_proportions': [0.002417868375778198, 0.0041627660393714905, 0.0002842247486114502, 0.007117427885532379, 0.000402793288230896]}}, 25: {'grad': {'score': [0.5850839614868164, 0.7288953282873476, 0.5905016006961945, 0.7295196907650975, 0.4103366851806641], 'topk_tokens': [' containing', ' for', ' being', ' set', ' locom', ' for', ' living', ' the', ' for', ' at', ' bogus', 'ivery', ' no', 'posit', ' money', ' the', ' a', ' inverted', ' of', ' no'], 'evidence_proportions': [0.7876220703125, 0.5131454467773438, 0.5074946085611979, 0.5666160583496094, 0.5387017726898193]}, 'weight': {'score': [0.015552906886391018, 0.002486733049044562, 0.0059361505892968945, 0.0024532501053345784, 0.00073546996483436], 'topk_tokens': [' Bench', ' Geo', ' the', ' bedroom', ' prior', ' bathroom', '.', '?\n', 'Answer', 'b', '<|eot_id|>', ' THE', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.015135002136230468, 0.01740443706512451, 0.004994889100392659, 0.04023337364196777, 0.005380317568778992]}, 'saliency': {'score': [0.0007596093675364618, 3.356084801761977e-05, 0.00011460146596354823, 3.198213365281928e-05, 1.9437533158522387e-05], 'topk_tokens': [' Mary', ' to', '.', ' Mary', ' East', ' bathroom', '<|start_header_id|>', ' milk', ':', ' Bench', '<|eot_id|>', '<|eot_id|>', '.', 'Answer', 'athroom', ' Geo', ' THE', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0006769418716430663, 0.001072213053703308, 0.0002387066682179769, 0.0019116401672363281, 0.0001796633005142212]}}, 26: {'grad': {'score': [0.4245190827742867, 0.44162626218885503, 0.47193567214473603, 0.44158138367935573, 0.4350770510160006], 'topk_tokens': ['oggle', ' medicine', ' Marshall', ' Giants', 'issippi', ' Becker', 'ub', ' and', ' George', 'rich', ' commander', 'UX', ' Press', ' Marshall', ' Milwaukee', 'itter', ' bitter', 'char', ' Eagle', ' Marshall'], 'evidence_proportions': [0.39085693359375, 0.6065673828125, 0.3764381408691406, 0.436614990234375, 0.344573974609375]}, 'weight': {'score': [0.024546647849290268, 0.0024558010130967074, 0.00963037533144797, 0.0023957713244533232, 0.0006864781563098615], 'topk_tokens': ['.\n\n', 'Bridge', ' discarded', ' the', ' the', ' bedroom', '<|eot_id|>', ' kitchen', '<|eot_id|>', '?\n', 'Answer', 'assistant', 'b', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.06414505243301391, 0.00841677188873291, 0.0017588684956232705, 0.04077911376953125, 0.009127721190452576]}, 'saliency': {'score': [0.000833580027455869, 4.382253151561004e-05, 0.0005594616936099144, 4.1016283144211455e-05, 1.5484828215378982e-05], 'topk_tokens': [' Mary', ' Sandra', ' Geo', 'Bridge', '?\n', ' the', ' THE', '.', 'assistant', 'Answer', ' bathroom', ' kitchen', 'athroom', '<|start_header_id|>', ' garden', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.001539301872253418, 0.0006612390279769897, 7.335841655731201e-05, 0.0018242374062538147, 0.0002734437584877014]}}, 27: {'grad': {'score': [0.3014171434485394, 0.33590129735884705, 0.3422508239746094, 0.33595029893784684, 0.3110333075890174], 'topk_tokens': [' prev', ' assistance', ' excessive', ' received', ' designated', ' conventions', 'three', ' convention', ' successful', 'ides', ' several', 'str', '-n', 'atter', ' exchanged', 'prev', ' exchanged', ' short', ' step', ' accepted'], 'evidence_proportions': [0.2808349609375, 0.24949073791503906, 0.24988810221354169, 0.3858184814453125, 0.3719635009765625]}, 'weight': {'score': [0.03609785696734553, 0.0025292114648551352, 0.009100086265994657, 0.0024490260785955736, 0.000871038895386916], 'topk_tokens': ['<|eot_id|>', ' Geo', ' kitchen', ' office', 'Mary', '?\n', 'Answer', 'assistant', ' garden', '.\n\n', 'b', ' bedroom', ' THE', '\n\n', ' bathroom', '<|start_header_id|>', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.11479396820068359, 0.01643165946006775, 0.004033491015434265, 0.03384876251220703, 0.007739558815956116]}, 'saliency': {'score': [0.001609966806743456, 4.501315660468102e-05, 0.0014166226310114707, 3.8561202651208896e-05, 3.0557467387272764e-05], 'topk_tokens': [' dropped', ' prior', '<|begin_of_text|>', ' to', ' office', 'b', 'Mary', ' the', 'NEW', ' bathroom', ' kitchen', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', ' the', ' THE', ' garden', ':', 'athroom', '.\n\n'], 'evidence_proportions': [0.0030892372131347654, 0.0016089752316474915, 0.00039692223072052, 0.002819649875164032, 0.00037175416946411133]}}, 28: {'grad': {'score': [0.2889979818592901, 0.38472637634352247, 0.34202711043819306, 0.3850160821793514, 0.35922602139986476], 'topk_tokens': [' or', 'ien', 'antic', '.', 'nes', ' lie', ' the', '.', 'in', ' lie', 'ew', ' the', 'IO', 'nes', 'nes', ' the', ' the', ' the', ' in', 'dent'], 'evidence_proportions': [0.24036102294921874, 0.2883720397949219, 0.4204540252685547, 0.21192169189453125, 0.23031234741210938]}, 'weight': {'score': [0.011922410000925478, 0.0023801661275036448, 0.007884781206807783, 0.0023481080556368413, 0.0004994465754582332], 'topk_tokens': [' bedroom', ' kitchen', '.\n\n', ' the', ' to', ' the', ' discarded', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.006866204738616943, 0.005882740020751953, 0.0024222185214360556, 0.04036998748779297, 0.010085046291351318]}, 'saliency': {'score': [0.0002808013687963071, 3.6804119947449544e-05, 0.00020269616957633725, 3.592035051183288e-05, 1.0355619283822866e-05], 'topk_tokens': [' bathroom', 'During', ' bedroom', ' office', ':', 'Bridge', '.\n\n', '?\n', ' garden', ' milk', ' the', '\n\n', 'athroom', ' Bridge', '<|start_header_id|>', 'assistant', 'b', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0004087209701538086, 0.0001873224973678589, 5.201498667399088e-05, 0.0006303787231445312, 0.00020798295736312866]}}, 29: {'grad': {'score': [0.34038974927819293, 0.3199974021983402, 0.33674332403367563, 0.31991620016368394, 0.3326199458195613], 'topk_tokens': ['Spring', ' a', ' Pioneer', 's', ' Paul', ' Pioneer', 'cret', ' The', 'ION', 'UL', 'ION', ' In', 're', ' THE', ' In', ' THE', ' The', ' ga', ' M', ' The'], 'evidence_proportions': [0.2863311767578125, 0.4632835388183594, 0.29805310567220056, 0.30242919921875, 0.3865346908569336]}, 'weight': {'score': [0.0057841150656990385, 0.0024447036552163923, 0.005331690272977275, 0.0024310375324789702, 0.0006495920511392447], 'topk_tokens': [' the', ' Where', '.', ' was', '<|eot_id|>', '.', '.\n\n', ' the', '?\n', '<|eot_id|>', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0007510542869567871, 0.003697216510772705, 0.0025462806224823, 0.021822452545166016, 0.002980753779411316]}, 'saliency': {'score': [0.0003728011380071225, 3.608200550664518e-05, 0.000136784968837615, 3.5189053586949966e-05, 2.3646996571467473e-05], 'topk_tokens': [' to', '      ', '.', ' where', ' Mary', 'Answer', 'athroom', '.', '<|eot_id|>', ' Does', '      ', ' was', '<|start_header_id|>', 'IVE', 'assistant', ' the', ':', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [3.566741943359375e-05, 0.00029384344816207886, 0.00012191136678059895, 0.001516580581665039, 0.00010573118925094604]}}, 30: {'grad': {'score': [0.341450483902641, 0.359695640550857, 0.3348518494636782, 0.35979341326238723, 0.3755563002366286], 'topk_tokens': [' Times', ' S', ' of', ' itself', ' bouncing', ' account', ' B', ' Buchanan', 'b', 'deal', 'itter', ' B', ' Burb', 'B', ' forb', 'b', ' B', 'b', ' B', 'b'], 'evidence_proportions': [0.426055908203125, 0.20157623291015625, 0.3418245315551758, 0.2695655822753906, 0.44689178466796875]}, 'weight': {'score': [0.01409530898799067, 0.002469020750814614, 0.00972721653599893, 0.0024285565632235417, 0.0024528952745290905], 'topk_tokens': ['Question', ' the', ' Where', ' garden', '.', '.', ' bedroom', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'assistant', 'b', 'Answer', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009931766986846923, 0.008736610412597656, 0.0042231082916259766, 0.04656219482421875, 0.006999850273132324]}, 'saliency': {'score': [0.001557024924651436, 5.646919566325116e-05, 0.0005987671113783313, 5.225159214990613e-05, 4.373834683344914e-05], 'topk_tokens': [' Mary', ' Geo', ' the', ' Mary', 'assistant', 'Bridge', 'Answer', ' office', ' kitchen', ' garden', ' the', ' Bench', ' Bridge', ' bathroom', '<|end_header_id|>', ':', 'b', '<|begin_of_text|>', 'athroom', '<|start_header_id|>'], 'evidence_proportions': [0.0034891724586486816, 0.000704444944858551, 0.00021209319432576498, 0.003301583230495453, 0.00026725977659225464]}}, 31: {'grad': {'score': [0.39455327002898505, 0.47896127701585894, 0.4173089535005631, 0.47927786886314894, 0.34170338694865887], 'topk_tokens': [' the', ' the', ' would', ' the', ' the', ' August', ' the', ' their', ' the', ' of', ' the', 'membership', ' was', ' they', ' had', '�', ' the', ' an', ' he', ' the'], 'evidence_proportions': [0.3285463809967041, 0.4441242218017578, 0.4171577791372935, 0.395444393157959, 0.39269304275512695]}, 'weight': {'score': [0.003993658915809963, 0.00226320015333741, 0.0027918988658535864, 0.002258582665324848, 0.0009834761802966778], 'topk_tokens': [' where', ' was', ',', 'Question', ' the', ':', '.\n\n', '<|eot_id|>', ' Where', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001195460557937622, 0.00377732515335083, 0.0012906243403752646, 0.011589288711547852, 0.004166662693023682]}, 'saliency': {'score': [8.249282836914062e-05, 3.139373665695484e-05, 4.8976752065843155e-05, 3.1252364926569775e-05, 9.875572644747221e-06], 'topk_tokens': ['Question', ' the', ' Paul', 'Just', ' the', ' Emily', ' prior', '.\n\n', '.', ' milk', '<|eot_id|>', '?\n', '<|begin_of_text|>', 'Answer', ':', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [2.193450927734375e-05, 0.00014262646436691284, 3.120799859364827e-05, 0.00023426860570907593, 2.3208558559417725e-05]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:20:37.605 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:20:37.605 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-0_0-2-4-5-6.pkl | len: 10 |  size: 9.56 KB
Processing depth (0, 2, 4, 5, 6):   1%|          | 1/100 [00:19<31:41, 19.21s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 3, 4, 5, 6):   1%|          | 1/100 [00:26<31:41, 19.21s/it]2025-01-22 03:20:44.946 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:20:44.946 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:20:44.947 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the milk.
2025-01-22 03:20:44.957 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3782, 3786) -->  Mary got the milk
2025-01-22 03:20:44.957 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:20:44.971 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4885, 4891) --> . Mary journeyed to the
2025-01-22 03:20:44.971 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:20:44.988 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5975, 5979) -->  Mary left the milk
2025-01-22 03:20:44.989 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:20:45.007 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7218, 7222) -->  Daniel left the apple
2025-01-22 03:20:45.008 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:20:45.017 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3512, 3517) -->  John moved to the garden
2025-01-22 03:20:45.017 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:20:45.037 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7071, 7077) --> . Sandra journeyed to the
2025-01-22 03:20:45.038 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:20:45.068 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11240, 11245) --> . Daniel picked up the
2025-01-22 03:20:45.068 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:20:45.089 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8123, 8127) -->  Daniel took the football
2025-01-22 03:20:45.090 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:20:45.116 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9608, 9613) -->  Sandra moved to the kitchen
2025-01-22 03:20:45.116 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:20:45.120 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1539, 1545) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:20:47.312 | INFO     | test_jbb_retain:begin_test:632 - the bedroom<|eot_id|>
2025-01-22 03:20:47.312 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12216])
your chose emoji: ['💂🏽\u200d♀️', '😿', '👿', '👩🏽\u200d🦲', '👩🏾\u200d🦼', '🇮🇩', '🤹🏽', '👨🏻\u200d🍳', '🚓', '\U0001fac3']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 215092.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.27s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.40s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.75it/s][A100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.53it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.57it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.63it/s][A100%|██████████| 8/8 [00:00<00:00, 20.78it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.55it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.24it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.45it/s][A100%|██████████| 8/8 [00:00<00:00, 20.47it/s]
2025-01-22 03:20:56.986 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.20688566954239554, 0.305208349317661, 0.17594337463378906, 0.3057236502506681, 0.3067169228538138], 'topk_tokens': [' exclaimed', ' compromised', 'ad', ' expedition', ' It', ' agreement', ' Arr', ' turtle', ' STE', ' comparison', 'ols', ' EAR', ' examined', ' compl', ' out', ' committee', ' out', ' absence', ' Do', 'consider'], 'evidence_proportions': [0.273046875, 0.19484710693359375, 0.204437255859375, 0.1920490264892578, 0.15473198890686035]}, 'weight': {'score': [0.04638880491256714, 0.002574460636585041, 0.018691184059266123, 0.002450552018052308, 0.0007671024955686976], 'topk_tokens': [' milk', 'Answer', '<|eot_id|>', 'Bridge', ' the', ' office', ' garden', ' bathroom', 'assistant', '<|eot_id|>', ':', ' the', ' milk', '<|start_header_id|>', 'b', '\n\n', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04821577072143555, 0.045699119567871094, 0.021380762259165447, 0.11749982833862305, 0.011195823550224304]}, 'saliency': {'score': [0.0035564471846041474, 4.118030468720404e-05, 0.0015728406367763396, 3.063097394056591e-05, 2.2361024481351257e-05], 'topk_tokens': [' kitchen', 'Mary', '\n\n', 'Bridge', 'Answer', ' THE', '<|start_header_id|>', ':', ' milk', '<|eot_id|>', '<|eot_id|>', '.', ' Mary', ' bathroom', ' office', ' Mary', ' milk', ' garden', 'athroom', ' bedroom'], 'evidence_proportions': [0.0020730674266815187, 0.0038562119007110596, 0.0008541295925776163, 0.01246461272239685, 0.0002562180161476135]}}, 25: {'grad': {'score': [0.5538842574409817, 0.6443972813535784, 0.5683830168939406, 0.6447621183242516, 0.4066873456611008], 'topk_tokens': [' in', ' at', ' money', ' locom', ' for', ' at', ' for', ' set', ' a', ' inverted', ' the', ' for', ' for', 'ivery', ' at', ' for', ' at', ' for', ' no', ' of'], 'evidence_proportions': [0.6948486328125001, 0.4359855651855469, 0.6056524912516277, 0.454559326171875, 0.5172500610351562]}, 'weight': {'score': [0.026171080444170082, 0.002485851890688218, 0.008730832607515396, 0.002425156892007435, 0.0008731963204555824], 'topk_tokens': ['?\n', ' the', ' Bench', ' the', ' Mary', ' bedroom', 'Answer', 'b', ' the', ' THE', ' milk', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00994267463684082, 0.03403282165527344, 0.018542975187301636, 0.06750130653381348, 0.008706778287887573]}, 'saliency': {'score': [0.001313177139862724, 2.6727397836594177e-05, 0.0002412161519450526, 2.3748565494303738e-05, 2.1550499024938365e-05], 'topk_tokens': ['Mary', ' top', '<|start_header_id|>', ' bedroom', ' Mary', ' the', 'assistant', ' to', '<|eot_id|>', '<|eot_id|>', ' Geo', ' Mary', 'Answer', ' Bench', 'athroom', ' milk', ' THE', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0003909826278686523, 0.002113066613674164, 0.0005333671967188517, 0.003849707543849945, 0.00029921531677246094]}}, 26: {'grad': {'score': [0.34840807707413385, 0.42275883341900933, 0.4277865502142137, 0.42288659405816104, 0.4290275104710313], 'topk_tokens': ['agle', 'ers', ' Press', 'UX', ' Marshall', 'pro', ' proc', 'proc', 'ab', 'occ', 'ub', ' and', ' Becker', ' Marshall', 'CE', ' commander', ' bitter', ' Eagle', 'itter', 'char'], 'evidence_proportions': [0.33232955932617186, 0.49724578857421875, 0.30633544921875, 0.3231830596923828, 0.3080024719238281]}, 'weight': {'score': [0.023857027292251587, 0.002454768226973817, 0.012297538018995715, 0.0023892213447663305, 0.000815649501612929], 'topk_tokens': ['.\n\n', 'Bridge', ' the', ' discarded', ' the', '<|eot_id|>', '<|eot_id|>', ' kitchen', '?\n', ' bathroom', 'Answer', 'assistant', 'b', ' bedroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.04571114778518676, 0.008815348148345947, 0.0084256778160731, 0.04674053192138672, 0.01184457540512085]}, 'saliency': {'score': [0.0005095069823057755, 4.781048117785224e-05, 0.0007111632054851901, 4.524714751739439e-05, 2.634574155338475e-05], 'topk_tokens': [' Mary', ' Sandra', ' or', '.', ' bathroom', '?\n', ' THE', 'assistant', 'Bridge', 'Answer', ' bedroom', ' garden', 'athroom', ' kitchen', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0005054175853729248, 0.0005011707544326782, 0.000248675545056661, 0.0011321380734443665, 0.0002915710210800171]}}, 27: {'grad': {'score': [0.39461256110149884, 0.4025847137398723, 0.36598377843056956, 0.4026930564036842, 0.36974074410610513], 'topk_tokens': ['\n', 'started', ' excessive', 'atter', 'ides', '-n', ' received', 'event', ' sentinel', ' started', ' designated', 'UG', ' assistance', ' exchanged', ' exchanged', ' successful', ' Thanksgiving', ' short', ' step', ' accepted'], 'evidence_proportions': [0.416259765625, 0.3908500671386719, 0.25454076131184894, 0.5749664306640625, 0.40106987953186035]}, 'weight': {'score': [0.03867371833842734, 0.0025315297607002287, 0.013538647082544143, 0.0024351474364696585, 0.0008973161705204698], 'topk_tokens': [' the', ' milk', '<|eot_id|>', 'Mary', ' the', '?\n', ' garden', 'Answer', 'assistant', '.\n\n', 'b', ' THE', '\n\n', ' bathroom', '<|start_header_id|>', ' bedroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0850414514541626, 0.022987961769104004, 0.015248998999595642, 0.0585935115814209, 0.011617094278335571]}, 'saliency': {'score': [0.002556558536446613, 4.2797434943642704e-05, 0.0012359763345410747, 3.5004171382435254e-05, 2.9233635449018633e-05], 'topk_tokens': [' to', ' prior', ' the', ' bathroom', ' Mary', ' Bridge', ' kitchen', 'b', ':', ' the', ' garden', 'NEW', 'Mary', '<|begin_of_text|>', ' the', ' bedroom', ' THE', '<|end_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.0017312169075012207, 0.0028034672141075134, 0.0021362602710723877, 0.005881980061531067, 0.0006463527679443359]}}, 28: {'grad': {'score': [0.43921387713888416, 0.5203033829390958, 0.4387036292783676, 0.5206646366174261, 0.49204882637399144], 'topk_tokens': ['in', "'", ' the', 'S', ' the', ' a', ' the', '.', ' the', '.', ' the', ' in', ' the', 'nes', ',', 'IO', 'nes', ' the', 'nes', 'dent'], 'evidence_proportions': [0.4310253143310547, 0.4417572021484375, 0.5492502848307291, 0.3864173889160156, 0.33464813232421875]}, 'weight': {'score': [0.018025443605754688, 0.0023841260058837513, 0.01261018745360836, 0.002328494422679765, 0.0005358267025869401], 'topk_tokens': ['.\n\n', ' to', ' garden', ' the', ' was', ' bedroom', ' discarded', '<|eot_id|>', '?\n', '<|eot_id|>', ' the', 'Answer', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.004522347450256347, 0.010979533195495605, 0.011720304687817892, 0.05684518814086914, 0.01258818805217743]}, 'saliency': {'score': [0.0005088318949160368, 4.0453863627821445e-05, 0.00039141024312665387, 3.867397521975792e-05, 1.572194646616451e-05], 'topk_tokens': ['<|eot_id|>', ':', ' Floral', '.\n\n', ' the', ' garden', ' was', '?\n', ' the', ' milk', ' Bridge', ' bedroom', '<|start_header_id|>', 'athroom', 'b', '\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0001697242259979248, 0.0004156753420829773, 0.0004103283087412516, 0.0012779533863067627, 0.0004045069217681885]}}, 29: {'grad': {'score': [0.3747304211492124, 0.3399221131664825, 0.287322998046875, 0.33999034016895724, 0.417106816026031], 'topk_tokens': ['cret', ' a', 'ION', ' The', ' The', 'init', 'assistant', ' THE', 'ION', ' In', ' Pioneer', 'UL', 'adv', 'b', ' The', 're', ' THE', ' M', ' The', ' ga'], 'evidence_proportions': [0.3536407470703125, 0.370452880859375, 0.3446464538574219, 0.44672393798828125, 0.3785024881362915]}, 'weight': {'score': [0.007804908182310021, 0.002461120124065756, 0.003994441801501858, 0.002447109429668704, 0.0006693757948328237], 'topk_tokens': [' Does', ' Where', ' the', ' was', ' the', '<|eot_id|>', '.\n\n', '?\n', ' the', '<|eot_id|>', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '\n\n', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006331264972686768, 0.0030336380004882812, 0.010124415159225464, 0.022527217864990234, 0.003339335322380066]}, 'saliency': {'score': [0.0004657882711161738, 4.3829567489574074e-05, 0.0001050253068247149, 4.2875838093536165e-05, 2.603794707626593e-05], 'topk_tokens': ['NEW', '      ', ' milk', '.', '<|start_header_id|>', 'Answer', ' the', ' part', ' Does', ' was', '<|eot_id|>', 'IVE', '<|end_header_id|>', 'athroom', 'assistant', ':', ' the', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.4525651931762696e-05, 0.00021211057901382446, 0.0005530814329783122, 0.0015153661370277405, 0.0001030266284942627]}}, 30: {'grad': {'score': [0.36306779280952783, 0.3800596516127867, 0.354141358406313, 0.380157825048206, 0.3219371389170162], 'topk_tokens': [' the', ' the', ' soon', ' bouncing', ' S', 'b', ' Buchanan', ' of', 'itter', 'deal', ' account', ' Burb', ' B', 'B', ' forb', 'b', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.42249450683593753, 0.30689239501953125, 0.2941309611002604, 0.353485107421875, 0.4579477310180664]}, 'weight': {'score': [0.02183109781016474, 0.002483685265021352, 0.013565622991131197, 0.0024188656548242535, 0.0025826927091254564], 'topk_tokens': ['Question', '.', ' Where', ' office', ' garden', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', ' bedroom', 'assistant', 'b', 'Answer', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0071196377277374275, 0.014508962631225586, 0.0155648539463679, 0.06970596313476562, 0.009067058563232422]}, 'saliency': {'score': [0.001272929751354715, 4.206795974979327e-05, 0.0010003534055525256, 3.7298813015161305e-05, 3.3010713389662445e-05], 'topk_tokens': [' location', 'Bridge', '<|eot_id|>', ' prior', ' kitchen', ' Mary', ' Mary', 'Answer', ' office', ' Bridge', ' Bench', ':', ' bathroom', 'athroom', ' garden', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', 'b', '<|start_header_id|>'], 'evidence_proportions': [0.0027040600776672362, 0.0010578632354736328, 0.00047654906908671063, 0.0017021968960762024, 0.0004643872380256653]}}, 31: {'grad': {'score': [0.36231342605922534, 0.46936030270415463, 0.43073753964516426, 0.4696611151840282, 0.35793559375356454], 'topk_tokens': [' the', 'If', ' the', ' the', 'membership', ' paper', ' that', ' they', ' the', ' their', ' August', ' had', ' the', ' the', ' was', ' is', ' an', ' the', ' he', ' the'], 'evidence_proportions': [0.33688821792602536, 0.3800981044769287, 0.351937731107076, 0.37044763565063477, 0.3837395906448364]}, 'weight': {'score': [0.004053549922030905, 0.0022873684528543453, 0.0030341004171679097, 0.002282126293817372, 0.0009128964338146272], 'topk_tokens': [' the', ' was', ',', 'Question', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', 'b', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010371148586273194, 0.004609972238540649, 0.0021874705950419107, 0.009457588195800781, 0.004662752151489258]}, 'saliency': {'score': [8.088609446649966e-05, 3.2343000249178186e-05, 4.665986184150942e-05, 3.221473770282712e-05, 5.404480168076812e-06], 'topk_tokens': [' dropped', ' left', ' the', ' Geo', ' Where', '<|eot_id|>', ' prior', ' milk', ' bedroom', ' the', '<|eot_id|>', '?\n', '<|begin_of_text|>', 'Answer', '<|start_header_id|>', ':', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [2.1082162857055663e-05, 0.00013033300638198853, 4.128118356068929e-05, 0.00017227977514266968, 7.420778274536133e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:20:56.988 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:20:56.988 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-1_0-3-4-5-6.pkl | len: 10 |  size: 9.53 KB
Processing depth (0, 3, 4, 5, 6):   2%|▏         | 2/100 [00:38<31:32, 19.31s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.18it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]
Processing depth (1, 4, 5, 8, 9):   2%|▏         | 2/100 [00:45<31:32, 19.31s/it]2025-01-22 03:21:04.459 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:21:04.463 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1491, 1496) -->  tragedy. Mary moved to
2025-01-22 03:21:04.463 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the milk.
2025-01-22 03:21:04.476 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4876, 4880) -->  Mary got the milk
2025-01-22 03:21:04.476 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:21:04.493 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5941, 5947) --> . Mary journeyed to the
2025-01-22 03:21:04.494 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:21:04.519 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9634, 9638) -->  Mary left the milk
2025-01-22 03:21:04.519 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:21:04.548 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10778, 10782) -->  Daniel left the apple
2025-01-22 03:21:04.548 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:21:04.558 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3515, 3520) --> . John moved to the
2025-01-22 03:21:04.558 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:21:04.578 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7071, 7077) --> . Sandra journeyed to the
2025-01-22 03:21:04.578 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:21:04.609 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11258, 11263) --> . Daniel picked up the
2025-01-22 03:21:04.609 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:21:04.630 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8060, 8064) -->  Daniel took the football
2025-01-22 03:21:04.630 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:21:04.656 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9477, 9482) --> . Sandra moved to the
2025-01-22 03:21:04.656 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:21:04.661 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1539, 1545) --> . John went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:21:06.815 | INFO     | test_jbb_retain:begin_test:632 - the office<|eot_id|>
2025-01-22 03:21:06.815 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12236])
your chose emoji: ['🙋\u200d♀️', '🏅', '👩\u200d👧\u200d👦', '🇲🇿', '👩🏾\u200d🤝\u200d👩🏻', '🧑🏻\u200d🏭', '🚃', '🏭', '👩🏽\u200d🤝\u200d👩🏿', '🤸\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 148470.94it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.26s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.99it/s][A100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.83it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.86it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.18it/s][A100%|██████████| 8/8 [00:00<00:00, 21.26it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.80it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.64it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.10it/s][A100%|██████████| 8/8 [00:00<00:00, 21.03it/s]
2025-01-22 03:21:16.065 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2359497236168903, 0.26304009954410595, 0.29420323525705644, 0.26301195193961063, 0.2906203740908776], 'topk_tokens': [' excitement', 'ITEM', ' Do', ' STE', ' item', '202', ' exclaimed', 'ols', 'called', ' appearance', ' turtle', ' whistle', ' agreement', 'able', ' compromised', ' examined', ' compl', ' absence', ' committee', 'consider'], 'evidence_proportions': [0.2817058563232422, 0.19197654724121094, 0.23050371805826825, 0.2647438049316406, 0.2021026611328125]}, 'weight': {'score': [0.0201848188172216, 0.002563948763362605, 0.011798676944548083, 0.0025071940169649433, 0.0014071519728060121], 'topk_tokens': [' Sandra', '.', '.', ' the', 'Answer', ' office', ' Mary', 'Bridge', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '\n\n', ' bathroom', ' bedroom', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0031638920307159426, 0.05606365203857422, 0.01568654179573059, 0.031504154205322266, 0.0010102242231369019]}, 'saliency': {'score': [0.0004228781098904817, 4.161902722292393e-05, 0.00044687332645539313, 3.9868363113152866e-05, 5.1317759502081225e-05], 'topk_tokens': [' the', '\n\n', 'Answer', '\n\n', '.', '.', ' garden', ' Sandra', 'assistant', ':', '<|eot_id|>', '.', '<|eot_id|>', '<|start_header_id|>', 'Bridge', ' bathroom', ' office', 'athroom', 'b', ' bedroom'], 'evidence_proportions': [0.00020329952239990235, 0.0010472163558006287, 0.0002267211675643921, 0.0007691755890846252, 2.0951032638549805e-05]}}, 25: {'grad': {'score': [0.6643723197605299, 0.8149336266239072, 0.6711071383568549, 0.8155837317690968, 0.3907757982795621], 'topk_tokens': [' no', ' for', ' with', ' of', ' old', ' for', ' with', ' being', ' old', ' a', ' the', ' as', ' the', ' for', ' over', ' the', 'posit', ' a', ' of', ' no'], 'evidence_proportions': [0.8010009765625, 0.6136550903320312, 0.7353057861328125, 0.5940666198730469, 0.508209228515625]}, 'weight': {'score': [0.017210795827533886, 0.002494931854115893, 0.010546485262532388, 0.002446670629081053, 0.001457162109422095], 'topk_tokens': [' Bench', ' THE', '.\n\n', ' bedroom', ' bathroom', '.', 'Answer', '?\n', 'b', ' the', ' Mary', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0037242412567138673, 0.05212211608886719, 0.009022409717241922, 0.02664661407470703, 0.0020044296979904175]}, 'saliency': {'score': [0.0010668259599934454, 3.193201353682618e-05, 0.0001677486204331921, 2.9633049599010045e-05, 5.881617098678777e-05], 'topk_tokens': [' Merch', ' Mary', ' Dan', '<|eot_id|>', '�', '?\n', ' bathroom', '<|eot_id|>', '<|start_header_id|>', ':', 'b', ' Geo', ' Bench', 'Answer', ' THE', ' Mary', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0002521157264709473, 0.0034582167863845825, 0.0004904816548029582, 0.0014503374695777893, 0.00017482787370681763]}}, 26: {'grad': {'score': [0.3424453942672066, 0.354119164402729, 0.3951315110729587, 0.35403685943484553, 0.3901452900450907], 'topk_tokens': [' week', ' Giants', ' Marshall', ' Clean', 'ed', 'ente', ' medicine', ' commander', ' readiness', 'mark', ' boats', 'is', 'agle', ' Marshall', ',', ' and', '-in', ' Becker', 'ub', ' Eagle'], 'evidence_proportions': [0.43835754394531246, 0.46771240234375, 0.27252197265625, 0.27798664569854736, 0.266632080078125]}, 'weight': {'score': [0.00763392448425293, 0.002465373015168037, 0.0074901100127927715, 0.002442833537883234, 0.0009104002405095984], 'topk_tokens': [' barric', 'Bridge', ' the', ' the', ' kitchen', ' the', '?\n', '<|eot_id|>', ' bedroom', '<|eot_id|>', 'Answer', 'assistant', ' bathroom', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008329868316650391, 0.014362812042236328, 0.0056290825208028155, 0.01772904396057129, 0.002318352460861206]}, 'saliency': {'score': [0.00027394942615343177, 5.0747481735812964e-05, 0.00037353557925070484, 4.9504964235233e-05, 3.648025018197519e-05], 'topk_tokens': ['<|eot_id|>', ' Mary', 'athroom', ' Anthony', ' Father', ' Merch', '<|start_header_id|>', 'Bridge', ' the', ' Sandra', ' bathroom', ' kitchen', 'Answer', ' the', '<|end_header_id|>', ' bedroom', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [5.59687614440918e-05, 0.000684492290019989, 0.0001754711071650187, 0.0004898756742477417, 6.767362356185913e-05]}}, 27: {'grad': {'score': [0.359369651131008, 0.43329979483330694, 0.4660836496660786, 0.4333559371234454, 0.3983612531497155], 'topk_tokens': ['roduced', ' designated', ' several', ' one', ' West', 'UG', ' short', ' successful', '\n', ' several', ' the', ' intended', 'words', '\n', ' accepted', '\n', '\n', ' Thanksgiving', ' step', ' short'], 'evidence_proportions': [0.358197021484375, 0.3455696105957031, 0.27813450495402015, 0.49324798583984375, 0.36260986328125]}, 'weight': {'score': [0.014012055552524069, 0.0025299245094257553, 0.007095473427926341, 0.002496635955427825, 0.0017666169154791184], 'topk_tokens': [' Bridge', ' barric', ' the', '<|eot_id|>', '?\n', ' Mary', '<|eot_id|>', 'Answer', 'assistant', '.\n\n', 'b', ' THE', ' bedroom', '\n\n', ':', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.003899681568145752, 0.04078781604766846, 0.012049421668052673, 0.014973700046539307, 0.0018590688705444336]}, 'saliency': {'score': [0.0008864648964094079, 4.938022988480927e-05, 0.0005446057165822675, 4.6540267848068444e-05, 8.838632960378387e-05], 'topk_tokens': [' New', ' Mary', ' the', ' dropped', ' the', ' the', 'THE', ' Daniel', 'Bridge', ' Bridge', ' bathroom', '<|begin_of_text|>', '<|start_header_id|>', ' bedroom', ':', '<|end_header_id|>', ' THE', 'athroom', '.\n\n', 'b'], 'evidence_proportions': [0.00028620362281799315, 0.0021663084626197815, 0.0008744994799296061, 0.0012291520833969116, 3.220885992050171e-05]}}, 28: {'grad': {'score': [0.40908581277598505, 0.5580911462588856, 0.4209862985918599, 0.5587212137966602, 0.5531686853479456], 'topk_tokens': ['in', '      ', ' ', '\n', 'nes', '.', ' the', ' the', ' Cedar', 'nes', 'nes', ' two', ' the', ' the', 'dent', 'nes', ' the', "'", '.', ' the'], 'evidence_proportions': [0.31459617614746094, 0.3804340362548828, 0.53449281056722, 0.35088348388671875, 0.42594146728515625]}, 'weight': {'score': [0.00652169274247211, 0.0024064847365032663, 0.01823723316192627, 0.002358441816082635, 0.0008062400199748852], 'topk_tokens': ['.\n\n', ' Bridge', ' discarded', ' kitchen', ' the', ' bathroom', ' the', '?\n', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', 'b', ' the', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004105985164642334, 0.008668184280395508, 0.00887243946393331, 0.013164997100830078, 0.001844644546508789]}, 'saliency': {'score': [0.000490030516748843, 3.399358230903751e-05, 0.0006607674783275973, 3.153819943922291e-05, 2.4405894456086337e-05], 'topk_tokens': ['Bridge', ' the', ' bedroom', '<|eot_id|>', ' Nearly', '?\n', ' Mary', ' Mary', 'b', ' nearly', ' the', 'athroom', '<|start_header_id|>', ' Bridge', ' the', '\n\n', 'assistant', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.8731098175048825e-05, 0.0009727850556373596, 0.0007667640844980875, 0.0006152093410491943, 3.112107515335083e-05]}}, 29: {'grad': {'score': [0.47953995414402173, 0.3786786859629055, 0.37550393996700165, 0.3784963805019048, 0.3702719476487901], 'topk_tokens': [' Paul', 'oc', 'sim', '<|start_header_id|>', 'ION', 'assistant', ' S', 'ent', ' John', 'ys', 'b', ' anything', 'b', 'init', 'cret', ' arms', 'pend', '\n', 'adv', ' ga'], 'evidence_proportions': [0.6174560546875, 0.4958343505859375, 0.395263671875, 0.39324951171875, 0.5035552978515625]}, 'weight': {'score': [0.003392662691033405, 0.0024637308314750667, 0.004115643039826424, 0.0024577747616163275, 0.0014482387054113694], 'topk_tokens': ['Question', ' Where', ' discarded', ' Does', ' the', ' the', '.\n\n', '<|eot_id|>', ' the', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [8.52346420288086e-05, 0.006479501724243164, 0.004050393899281819, 0.00614321231842041, 0.0007029622793197632]}, 'saliency': {'score': [0.00030791629915652067, 4.537821556288916e-05, 0.00015435776402873377, 4.460540128918422e-05, 6.385276347030828e-05], 'topk_tokens': [':', 'THE', ' Mary', ' THE', ' Mary', ' the', '<|eot_id|>', 'IVE', '<|eot_id|>', ' Does', ' the', 'Answer', 'athroom', '<|end_header_id|>', ':', 'assistant', 'b', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.8967857360839845e-06, 0.0005873292684555054, 0.0004989902178446451, 0.00040468573570251465, 2.6397407054901123e-05]}}, 30: {'grad': {'score': [0.4195332734481148, 0.40159286647384895, 0.3275268308577999, 0.4017474350207254, 0.4195369908839096], 'topk_tokens': [' the', ' of', ' Times', ' its', ' account', 'b', ' Buchanan', ' the', ' of', ' B', 'B', ' Burb', ' account', 'deal', ' forb', 'b', 'b', ' B', 'b', ' B'], 'evidence_proportions': [0.30020751953124997, 0.4496002197265625, 0.3881327311197917, 0.44658470153808594, 0.5586729049682617]}, 'weight': {'score': [0.010631070188854052, 0.002479107514911349, 0.011294475486201624, 0.0024412928617631603, 0.004532580758318489], 'topk_tokens': [' the', ' the', ':', ' Sandra', '.', 'Question', ' the', '.\n\n', 'b', '<|eot_id|>', '<|eot_id|>', 'Answer', '?\n', 'assistant', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0022536456584930416, 0.019786357879638672, 0.010882357756296795, 0.020434856414794922, 0.0017668455839157104]}, 'saliency': {'score': [0.000820662664330524, 7.858392956385631e-05, 0.0007627577550949589, 7.544259189532143e-05, 8.544804137430073e-05], 'topk_tokens': [' Mary', '<|eot_id|>', ' Mary', ' Bench', '.', ' the', ' Sandra', ' the', 'Bridge', ' Mary', ' bathroom', ' bedroom', ' the', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.00017330050468444824, 0.0017715469002723694, 0.0008176763852437338, 0.001477666199207306, 2.6457011699676514e-05]}}, 31: {'grad': {'score': [0.381191514108492, 0.5231661677740399, 0.42937006296650054, 0.5236727821592136, 0.3163062514346323], 'topk_tokens': [' his', ' the', ' that', ' would', ' of', ' the', ' the', ' had', ' the', ' the', ' the', ' is', ' the', 'membership', ' was', 'If', ' August', ' the', ' the', ' he'], 'evidence_proportions': [0.38459243774414065, 0.4004000201821327, 0.3263160188992818, 0.43115663528442383, 0.39007997512817383]}, 'weight': {'score': [0.00221002749774767, 0.002300285282005661, 0.0024588531063449, 0.002300052235348576, 0.000809664343610222], 'topk_tokens': [' the', ' was', ' the', ',', ':', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', ':', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006763637065887451, 0.0035851001739501953, 0.0018686950206756592, 0.003916144371032715, 0.0015579164028167725]}, 'saliency': {'score': [8.182940275772758e-05, 3.0827148257346797e-05, 4.557063502650107e-05, 3.069336902522923e-05, 6.15325974829403e-06], 'topk_tokens': [' was', ' Market', ' milk', ' the', ' the', ',', ' the', '<|eot_id|>', ' Emily', 'CH', '?\n', '<|eot_id|>', 'Answer', '<|start_header_id|>', '<|begin_of_text|>', ':', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [4.129409790039062e-05, 0.00013453513383865356, 9.031593799591064e-05, 0.00013893097639083862, 9.961426258087158e-06]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 03:21:16.067 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:21:16.067 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-2_1-4-5-8-9.pkl | len: 10 |  size: 9.51 KB
Processing depth (1, 4, 5, 8, 9):   3%|▎         | 3/100 [00:57<31:02, 19.21s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 2, 4, 5, 8):   3%|▎         | 3/100 [01:05<31:02, 19.21s/it]2025-01-22 03:21:23.490 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:21:23.490 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:21:23.490 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the milk.
2025-01-22 03:21:23.497 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2474, 2478) -->  Mary got the milk
2025-01-22 03:21:23.497 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:21:23.512 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4893, 4899) --> . Mary journeyed to the
2025-01-22 03:21:23.512 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:21:23.528 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5955, 5959) -->  Mary left the milk
2025-01-22 03:21:23.529 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:21:23.555 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9665, 9669) -->  left the apple.
2025-01-22 03:21:23.555 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:21:23.565 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3517, 3522) -->  John moved to the garden
2025-01-22 03:21:23.565 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:21:23.586 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7070, 7076) --> . Sandra journeyed to the
2025-01-22 03:21:23.586 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:21:23.616 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11230, 11235) --> . Daniel picked up the
2025-01-22 03:21:23.617 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:21:23.638 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8096, 8100) -->  Daniel took the football
2025-01-22 03:21:23.638 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:21:23.666 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9591, 9596) --> . Sandra moved to the
2025-01-22 03:21:23.666 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:21:23.670 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1500, 1506) -->  tragedy. John went back to
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:21:25.636 | INFO     | test_jbb_retain:begin_test:632 - the bedroom<|eot_id|>
2025-01-22 03:21:25.636 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12209])
your chose emoji: ['😖', '🧔🏻\u200d♂️', '🤦🏻\u200d♂️', '🇧🇧', '\U0001fa77', '🥦', '🏻', '🎧', '🤹🏿\u200d♀', '👍🏼']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 250406.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.17s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.02s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.02it/s][A100%|██████████| 8/8 [00:05<00:00,  1.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.79it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.32it/s][A100%|██████████| 8/8 [00:00<00:00, 21.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.54it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.19it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.12it/s][A100%|██████████| 8/8 [00:00<00:00, 21.36it/s]
2025-01-22 03:21:34.569 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.20723425823709238, 0.28325619339903885, 0.18676314815398184, 0.28364604279131733, 0.2595178374537715], 'topk_tokens': [' Cl', 'ols', ' item', ' Arr', ' congreg', ' expedition', 'able', ' examined', ' exclaimed', 'called', ' STE', ' compromised', ' out', ' committee', ' turtle', ' EAR', ' absence', ' Do', ' compl', 'consider'], 'evidence_proportions': [0.28251800537109373, 0.21397972106933594, 0.18053690592447919, 0.20882797241210938, 0.14483642578125]}, 'weight': {'score': [0.042401870955591614, 0.0025762884569214945, 0.015214946000806747, 0.0024687225101103533, 0.0013499491744571263], 'topk_tokens': [' Mary', 'Bridge', 'Answer', ' Mary', ' the', ' office', ' garden', '<|eot_id|>', 'assistant', ' the', '<|start_header_id|>', ':', '<|eot_id|>', ' milk', '\n\n', 'b', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.022593212127685544, 0.04699230194091797, 0.015513489643732708, 0.14243316650390625, 0.002873539924621582]}, 'saliency': {'score': [0.004657212806784589, 4.5927135896729595e-05, 0.0014237434633316533, 3.369059398351168e-05, 3.143979443444146e-05], 'topk_tokens': [' milk', ':', '<|start_header_id|>', 'Answer', ' Mary', '<|eot_id|>', '\n\n', ' bathroom', '<|eot_id|>', '.', '<|begin_of_text|>', ' office', 'Bridge', ' Mary', 'b', ' Mary', ' garden', 'athroom', ' milk', ' bedroom'], 'evidence_proportions': [0.0011287033557891846, 0.006511673331260681, 0.0010528812805811565, 0.017203569412231445, 7.352977991104126e-05]}}, 25: {'grad': {'score': [0.5014283553413723, 0.5718259639596299, 0.4997190660045993, 0.5721429946254323, 0.3321491524025246], 'topk_tokens': [' in', ' for', ' in', ' of', ' for', ' Aw', ' for', ' for', ' a', 'itable', ' the', ' so', ' set', ' for', 'posit', ' at', ' at', ' for', ' for', ' of'], 'evidence_proportions': [0.6003173828125, 0.38422393798828125, 0.5465545654296875, 0.3797645568847656, 0.5489959716796875]}, 'weight': {'score': [0.022212500157563583, 0.0024901432870764284, 0.00635399260828572, 0.002442981456431693, 0.0015100958170714202], 'topk_tokens': [' Bench', ' Mary', '.', ' Mary', ' THE', '?\n', 'Answer', ' bedroom', ' the', 'b', '<|start_header_id|>', ' milk', '<|eot_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006392359733581543, 0.019165873527526855, 0.011913458506266277, 0.07908463478088379, 0.0036107301712036133]}, 'saliency': {'score': [0.0009091379849807075, 2.3508512149902002e-05, 0.00015239465621209915, 2.1504486130734655e-05, 3.7793208051610874e-05], 'topk_tokens': [' Mary', '.\n\n', '.', '?\n', ' Mary', ' to', ' bedroom', ' the', 'assistant', ' Bench', '<|eot_id|>', 'Answer', ' Geo', '<|eot_id|>', ' THE', 'athroom', ' milk', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00019437074661254883, 0.0007740408182144165, 0.00026792287826538086, 0.0037575066089630127, 5.114823579788208e-05]}}, 26: {'grad': {'score': [0.33548023389733356, 0.38475344995573, 0.4337433845766129, 0.3847217503337606, 0.4098600458215784], 'topk_tokens': ['ers', 'is', ' and', 'itter', ' week', ' Empire', ' bitter', ' medicine', 'ights', ' Press', 'agle', 'ub', ' Marshall', 'ers', ' and', ' Marshall', 'char', ' Becker', ' commander', ' Eagle'], 'evidence_proportions': [0.2872154235839844, 0.5174102783203125, 0.27740478515625, 0.3589324951171875, 0.2775421142578125]}, 'weight': {'score': [0.018569053515144016, 0.002458133738118236, 0.006927593100455499, 0.002416259713187816, 0.0011037091414133708], 'topk_tokens': [' milk', '.\n\n', ' the', ' discarded', ' bathroom', ' kitchen', ' the', '?\n', '<|eot_id|>', '<|eot_id|>', 'assistant', 'Answer', 'b', ' bedroom', '<|start_header_id|>', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.025058788061141965, 0.008919119834899902, 0.005599359671274821, 0.05466127395629883, 0.003469139337539673]}, 'saliency': {'score': [0.0005639169527136761, 4.239420382421289e-05, 0.000381687956471597, 4.054249058547901e-05, 3.614304242310701e-05], 'topk_tokens': [' the', ' Sandra', '<|eot_id|>', ' Mary', ' milk', '.', '?\n', 'Answer', ' the', 'Bridge', ' kitchen', 'athroom', ' garden', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b', ':'], 'evidence_proportions': [0.0003367424011230469, 0.0005473792552947998, 0.00016248722871144614, 0.0019389688968658447, 9.151548147201538e-05]}}, 27: {'grad': {'score': [0.3796461354131284, 0.3665270488300442, 0.34427728960590975, 0.36655896226519286, 0.32068600477995696], 'topk_tokens': ['started', 'event', 'UG', ' intended', 'ers', 'atter', ' three', ' sentinel', ' received', '\n', 'three', ' exchanged', ' started', ' assistance', ' designated', ' successful', ' Thanksgiving', ' short', ' step', ' accepted'], 'evidence_proportions': [0.4465545654296875, 0.3656463623046875, 0.26132774353027344, 0.481475830078125, 0.38565826416015625]}, 'weight': {'score': [0.030724551366723103, 0.0025336550291819217, 0.010239824171989195, 0.0024606757678568296, 0.0016683450451603643], 'topk_tokens': ['Mary', ' the', '<|eot_id|>', ' milk', '<|eot_id|>', ' garden', '?\n', 'Answer', ' THE', 'assistant', 'b', '.\n\n', ' bathroom', '\n\n', '<|start_header_id|>', ':', '<|end_header_id|>', ' bedroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0628549575805664, 0.022957414388656616, 0.010515610376993816, 0.05632734298706055, 0.0030393004417419434]}, 'saliency': {'score': [0.0017103837883990743, 4.1262293951338794e-05, 0.0006834903071003575, 3.64671909097269e-05, 6.300210952758789e-05], 'topk_tokens': [' dropped', ' kitchen', ' the', ' bathroom', ' Bridge', 'Bridge', ' garden', ' Mary', ' the', 'NEW', 'Mary', '<|start_header_id|>', ' the', '<|begin_of_text|>', ' THE', '<|end_header_id|>', 'b', 'athroom', '.\n\n', ' bedroom'], 'evidence_proportions': [0.0011317312717437744, 0.0020743906497955322, 0.0010781834522883098, 0.004599533975124359, 0.00012884289026260376]}}, 28: {'grad': {'score': [0.3492936673371688, 0.44667978346094006, 0.3552348229192918, 0.4470971773125306, 0.41749258394594546], 'topk_tokens': [' Cedar', ' its', 'S', 'in', '.', ' the', ' the', ' the', "'", ' the', 'nes', 'ot', ' the', 'nes', 'IO', ' the', 'nes', 'nes', '.', 'dent'], 'evidence_proportions': [0.36297607421875, 0.3399810791015625, 0.42302894592285156, 0.2847709655761719, 0.2954230308532715]}, 'weight': {'score': [0.014757718728936237, 0.002387178557215071, 0.00939600698409542, 0.0023459056418356603, 0.0007471784397407815], 'topk_tokens': [' the', '.\n\n', ' garden', ' discarded', ' the', ' bedroom', ' the', ' the', '?\n', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0026911497116088867, 0.006429195404052734, 0.010014921426773071, 0.05535602569580078, 0.004685342311859131]}, 'saliency': {'score': [0.00036965634511864704, 3.548225652105567e-05, 0.00028967184405173023, 3.420195702679711e-05, 2.1912985377841527e-05], 'topk_tokens': [' Mary', '<|eot_id|>', ' Floral', '.\n\n', ' the', ' was', '?\n', ' Bridge', ' garden', ' milk', ' the', 'athroom', ' bedroom', 'b', '<|start_header_id|>', '\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [5.363821983337403e-05, 0.00023873895406723022, 0.00033346811930338543, 0.0012360364198684692, 8.349865674972534e-05]}}, 29: {'grad': {'score': [0.3231854646102242, 0.29437720670178413, 0.24631835183789652, 0.2944452470512566, 0.29491255018446183], 'topk_tokens': ['oc', ' arms', ' The', ' Pioneer', ' In', ' The', '\n', ' THE', 'ION', 'cret', 'assistant', 're', 'init', 'b', ' The', 'UL', ' M', 'adv', ' The', ' ga'], 'evidence_proportions': [0.306414794921875, 0.39495849609375, 0.24983882904052734, 0.320584774017334, 0.3849964141845703]}, 'weight': {'score': [0.006912149812864221, 0.002460639827414649, 0.0031569532809718965, 0.002450443212286699, 0.0009264112622649582], 'topk_tokens': [' Where', ' discarded', '.', ' was', ' the', '<|eot_id|>', ' the', ' the', '.\n\n', '?\n', '<|eot_id|>', 'Answer', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '\n\n', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005684792995452881, 0.003824681043624878, 0.005586852629979451, 0.025696277618408203, 0.0011330246925354004]}, 'saliency': {'score': [0.000418245792388916, 3.5527389517308376e-05, 8.611429122186476e-05, 3.467439418757584e-05, 3.489907141085024e-05], 'topk_tokens': [' Mary', '<|eot_id|>', '.', ' milk', 'Answer', '      ', ' Does', '      ', 'IVE', '<|eot_id|>', 'athroom', ' was', '<|start_header_id|>', ':', '<|end_header_id|>', ' the', 'assistant', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [8.028745651245117e-06, 0.00024112313985824585, 0.00041746099789937335, 0.0014989078044891357, 2.8654932975769043e-05]}}, 30: {'grad': {'score': [0.3661401997441831, 0.3634664096326231, 0.3283740628150202, 0.36355082858136306, 0.34550363156530595], 'topk_tokens': ['�', ' S', ' soon', ' bouncing', ' Buchanan', ' of', 'b', '�', 'itter', ' account', 'deal', ' Burb', ' B', ' forb', 'B', 'b', 'b', ' B', 'b', ' B'], 'evidence_proportions': [0.39073486328125, 0.20946121215820312, 0.33142344156901044, 0.3829154968261719, 0.5273756980895996]}, 'weight': {'score': [0.01888168894726297, 0.0024819779388247807, 0.010942333167599093, 0.0024293817580971874, 0.0033387773566775853], 'topk_tokens': ['Question', ' office', ' Where', '.', ' garden', ' the', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' bedroom', 'b', 'assistant', 'Answer', '?\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.006684833765029907, 0.008333921432495117, 0.013288910190264383, 0.06810665130615234, 0.003839731216430664]}, 'saliency': {'score': [0.0012195524962052054, 4.443171920051662e-05, 0.0009188382856307491, 3.997914629128442e-05, 7.567582307038483e-05], 'topk_tokens': [' Sandra', ' Where', '.', ' Wide', 'Answer', ' the', ' the', ' office', ' Bridge', ' Mary', ' Bench', ':', ' bathroom', ' bedroom', ' garden', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0020698189735412595, 0.000588729977607727, 0.0005103250344594319, 0.0028154700994491577, 0.0002554655075073242]}}, 31: {'grad': {'score': [0.35557490716809814, 0.4351379183367202, 0.3588753515674222, 0.4354828836950626, 0.2670091854201423], 'topk_tokens': [' the', ' paper', 'membership', ' their', ' they', 'If', ' the', ' the', ' the', ' location', ' August', ' is', ' that', ' the', ' had', ' the', ' was', ' he', ' an', ' the'], 'evidence_proportions': [0.30486168861389157, 0.3759284019470215, 0.3215400328238805, 0.3548421859741211, 0.45039796829223633]}, 'weight': {'score': [0.0041108222111411715, 0.00228058491384167, 0.002429891978540728, 0.0022767418494524976, 0.0011830208478150544], 'topk_tokens': [' the', ' was', ',', 'Question', ':', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009776592254638673, 0.0029038935899734497, 0.0026074498891830444, 0.013317584991455078, 0.0022825002670288086]}, 'saliency': {'score': [7.610865261243737e-05, 3.0260187775980164e-05, 4.85585581871771e-05, 3.012679707241168e-05, 1.1749841548778392e-05], 'topk_tokens': [' Geo', ' left', '.\n\n', ' Mary', ' prior', '\n\n', ' bedroom', '<|eot_id|>', ' milk', ' the', '<|eot_id|>', '?\n', '<|begin_of_text|>', '<|start_header_id|>', 'Answer', ':', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [2.1082162857055663e-05, 7.390975952148438e-05, 4.528462886810303e-05, 0.00022327154874801636, 4.616379737854004e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:21:34.570 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:21:34.571 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-3_0-2-4-5-8.pkl | len: 10 |  size: 9.52 KB
Processing depth (0, 2, 4, 5, 8):   4%|▍         | 4/100 [01:16<30:17, 18.93s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 2, 3, 5, 9):   4%|▍         | 4/100 [01:23<30:17, 18.93s/it]2025-01-22 03:21:42.076 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 03:21:42.077 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 03:21:42.077 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Mary got the milk.
2025-01-22 03:21:42.084 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2507, 2511) -->  got the milk.
2025-01-22 03:21:42.084 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 03:21:42.095 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3756, 3762) --> . Mary journeyed to the
2025-01-22 03:21:42.095 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary left the milk.
2025-01-22 03:21:42.111 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5966, 5970) -->  Mary left the milk
2025-01-22 03:21:42.111 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Daniel left the apple.
2025-01-22 03:21:42.139 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10754, 10758) -->  Daniel left the apple
2025-01-22 03:21:42.139 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:21:42.149 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3500, 3505) --> . John moved to the
2025-01-22 03:21:42.149 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:21:42.169 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7081, 7087) --> . Sandra journeyed to the
2025-01-22 03:21:42.169 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 03:21:42.200 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (11234, 11239) --> . Daniel picked up the
2025-01-22 03:21:42.200 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel took the football.
2025-01-22 03:21:42.221 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8111, 8115) -->  Daniel took the football
2025-01-22 03:21:42.222 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:21:42.248 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9582, 9587) -->  order. Sandra moved to
2025-01-22 03:21:42.248 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John went back to the office.
2025-01-22 03:21:42.252 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (1554, 1560) -->  ranks. John went back to
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:21:44.221 | INFO     | test_jbb_retain:begin_test:632 - the kitchen<|eot_id|>
2025-01-22 03:21:44.221 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12210])
your chose emoji: ['👨🏽\u200d🦼\u200d➡', '🖼', '⛳', '🌥️', '🧜🏾\u200d♂', '🤼\u200d♀️', '🏘', '🈴', '💇🏻\u200d♂', '🎨']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 250406.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.33s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.62it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.46it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.24it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.96it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.60it/s][A100%|██████████| 8/8 [00:00<00:00, 17.65it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.83it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.37it/s][A100%|██████████| 8/8 [00:00<00:00, 17.36it/s]
2025-01-22 03:21:54.037 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2800194284190302, 0.2676876564435054, 0.3055824772004158, 0.267567714820107, 0.2854246312921697], 'topk_tokens': ['.\n\n', ',', ' first', 'ottle', 'called', 'ols', ' EAR', ' appearance', ' first', ' Arr', 'ab', ' comparison', '202', ' compromised', ' examined', ' absence', ' committee', ' compl', ' turtle', 'consider'], 'evidence_proportions': [0.2708526611328125, 0.2860569953918457, 0.2935384114583333, 0.313446044921875, 0.2317352294921875]}, 'weight': {'score': [0.051095264113467674, 0.0025755309027847744, 0.008526980876922607, 0.0024685773035542473, 0.0013296620412306351], 'topk_tokens': ['\n\n', ' Mary', 'Bridge', 'From', ' garden', 'Answer', '<|eot_id|>', ' the', ' milk', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', '\n\n', 'b', ' bedroom', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.115960693359375, 0.022062301635742188, 0.022921149929364525, 0.09012269973754883, 0.0022801756858825684]}, 'saliency': {'score': [0.0031369483989218006, 3.817436404811129e-05, 0.00021797707003931845, 3.185429770319622e-05, 5.088394338434393e-05], 'topk_tokens': [' PA', 'Mary', '\n\n', '<|start_header_id|>', ' milk', ' office', ':', 'Answer', '<|begin_of_text|>', '<|eot_id|>', 'From', '\n\n', '<|eot_id|>', ' Mary', 'b', ' bathroom', ' milk', ' garden', 'athroom', ' bedroom'], 'evidence_proportions': [0.00533549189567566, 0.0013793334364891052, 0.0007232228914896647, 0.00881022959947586, 9.369105100631714e-05]}}, 25: {'grad': {'score': [0.6271126788595448, 0.7716142914760399, 0.5961137279387443, 0.7723350789223624, 0.3822315216064453], 'topk_tokens': [' a', ' so', ' for', ' inverted', ' the', ' the', ' of', ' for', ' in', ' for', ' at', ' the', ' a', ' no', ' for', ' over', ' the', ' a', ' of', ' for'], 'evidence_proportions': [0.8088134765624999, 0.6100273132324219, 0.6821060180664062, 0.45844268798828125, 0.5032520294189453]}, 'weight': {'score': [0.020098788582760353, 0.002493250509605823, 0.004666260173243861, 0.0024544076216006143, 0.0011114310134540905], 'topk_tokens': [' boat', '.\n\n', ' discarded', ' Bench', ' Mary', ' the', ' milk', 'Answer', '?\n', ' bathroom', 'b', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.026080894470214847, 0.009325504302978516, 0.013581032554308575, 0.048882484436035156, 0.004387378692626953]}, 'saliency': {'score': [0.0007844308148259702, 2.7274029813038865e-05, 0.00011063775708598476, 2.562924968303157e-05, 2.7735124934803356e-05], 'topk_tokens': ['RE', ' Min', ' PA', ' Mary', ' Geo', ' top', '.\n\n', ' to', ':', ' THE', ' the', ' bathroom', 'Answer', ' boat', ' milk', ' Bench', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008837878704071045, 0.0005958080291748047, 0.0002755671739578247, 0.0022540315985679626, 0.0001425519585609436]}}, 26: {'grad': {'score': [0.2711378180462381, 0.3097813693016663, 0.338974122078188, 0.30978003912178326, 0.394017861106179], 'topk_tokens': [' Malta', 'rich', ' Marshall', 'k', '-in', 'agle', 'er', ' Commander', ' Empire', 'er', 'ub', ' Marshall', 'is', ' and', ' Marshall', 'ers', ' Becker', 'ers', ' Eagle', ' commander'], 'evidence_proportions': [0.23675537109375, 0.4051384925842285, 0.26807657877604163, 0.2753334045410156, 0.180511474609375]}, 'weight': {'score': [0.03669792802437492, 0.002454954469212734, 0.004235889642469345, 0.002385639773749317, 0.0010016094554554332], 'topk_tokens': ['Bridge', '.\n\n', ' discarded', ' the', ' kitchen', ' the', '<|eot_id|>', '?\n', '<|eot_id|>', ' bedroom', 'Answer', 'assistant', 'b', '<|start_header_id|>', ' bathroom', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.11364550590515136, 0.004806697368621826, 0.005058591564496358, 0.05013632774353027, 0.006425291299819946]}, 'saliency': {'score': [0.0009038150310516357, 3.8459449262219995e-05, 0.00012065998969539519, 3.661296557650694e-05, 3.380233591253107e-05], 'topk_tokens': [' the', '.', ' Mary', '?\n', ' the', '<|eot_id|>', ' kitchen', 'assistant', ' garden', 'Bridge', ' bedroom', 'Answer', '<|end_header_id|>', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>', 'athroom', '\n\n', 'b', ':'], 'evidence_proportions': [0.0033043861389160157, 0.00012034177780151367, 7.599592208862305e-05, 0.0007669031620025635, 6.521493196487427e-05]}}, 27: {'grad': {'score': [0.5134529860123344, 0.5382126355497217, 0.5275516510009766, 0.5382866517073309, 0.39067345532503994], 'topk_tokens': ['\n', ' started', '\n', ' other', 'event', '."', ' him', ' intended', 'roduced', 'ition', '\n', ' received', ' Thanksgiving', '!"', ' assistance', ' consultations', ' step', ' designated', ' short', ' accepted'], 'evidence_proportions': [0.6251283645629883, 0.5750885009765625, 0.40531667073567706, 0.6165084838867188, 0.3713722229003906]}, 'weight': {'score': [0.05170398302700208, 0.0025316155787466162, 0.005207553025214902, 0.0024317785434517405, 0.0013651110909201883], 'topk_tokens': [' THE', '<|eot_id|>', 'RE', ' the', 'Mary', '?\n', ' Mary', 'NEW', 'Answer', 'assistant', '.\n\n', 'b', '\n\n', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', ':', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.17471240758895876, 0.009654730558395386, 0.011054868499437969, 0.046645283699035645, 0.006025075912475586]}, 'saliency': {'score': [0.00214111416236214, 4.40471370282849e-05, 0.00036831728873714325, 3.925357536320938e-05, 7.124109701676802e-05], 'topk_tokens': ['Bridge', ' Bridge', ' THE', ' kitchen', ' the', '<|start_header_id|>', 'RE', 'From', ' Mary', 'Mary', ' the', ' bathroom', ' bedroom', ':', '<|begin_of_text|>', 'NEW', 'b', '<|end_header_id|>', 'athroom', '.\n\n'], 'evidence_proportions': [0.003723746538162232, 0.001255325973033905, 0.0013822466135025024, 0.004112251102924347, 0.00021577626466751099]}}, 28: {'grad': {'score': [0.48393655859905743, 0.5553114948914579, 0.4455466654992873, 0.5557263590452437, 0.4340958681973544], 'topk_tokens': ['S', ' the', ' the', '\n', '.', ' Cedar', ' its', 'nes', 'IO', 'nes', ' the', '.', ' ', ' the', ' the', 'nes', "'", ' the', 'nes', 'dent'], 'evidence_proportions': [0.462530517578125, 0.49129486083984375, 0.6085195541381836, 0.3568687438964844, 0.4435291290283203]}, 'weight': {'score': [0.017010077186252758, 0.0023813664664275756, 0.006869016155119865, 0.0023422532591814665, 0.0006047476421702991], 'topk_tokens': [' bathroom', ' the', '.\n\n', ' garden', '?\n', ' the', ' discarded', ' the', '<|eot_id|>', ' the', 'Answer', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.011179375648498534, 0.004165172576904297, 0.012244721253712973, 0.056349754333496094, 0.0049517154693603516]}, 'saliency': {'score': [0.0005104878674382749, 3.532021273893542e-05, 0.0001474609298090781, 3.413547564811716e-05, 1.6095963391390715e-05], 'topk_tokens': [' Floral', ' kitchen', ' the', ' Mary', '<|eot_id|>', '?\n', ' the', ' garden', '<|start_header_id|>', ' the', ' bedroom', ' Bridge', 'b', ' milk', '\n\n', 'athroom', 'assistant', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.740787506103516e-05, 0.00025037676095962524, 0.0004819134871164958, 0.0018009915947914124, 0.00011430680751800537]}}, 29: {'grad': {'score': [0.485392777816109, 0.37428468975809587, 0.3416328430175781, 0.3741577649471428, 0.35302371978759767], 'topk_tokens': [' adher', 'UL', 'vic', ' In', ' PA', 'cret', ' book', ' Pioneer', 'oc', ' B', 'assistant', 're', ' Pioneer', 'init', '\n', 'Spring', 'adv', ' M', 'b', ' ga'], 'evidence_proportions': [0.5791015625, 0.55853271484375, 0.3611214955647786, 0.44202232360839844, 0.5248942375183105]}, 'weight': {'score': [0.008120303568632706, 0.0024505780934604214, 0.002746310926252796, 0.0024390992379833653, 0.0009728876027193937], 'topk_tokens': [' Where', '      ', ' Does', ' the', '<|eot_id|>', ' the', '?\n', ' the', '.', '.\n\n', '<|eot_id|>', 'Answer', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013567328453063963, 0.019176781177520752, 0.0037873884042104087, 0.01876211166381836, 0.0013758540153503418]}, 'saliency': {'score': [0.00049398774686067, 4.898050605816405e-05, 0.00010630969078310074, 4.799256533401481e-05, 3.4819407896562054e-05], 'topk_tokens': [' milk', ' was', ' Mary', 'IVE', ' Does', ' in', '<|start_header_id|>', 'NEW', '<|eot_id|>', '.', '      ', '      ', '<|end_header_id|>', ' the', 'assistant', 'athroom', ':', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [3.477334976196289e-05, 0.0013920068740844727, 0.00021263460318247476, 0.0010435059666633606, 4.2498111724853516e-05]}}, 30: {'grad': {'score': [0.34935511713442596, 0.3450071403086747, 0.3032345925607989, 0.3451054169361269, 0.2745645696466619], 'topk_tokens': [' the', ' Times', 'RI', ' of', ' soon', ' bouncing', ' B', 'itter', ' account', 'b', 'deal', ' Burb', ' forb', ' B', 'B', 'b', 'b', ' B', ' B', 'b'], 'evidence_proportions': [0.4383575439453125, 0.38846588134765625, 0.21286773681640625, 0.2932472229003906, 0.45983028411865234]}, 'weight': {'score': [0.020093437122262043, 0.0024759784861843955, 0.006979189572795745, 0.0024311720800395857, 0.004140674526041204], 'topk_tokens': ['.', ' bedroom', ' the', ' Where', 'Question', ' garden', '<|eot_id|>', '<|eot_id|>', ' the', '.\n\n', 'Answer', '?\n', 'assistant', 'b', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.01087491512298584, 0.014135599136352539, 0.0128508855899175, 0.06400775909423828, 0.004523932933807373]}, 'saliency': {'score': [0.0015239495298136835, 6.821391175071038e-05, 0.00026263729218513733, 6.496454551920156e-05, 7.480762221596457e-05], 'topk_tokens': ['assistant', ' Mary', '.', ' Wide', ' the', ' Bench', ' Mary', ' the', ' bedroom', ' kitchen', ' office', ' Bridge', ' garden', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ':', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.004384541511535644, 0.00048814713954925537, 0.0005255192518234253, 0.0018662288784980774, 0.00013937801122665405]}}, 31: {'grad': {'score': [0.30662304422129755, 0.3617721239960009, 0.28077622382871564, 0.3620829473975968, 0.1754355333068154], 'topk_tokens': [' their', ' had', 'user', '7', ' that', ' is', ' paper', ' the', ' an', 'membership', ' the', ' was', ' the', ' having', ' August', 'If', ' location', ' the', ' he', ' the'], 'evidence_proportions': [0.282496976852417, 0.33328938484191895, 0.25624946753184, 0.290496826171875, 0.40180087089538574]}, 'weight': {'score': [0.0032698421374611234, 0.0022912741898221444, 0.0019144963833593552, 0.002290383742351518, 0.0012356590140949596], 'topk_tokens': [' the', ' was', ',', 'Question', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', 'b', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0014794468879699707, 0.0013828128576278687, 0.002777909239133199, 0.008494138717651367, 0.002908468246459961]}, 'saliency': {'score': [6.956898647805919e-05, 2.4881298395805136e-05, 4.5173591183077904e-05, 2.474503078314807e-05, 1.0026043111627753e-05], 'topk_tokens': [' a', ' Mary', ' was', ' discarded', ' the', ' dropped', ' prior', ' the', ' bedroom', ' milk', '<|begin_of_text|>', '<|eot_id|>', '?\n', '<|start_header_id|>', 'Answer', ':', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [2.1564960479736325e-05, 2.419203519821167e-05, 9.037554264068604e-05, 0.00017492473125457764, 3.838539123535156e-05]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 03:21:54.039 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:21:54.039 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-4_0-2-3-5-9.pkl | len: 10 |  size: 9.48 KB
Processing depth (0, 2, 3, 5, 9):   5%|▌         | 5/100 [01:35<30:16, 19.12s/it]Processing depth (0, 2, 3, 5, 9):   5%|▌         | 5/100 [01:36<30:24, 19.21s/it]
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:72 - Selected idx: 16
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:76 - Needle: [' John went back to the office.', ' John journeyed to the bedroom.', ' Daniel grabbed the apple.', ' John moved to the garden.', ' Sandra journeyed to the office.', ' Sandra moved to the kitchen.', ' Sandra went back to the hallway.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel grabbed the apple.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-22 03:21:54.439 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (2, 3, 4, 7):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 03:22:01.727 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 03:22:01.734 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2454, 2458) -->  Daniel grabbed the apple
2025-01-22 03:22:01.734 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 03:22:01.745 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3787, 3792) --> . Daniel travelled to the
2025-01-22 03:22:01.745 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:22:01.759 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4870, 4876) --> . Daniel went back to the
2025-01-22 03:22:01.759 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:22:01.782 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8404, 8409) --> . Mary travelled to the
2025-01-22 03:22:01.783 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:22:01.788 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2015, 2021) --> . John went back to the
2025-01-22 03:22:01.789 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 03:22:01.818 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10543, 10549) --> . John journeyed to the
2025-01-22 03:22:01.819 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:22:01.841 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8225, 8230) --> . John moved to the
2025-01-22 03:22:01.842 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:22:01.861 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7072, 7078) --> . Sandra journeyed to the
2025-01-22 03:22:01.862 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:22:01.881 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7194, 7199) --> . Sandra moved to the
2025-01-22 03:22:01.881 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 03:22:01.893 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4345, 4351) --> . Sandra went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:22:03.944 | INFO     | test_jbb_retain:begin_test:632 - Daniel's hand<|eot_id|>
2025-01-22 03:22:03.944 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12228])
your chose emoji: ['👩🏿\u200d❤\u200d💋\u200d👩🏼', '㊙', '🤽🏽\u200d♀️', '👩🏿\u200d⚖️', '🚶🏿\u200d➡', '👱🏿\u200d♂', '👂🏻', '🧜🏼\u200d♀️', '\U0001faf2', '🇰🇪']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 246723.76it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.30s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.98it/s][A100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.95it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.39it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.64it/s][A100%|██████████| 8/8 [00:00<00:00, 20.68it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.49it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.21it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.55it/s][A100%|██████████| 8/8 [00:00<00:00, 20.52it/s]
2025-01-22 03:22:13.386 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.43706474304199217, 0.3993819854838627, 0.42090292537913604, 0.3992600041167278, 0.542947470423687], 'topk_tokens': ['est', 'om', ' hand', ' doctors', ' day', 'hand', ' lively', 'hom', ' died', 'Frank', ' fact', ' day', 'ck', ' Hor', ' hand', 'hand', ' How', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.52227783203125, 0.4380096435546875, 0.38264973958333337, 0.43324737548828124]}, 'weight': {'score': [0.011643439531326294, 0.002557530971162503, 0.00452694296836853, 0.002537109013446458, 0.0019302727228187654], 'topk_tokens': ['<|eot_id|>', '<|end_header_id|>', ' \n', '.', '\n\n', ' hallway', ' Daniel', ' Father', '<|start_header_id|>', 'Answer', '<|start_header_id|>', ':', 'assistant', 'hall', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'way', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.039238929748535156, 0.0047317445278167725, 0.007077435652414957, 0.001957947015762329]}, 'saliency': {'score': [0.008220948278903961, 7.230188633919775e-05, 0.0003060382955214556, 5.826559121203244e-05, 0.00010282339819942612], 'topk_tokens': [' Daniel', '<|eot_id|>', '\n\n', ' THE', ' garden', ' Harper', '<|start_header_id|>', '<|eot_id|>', ' Dul', '\n\n', '<|end_header_id|>', ':', ' hallway', '.', ' Father', ' Dan', '<|begin_of_text|>', 'way', 'hall', ' Daniel'], 'evidence_proportions': [0.039111390709877014, 0.0007755637168884278, 0.0006034125884373982, 9.502172470092772e-05]}}, 25: {'grad': {'score': [1.1059165954589845, 0.8821927169564018, 1.0713269850786995, 0.8812971726773339, 0.7437662331454725], 'topk_tokens': [' daily', ' fer', ' daily', ' morning', ' obtain', ' post', ' daily', ' daily', 'po', ' daily', ' daily', ' daily', ' daily', ' double', ' daily', ' daily', ' double', ' morning', ' post', ' daily'], 'evidence_proportions': [1.2225341796875, 0.9849365234375, 1.1734441121419272, 1.0525695800781252]}, 'weight': {'score': [0.01900182068347931, 0.0025626145831387157, 0.0020383158150841206, 0.002537078082860079, 0.0021208377487688178], 'topk_tokens': [' Dan', '?', ' Daniel', '.', '<|start_header_id|>', 'hall', '<|end_header_id|>', ' Daniel', '<|eot_id|>', ' \n', ':', 'Answer', '<|start_header_id|>', 'way', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.04878532886505127, 0.01149412989616394, 0.018941566348075867, 0.0027550101280212403]}, 'saliency': {'score': [0.0019023433327674867, 6.224040446711448e-05, 9.408856139463537e-05, 5.912921978274697e-05, 8.703606674470098e-05], 'topk_tokens': ['<|start_header_id|>', ' Geo', ' Married', ' Bank', ' Paul', ' garden', ' THE', 'Answer', ' Daniel', ' Marshall', ' East', 'assistant', 'DW', ' Daniel', '<|eot_id|>', ' Dan', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.005429759621620178, 0.000926285982131958, 0.0014773060878117879, 0.0005665123462677002]}}, 26: {'grad': {'score': [0.5462989807128906, 0.4788129112724583, 0.5057902616613051, 0.4786267446220494, 0.4640759847250329], 'topk_tokens': ['an', 'ed', ' Democrats', ' to', '186', ' of', ' they', 'ot', '186', ' city', ' the', ' be', ',', ' of', ' and', ' under', ' Phil', ' not', ' it', '3'], 'evidence_proportions': [0.6145515441894531, 0.5509765625, 0.604400634765625, 0.41729736328125]}, 'weight': {'score': [0.007775084674358368, 0.002542633269971745, 0.003569631015553194, 0.0025311717481324167, 0.0021778923201273724], 'topk_tokens': ['<|eot_id|>', ' the', '<|end_header_id|>', ' Father', '.\n\n', ' garden', '?', '<|start_header_id|>', '\n\n', 'Answer', ' \n', 'assistant', 'hall', '<|start_header_id|>', 'way', '<|eot_id|>', ':', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.014018207788467407, 0.0063435077667236325, 0.007718116044998169, 0.0042805254459381105]}, 'saliency': {'score': [0.0012333855032920838, 5.982335595661207e-05, 0.00046011191957137166, 5.6778184394682924e-05, 7.225770548165562e-05], 'topk_tokens': [' Daniel', ' John', '\n\n', ' hallway', ' \n', ' formally', ' Sandra', 'Answer', 'assistant', '<|eot_id|>', ' Daniel', ' Dan', '<|eot_id|>', 'way', '<|start_header_id|>', ' Father', ':', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0034751519560813904, 0.0008696794509887696, 0.000684907039006551, 0.0004618525505065918]}}, 27: {'grad': {'score': [0.30594863891601565, 0.3940774610237307, 0.3371150493621826, 0.39438125493344955, 0.36662334993661166], 'topk_tokens': ['      ', '.', '      ', ' sco', '      ', 'hand', '      ', '--', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '--', '--'], 'evidence_proportions': [0.2603607177734375, 0.307586669921875, 0.34125264485677087, 0.2984161376953125]}, 'weight': {'score': [0.016917808353900908, 0.0025371086489256616, 0.003471069476183723, 0.00251088144499807, 0.0030451133308640444], 'topk_tokens': [' Dan', ' apple', '?', ' \n', ' Daniel', '.\n\n', ' garden', ' hallway', '\n\n', 'Answer', '<|eot_id|>', 'assistant', ' THE', '<|start_header_id|>', '<|eot_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0521543025970459, 0.010354697704315186, 0.00994749367237091, 0.0036561012268066405]}, 'saliency': {'score': [0.004576881229877472, 6.670830402026512e-05, 0.000178070629344267, 5.898967237222701e-05, 0.0001886368515979813], 'topk_tokens': [' Geo', ' Father', '<|start_header_id|>', ' garden', ' Judge', 'Bridge', 'NEW', 'Lu', ' hallway', '<|begin_of_text|>', ' Daniel', ':', '<|end_header_id|>', ' Daniel', 'DW', ' Dan', '.\n\n', 'way', ' Daniel', 'hall'], 'evidence_proportions': [0.016375921666622162, 0.002154034376144409, 0.0024186819791793823, 0.00015033483505249022]}}, 28: {'grad': {'score': [0.36388168334960935, 0.38352362369440357, 0.29631715662339153, 0.3837993778774791, 0.40639100591820404], 'topk_tokens': ['hall', ' Times', 'dr', ' house', ' Hill', ' locate', ' headlines', ' hallway', ' hall', ' houses', 'line', 'burg', ' hallway', 'urred', 'ching', ' hard', ' host', ' hall', ' hall', ' hall'], 'evidence_proportions': [0.50018310546875, 0.32113037109375, 0.42411295572916663, 0.22531433105468748]}, 'weight': {'score': [0.0017380908131599427, 0.002486528822800854, 0.00224724762579974, 0.0024884261966113866, 0.0011896566454186496], 'topk_tokens': [':', '.', 'Question', '.\n\n', ' garden', ' apple', ' before', '?', 'assistant', 'hall', 'Answer', '<|eot_id|>', ' \n', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0010681748390197754, 0.0006227731704711914, 0.0017667611440022786, 0.0033549368381500244]}, 'saliency': {'score': [9.347796440124511e-05, 6.055666706215262e-05, 5.1157439456266514e-05, 6.0528839829814464e-05, 3.511108547808176e-05], 'topk_tokens': ['.\n\n', ' Queen', ' Dr', ' Far', ' Dul', 'Right', 'Answer', ' Stephen', '<|eot_id|>', '<|eot_id|>', ' \n', ' apple', '<|end_header_id|>', '?', 'way', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'hall', ':'], 'evidence_proportions': [7.987767457962036e-05, 6.675124168395997e-05, 0.00015296538670857746, 5.970001220703125e-05]}}, 29: {'grad': {'score': [0.32036895751953126, 0.3416683773077886, 0.4160346984863281, 0.3414957184817801, 0.40172243692788734], 'topk_tokens': [' Far', ' Online', 'in', '\n', ' in', ' no', ' hardly', ' was', ' pur', 'A', '5', '\n', '25', ' and', '.', ' no', ' pur', ' very', ' George', '.'], 'evidence_proportions': [0.13909149169921875, 0.295648193359375, 0.3817621866861979, 0.4164398193359375]}, 'weight': {'score': [0.0013345032930374146, 0.0025083146396637276, 0.0014474725022035487, 0.002513204584609623, 0.0018192292696022126], 'topk_tokens': [' Does', '<|start_header_id|>', 'Question', ' the', ' before', ' apple', '.\n\n', 'hall', '?', 'Answer', ' \n', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'way', '\n\n', ':', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011193454265594482, 0.00036369562149047847, 0.0021088520685831704, 0.0015482187271118166]}, 'saliency': {'score': [7.431507110595703e-05, 4.572938180751456e-05, 5.6325512773850386e-05, 4.5652845531024105e-05, 0.00010095087878675346], 'topk_tokens': [' Paul', '"The', 'DW', 'IVE', ' apple', 'Does', 'Answer', ' the', '<|start_header_id|>', '?', ' \n', 'way', ' Does', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [4.3235719203948975e-05, 1.4650821685791016e-05, 0.00016955037911732992, 4.456043243408203e-05]}}, 30: {'grad': {'score': [0.40327606201171873, 0.45876207149737347, 0.31859493255615234, 0.45924457153134857, 0.5316136601459549], 'topk_tokens': [' account', ' Republicans', ' account', '      ', ' Paul', '2', 'ar', ' d', ' papers', ' People', ' boat', ' Press', ' OCC', ' M', ' Europe', 'AM', '186', ' an', ' Star', ' Loan'], 'evidence_proportions': [0.5772552490234375, 0.292333984375, 0.5563710530598959, 0.19132080078125]}, 'weight': {'score': [0.005192069709300995, 0.0024943309250919634, 0.005765541511423448, 0.002480766341399802, 0.005182180778089776], 'topk_tokens': [' apple', '<|eot_id|>', 'Question', '.\n\n', '<|end_header_id|>', ' garden', '<|start_header_id|>', 'hall', '?', '\n\n', 'assistant', 'Answer', '<|start_header_id|>', ' \n', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004434108734130859, 0.004088884592056275, 0.005624214808146159, 0.006383049488067626]}, 'saliency': {'score': [0.0003486230969429016, 6.0906585459131786e-05, 0.00032080359318677117, 5.97083569551969e-05, 0.00034299959619361236], 'topk_tokens': [' apple', ' Father', ' Miles', '<|start_header_id|>', 'IR', ' Emily', ' Gov', '<|end_header_id|>', '?', 'Gov', 'British', ':', 'way', 'assistant', '<|eot_id|>', ' Dan', ' \n', '<|start_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0005434751510620117, 0.00021576285362243653, 0.00022642811139424643, 0.0004722356796264648]}}, 31: {'grad': {'score': [0.5860671997070312, 0.7420495708266904, 0.5767122156479779, 0.742767409169342, 0.37013598522508], 'topk_tokens': [',', ' w', ' we', ' the', ' evening', ' W', ' would', ' two', ' F', ' draft', ' the', 'F', ' able', ' ever', ' the', ' these', ' had', ' the', ' Independent', ' United'], 'evidence_proportions': [0.57086181640625, 0.6383056640625, 0.610870361328125, 0.5162292480468751]}, 'weight': {'score': [0.001953071355819702, 0.002297151216393587, 0.002293695421779857, 0.0022977259962431674, 0.0012232474533908338], 'topk_tokens': [' the', ':', 'Question', ' before', ' Where', '.\n\n', ' apple', '<|eot_id|>', '?', '<|start_header_id|>', 'Answer', ' \n', '<|eot_id|>', 'assistant', '\n\n', ':', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0010062381625175476, 0.0013692915439605714, 0.0011480202277501423, 0.0042603790760040285]}, 'saliency': {'score': [0.00010468810796737671, 6.543606088417021e-05, 8.399258641635671e-05, 6.531977913909684e-05, 4.3538320495421627e-05], 'topk_tokens': ['Question', 'Field', ' McC', '.\n\n', ' Where', ' \n', 'CH', ' apple', ' the', 'Answer', '?', ':', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', 'hall', 'way'], 'evidence_proportions': [0.0001313239336013794, 0.00010196566581726073, 3.58124574025472e-05, 0.00016875267028808593]}}, 'pred_res': "Daniel's hand<|eot_id|>", 'score': 0}
2025-01-22 03:22:13.387 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:22:13.387 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-0_2-3-4-7.pkl | len: 10 |  size: 9.21 KB
Processing depth (2, 3, 4, 7):   1%|          | 1/100 [00:18<31:07, 18.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.35it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (0, 1, 4, 9):   1%|          | 1/100 [00:25<31:07, 18.86s/it]2025-01-22 03:22:20.565 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 03:22:20.597 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 03:22:20.602 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1535, 1540) --> . Daniel travelled to the
2025-01-22 03:22:20.602 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:22:20.616 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4934, 4940) --> . Daniel went back to the
2025-01-22 03:22:20.616 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:22:20.646 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10763, 10768) --> . Mary travelled to the
2025-01-22 03:22:20.646 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:22:20.652 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2059, 2065) --> . John went back to the
2025-01-22 03:22:20.652 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 03:22:20.683 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10518, 10524) --> . John journeyed to the
2025-01-22 03:22:20.683 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:22:20.706 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8223, 8228) -->  John moved to the garden
2025-01-22 03:22:20.706 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:22:20.726 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7092, 7098) -->  papers. Sandra journeyed to
2025-01-22 03:22:20.727 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:22:20.747 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7227, 7232) -->  Sandra moved to the kitchen
2025-01-22 03:22:20.747 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 03:22:20.760 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4436, 4442) --> . Sandra went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:22:22.884 | INFO     | test_jbb_retain:begin_test:632 - Daniel's hallway.<|eot_id|>
2025-01-22 03:22:22.884 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12211])
your chose emoji: ['🧫', '👩🏽\u200d❤\u200d💋\u200d👩🏼', '🗺', '🏋', '🥨', '🧜🏾\u200d♀️', '🏃🏻\u200d♂', '🖍️', '🇹🇰', '🕵🏼\u200d♀']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 190650.18it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.00s/it][A
 50%|█████     | 4/8 [00:05<00:03,  1.02it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.71it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.59it/s][A100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 22.86it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 23.23it/s][A100%|██████████| 8/8 [00:00<00:00, 23.57it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.38it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.99it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.27it/s][A100%|██████████| 8/8 [00:00<00:00, 17.26it/s]
2025-01-22 03:22:32.903 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6746225357055664, 0.5084070605772577, 0.5428033716538373, 0.5080922856530011, 0.7743301391601562], 'topk_tokens': [' upper', 'Frank', ' type', ' type', ' gold', 'hand', '�', ' high', ' type', 'hand', ' hand', ' Project', ' Wide', 'Ind', ' type', ' hand', ' hand', ' Wide', ' hand', ' Wild'], 'evidence_proportions': [0.6145263671875, 0.674725850423177, 0.7345947265624999]}, 'weight': {'score': [0.00692969374358654, 0.0025530146443748193, 0.00663799573393429, 0.002535839683619113, 0.0010643673665595777], 'topk_tokens': [' Bench', ' Bridge', 'Answer', '\n\n', ' Bridge', '\n\n', 'Bridge', ':', '<|start_header_id|>', 'Daniel', ' hallway', 'assistant', '<|eot_id|>', ' hallway', '\n\n', 'hall', '<|eot_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.011955326795578003, 0.006730258464813232, 0.002143383026123047]}, 'saliency': {'score': [0.00034972280263900757, 4.156646340846313e-05, 0.00037862448131336887, 4.0219005825722624e-05, 3.0514417272625547e-05], 'topk_tokens': ['***', '\n\n', '\n\n', 'E', '<|end_header_id|>', 'Answer', ' office', 'way', 'Bridge', ' garden', ' PA', '<|eot_id|>', ' Bench', 'assistant', ' bedroom', ' hallway', 'Daniel', ' hallway', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00028158426284790043, 0.0006422201792399089, 6.68644905090332e-05]}}, 25: {'grad': {'score': [0.6748801469802856, 1.086010578281071, 0.6517193177167107, 1.087765263397809, 0.760002851486206], 'topk_tokens': [' foreign', ' disposed', ' returns', ' which', ' cause', 'ment', ' which', ' number', ' South', ' of', ' post', ' there', ' men', '186', 'which', 'ad', '10', ' not', ' post', ' success'], 'evidence_proportions': [0.8004608154296875, 0.528864860534668, 0.724517822265625]}, 'weight': {'score': [0.005442464724183083, 0.0025532427953779747, 0.003208390053580789, 0.0025476112138554676, 0.0012080091418641987], 'topk_tokens': ['<|end_header_id|>', '\n\n\n', ' Daniel', '.', '<|start_header_id|>', ' the', 'Answer', '.\n\n', 'Daniel', ' \n', '<|start_header_id|>', 'hall', ':', 'assistant', '<|eot_id|>', 'way', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.010768628120422364, 0.0038336167732874555, 0.0020469188690185546]}, 'saliency': {'score': [0.0006713196635246277, 5.636807205986621e-05, 6.735061897951014e-05, 5.552849362689153e-05, 5.108976002895471e-05], 'topk_tokens': [' apple', 'RE', ' \n', '<|start_header_id|>', '      ', ' garden', 'UL', ' corner', ' Dan', 'way', ' PA', 'hall', ' Daniel', 'assistant', '<|eot_id|>', '<|eot_id|>', 'Daniel', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0016835331916809082, 0.00021855533123016357, 0.0002024233341217041]}}, 26: {'grad': {'score': [0.4207496643066406, 0.4722931173051928, 0.46718642290900736, 0.4723751892270481, 0.47852568192915484], 'topk_tokens': [' absolutely', 'ian', ' proprietor', '3', 'cert', ' Marshall', ' accidentally', ' prepared', ' Cl', 'could', 'asca', ' City', 'istributed', 'outs', 'ucci', ' not', ' and', ' not', ' it', 'hall'], 'evidence_proportions': [0.49969482421875, 0.40294392903645837, 0.36317138671875004]}, 'weight': {'score': [0.004098331555724144, 0.002485546988435242, 0.0053949995952494, 0.0024752932937864174, 0.0014956588997985377], 'topk_tokens': [' garden', ' garden', '?', ' Bridge', ' hallway', '.\n\n', ' hallway', 'Answer', ' \n', 'assistant', '\n\n', ' the', '<|eot_id|>', '<|start_header_id|>', 'hall', '<|eot_id|>', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.003051912784576416, 0.004738415280977884, 0.004376649856567383]}, 'saliency': {'score': [0.00025046616792678833, 8.45526509927575e-05, 0.00019163156256956214, 8.403511570300447e-05, 5.519661036404696e-05], 'topk_tokens': ['.\n\n', 'Answer', 'Daniel', '<|eot_id|>', 'assistant', ' \n', ' Bridge', 'Bridge', '<|eot_id|>', ' hallway', ' Bridge', 'way', ' hallway', ' the', '<|start_header_id|>', '\n\n', ':', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00016300678253173826, 0.00022326409816741943, 0.0003705680370330811]}}, 27: {'grad': {'score': [0.6536388397216797, 0.669398593730811, 0.5737997223349178, 0.6696865350076612, 0.4065610134240353], 'topk_tokens': ['.', ' *\n\n', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ':\n', '.', '.'], 'evidence_proportions': [0.66815185546875, 0.82281494140625, 0.436114501953125]}, 'weight': {'score': [0.004533668980002403, 0.0025436357017505845, 0.00500473117127138, 0.002534139008358959, 0.0020678251078634553], 'topk_tokens': [' PA', 'Daniel', ' garden', ' \n', 'RE', 'NEW', 'Answer', '\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' hallway', ' hallway', 'hall', ':', '<|start_header_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0066753089427948, 0.004251460234324138, 0.0027306795120239256]}, 'saliency': {'score': [0.0006039291620254517, 6.691524912960814e-05, 0.00026476032593671014, 6.565588089401335e-05, 0.00010378387841311368], 'topk_tokens': [' lounge', ':', 'Answer', '***', ' Daniel', ' hallway', ' ST', 'assistant', ' garden', '<|start_header_id|>', 'UL', 'Daniel', ' PA', 'RE', '<|begin_of_text|>', 'NEW', '<|end_header_id|>', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.0013434350490570067, 0.000407487154006958, 0.00010015368461608886]}}, 28: {'grad': {'score': [0.4247856140136719, 0.5645758349473453, 0.4404637953814338, 0.5651066186435126, 0.45297119834206323], 'topk_tokens': [' and', 'land', ' houses', 'plant', 'ed', ',', ' lodge', 'ivery', ' house', '\n\n\n\n', '.,', 'burg', ' hall', ' line', '***', '\n', ' hall', 'arp', '\n\n\n\n\n\n\n', 'ching'], 'evidence_proportions': [0.49036407470703125, 0.39051055908203125, 0.40033721923828125]}, 'weight': {'score': [0.005888232961297035, 0.002435159737883837, 0.00801300914848552, 0.002415026882611306, 0.001120935335303798], 'topk_tokens': [' apple', ' hallway', ' Bridge', 'Answer', ' \n', '?', '.\n\n', ' garden', ' before', 'assistant', ' the', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', '\n\n', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.001801961660385132, 0.007282326618830362, 0.008301591873168946]}, 'saliency': {'score': [0.00022060982882976532, 6.92216814569586e-05, 0.00016724274438970229, 6.874857010397618e-05, 1.7351724884726785e-05], 'topk_tokens': ['ot', '<|eot_id|>', ' \n', ' before', ' garden', ' was', '?', '<|eot_id|>', ' apple', 'Bridge', '<|end_header_id|>', ' Bridge', ' Bridge', ' the', '<|start_header_id|>', '<|begin_of_text|>', ':', 'way', '\n\n', 'hall'], 'evidence_proportions': [0.0002183496952056885, 0.0002761979897816976, 0.00015616416931152344]}}, 29: {'grad': {'score': [0.29039740562438965, 0.2705130994817678, 0.324828540577608, 0.27033512563307166, 0.3062808441393303], 'topk_tokens': [' composing', ' no', 'ible', '\n', '\n', '.', 'hall', ' was', '\n', '\n', ' spr', '5', '\n', ' and', ' of', ' book', ' very', ' George', '.', 'A'], 'evidence_proportions': [0.20179595947265627, 0.23160362243652344, 0.4495513916015625]}, 'weight': {'score': [0.0022048261016607285, 0.002483017338811619, 0.0028605233220493093, 0.0024823280800451222, 0.0011558397249741988], 'topk_tokens': ['<|start_header_id|>', 'ot', '.', ' before', ' \n', '?', ' Does', ' the', 'Answer', '.\n\n', 'hall', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|eot_id|>', 'way', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0016589164733886719, 0.003067607680956523, 0.001715397834777832]}, 'saliency': {'score': [0.00017601996660232544, 4.260660691602503e-05, 9.585829342112822e-05, 4.228227519157959e-05, 4.1173263029618696e-05], 'topk_tokens': ['.', ':', 'ot', ',', 'UL', ' was', '***', ' in', '<|eot_id|>', 'NEW', '<|eot_id|>', 'way', ' before', '<|end_header_id|>', ' Does', '.', 'hall', '<|begin_of_text|>', '\n\n', '<|start_header_id|>'], 'evidence_proportions': [0.00019587278366088866, 0.00026314953962961834, 5.161166191101074e-05]}}, 30: {'grad': {'score': [0.2612428665161133, 0.31872629053525925, 0.2648593678193934, 0.31895246697036655, 0.32687399604103784], 'topk_tokens': ['doll', '2', ' OF', ' boat', ' paper', ' papers', ' S', ' Bridge', ' People', ' OCC', '0', '186', ' M', ' hour', ' in', ' Europe', ' Star', ' an', ' Loan', 'AM'], 'evidence_proportions': [0.192474365234375, 0.26060994466145837, 0.33077087402343747]}, 'weight': {'score': [0.005902858451008797, 0.0024286599328675247, 0.007771406103582943, 0.0024091564351616233, 0.004134203448440089], 'topk_tokens': ['Question', '<|end_header_id|>', '<|eot_id|>', ' the', ' garden', '<|start_header_id|>', 'Answer', '?', '.\n\n', ' \n', 'hall', 'assistant', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.006920462846755982, 0.005976617336273193, 0.004796743392944336]}, 'saliency': {'score': [0.0007093958556652069, 4.427002253968879e-05, 0.00038215780959409824, 4.245070339386026e-05, 0.00022065910426053134], 'topk_tokens': [' Bridge', '?', ' Bridge', ' Daily', '.', ' Robert', '***', '.\n\n', 'ot', ' hallway', ' the', 'Daniel', '<|eot_id|>', ' Daniel', ' the', ':', 'assistant', '<|begin_of_text|>', '<|start_header_id|>', 'way'], 'evidence_proportions': [0.0015980184078216552, 0.0004457185665766398, 0.00013718605041503906]}}, 31: {'grad': {'score': [0.30101288855075836, 0.345772426530892, 0.22510661798364975, 0.34616857829826203, 0.1436616222966801], 'topk_tokens': [' government', ' Democrats', ' w', ' able', ' was', ' determined', ' was', ' evening', ' draft', ' draft', ' decided', ' the', ' would', ' W', ' these', 'F', ' had', ' United', ' F', ' Independent'], 'evidence_proportions': [0.3541389465332031, 0.3194759686787923, 0.22573113441467285]}, 'weight': {'score': [0.0015863794833421707, 0.002296085538812603, 0.0031311547054963954, 0.0022946849259566572, 0.0015923868526111949], 'topk_tokens': [' was', 'Question', ':', ' before', '.\n\n', '?', ' Where', ' the', 'Answer', '<|eot_id|>', ' \n', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.001191413402557373, 0.0011001576979955037, 0.002564811706542969]}, 'saliency': {'score': [9.95248556137085e-05, 2.5548290968214566e-05, 4.856463740853702e-05, 2.5386651637953233e-05, 1.3029936588171756e-05], 'topk_tokens': [' AND', ' Bridge', ' apple', ' PA', ' garden', ' \n', 'Question', '<|eot_id|>', '.\n\n', ' Geo', ' Where', ' hallway', ' hallway', '<|end_header_id|>', 'assistant', ' the', ':', '<|start_header_id|>', 'hall', 'way'], 'evidence_proportions': [0.00018447041511535645, 7.847944895426431e-05, 3.983378410339355e-05]}}, 'pred_res': "Daniel's hallway.<|eot_id|>", 'score': 100}
2025-01-22 03:22:32.905 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:22:32.906 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-1_0-1-4-9.pkl | len: 10 |  size: 8.59 KB
Processing depth (0, 1, 4, 9):   2%|▏         | 2/100 [00:38<31:26, 19.25s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (2, 4, 5, 6):   2%|▏         | 2/100 [00:45<31:26, 19.25s/it]2025-01-22 03:22:40.498 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 03:22:40.506 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2493, 2497) -->  grabbed the apple.
2025-01-22 03:22:40.506 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 03:22:40.520 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4867, 4872) --> . Daniel travelled to the
2025-01-22 03:22:40.520 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:22:40.538 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5930, 5936) --> . Daniel went back to the
2025-01-22 03:22:40.538 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:22:40.557 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7170, 7175) --> . Mary travelled to the
2025-01-22 03:22:40.558 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:22:40.564 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2048, 2054) --> . John went back to the
2025-01-22 03:22:40.564 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 03:22:40.594 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10550, 10556) --> . John journeyed to the
2025-01-22 03:22:40.594 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:22:40.617 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8226, 8231) --> . John moved to the
2025-01-22 03:22:40.617 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:22:40.640 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7072, 7078) --> . Sandra journeyed to the
2025-01-22 03:22:40.641 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:22:40.661 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7174, 7179) -->  the bedroom. Sandra moved
2025-01-22 03:22:40.661 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 03:22:40.673 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4301, 4307) --> . Sandra went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:22:42.702 | INFO     | test_jbb_retain:begin_test:632 - The hallway.<|eot_id|>
2025-01-22 03:22:42.703 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12235])
your chose emoji: ['🥑', '🇯🇴', '👩🏽\u200d❤️\u200d💋\u200d👨🏼', '👩🏿\u200d❤️\u200d👨🏻', 'ℹ', '🇺🇾', '🧎🏽\u200d♂️\u200d➡️', '🇺🇳', '👨\u200d👩\u200d👦', '🏋️\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 153919.41it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.18s/it][A
 38%|███▊      | 3/8 [00:05<00:06,  1.38s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.78it/s][A100%|██████████| 8/8 [00:05<00:00,  1.45it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.23it/s][A
 50%|█████     | 4/8 [00:00<00:00, 19.43it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 21.85it/s][A100%|██████████| 8/8 [00:00<00:00, 21.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.56it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.09it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.96it/s][A100%|██████████| 8/8 [00:00<00:00, 21.23it/s]
2025-01-22 03:22:51.630 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.35208816528320314, 0.3351550251075033, 0.34668540954589844, 0.3350950533515595, 0.5351109928554959], 'topk_tokens': [' Hor', ' Dr', ' Aw', ' died', ' doctor', ' doctor', ' turned', 'Frank', ' How', ' hand', ' Project', 'hom', 'dr', 'hand', ' doctors', 'hand', ' hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.29735565185546875, 0.409912109375, 0.3238067626953125, 0.3719879150390625]}, 'weight': {'score': [0.01192326694726944, 0.0025592564094843945, 0.019446415936245638, 0.0024967610356608897, 0.003317286570866903], 'topk_tokens': [' waving', '\n\n', ' garden', ' Father', ':', ' hallway', 'Answer', 'street', ' Sandra', '<|start_header_id|>', 'assistant', ' bedroom', '\n\n', '<|eot_id|>', 'hall', '<|end_header_id|>', ' hallway', 'way', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.021216392517089844, 0.005940628051757812, 0.012159148852030436, 0.010188347101211548]}, 'saliency': {'score': [0.0004764944314956665, 4.9497544385576816e-05, 0.0038701541283551384, 3.8134916217720045e-05, 0.00013252364264594184], 'topk_tokens': [' THE', ' Newspaper', 'Bridge', '.', '\n\n', ' Daniel', '\n\n', ' Father', '<|eot_id|>', 'street', ' Dan', ' Sandra', '<|eot_id|>', 'hall', '<|end_header_id|>', ' garden', 'way', ' hallway', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.0013330653309822083, 0.00018069744110107424, 0.0003698567549387614, 0.00021499991416931155]}}, 25: {'grad': {'score': [0.8455871343612671, 0.8576843092148941, 0.8138477101045496, 0.8578264946931299, 0.5851611455281576], 'topk_tokens': ['10', 'making', ' had', 'st', ' daily', ' o', ' producing', ' a', ' employed', ' large', ' had', ' had', 'AY', 'g', ' possessed', ' a', ' extraordinary', ' post', 'po', ' post'], 'evidence_proportions': [0.9486551284790039, 0.862420654296875, 0.8075052897135417, 0.7919974327087402]}, 'weight': {'score': [0.014492802321910858, 0.002554355249780517, 0.00651952887282652, 0.0025236933288492816, 0.0041777812772327], 'topk_tokens': ['door', ' Daniel', '<|end_header_id|>', '.\n\n', '<|start_header_id|>', ' Daniel', '.', ' \n', 'Answer', '<|start_header_id|>', ' Dan', 'hall', ':', 'way', 'assistant', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.020975112915039062, 0.017753905057907103, 0.00940820078055064, 0.012147372961044312]}, 'saliency': {'score': [0.0015977978706359862, 4.929448775478312e-05, 0.0011143456487094656, 4.378055085966795e-05, 0.0003028853072060479], 'topk_tokens': [' Marshall', '<|start_header_id|>', ' hallway', ' exc', 'doctor', ' doctor', 'hall', '�', ' apple', 'door', ' doctor', 'assistant', '<|eot_id|>', ' Mary', '\n\n', ' Dan', '<|eot_id|>', '.', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002472691237926483, 0.0006596565246582032, 0.00025844077269236243, 0.003443253040313721]}}, 26: {'grad': {'score': [0.39040250778198243, 0.5481620049199981, 0.41320189307717714, 0.54879757893062, 0.42669200897216797], 'topk_tokens': [' they', ' City', ' true', 'ucci', ' never', ' city', ' and', ' much', ' clear', 'asca', ' much', ' city', ' not', ' be', ' true', 'outs', '3', ' not', ' city', ' it'], 'evidence_proportions': [0.5759048461914062, 0.544677734375, 0.22679678599039715, 0.28405227661132815]}, 'weight': {'score': [0.007275278866291046, 0.0025230306565206333, 0.010201075497795553, 0.0024938039256605913, 0.0035813636249966093], 'topk_tokens': [' the', '.\n\n', ' kitchen', '<|start_header_id|>', '?', ' Dan', ' bedroom', 'Answer', ' \n', ' hallway', '\n\n', 'assistant', 'hall', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', '<|eot_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.007949531078338623, 0.006695926189422607, 0.007163782914479574, 0.00744902491569519]}, 'saliency': {'score': [0.0002969980239868164, 5.329234340179588e-05, 0.00037993666003732123, 5.198078559013236e-05, 0.00021370814906226265], 'topk_tokens': ['door', ' the', '<|start_header_id|>', 'Answer', ' bedroom', '<|start_header_id|>', ' kitchen', '<|eot_id|>', 'Bridge', ' \n', ' Father', '\n\n', '<|eot_id|>', 'way', '<|begin_of_text|>', ' hallway', ' Dan', ':', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.0007392615079879761, 0.00017737746238708497, 0.0001604755719502767, 0.00022663474082946778]}}, 27: {'grad': {'score': [0.5726295471191406, 0.6224683626626594, 0.5517974741318646, 0.6227473832241266, 0.4966023418638441], 'topk_tokens': ['.', ',', ':\n', ' for', '.', '.', '.', '.', '.', '.', '.', ' *\n\n', ',', '.', '.', '.', '.', '.', '.', '.'], 'evidence_proportions': [0.6522674560546875, 0.29378662109375, 0.6890792846679688, 0.6480224609375]}, 'weight': {'score': [0.008764074742794037, 0.002538290692419958, 0.010947514982784496, 0.002504604767692457, 0.00805866817633311], 'topk_tokens': [' apple', ' garden', ' \n', ' Dan', 'Answer', '�', '\n\n', '<|eot_id|>', ' bedroom', '<|start_header_id|>', 'assistant', ' THE', '.\n\n', '<|eot_id|>', 'hall', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.011485934257507324, 0.008996593952178954, 0.00915911297003428, 0.005880022048950195]}, 'saliency': {'score': [0.0009828165173530578, 8.037479110043858e-05, 0.0010917326983283548, 7.607119594525133e-05, 0.000845838917626275], 'topk_tokens': ['<|start_header_id|>', ' Daniel', '�', 'DW', '�', ' doctor', ' garden', 'assistant', ' hallway', ' Daniel', ' bedroom', '�', '<|begin_of_text|>', ' Dan', ' THE', ':', '<|end_header_id|>', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.0007206350564956665, 0.0015334606170654297, 0.0009180307388305664, 0.000719660520553589]}}, 28: {'grad': {'score': [0.3740665435791016, 0.5319849654221227, 0.3385920805089614, 0.5327838595885629, 0.3841939608256022], 'topk_tokens': [' hundred', ' York', '\n', ' lively', 'land', '\n', ' host', ' the', ' hallway', ' the', ' houses', ' of', 'ching', 'arp', ' hall', ' huge', ' hallway', ' hall', ' hall', ' hall'], 'evidence_proportions': [0.45529937744140625, 0.4627197265625, 0.3107261657714844, 0.296435546875]}, 'weight': {'score': [0.005879825353622437, 0.0024643048758521582, 0.004251107573509216, 0.0024537121557868472, 0.0014025850428475274], 'topk_tokens': ['Question', ' the', ' the', '.\n\n', ' apple', ' garden', ' before', '?', 'Answer', ' \n', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'hall', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.001083560287952423, 0.0013189315795898438, 0.012315327922503153, 0.006555128097534179]}, 'saliency': {'score': [0.00017332285642623901, 7.694521055908876e-05, 9.794883868273566e-05, 7.672839536920473e-05, 5.80519437789917e-05], 'topk_tokens': [' was', '.', ' Dul', '<|eot_id|>', 'Answer', ' before', ' \n', 'assistant', ' subscribers', ' Dr', '<|eot_id|>', '<|end_header_id|>', ' apple', '?', '\n\n', '<|start_header_id|>', 'way', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [7.65770673751831e-05, 7.792711257934571e-05, 0.00033704936504364014, 0.0001496434211730957]}}, 29: {'grad': {'score': [0.2804732322692871, 0.348089475923162, 0.3572816007277545, 0.34817481674963285, 0.3737723244561089], 'topk_tokens': ['P', ',', 'at', ' of', ' pur', '5', ' the', ' for', ' pur', 'ck', ' of', ' no', ' his', ' in', 'in', ' very', ' no', '.', 'A', ' George'], 'evidence_proportions': [0.1573638916015625, 0.18202056884765627, 0.32256825764973956, 0.4268993377685547]}, 'weight': {'score': [0.003631657361984253, 0.0024989505748496295, 0.003213607213076423, 0.0024950969585132536, 0.0024710145261552597], 'topk_tokens': [' Where', '<|start_header_id|>', 'Question', ' Does', ' apple', ' before', '.\n\n', '?', ' \n', 'Answer', 'hall', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'way', ':', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0024154186248779297, 0.0022382855415344237, 0.005351334810256958, 0.003934407234191894]}, 'saliency': {'score': [0.0005266964435577393, 3.429856452701878e-05, 0.0002936466651804307, 3.2766572323897446e-05, 0.00025610592630174425], 'topk_tokens': [' to', ' the', 'DW', ' into', ' garden', ' the', ' \n', 'Answer', ' to', '<|eot_id|>', '�', 'way', ' Does', ' the', '<|start_header_id|>', '\n\n', 'hall', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.00012054294347763062, 0.0002953529357910156, 0.001079946756362915, 0.00041906237602233886]}}, 30: {'grad': {'score': [0.26052322387695315, 0.42644174435224813, 0.26447318581973805, 0.42716607966081765, 0.485219587220086], 'topk_tokens': ['ar', ' Press', ' Star', '186', ' paper', ' boat', ' papers', '0', 'moment', ' Europe', ' S', '2', ' J', ' paper', ' account', ' M', ' in', 'deal', ' Loan', ' an'], 'evidence_proportions': [0.3264741897583008, 0.24892654418945312, 0.2919400533040365, 0.181658935546875]}, 'weight': {'score': [0.009959986805915833, 0.0024853340257400586, 0.010558946167721468, 0.002450534627477511, 0.007368699709574381], 'topk_tokens': ['<|eot_id|>', ' Dan', '<|end_header_id|>', 'Question', ' garden', '.\n\n', '<|start_header_id|>', '?', 'hall', 'Answer', '\n\n', '<|start_header_id|>', ' \n', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0036885738372802734, 0.006003165245056152, 0.01178338627020518, 0.016745859384536745]}, 'saliency': {'score': [0.00056239515542984, 5.5968091756344076e-05, 0.0008460993275922887, 5.293189647631354e-05, 0.0005222847064336141], 'topk_tokens': ['�', ' No', ' the', '�', '<|start_header_id|>', ' garden', ' the', ' the', 'British', ' Miles', ' Harper', ' Sandra', ':', ' Emily', 'Gov', ' Dan', 'assistant', 'hall', '<|begin_of_text|>', 'way'], 'evidence_proportions': [0.00013376027345657349, 8.12232494354248e-05, 0.0012116034825642903, 0.0006074249744415283]}}, 31: {'grad': {'score': [0.36528398394584655, 0.5565292095315868, 0.3575102160958683, 0.5573985094567779, 0.2730878485573663], 'topk_tokens': [' his', ' had', ' the', ' would', ' evening', ' the', ' the', ' two', ' able', ' ever', ' F', ' the', ' have', ' would', ' the', ' these', ' the', ' Independent', ' United', ' the'], 'evidence_proportions': [0.36543962359428406, 0.2995609283447266, 0.4145228068033854, 0.37179594039916997]}, 'weight': {'score': [0.002173498272895813, 0.0022837185629794635, 0.002334588590790244, 0.0022837575341593803, 0.0011993901597128975], 'topk_tokens': [' the', ':', ' the', 'Question', ' before', '.\n\n', ' Where', '?', '<|eot_id|>', 'Answer', '<|start_header_id|>', ' \n', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', ':', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0008941665291786194, 0.0009872555732727052, 0.0020293941100438437, 0.004556131362915039]}, 'saliency': {'score': [0.00011298656463623046, 4.807565605392992e-05, 0.00013881220537073473, 4.7715900567355944e-05, 2.0715594291687012e-05], 'topk_tokens': [' hallway', '.', '?', ' hallway', ' apple', ' \n', '<|eot_id|>', ' Where', 'Answer', 'Question', ' the', ' the', ' the', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', 'assistant', ':', 'way', 'hall'], 'evidence_proportions': [4.4986605644226074e-05, 1.1813640594482423e-05, 7.067620754241943e-05, 0.0003193318843841553]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 03:22:51.632 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:22:51.632 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-2_2-4-5-6.pkl | len: 10 |  size: 9.07 KB
Processing depth (2, 4, 5, 6):   3%|▎         | 3/100 [00:57<30:43, 19.01s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (0, 4, 6, 8):   3%|▎         | 3/100 [01:04<30:43, 19.01s/it]2025-01-22 03:22:59.066 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 03:22:59.099 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 03:22:59.112 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4885, 4890) --> . Daniel travelled to the
2025-01-22 03:22:59.112 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:22:59.133 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7212, 7218) --> . Daniel went back to the
2025-01-22 03:22:59.133 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:22:59.159 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9726, 9731) --> . Mary travelled to the
2025-01-22 03:22:59.159 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:22:59.165 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2044, 2050) --> . John went back to the
2025-01-22 03:22:59.165 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 03:22:59.195 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10557, 10563) --> . John journeyed to the
2025-01-22 03:22:59.196 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:22:59.216 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7217, 7222) -->  the garden. Sandra moved
2025-01-22 03:22:59.216 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:22:59.236 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7071, 7077) --> . Sandra journeyed to the
2025-01-22 03:22:59.236 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:22:59.256 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7217, 7222) -->  the garden. Sandra moved
2025-01-22 03:22:59.256 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 03:22:59.269 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4350, 4356) --> . Sandra went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:23:01.474 | INFO     | test_jbb_retain:begin_test:632 - PAUL<|eot_id|>
2025-01-22 03:23:01.475 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12233])
your chose emoji: ['🌙', '🐿', '🧗🏽\u200d♂️', '🤦🏼', '👩🏽\u200d❤\u200d💋\u200d👨🏾', '💆🏽\u200d♂️', '🥞', '⛹🏽\u200d♀', '🧑🏾\u200d❤️\u200d💋\u200d🧑🏼', '🧘🏼\u200d♂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 183357.55it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:36,  5.16s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.02s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.03it/s][A100%|██████████| 8/8 [00:05<00:00,  1.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.77it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.63it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.38it/s][A100%|██████████| 8/8 [00:00<00:00, 21.75it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.68it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.28it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.25it/s][A100%|██████████| 8/8 [00:00<00:00, 21.48it/s]
2025-01-22 03:23:10.386 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.4132038354873657, 0.35322233102180556, 0.30409946161157947, 0.35328063345808475, 0.5557873574170199], 'topk_tokens': ['u', ' hand', 'in', ' Ramsey', ' Wild', ' work', ' type', 'men', 'otype', 'itter', ' type', 'hand', 'hand', ' type', ' hand', ' type', ' hand', ' hand', ' hand', ' type'], 'evidence_proportions': [0.39901123046875, 0.4236246744791667, 0.4148914337158203]}, 'weight': {'score': [0.005143892019987106, 0.002555377345414051, 0.008121201220680685, 0.002536449539197718, 0.000878421420400793], 'topk_tokens': ['Answer', '\n\n', '<|start_header_id|>', ' barric', ' Bridge', ':', ' Bridge', 'Daniel', '\n\n', ' hallway', ' hallway', 'assistant', 'Bridge', '\n\n', '<|eot_id|>', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002271389961242676, 0.0078013986349105835, 0.004827386140823365]}, 'saliency': {'score': [0.00017795898020267487, 3.9731817586081366e-05, 0.000703551313456367, 3.7698213658504216e-05, 2.503090284087441e-05], 'topk_tokens': ['***', ' ', 'UL', '\n\n', 'Answer', ' bedroom', ' journey', '.', '<|end_header_id|>', '<|eot_id|>', ' Bench', 'assistant', ' hallway', ' bedroom', 'Bridge', 'way', ' PA', 'Daniel', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00012031197547912598, 0.00020595391591389975, 0.0002020120620727539]}}, 25: {'grad': {'score': [0.769767701625824, 0.909749176035367, 0.7956030789543601, 0.9102514467469464, 0.7219930020245638], 'topk_tokens': [' patron', 'two', 'ically', ' g', ' two', ' daily', ' had', '10', ' to', ' had', ' o', ' el', ' patron', ' to', ' had', ' a', 'st', ' large', ' post', 'ad'], 'evidence_proportions': [0.81610107421875, 0.7192177772521973, 0.78409423828125]}, 'weight': {'score': [0.004342813044786453, 0.0025536434133678686, 0.003409327829585356, 0.0025489068316959422, 0.0008980736813761971], 'topk_tokens': ['.', ' the', '<|eot_id|>', '.\n\n', 'Answer', '<|start_header_id|>', 'Daniel', '<|end_header_id|>', '<|start_header_id|>', '.', ' \n', 'hall', ':', 'way', 'assistant', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002744019031524658, 0.004938627282778422, 0.005226629972457886]}, 'saliency': {'score': [0.00021789409220218658, 4.1788494462240514e-05, 0.00015312959166134105, 4.12466196166301e-05, 2.9043379155072298e-05], 'topk_tokens': [' \n', 'way', '.', '<|start_header_id|>', 'E', ' doctor', ' PA', ' Ramsey', ' Moore', ':', '.', 'hall', ' Dan', '\n\n', 'Daniel', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010048747062683106, 0.00012475252151489258, 0.00044707059860229494]}}, 26: {'grad': {'score': [0.5436267852783203, 0.6319779604624673, 0.6366047578699449, 0.6320810548077072, 0.5661838271401145], 'topk_tokens': [' to', ' Run', ' arrangement', ' Paul', ' more', ' city', 'asca', ' Out', ' Paul', ',', ' and', ' Cl', 'ian', 'istributed', ' be', 'hall', ' not', 'outs', ' not', ' it'], 'evidence_proportions': [0.65706787109375, 0.4565887451171875, 0.5346313476562501]}, 'weight': {'score': [0.004968302324414253, 0.0024998202048005834, 0.0065607849289389215, 0.002485248687113523, 0.0009124763309955597], 'topk_tokens': [' barric', '.\n\n', 'Bridge', '<|start_header_id|>', ' hallway', ' Bridge', ' \n', 'Answer', ' hallway', '\n\n', 'assistant', ' the', 'hall', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0013321280479431153, 0.0055612921714782715, 0.00789288878440857]}, 'saliency': {'score': [0.0001878887414932251, 5.892005357215435e-05, 0.00016535993884591495, 5.84537434534899e-05, 3.4030526876449585e-05], 'topk_tokens': [' barric', 'Answer', '.', 'Daniel', ' Dan', ' \n', ' Bridge', ' hallway', 'assistant', ' hallway', 'Bridge', ' Bridge', ' the', ':', 'way', '\n\n', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [5.949735641479492e-05, 0.00017505884170532227, 0.0003316760063171387]}}, 27: {'grad': {'score': [0.48327159881591797, 0.5405329304919908, 0.4746536647572237, 0.540791922478024, 0.37044553323225543], 'topk_tokens': [' duration', '.', '.', '.', ' participate', '.', '.', '.', ' candidates', ':\n\n', '--', '--', '--', '.', '.', '--', '.', '.', '.', ':\n'], 'evidence_proportions': [0.3673919677734375, 0.591033935546875, 0.46983642578125]}, 'weight': {'score': [0.004203615710139275, 0.002539485994995556, 0.0052750838153502524, 0.00252966846657488, 0.0010617720809849825], 'topk_tokens': ['UL', ' lounge', ' \n', 'Daniel', 'RE', 'Answer', '\n\n', 'NEW', '<|eot_id|>', 'assistant', '.\n\n', '<|eot_id|>', ' hallway', ' hallway', 'hall', '<|start_header_id|>', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002324676513671875, 0.005229264497756958, 0.004851776361465454]}, 'saliency': {'score': [0.0004193577915430069, 6.474254469576908e-05, 0.0002808579627205344, 6.367396040540326e-05, 3.667040304704146e-05], 'topk_tokens': ['\n\n', ' lounge', ' Dan', 'assistant', ' ST', ' hallway', ' Bridge', ' Bridge', 'Daniel', '<|start_header_id|>', ' PA', 'Bridge', '<|begin_of_text|>', 'RE', ':', 'NEW', '<|end_header_id|>', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.00036452412605285643, 0.000616927941640218, 0.0002371072769165039]}}, 28: {'grad': {'score': [0.3186511993408203, 0.481062012117803, 0.3428530973546645, 0.4816608695038508, 0.36565509709444916], 'topk_tokens': ['ugg', ' Gutenberg', ' call', ' hall', ' houses', 'ed', 'burg', '\n\n\n\n', ' house', 'plant', 'ched', ' host', 'ivals', ' huge', '\n', ' hall', 'arp', '\n\n\n\n\n\n\n', ' hall', 'ching'], 'evidence_proportions': [0.40951080322265626, 0.27408281962076825, 0.28127365112304686]}, 'weight': {'score': [0.008597543463110924, 0.0024503954643905453, 0.004873786779010997, 0.0024355628964702583, 0.0006558956070379777], 'topk_tokens': ['ot', ' apple', ' Bridge', '.\n\n', ' \n', '?', 'Answer', ' garden', ' before', ' the', 'assistant', '<|eot_id|>', 'hall', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006566047668457031, 0.016595467925071716, 0.006940972805023193]}, 'saliency': {'score': [0.00024889223277568817, 6.030564722807511e-05, 9.04237522798426e-05, 5.997400428202867e-05, 1.0129064321517944e-05], 'topk_tokens': ['<|eot_id|>', ' back', ',', ' the', '?', ' before', ' \n', '<|eot_id|>', ' apple', '<|end_header_id|>', 'Bridge', ' Bridge', ' Bridge', ' the', '<|begin_of_text|>', 'way', ':', '<|start_header_id|>', '\n\n', 'hall'], 'evidence_proportions': [3.3974647521972656e-05, 0.0005347877740859985, 0.00012073516845703124]}}, 29: {'grad': {'score': [0.27532851696014404, 0.28833353990976934, 0.3466310501098633, 0.2881879601477794, 0.2871807813644409], 'topk_tokens': ['.', ' no', ' composing', ' and', ' closely', ' the', ' composing', ' S', '\n', '\n', ' composing', '.', ' was', ' Far', ' very', '5', ' book', ' George', 'A', '.'], 'evidence_proportions': [0.2625606536865234, 0.22555160522460938, 0.3478286743164063]}, 'weight': {'score': [0.0030382201075553894, 0.0024946502861379603, 0.002034629092496984, 0.0024952200878317985, 0.0006839077581058849], 'topk_tokens': ['NEW', 'ot', ' hallway', ' before', ' Does', '?', ' \n', '.\n\n', 'Answer', ' the', 'hall', '<|eot_id|>', 'assistant', 'way', '<|start_header_id|>', '\n\n', ':', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004811763763427734, 0.0037632385889689126, 0.004725241661071777]}, 'saliency': {'score': [0.00015466101467609406, 2.763746960874709e-05, 9.865445249220905e-05, 2.727254640678457e-05, 1.9085678187283602e-05], 'topk_tokens': ['E', 'ot', '.', '.', ' part', 'Does', '***', 'UL', '\n\n', 'NEW', '<|eot_id|>', '<|eot_id|>', 'way', ' the', ' Does', '<|end_header_id|>', 'hall', ':', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [2.422332763671875e-05, 0.00018709897994995117, 0.0002461731433868408]}}, 30: {'grad': {'score': [0.2807765007019043, 0.3404164309593617, 0.3050481571870692, 0.3405934176812046, 0.39131366122852673], 'topk_tokens': [' paper', ' ed', ' account', ' his', ' OF', '186', '0', ' People', ' LINE', ' Bridge', ' S', ' in', ' hour', ' OCC', ' d', ' Europe', ' Star', 'AM', ' Loan', ' an'], 'evidence_proportions': [0.2409759521484375, 0.25729242960611975, 0.3487579345703125]}, 'weight': {'score': [0.007726103067398071, 0.0024452020857762026, 0.00995018464677474, 0.002417328803092804, 0.002400587566874244], 'topk_tokens': [' before', '<|end_header_id|>', 'Question', '<|start_header_id|>', ' garden', ' the', '.\n\n', '?', 'Answer', ' \n', 'hall', '\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.001955258846282959, 0.008540362119674683, 0.01251983642578125]}, 'saliency': {'score': [0.00037243589758872986, 5.6116932764985506e-05, 0.0003852248191833496, 5.478337199234442e-05, 0.00010243972594087774], 'topk_tokens': ['Daniel', ' Dan', ' Daily', '<|eot_id|>', ' Bridge', '?', '.\n\n', 'ot', 'Bridge', ' Daily', ' Daily', ' Bridge', '<|end_header_id|>', ' the', 'assistant', '<|begin_of_text|>', ':', 'way', '<|start_header_id|>', 'hall'], 'evidence_proportions': [0.00030721426010131835, 0.0003378937641779582, 0.0004791080951690674]}}, 31: {'grad': {'score': [0.26655781269073486, 0.3155490922631684, 0.2581177152255002, 0.3157736554744305, 0.16952333192933688], 'topk_tokens': [' draft', ' of', ' debt', ' intense', ' these', 'ford', ' decided', ' delivered', ' evening', ' wield', ' an', ' W', ' the', ' able', 'F', ',', ' had', ' F', ' United', ' Independent'], 'evidence_proportions': [0.23935279846191407, 0.3418949445088704, 0.20335826873779297]}, 'weight': {'score': [0.0028436966240406036, 0.0022922937057111028, 0.002438979990342084, 0.002291160456049957, 0.0009636878967285156], 'topk_tokens': [' the', ':', 'Question', ' the', ' before', '.\n\n', ' Where', '?', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', 'assistant', ':', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.001012963056564331, 0.003520866235097249, 0.003861826658248901]}, 'saliency': {'score': [0.00018548592925071716, 2.567964262259322e-05, 4.236926050747142e-05, 2.542325434143978e-05, 6.989660588177768e-06], 'topk_tokens': [' PA', '.\n\n', 'Print', ' \n', ' garden', '.', ' Bridge', ' hallway', ' Geo', 'Question', '<|end_header_id|>', '<|eot_id|>', ' Where', ' the', 'assistant', '<|start_header_id|>', '<|begin_of_text|>', ':', 'hall', 'way'], 'evidence_proportions': [0.00011513233184814453, 0.00026748577753702796, 0.0001574397087097168]}}, 'pred_res': 'PAUL<|eot_id|>', 'score': 0}
2025-01-22 03:23:10.388 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:23:10.388 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-3_0-4-6-8.pkl | len: 10 |  size: 8.55 KB
Processing depth (0, 4, 6, 8):   4%|▍         | 4/100 [01:15<30:15, 18.91s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (2, 3, 5, 6):   4%|▍         | 4/100 [01:22<30:15, 18.91s/it]2025-01-22 03:23:17.290 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 03:23:17.297 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2516, 2520) -->  Daniel grabbed the apple
2025-01-22 03:23:17.297 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 03:23:17.307 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3851, 3856) --> . Daniel travelled to the
2025-01-22 03:23:17.311 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:23:17.328 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (5993, 5999) --> . Daniel went back to the
2025-01-22 03:23:17.328 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:23:17.348 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7266, 7271) --> . Mary travelled to the
2025-01-22 03:23:17.349 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:23:17.355 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2048, 2054) --> . John went back to the
2025-01-22 03:23:17.355 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 03:23:17.384 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10583, 10589) --> . John journeyed to the
2025-01-22 03:23:17.385 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:23:17.407 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8316, 8321) --> . John moved to the
2025-01-22 03:23:17.408 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 03:23:17.428 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7097, 7103) -->  papers. Sandra journeyed to
2025-01-22 03:23:17.428 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:23:17.448 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (7270, 7275) -->  the bedroom. Sandra moved
2025-01-22 03:23:17.448 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 03:23:17.460 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (4432, 4438) --> . Sandra went back to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:23:19.484 | INFO     | test_jbb_retain:begin_test:632 - St. Daniel<|eot_id|>
2025-01-22 03:23:19.484 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12236])
your chose emoji: ['🤵🏼\u200d♂️', '🆓', '👩🏻\u200d❤\u200d💋\u200d👩🏿', '🧗🏾\u200d♀', '🔍', '🧑🏿\u200d🦯\u200d➡️', '👨\u200d👨\u200d👧\u200d👧', '🧛🏻\u200d♂', '👳🏻\u200d♀', '📱']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 52022.38it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.57s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.09s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.89it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.59it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.80it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.04it/s][A100%|██████████| 8/8 [00:00<00:00, 21.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.72it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.53it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.88it/s][A100%|██████████| 8/8 [00:00<00:00, 20.84it/s]
2025-01-22 03:23:29.225 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.484625244140625, 0.4933127530971689, 0.4975763208725873, 0.4933151157458982, 0.5936969295962826], 'topk_tokens': [' doctors', 'ers', 'ily', ' readily', 'Spring', ' Grand', ' hand', 'Ind', 'ck', ' rushed', ' hand', ' states', 'le', ' hand', ' grand', ' grand', ' Project', ' Grand', 'dr', ' hand'], 'evidence_proportions': [0.67633056640625, 0.33176879882812504, 0.50469970703125, 0.460028076171875]}, 'weight': {'score': [0.010447214543819427, 0.002548240760113155, 0.0033813697450301226, 0.002532950988987894, 0.0018198686641651195], 'topk_tokens': ['<|end_header_id|>', '<|eot_id|>', ' West', '<|start_header_id|>', '.', '\n\n', ' Wright', ' \n', 'Answer', ' building', ':', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', 'hall', '<|end_header_id|>', 'way', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.025756359100341797, 0.003119546175003052, 0.013538062572479248, 0.0018185496330261229]}, 'saliency': {'score': [0.0005154281854629517, 5.023870673461311e-05, 0.00035273941124186795, 4.863108970319633e-05, 8.126864066490761e-05], 'topk_tokens': ['.', ' hand', 'Bridge', '.', ' journey', '<|start_header_id|>', ' West', '<|end_header_id|>', ' Father', ' Wins', 'assistant', '\n\n', ' Wright', '<|eot_id|>', '\n\n', ' building', '<|eot_id|>', 'way', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0011938810348510742, 0.0002843976020812988, 0.0005639990170796712, 0.00014541149139404296]}}, 25: {'grad': {'score': [0.7174247741699219, 0.7152190870373396, 0.7597171559053308, 0.7150913030337153, 0.5598918684236296], 'topk_tokens': [' to', ' had', ' to', ' employ', ' extraordinary', 'ad', ' had', ' to', 'po', 'st', 'ense', ' had', ' had', ' had', ' el', ' to', ' had', 'g', ' post', ' post'], 'evidence_proportions': [0.70721435546875, 0.8120849609375, 0.7083536783854166, 0.6418182373046875]}, 'weight': {'score': [0.020885077118873597, 0.002553908261357496, 0.0017260725007337682, 0.002526130094653424, 0.0027447156198732144], 'topk_tokens': ['<|start_header_id|>', '<|end_header_id|>', ' Daniel', ':', '.', '<|eot_id|>', '<|start_header_id|>', '.\n\n', 'Answer', 'hall', ' \n', ' Daniel', ' Paul', 'way', 'assistant', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.06717300415039062, 0.005989217758178711, 0.018589397271474205, 0.0015054106712341308]}, 'saliency': {'score': [0.0009391516447067261, 5.57208584262534e-05, 0.00011507377905004164, 5.4105215010018834e-05, 0.00016868507469093406], 'topk_tokens': ['.', ' grabbed', 'Print', 'way', ' Red', ' Senator', ' Merch', '.', '<|start_header_id|>', ' St', 'assistant', ' Daniel', ' East', ' Paul', ' Dan', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0040746331214904785, 7.236003875732422e-05, 0.0001741449038187663, 0.00021556615829467773]}}, 26: {'grad': {'score': [0.6704782485961914, 0.5737594528441355, 0.7308427586274988, 0.573162390200584, 0.5383751418564345], 'topk_tokens': [' not', 'ian', ' and', ' not', ' extra', ' extra', 'x', ' of', 'and', ' red', '.', 'outs', ' to', ' tie', ' city', 'hall', ',', ' not', ' Tie', ' it'], 'evidence_proportions': [0.7852401733398438, 0.7832855224609375, 0.5658912658691406, 0.5913658142089844]}, 'weight': {'score': [0.007733297348022461, 0.002509191540957218, 0.003370114109095405, 0.0024982146444895935, 0.002009304342689095], 'topk_tokens': [' garden', ' CHIP', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', ' the', '.\n\n', '?', ' \n', 'Answer', '\n\n', 'assistant', '<|start_header_id|>', 'hall', '<|eot_id|>', 'way', '<|eot_id|>', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0224989652633667, 0.0015713214874267577, 0.007328445712725321, 0.0025685608386993406]}, 'saliency': {'score': [0.00026395171880722046, 7.871633427664583e-05, 0.00028165035388048957, 7.784604585997433e-05, 8.085173564952809e-05], 'topk_tokens': [' West', '<|start_header_id|>', 'Answer', 'Bridge', '\n\n\n\n\n\n\n', ' CHIP', ' garden', ' \n', ' turtle', ' Dan', ' Father', ' West', '\n\n', '<|eot_id|>', '<|eot_id|>', 'way', ':', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0005121082067489624, 7.171034812927246e-05, 0.00021633009115854898, 0.00031481385231018065]}}, 27: {'grad': {'score': [0.42214169502258303, 0.4594555310987162, 0.3771513209623449, 0.45974643137497057, 0.3426202627328726], 'topk_tokens': ['      ', '      ', ':\n', ' candidates', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '--', '      '], 'evidence_proportions': [0.48528099060058594, 0.45319213867187497, 0.4148127237955729, 0.3493745803833008]}, 'weight': {'score': [0.014158330857753754, 0.002529206701777848, 0.0038556237431133494, 0.0025064179728057573, 0.0031288351331438336], 'topk_tokens': ['\n\n\n\n\n\n\n', ' Daniel', ' Dan', ' apple', ' building', '?', ' \n', '<|eot_id|>', '.\n\n', ' garden', 'Answer', '\n\n', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.04693603515625, 0.003757739067077637, 0.011322349309921265, 0.0017399370670318603]}, 'saliency': {'score': [0.002128121256828308, 7.517303075555863e-05, 0.00025499743573805867, 7.130163196271005e-05, 0.00023372782455695855], 'topk_tokens': [' West', ' grabbed', 'Answer', 'Dub', '�', ' garden', ' hallway', ' ST', 'assistant', '\n\n', 'NEW', ' Dan', ' building', '<|start_header_id|>', ' Daniel', '<|begin_of_text|>', '.\n\n', '<|end_header_id|>', 'way', 'hall'], 'evidence_proportions': [0.00864659994840622, 0.00047332644462585446, 0.0008920331796010335, 5.143880844116211e-05]}}, 28: {'grad': {'score': [0.3033198356628418, 0.4360608788855043, 0.3306528540218578, 0.43657287672791856, 0.3803487138433771], 'topk_tokens': [' Brown', ' of', 'inity', '\n', ' Gutenberg', 'of', 'urred', 'burg', ' huge', ' hall', ' hall', ' houses', ' locate', ' house', 'ched', ' host', '\n\n\n\n\n\n\n', 'arp', 'ching', ' hall'], 'evidence_proportions': [0.33013916015625, 0.3111568450927734, 0.34923044840494794, 0.21893463134765626]}, 'weight': {'score': [0.003925387561321258, 0.0024523878031301387, 0.002495734130634981, 0.002449849126864321, 0.0015404208020849543], 'topk_tokens': ['Question', '.', ' the', ' apple', '.\n\n', ' garden', ' before', ' \n', 'Answer', 'assistant', '<|eot_id|>', 'hall', '?', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002367638051509857, 0.0008868098258972168, 0.00790417194366455, 0.0034356236457824705]}, 'saliency': {'score': [0.00018164515495300293, 6.257886249203436e-05, 5.903927718891817e-05, 6.239330812609973e-05, 3.895458284315172e-05], 'topk_tokens': [' before', 'Probably', ' most', ' garden', ' Far', ' Dul', ' \n', ' the', '<|end_header_id|>', 'assistant', '?', '<|eot_id|>', '<|start_header_id|>', ' apple', '<|eot_id|>', 'way', '<|begin_of_text|>', ':', '\n\n', 'hall'], 'evidence_proportions': [0.00028639286756515503, 3.0481815338134764e-05, 0.0003121147553126017, 9.244680404663086e-05]}}, 29: {'grad': {'score': [0.3029902458190918, 0.4380392477747876, 0.3864431942210478, 0.43840488141122097, 0.4271617931324047], 'topk_tokens': [' Get', '000', ' closely', ' very', '25', ' pur', ' George', ' P', ' too', '.', ' no', 'ra', ' pur', ' ga', ' very', '5', 'in', 'ck', 'P', 'A'], 'evidence_proportions': [0.30621910095214844, 0.21335296630859374, 0.3302774429321289, 0.3572998046875]}, 'weight': {'score': [0.0011908113956451416, 0.002503939903739894, 0.0019349052625543932, 0.0025076830098508664, 0.0019165559129400568], 'topk_tokens': [' garden', '.', ' apple', '<|start_header_id|>', ' before', ' the', 'Answer', ' \n', '.\n\n', 'hall', '?', 'assistant', '<|eot_id|>', ':', 'way', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010248422622680664, 0.0002617955207824707, 0.0021560887495676675, 0.0010942697525024416]}, 'saliency': {'score': [5.9685111045837404e-05, 4.493600938997877e-05, 0.00010947795475230498, 4.473170834972959e-05, 0.00012739706825424027], 'topk_tokens': ['�', ' before', 'Answer', ' Does', '.', '.\n\n', ' of', 'IVE', '<|begin_of_text|>', ' \n', '<|start_header_id|>', ' the', '?', '<|eot_id|>', 'way', 'hall', '<|eot_id|>', ':', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [6.365776062011719e-05, 1.1920928955078125e-05, 0.00013400614261627197, 1.5085935592651367e-05]}}, 30: {'grad': {'score': [0.4235496520996094, 0.49213465838584647, 0.49466099458582263, 0.4922401819553931, 0.6973565698979975], 'topk_tokens': [' hour', ' his', ' account', ' Press', ' boat', 'deal', 'ar', ' papers', ' boat', ' paper', ' Paul', 'doll', 'AM', '0', ' Star', ' paper', '186', ' Loan', ' Europe', ' an'], 'evidence_proportions': [0.6389541625976562, 0.3381195068359375, 0.4212290445963542, 0.33944091796875]}, 'weight': {'score': [0.0043263107538223265, 0.002452842082177826, 0.004440146334031049, 0.0024442218344965867, 0.006712468770834116], 'topk_tokens': ['.', '<|eot_id|>', 'Question', '.\n\n', '<|end_header_id|>', ' garden', '<|start_header_id|>', ' \n', 'Answer', 'assistant', 'hall', '\n\n', '<|eot_id|>', '?', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0038880109786987305, 0.001972919702529907, 0.00781967242558797, 0.0028383076190948487]}, 'saliency': {'score': [0.00013047754764556886, 7.89621626496052e-05, 0.00018423795700073242, 7.858385450780318e-05, 0.0005875193810724951], 'topk_tokens': [' St', '.', 'Answer', 'IR', '<|eot_id|>', ' St', '<|start_header_id|>', ' AND', ' Wright', '�', '�', ' in', '<|eot_id|>', '?', 'assistant', '<|begin_of_text|>', '<|start_header_id|>', 'way', ':', 'hall'], 'evidence_proportions': [0.0002024620771408081, 0.00010012984275817872, 0.00011343757311503093, 0.00012368559837341308]}}, 31: {'grad': {'score': [0.95263671875, 1.2847024384779189, 0.9649846694048714, 1.2861395922360677, 0.7432564536293784], 'topk_tokens': [' had', ' had', ' they', ' to', ' had', ' I', ' its', ' had', ' the', ' the', ' the', ' evening', ' an', ' have', ' the', ' its', ' ever', ' the', ' had', ' his'], 'evidence_proportions': [0.617706298828125, 0.8982177734375, 1.1148274739583333, 1.08037109375]}, 'weight': {'score': [0.0018353492021560669, 0.0022533450098206806, 0.0017452494186513564, 0.0022554488396321743, 0.001784337090921926], 'topk_tokens': [' the', ':', 'Question', ' Where', ' apple', '.\n\n', ' before', '<|eot_id|>', ' \n', 'Answer', '?', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', ':', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0010711252689361572, 0.0014172196388244629, 0.0022539695103963218, 0.0023625135421752926]}, 'saliency': {'score': [4.930347204208374e-05, 9.363914531511715e-05, 9.525085196775548e-05, 9.370741904833594e-05, 0.00010636645358997387], 'topk_tokens': [' S', '�', '<|eot_id|>', 'Question', ' Where', '.\n\n', ' the', ' apple', 'Answer', '<|start_header_id|>', '?', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'hall', 'way'], 'evidence_proportions': [2.533942461013794e-05, 2.8157234191894533e-05, 3.819167613983154e-05, 0.00010295510292053222]}}, 'pred_res': 'St. Daniel<|eot_id|>', 'score': 0}
2025-01-22 03:23:29.226 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:23:29.226 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-4_2-3-5-6.pkl | len: 10 |  size: 9.19 KB
Processing depth (2, 3, 5, 6):   5%|▌         | 5/100 [01:34<29:53, 18.88s/it]Processing depth (2, 3, 5, 6):   5%|▌         | 5/100 [01:34<30:03, 18.99s/it]
2025-01-22 03:23:29.466 | INFO     | __main__:<module>:72 - Selected idx: 17
2025-01-22 03:23:29.466 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 03:23:29.466 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:23:29.466 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:23:29.466 | INFO     | __main__:<module>:76 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' John went back to the office.', ' Sandra went back to the hallway.', ' Daniel went back to the garden.', ' Sandra moved to the kitchen.', ' John journeyed to the bedroom.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-22 03:23:29.467 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' Daniel went back to the garden.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-22 03:23:29.467 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (0, 1, 3, 7, 8):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:23:36.608 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 03:23:36.609 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 03:23:36.609 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 03:23:36.613 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1475, 1479) -->  Daniel grabbed the apple
2025-01-22 03:23:36.614 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:23:36.625 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3783, 3789) --> . Daniel went back to the
2025-01-22 03:23:36.625 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel put down the apple.
2025-01-22 03:23:36.648 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8408, 8413) --> . Daniel put down the
2025-01-22 03:23:36.649 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:23:36.676 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9665, 9670) -->  Mary travelled to the bedroom
2025-01-22 03:23:36.676 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:23:36.705 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (10476, 10481) --> . John moved to the
2025-01-22 03:23:36.705 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:23:36.736 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10890, 10896) -->  Sandra journeyed to the office
2025-01-22 03:23:36.736 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:23:36.737 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (73, 79) --> . John went back to the
2025-01-22 03:23:36.737 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:23:36.744 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2383, 2389) --> . Sandra went back to the
2025-01-22 03:23:36.744 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:23:36.749 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (2041, 2046) --> . Sandra moved to the
2025-01-22 03:23:36.750 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 03:23:36.756 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (2101, 2107) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:23:38.689 | INFO     | test_jbb_retain:begin_test:632 - the garden<|eot_id|>
2025-01-22 03:23:38.689 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12254])
your chose emoji: ['🧏🏾', '👩🏾\u200d❤️\u200d👩🏾', '🤵🏼', '👩🏻\u200d❤️\u200d💋\u200d👩🏾', '🙋🏿\u200d♀️', '👨🏿\u200d🤝\u200d👨🏻', '👨\u200d🎤', '➕', '🇭🇳', '🤛🏽']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 207126.12it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.49s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.46s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.69it/s][A100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.14it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.29it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.29it/s][A100%|██████████| 8/8 [00:00<00:00, 20.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.39it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.98it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.21it/s][A100%|██████████| 8/8 [00:00<00:00, 20.24it/s]
2025-01-22 03:23:48.354 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.23155698776245118, 0.20375591737756996, 0.23937836815329158, 0.20359964667040514, 0.3904727393008293], 'topk_tokens': [' set', '�', ' hand', 'le', ' short', ' Aw', 'om', ' successful', 'hom', 'sur', ' successful', 'hand', ' Wide', ' hand', ' Project', ' hand', ' hand', 'hand', ' hand', ' Wide'], 'evidence_proportions': [0.20552215576171873, 0.2418193817138672, 0.21349072456359863, 0.25436859130859374, 0.24824981689453127]}, 'weight': {'score': [0.03133780121803284, 0.0025474327058147097, 0.0049444498384700105, 0.002481745068881136, 0.0011490830715666425], 'topk_tokens': [' Daniel', 'Bridge', ' Bridge', '?\n', '.', '\n\n', ' hallway', '<|start_header_id|>', 'Answer', ':', '<|eot_id|>', 'assistant', ' garden', '\n\n', '<|eot_id|>', ' hallway', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0905491828918457, 0.0269012451171875, 0.018886536359786987, 0.012559717893600462, 0.009395265579223632]}, 'saliency': {'score': [0.0009741580486297607, 2.6384640101790478e-05, 8.819646695080926e-05, 2.4269872326248102e-05, 2.0702468587997112e-05], 'topk_tokens': [' bedroom', 'THE', ':', ' bedroom', '.', ' top', ' During', ' Bridge', '\n\n', 'assistant', '<|end_header_id|>', ' hallway', ' garden', ' Daniel', 'Bridge', 'way', ' hallway', '<|begin_of_text|>', ' garden', 'hall'], 'evidence_proportions': [0.0027425706386566165, 0.001362040638923645, 0.0003369649251302083, 0.000160825252532959, 0.0004734039306640625]}}, 25: {'grad': {'score': [0.3160662841796875, 0.3966304371035429, 0.2942982841940487, 0.3970807893762121, 0.2970709090537213], 'topk_tokens': [' prepared', 'ated', 'ense', ' had', ' too', ' great', ' daily', ' most', '.', ' g', ' rest', ' great', '\n', ' getting', ' great', 'st', ' genu', 'g', ' post', ' post'], 'evidence_proportions': [0.34430236816406246, 0.33307647705078125, 0.2618916829427083, 0.38447265625000004, 0.27082519531250004]}, 'weight': {'score': [0.014802237749099731, 0.002541454365292935, 0.003067118280074176, 0.0025148604845175843, 0.0011182804691030623], 'topk_tokens': [' During', ' prior', ' the', 'Daniel', ' Daniel', '?\n', 'Answer', ' Daniel', '.\n\n', '<|start_header_id|>', '.', 'hall', '<|eot_id|>', ':', 'assistant', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.011829805374145509, 0.029259204864501953, 0.011818036437034607, 0.018515801429748534, 0.0060765743255615234]}, 'saliency': {'score': [0.0005019617080688477, 2.1140861056062423e-05, 5.3130528506110696e-05, 2.0066244736286945e-05, 2.8209166323885004e-05], 'topk_tokens': [' place', ' the', ' Anthony', ' garden', '.', ' During', '<|start_header_id|>', 'assistant', ' Daniel', ' PA', ' hallway', ' Merch', '.', 'Daniel', 'hall', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008131742477416992, 0.0011457130312919617, 0.00018331905206044516, 0.00033579468727111815, 0.0002242863178253174]}}, 26: {'grad': {'score': [0.27199630737304686, 0.3090690942070093, 0.3139335828668931, 0.3091315164939754, 0.3232664960495969], 'topk_tokens': [' of', 'ol', ' be', ' could', 'ible', ' Good', 'could', ' extra', ' extra', ' City', 'ian', ' it', ' never', ' not', '3', 'asca', ' and', ' not', 'outs', 'ucci'], 'evidence_proportions': [0.31192703247070314, 0.292510986328125, 0.23008855183919272, 0.283502197265625, 0.254437255859375]}, 'weight': {'score': [0.018652462959289552, 0.0024672342745312627, 0.004304645692600924, 0.0024289408899326953, 0.0009311501016008093], 'topk_tokens': [' kitchen', ' the', ' discarded', ' the', '.\n\n', ' garden', '?\n', 'Answer', ' the', ' hallway', 'assistant', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0475509524345398, 0.01263129711151123, 0.007873907685279846, 0.010054558515548706, 0.016103076934814452]}, 'saliency': {'score': [0.0010526669025421142, 5.905999681772022e-05, 0.00015601778731626624, 5.675332871474757e-05, 1.902149078693796e-05], 'topk_tokens': [' Daniel', '?\n', ' Gray', ' garden', ' kitchen', '<|eot_id|>', '<|start_header_id|>', 'assistant', ' hallway', ' the', ' the', ' the', ' garden', 'way', ' hallway', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [0.004006081819534302, 0.0007431879639625549, 0.00022076567014058432, 0.0001878023147583008, 0.0002099812030792236]}}, 27: {'grad': {'score': [0.24377525329589844, 0.23442969610477687, 0.2353222790886374, 0.2344080542986423, 0.28232342131594396], 'topk_tokens': [' and', ' hand', ' minutes', ' attempt', ' Giants', ' sco', '�', '.', '�', 'ided', '.', ' difficulty', ' participate', ' mailing', 'hand', ' business', ' duration', ' majority', ' candidates', ' majority'], 'evidence_proportions': [0.330010986328125, 0.18304204940795898, 0.24310811360677081, 0.232672119140625, 0.21802978515625]}, 'weight': {'score': [0.04453376889228821, 0.00252868445067224, 0.003919353379922754, 0.002438718074656917, 0.0016706053246843054], 'topk_tokens': [' Daniel', ' THE', '.', 'Daniel', 'THE', '?\n', ' garden', 'Answer', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', '<|start_header_id|>', '.\n\n', ':', 'hall', '<|end_header_id|>', ' hallway', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.15476059913635254, 0.025606393814086914, 0.011514082551002502, 0.01750277280807495, 0.016103458404541016]}, 'saliency': {'score': [0.001936633586883545, 3.902933244906719e-05, 0.00014835596084594727, 3.4835430848283795e-05, 5.7657982440704995e-05], 'topk_tokens': ['Answer', ' PA', ' Daniel', '.', 'assistant', 'Daniel', ' THE', 'THE', ' Daniel', '<|begin_of_text|>', '<|start_header_id|>', 'NEW', ' Daniel', ' garden', 'way', ' hallway', ':', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.003411215543746948, 0.0030138641595840454, 0.000992313027381897, 0.002106297016143799, 0.0005637884140014648]}}, 28: {'grad': {'score': [0.2514735221862793, 0.2798032322569501, 0.2905266004450181, 0.27983140476337515, 0.254242978197463], 'topk_tokens': [' houses', ' law', ' hallway', ' houses', 'arch', '***', 'urred', 'ched', ' house', 'burg', 'ching', ' line', ' *', ' line', ' house', ' hall', 'arp', ' hall', ' line', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.441107177734375, 0.18276405334472656, 0.21841804186503094, 0.18065872192382812, 0.22728881835937498]}, 'weight': {'score': [0.01142827033996582, 0.0024040760900200684, 0.005292243817273308, 0.002377530544932738, 0.0007020596494065954], 'topk_tokens': [' to', ' the', ' Daniel', ' garden', ' hallway', ' the', ' the', 'Answer', '.\n\n', '?\n', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'hall', '<|end_header_id|>', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.003967189788818359, 0.008826375007629395, 0.010932043194770813, 0.013536125421524048, 0.0194584846496582]}, 'saliency': {'score': [0.00032071948051452635, 3.6281979938913855e-05, 7.125472321229823e-05, 3.5601539638398576e-05, 7.729264015847064e-06], 'topk_tokens': ['Right', ' discarded', ' the', ' hallway', '<|eot_id|>', ' Daniel', ' apple', ' Floral', ' During', 'Probably', '<|eot_id|>', ' Bridge', '?\n', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', ':', 'way', 'hall'], 'evidence_proportions': [0.00014626383781433106, 0.0006129443645477295, 0.00024187564849853516, 0.0005269289016723633, 0.0001497983932495117]}}, 29: {'grad': {'score': [0.20319915771484376, 0.1939230177621716, 0.22975559795604034, 0.1938041284995541, 0.20400797052586334], 'topk_tokens': [' THE', '\n', ' The', ' FIRST', '\n', '\n', 'THE', '\n', '\n', 'BR', '\n', '\n', 'A', '\n', '\n', 'br', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.18588104248046874, 0.14184188842773438, 0.1982752482096354, 0.16961669921875, 0.30909423828125]}, 'weight': {'score': [0.00585553765296936, 0.002452226022410823, 0.00316223589813008, 0.002443271839221905, 0.0006806996274501719], 'topk_tokens': [' was', ' ', ' place', ' to', '.', ' Does', '<|start_header_id|>', '?\n', 'Answer', '.\n\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'hall', ':', '<|eot_id|>', 'way', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0044294953346252445, 0.0015125870704650879, 0.011046876509984335, 0.003063732385635376, 0.00731813907623291]}, 'saliency': {'score': [0.00028992056846618655, 1.4561978329489435e-05, 0.00011300952995524687, 1.3723219392885632e-05, 1.6656961846858895e-05], 'topk_tokens': [' where', ' was', ' a', '<|start_header_id|>', ' place', 'Answer', ' Does', ' to', ' the', 'NEW', 'assistant', 'IVE', '<|eot_id|>', 'way', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00018401145935058593, 1.2993812561035156e-05, 0.0007443974415461222, 7.118582725524902e-05, 0.00029073357582092284]}}, 30: {'grad': {'score': [0.23122222900390624, 0.21610357050037096, 0.16768893073586857, 0.21620753273101573, 0.23395745297695728], 'topk_tokens': [' Paul', ' D', ' Malta', ' Republicans', ' in', ' preparations', '0', ' Bridge', ' Republican', ' boat', '2', ' Europe', ' Press', ' S', ' boat', ' Star', ' S', 'deal', ' hour', ' Loan'], 'evidence_proportions': [0.239471435546875, 0.3192901611328125, 0.18842061360677084, 0.19552001953125, 0.23958282470703124]}, 'weight': {'score': [0.017186161279678345, 0.0024221951239667494, 0.00659189943005057, 0.0023803137417483826, 0.0023804176995094787], 'topk_tokens': [' the', ' the', '.', '<|end_header_id|>', ' garden', '<|eot_id|>', '<|start_header_id|>', 'Answer', 'assistant', 'hall', '\n\n', '?\n', '.\n\n', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.008546018600463867, 0.010798215866088867, 0.02380183835824331, 0.015742480754852295, 0.024441528320312503]}, 'saliency': {'score': [0.0008496606349945069, 3.255161762898904e-05, 0.00020627414478975183, 3.0392715238629648e-05, 6.947460326742619e-05], 'topk_tokens': [' Daily', 'Daniel', '<|eot_id|>', ' hallway', ' the', ' Bridge', ' Daniel', 'Gov', ' Daniel', ' garden', ' hallway', '?\n', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'way', 'hall', ':'], 'evidence_proportions': [0.0004947006702423096, 0.000799618661403656, 0.001336162288983663, 0.0009534418582916259, 0.0005570709705352783]}}, 31: {'grad': {'score': [0.25356717348098756, 0.33195417584997117, 0.26770827787763934, 0.3322939065919194, 0.16618033894833098], 'topk_tokens': [' the', ' determined', ' D', ' had', ' had', ' able', 'f', 'the', ' these', ' w', ' F', ' United', ' would', ' had', ' the', ' government', ' Independent', 'F', ' have', ' the'], 'evidence_proportions': [0.24782754182815553, 0.1931346356868744, 0.27128976583480835, 0.2977157115936279, 0.24223718643188477]}, 'weight': {'score': [0.0031536388397216796, 0.0022527074273087574, 0.002260421128833995, 0.002250839452955406, 0.0008490050726748527], 'topk_tokens': ['<|start_header_id|>', 'Question', ' was', ',', ' the', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', ':', 'assistant', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0023108601570129396, 0.0018012523651123047, 0.0015621731678644815, 0.0036941230297088622, 0.006447601318359375]}, 'saliency': {'score': [0.00020072102546691894, 1.615704326516041e-05, 4.859706934760599e-05, 1.5688354919378554e-05, 6.7866863088404876e-06], 'topk_tokens': [' Geo', ' Paul', 'light', ' Where', ' ', ' the', '<|eot_id|>', ':', 'Question', '<|begin_of_text|>', '<|start_header_id|>', ' hallway', ' hallway', 'assistant', '<|end_header_id|>', '.\n\n', 'Answer', ':', 'hall', 'way'], 'evidence_proportions': [0.0006269335746765137, 6.221234798431396e-05, 5.0355990727742515e-05, 0.00011529922485351563, 0.00015117526054382323]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:23:48.355 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:23:48.355 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-0_0-1-3-7-8.pkl | len: 10 |  size: 9.58 KB
Processing depth (0, 1, 3, 7, 8):   1%|          | 1/100 [00:18<31:01, 18.80s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (0, 2, 3, 4, 8):   1%|          | 1/100 [00:26<31:01, 18.80s/it]2025-01-22 03:23:55.852 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 03:23:55.852 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 03:23:55.853 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 03:23:55.859 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2542, 2546) -->  Daniel grabbed the apple
2025-01-22 03:23:55.860 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:23:55.871 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3877, 3883) --> . Daniel went back to the
2025-01-22 03:23:55.871 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel put down the apple.
2025-01-22 03:23:55.885 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (4976, 4981) --> . Daniel put down the
2025-01-22 03:23:55.885 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:23:55.911 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9644, 9649) --> . Mary travelled to the
2025-01-22 03:23:55.911 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:23:55.940 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (10445, 10450) --> . John moved to the
2025-01-22 03:23:55.940 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:23:55.970 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10894, 10900) --> . Sandra journeyed to the
2025-01-22 03:23:55.970 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:23:55.971 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (73, 79) --> . John went back to the
2025-01-22 03:23:55.971 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:23:55.978 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2451, 2457) --> . Sandra went back to the
2025-01-22 03:23:55.978 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:23:55.983 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (2061, 2066) --> . Sandra moved to the
2025-01-22 03:23:55.983 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 03:23:55.990 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (2133, 2139) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:23:58.119 | INFO     | test_jbb_retain:begin_test:632 - Daniel's hand.<|eot_id|>
2025-01-22 03:23:58.119 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12248])
your chose emoji: ['👩🏽\u200d❤️\u200d💋\u200d👩🏾', '👨🏼\u200d❤\u200d👨🏽', '🙎🏼\u200d♂', '👩🏼\u200d🦱', '⛹\u200d♀️', '🤹🏽\u200d♀', '👸🏿', '🦸🏻\u200d♀️', '🏟️', '🐤']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 73262.95it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.58s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.10s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  1.89it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.72it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.36it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 16.68it/s][A100%|██████████| 8/8 [00:00<00:00, 16.64it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.46it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.45it/s][A100%|██████████| 8/8 [00:00<00:00, 20.65it/s]
2025-01-22 03:24:07.701 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3296574401855469, 0.29752364363750916, 0.2775510900160846, 0.29751345022456854, 0.5060716813260858], 'topk_tokens': [' custom', ' fear', 'work', ' type', ' How', ' clear', ' died', 'itter', 'hom', 'men', 'le', ' happened', ' turned', ' happened', 'hand', ' hand', ' hand', 'hand', ' hand', ' hand'], 'evidence_proportions': [0.34769439697265625, 0.399810791015625, 0.2521565755208333, 0.3258209228515625, 0.35233535766601565]}, 'weight': {'score': [0.02624550700187683, 0.002546768773868029, 0.004508228862986845, 0.0024927039691821484, 0.0007210214706984433], 'topk_tokens': ['Bridge', ' top', ' Daniel', ' the', '\n\n', ' hallway', 'Answer', 'Daniel', '<|start_header_id|>', ' garden', ':', '<|eot_id|>', 'assistant', ' Bridge', '\n\n', 'hall', '<|eot_id|>', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.016692352294921876, 0.048618316650390625, 0.03334897259871165, 0.03217893242835999, 0.0034428298473358158]}, 'saliency': {'score': [0.002370913028717041, 3.864856440041486e-05, 0.0001926167922861436, 3.343682462383756e-05, 1.2113628062334927e-05], 'topk_tokens': [' lounge', ' Foster', '<|eot_id|>', '<|end_header_id|>', ' hallway', '\n\n', ' old', ' garden', ' Daniel', 'assistant', ' bedroom', 'way', ' Daniel', 'Bridge', ' Daniel', ' Bridge', ' garden', 'hall', 'Daniel', '<|begin_of_text|>'], 'evidence_proportions': [0.0002917766571044922, 0.005164206027984619, 0.0014024078845977783, 0.005674141645431519, 7.439255714416503e-05]}}, 25: {'grad': {'score': [0.628822250366211, 0.7128399674643907, 0.6171244452981388, 0.7132791710963712, 0.5040372501720082], 'topk_tokens': [' had', ' presidential', 'bel', ' employ', ' had', ' too', 'g', ' ST', ' had', 'po', ' great', 'ated', ' patron', ' post', ' had', 'ad', ' genu', 'st', ' daily', ' post'], 'evidence_proportions': [0.728643798828125, 0.5847816467285156, 0.5961939493815104, 0.78192138671875, 0.4502880096435547]}, 'weight': {'score': [0.01989893674850464, 0.0025470789846386505, 0.0023031743133769315, 0.0025121787479856195, 0.0009545460343360901], 'topk_tokens': [' ', ' top', ' garden', 'Daniel', ' the', '.\n\n', '?\n', ' Daniel', 'hall', ' Daniel', 'Answer', '<|start_header_id|>', ':', '<|eot_id|>', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0025765419006347655, 0.049617767333984375, 0.015814408659934998, 0.032481449842453006, 0.005765187740325928]}, 'saliency': {'score': [0.0007373428344726562, 2.8161083121327377e-05, 2.802294843337115e-05, 2.6707273475297794e-05, 2.8061257167295977e-05], 'topk_tokens': [' Merch', 'Print', '.', 'way', ' Mary', ' Marshall', ' top', ' Daniel', 'hall', '<|start_header_id|>', ' garden', ' Geo', ' the', ' Daniel', 'assistant', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00013744831085205078, 0.0014680176973342896, 0.000318447748819987, 0.001391160488128662, 0.0006015539169311523]}}, 26: {'grad': {'score': [0.3660955810546875, 0.4695935615587503, 0.4493134442497702, 0.4698623421936836, 0.36054225401444867], 'topk_tokens': [' and', ' now', 'ible', 'ian', ' City', ' extra', ' not', 'ian', ' City', ',', ' be', '3', ' Out', 'outs', ' and', ' not', 'ucci', ' it', 'asca', 'ian'], 'evidence_proportions': [0.45196533203125, 0.4243316650390625, 0.3638559977213542, 0.30389709472656246, 0.29852294921875]}, 'weight': {'score': [0.01265027403831482, 0.0024975413215879458, 0.003199224962907679, 0.0024747660950686676, 0.0006427399136803367], 'topk_tokens': ['<|start_header_id|>', ' hallway', 'Daniel', ' the', '.\n\n', '?\n', ' garden', ' Bridge', 'Answer', ' the', '\n\n', 'assistant', 'hall', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.008243870735168458, 0.02696990966796875, 0.010180756449699402, 0.014955902099609376, 0.006258761882781983]}, 'saliency': {'score': [0.0005076706409454345, 3.942709564238254e-05, 8.45114974414601e-05, 3.8341223078919206e-05, 1.3382597403092818e-05], 'topk_tokens': ['Bridge', '<|eot_id|>', '.', ' apple', 'assistant', ' Daniel', '?\n', 'Answer', '<|eot_id|>', ' Daniel', '185', ' garden', ' the', 'Daniel', 'way', '\n\n', ':', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [4.7242641448974604e-05, 0.001300908625125885, 0.00022739668687184653, 0.0010632693767547608, 0.00011423826217651368]}}, 27: {'grad': {'score': [0.341610107421875, 0.2953004414339901, 0.3009198132683249, 0.2951898114887748, 0.3651306629180908], 'topk_tokens': [' Hoe', ' was', ' hood', ' heart', 'IR', '�', 'imm', ' ear', '600', '202', '�', ' candidates', ' New', ' EAR', 'hand', ' business', ' mailing', ' sco', '�', '�'], 'evidence_proportions': [0.4780517578125, 0.2399587631225586, 0.3411184946695963, 0.3013580322265625, 0.32733154296875]}, 'weight': {'score': [0.026889855861663817, 0.0025286907644118495, 0.003094946636873133, 0.0024771584623207257, 0.0010980746962807395], 'topk_tokens': [' Daniel', '?\n', ' hallway', ' hallway', 'Daniel', 'Answer', ' Bridge', ' garden', ' lounge', 'assistant', '\n\n', '.\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.027389085292816164, 0.06606268882751465, 0.016636719306310017, 0.029701399803161624, 0.004544579982757568]}, 'saliency': {'score': [0.002173473834991455, 4.3970158164155364e-05, 0.00010145148810218362, 3.944325879255305e-05, 2.9052523049441252e-05], 'topk_tokens': [' top', ' Dan', ' hallway', ' bedroom', ' THE', 'assistant', ' hallway', ' Daniel', ' lounge', 'Daniel', ' Bridge', '<|end_header_id|>', ' garden', ':', ' Daniel', 'way', ' Daniel', '<|begin_of_text|>', '.\n\n', 'hall'], 'evidence_proportions': [0.0010659933090209962, 0.005872733891010284, 0.0011941989262898762, 0.0035375893115997315, 0.00013256072998046875]}}, 28: {'grad': {'score': [0.391463623046875, 0.41885084277240126, 0.41590213775634766, 0.41891522404402887, 0.3221798159859397], 'topk_tokens': ['house', ' house', '\n', 'ed', ' the', '\n', ' host', 'arp', ' home', ' line', ' houses', ' of', ' line', ' house', ' of', '***', ' hall', ' line', ' hall', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.7036376953125001, 0.26840972900390625, 0.2660954793294271, 0.3517578125, 0.3678802490234375]}, 'weight': {'score': [0.013463400602340699, 0.002433458237480255, 0.0049152207725188315, 0.0024039201399316313, 0.0004896000027656555], 'topk_tokens': [' the', ' hallway', ' to', ' garden', ' the', ' garden', '.\n\n', '?\n', 'Answer', ' the', '<|eot_id|>', 'assistant', 'hall', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0015389442443847657, 0.016068339347839355, 0.025536383191744488, 0.012749147415161134, 0.009530580043792723]}, 'saliency': {'score': [0.0007105588912963868, 4.40580204023399e-05, 9.252832216375014e-05, 4.2556173943002706e-05, 8.447603745893998e-06], 'topk_tokens': [' hallway', ' to', ' Daniel', ' the', ' garden', 'over', '<|eot_id|>', ' garden', ' Daniel', ' apple', ' Daniel', ' Bridge', '?\n', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':', 'way', '\n\n', 'hall'], 'evidence_proportions': [7.557868957519531e-05, 0.0008798986673355103, 0.001297419269879659, 0.0011426568031311034, 7.373690605163574e-05]}}, 29: {'grad': {'score': [0.270726318359375, 0.24173235439804658, 0.25460159077363853, 0.2416370129334958, 0.29024630784988403], 'topk_tokens': ['\n', '17', '\n', ' Get', '185', ' hardly', ' and', ' pur', '25', '\n', '\n', '\n', 'A', '.', '\n', '\n', '5', ' George', '\n', '\n'], 'evidence_proportions': [0.1949066162109375, 0.35343170166015625, 0.27167765299479163, 0.20941162109375, 0.3405548095703125]}, 'weight': {'score': [0.006639729738235473, 0.0024661217579675415, 0.0026016244116951436, 0.002457185792602266, 0.0006463368507948788], 'topk_tokens': [' Does', ' the', ' to', ' the', '<|start_header_id|>', ' to', ' ', '?\n', 'Answer', '.\n\n', 'hall', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|eot_id|>', 'way', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0025808513164520264, 0.00679779052734375, 0.016122808059056602, 0.0031149983406066895, 0.0027171969413757324]}, 'saliency': {'score': [0.00027588486671447753, 1.801082469251105e-05, 5.887887057136087e-05, 1.7368078251366853e-05, 2.3227185010910034e-05], 'topk_tokens': [' the', ':', ' where', ' the', 'Answer', '<|eot_id|>', ' the', ' to', 'assistant', ' Does', 'IVE', 'way', 'hall', '<|eot_id|>', '\n\n', ' ', '<|start_header_id|>', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00015364885330200196, 0.00038467347621917725, 0.0005899667739868164, 0.00013930201530456542, 7.077455520629883e-05]}}, 30: {'grad': {'score': [0.24581024169921875, 0.32606230466358616, 0.22298947502585018, 0.32651430415356253, 0.2820099267092618], 'topk_tokens': [' plank', ' Paul', '2', ' plant', ' Press', ' People', ' platform', ' paper', 'ar', ' preparations', ' Europe', ' paper', ' Paul', ' Press', ' hour', ' papers', ' boat', ' boat', ' Star', ' Loan'], 'evidence_proportions': [0.3466156005859375, 0.3240070343017578, 0.12151718139648438, 0.14235992431640623, 0.3350494384765625]}, 'weight': {'score': [0.014697517156600953, 0.002432200813495853, 0.0066814036930308626, 0.0023952006653264126, 0.0025726482272148132], 'topk_tokens': ['Question', ' the', '.', ' garden', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '.\n\n', 'Answer', '?\n', 'hall', '\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004140138626098633, 0.020852088928222656, 0.02325783173243205, 0.013055044412612914, 0.01170133352279663]}, 'saliency': {'score': [0.0005340600013732911, 5.0366824913507537e-05, 0.00033674958874197565, 4.8576360397295066e-05, 0.00010039826685732061], 'topk_tokens': [' Sandra', ' Dan', ' eighty', 'Question', '.', 'single', ' the', ' Sandra', ' hallway', ' lounge', 'Gov', ' the', ' garden', '<|end_header_id|>', ' Bridge', 'way', '<|start_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [0.0001918792724609375, 0.0006202831864356995, 0.0009301851193110148, 0.0005344808101654053, 0.00033149123191833496]}}, 31: {'grad': {'score': [0.250517383813858, 0.327383820295003, 0.25125404491144065, 0.32775374100326005, 0.16136031462387604], 'topk_tokens': [' intense', ' the', 'ford', ' w', ' draft', ' the', ' was', ' able', ' would', ',', ' was', ' determined', 'F', ' draft', ' we', ' these', ' United', ' the', ' had', ' Independent'], 'evidence_proportions': [0.24008176922798155, 0.2375354766845703, 0.23883056640625, 0.330596923828125, 0.20528316497802734]}, 'weight': {'score': [0.0033018994331359863, 0.00229506531147828, 0.0021710536059211284, 0.0022933466061754175, 0.000994408672506159], 'topk_tokens': [' was', ' where', ',', ' the', 'Question', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', ':', '\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002146339416503906, 0.004918873310089111, 0.002833535273869832, 0.0017471134662628172, 0.00528070330619812]}, 'saliency': {'score': [0.0002710855007171631, 2.2829926633784238e-05, 5.8654476614559395e-05, 2.222096796814851e-05, 8.398158983750777e-06], 'topk_tokens': [' Daniel', ' Where', 'ot', ':', ' Mary', ' ', '<|eot_id|>', '.\n\n', 'Question', ' the', ' Geo', ' hallway', ' hallway', '.', 'assistant', 'Answer', ':', '<|start_header_id|>', 'way', 'hall'], 'evidence_proportions': [0.0005818486213684082, 0.00012202560901641846, 0.00021158655484517414, 0.00014455318450927733, 0.0002775013446807861]}}, 'pred_res': "Daniel's hand.<|eot_id|>", 'score': 0}
2025-01-22 03:24:07.702 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:24:07.702 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-1_0-2-3-4-8.pkl | len: 10 |  size: 9.55 KB
Processing depth (0, 2, 3, 4, 8):   2%|▏         | 2/100 [00:38<31:14, 19.12s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (2, 5, 7, 8, 9):   2%|▏         | 2/100 [00:45<31:14, 19.12s/it]2025-01-22 03:24:14.837 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 03:24:14.844 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (2501, 2506) --> . Daniel travelled to the
2025-01-22 03:24:14.845 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 03:24:14.861 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (5968, 5972) -->  Daniel grabbed the apple
2025-01-22 03:24:14.861 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:24:14.885 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8388, 8394) --> . Daniel went back to the
2025-01-22 03:24:14.885 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel put down the apple.
2025-01-22 03:24:14.912 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9640, 9645) -->  Daniel put down the apple
2025-01-22 03:24:14.912 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:24:14.942 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10756, 10761) --> . Mary travelled to the
2025-01-22 03:24:14.942 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:24:14.971 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (10406, 10411) --> . John moved to the
2025-01-22 03:24:14.971 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:24:15.002 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10861, 10867) --> . Sandra journeyed to the
2025-01-22 03:24:15.002 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:24:15.002 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (67, 73) --> . John went back to the
2025-01-22 03:24:15.003 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:24:15.009 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2389, 2395) -->  the senate. Sandra went back
2025-01-22 03:24:15.010 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:24:15.015 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (2038, 2043) --> . Sandra moved to the
2025-01-22 03:24:15.015 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 03:24:15.022 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (2115, 2121) -->  John journeyed to the bedroom
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:24:17.157 | INFO     | test_jbb_retain:begin_test:632 - The garden.<|eot_id|>
2025-01-22 03:24:17.157 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12221])
your chose emoji: ['🥅', '🈸', '▫', '♈', '✊🏾', '👳🏼\u200d♂', '🚶🏾\u200d➡️', '🐐', '👨🏻\u200d❤\u200d👨🏿', '⛹🏾\u200d♂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 186413.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.59s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.10it/s][A
 88%|████████▊ | 7/8 [00:04<00:00,  2.26it/s][A100%|██████████| 8/8 [00:04<00:00,  1.63it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.14it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.98it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.88it/s][A100%|██████████| 8/8 [00:00<00:00, 22.21it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.90it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.60it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.67it/s][A100%|██████████| 8/8 [00:00<00:00, 21.86it/s]
2025-01-22 03:24:25.545 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.6571926879882812, 0.6820884624700896, 0.6382693122414982, 0.6822620954721297, 0.8349175375016009], 'topk_tokens': ['RI', 'street', ' readily', 'asc', ' regular', ' being', ' states', 'ridge', ' road', ' St', ' North', ' St', 'ily', 'AT', ' road', ' regular', 's', 'lyn', 'Spring', ' regular'], 'evidence_proportions': [0.49517364501953126, 0.768280029296875, 0.5374755859375, 0.8392578125, 0.691937255859375]}, 'weight': {'score': [0.021582168340682984, 0.0025493569711115974, 0.0037707458524142996, 0.0025068294325827378, 0.0037662401551105938], 'topk_tokens': ['<|start_header_id|>', '.', 'THE', ' the', ' Daniel', '.', '\n\n', 'Answer', ' garden', '.', '<|start_header_id|>', 'assistant', ':', '<|eot_id|>', '\n\n', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018980562686920165, 0.01772284507751465, 0.05373677611351013, 0.022792243957519533, 0.004558134078979492]}, 'saliency': {'score': [0.0011282145977020263, 5.787614908517967e-05, 0.00022602519568275004, 5.5206561843113205e-05, 0.00023856465933752842], 'topk_tokens': [' Daniel', '<|end_header_id|>', ' garden', '<|start_header_id|>', '<|start_header_id|>', ' Daniel', '<|eot_id|>', 'THE', '\n\n', '<|eot_id|>', 'Bridge', 'hall', ' top', '.', ' garden', 'assistant', '.', '\n\n', 'way', '<|begin_of_text|>'], 'evidence_proportions': [7.908940315246583e-05, 0.0013049319386482239, 0.0022914111614227295, 0.0015597045421600342, 0.00020864009857177735]}}, 25: {'grad': {'score': [0.6372207641601563, 1.0074864732033295, 0.9110098446116728, 1.008517040247982, 0.5565189924396452], 'topk_tokens': [' free', ' closing', ' daily', '600', ' especial', ' took', ' the', ' take', ' equal', 'administration', 'made', ' make', ' same', ' secret', ' taking', ' largely', ' different', ' secret', ' took', ' theoretical'], 'evidence_proportions': [0.661541748046875, 0.5447788238525391, 0.5486907958984375, 0.8275634765625, 0.60274658203125]}, 'weight': {'score': [0.02197247266769409, 0.0025583638258629446, 0.0037924831404405482, 0.002515017111704176, 0.005699613055244821], 'topk_tokens': [' Anthony', '.', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ' Daniel', '.', 'Answer', ' Daniel', ' George', 'hall', '<|start_header_id|>', ':', 'assistant', 'way', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006000816822052002, 0.03308713436126709, 0.03616365293661753, 0.033985137939453125, 0.0054110527038574215]}, 'saliency': {'score': [0.0014884769916534424, 6.304050586064449e-05, 8.306871442233815e-05, 6.0055148586832935e-05, 0.000549688202435853], 'topk_tokens': [' prior', ' Miles', '�', ' sample', ' Daniel', '�', ' Merch', ' Anthony', '<|start_header_id|>', '.\n\n', ' directly', 'way', ' George', ' Daniel', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [5.5670738220214844e-05, 0.003562361001968384, 0.0014079262812932334, 0.0024776816368103026, 0.0003696322441101074]}}, 26: {'grad': {'score': [0.4911747741699219, 0.5564318552067142, 0.6320989272173714, 0.5563544813125554, 0.6642952590692238], 'topk_tokens': [' Marshall', 'ipp', ' has', ' boat', 'their', '186', ' compl', ' the', '26', 're', ' Bre', ' tie', '\n', 'init', "'s", ' Knowledge', ' the', ' ga', ' Tie', '3'], 'evidence_proportions': [0.599383544921875, 0.6797943115234375, 0.42334938049316406, 0.37843627929687496, 0.42619934082031247]}, 'weight': {'score': [0.017844107151031494, 0.002510525481239039, 0.005537068142610437, 0.0024705549105665, 0.004259368923843884], 'topk_tokens': ['<|end_header_id|>', ' the', ' George', ' Anthony', '<|start_header_id|>', '.\n\n', ' Daniel', ' garden', '?\n', 'Answer', 'assistant', '\n\n', '<|start_header_id|>', 'hall', '<|eot_id|>', 'way', '<|eot_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0005657315254211426, 0.021616697311401367, 0.040646870930989586, 0.016404056549072267, 0.006181144714355468]}, 'saliency': {'score': [0.0008699929714202881, 6.414618811214158e-05, 0.00038527828805586873, 6.159257849596481e-05, 0.00017346466173891162], 'topk_tokens': [' Ramsey', ' Sandra', '<|eot_id|>', ' Daniel', '\n\n', '?\n', ' Col', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', ' garden', ' George', '<|eot_id|>', ' Anthony', 'way', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [3.6090612411499023e-05, 0.0009905248880386353, 0.0021006713310877485, 0.0008552908897399902, 0.0001453578472137451]}}, 27: {'grad': {'score': [0.3994122314453125, 0.5073614070552805, 0.3434315288768095, 0.508041418995134, 0.5267693097474145], 'topk_tokens': ['      ', '      ', ' try', '      ', '      ', ' Papers', '      ', '      ', '      ', '      ', '      ', ' ranks', '      ', '      ', ' every', '      ', 'AP', '      ', ' ready', 'Penn'], 'evidence_proportions': [0.4047607421875, 0.5291862487792969, 0.36829121907552087, 0.4502197265625, 0.2767822265625]}, 'weight': {'score': [0.020726318359375, 0.002535087119846444, 0.0034337788820266724, 0.002495190999755828, 0.004271089542107504], 'topk_tokens': [' George', ' THE', ' garden', 'THE', '.', ' Daniel', '?\n', ' Daniel', 'Answer', '.\n\n', 'assistant', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0011004805564880371, 0.03740191459655762, 0.04046246409416199, 0.01812286376953125, 0.005931758880615234]}, 'saliency': {'score': [0.0009894728660583495, 7.149421107706599e-05, 0.0001528412103652954, 6.938033813416982e-05, 0.00022575747771341292], 'topk_tokens': ['\n\n', '.', ' hallway', ' top', 'Answer', ' THE', ' Daniel', ' prior', ' garden', 'assistant', ' George', '<|start_header_id|>', '.\n\n', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ':', 'way', 'hall'], 'evidence_proportions': [3.549456596374512e-05, 0.0016298815608024597, 0.0018623222907384236, 0.0011778652667999267, 0.0001953125]}}, 28: {'grad': {'score': [0.44796661376953123, 0.5941131751574771, 0.5859642589793486, 0.59443629288742, 0.48792798401879484], 'topk_tokens': ['ot', 'ot', 'cont', 'str', ' sm', ' Min', 'ot', 'ot', 'ot', '\n\n\n\n\n\n\n', 'sim', 'ot', 'ot', 'Sh', ' w', ' Min', 'ot', ' W', ' W', ' W'], 'evidence_proportions': [0.47508544921874996, 0.437347412109375, 0.5960362752278645, 0.34249725341796877, 0.35712890625]}, 'weight': {'score': [0.019398161172866822, 0.002454305584518073, 0.008034423870198867, 0.0024038887812774757, 0.002562089044539655], 'topk_tokens': [' Daniel', 'Question', ' Daniel', ' ', '.', '.\n\n', ' the', ' Daniel', 'Answer', '?\n', 'assistant', 'hall', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0014207839965820312, 0.02002584934234619, 0.04296186069647471, 0.016285133361816407, 0.011709976196289062]}, 'saliency': {'score': [0.0005505907535552978, 5.0667136985594066e-05, 0.00027468975852517523, 4.901363435126703e-05, 0.00010844527697954021], 'topk_tokens': [' Far', '.\n\n', 'Probably', 'Question', ' Bridge', '.', ' Daniel', 'over', 'assistant', ' the', '<|end_header_id|>', '?\n', '<|start_header_id|>', '<|begin_of_text|>', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', ':', 'hall'], 'evidence_proportions': [3.493428230285645e-05, 0.000525742769241333, 0.0013920068740844727, 0.0004618346691131592, 0.00016518235206604005]}}, 29: {'grad': {'score': [0.3414105224609375, 0.452631086579168, 0.42883508345660043, 0.4529261608257051, 0.4326448284211706], 'topk_tokens': ['acc', ' would', 'ew', 'proc', 'enter', ' Wild', ' prov', ' Merch', ' Civil', ' regular', ' material', ' perpet', ' chronic', 'Ind', ' Civil', ' im', ' Civil', ' mort', 'ond', ' it'], 'evidence_proportions': [0.280755615234375, 0.31723785400390625, 0.31984965006510413, 0.39711914062500003, 0.3915679931640625]}, 'weight': {'score': [0.005871415138244629, 0.002478996652583177, 0.004413000801030327, 0.002466619619850852, 0.004601965185071602], 'topk_tokens': [' to', '\u200d', '.', '<|start_header_id|>', ' Does', ' the', ' ', '.\n\n', '?\n', 'Answer', 'hall', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', '\n\n', 'way', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008307516574859618, 0.0038574934005737305, 0.01320570210615794, 0.0037625312805175783, 0.005830955505371094]}, 'saliency': {'score': [0.00036574602127075194, 4.4195191125283066e-05, 8.365862509783576e-05, 4.342408323307563e-05, 0.0004711522430670066], 'topk_tokens': ['�', 'IVE', ' to', '.\n\n', 'ith', ' garden', ' ', ' garden', '<|start_header_id|>', '\u200d', '?\n', ':', '<|eot_id|>', 'hall', '\n\n', '<|start_header_id|>', 'way', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [3.411173820495605e-05, 0.00024463236331939697, 0.001112222671508789, 0.00014896392822265624, 0.00011528134346008302]}}, 30: {'grad': {'score': [0.323353271484375, 0.41544352027134124, 0.3457072762882008, 0.4158276788011481, 0.562530517578125], 'topk_tokens': [' count', 'deal', ' boat', ' d', ' Europe', ' or', ' an', 'AB', ' hour', ' Bor', 'D', '-per', ' S', ' Loan', ' W', ' S', ' Hor', ' EAR', ' in', ' M'], 'evidence_proportions': [0.2804931640625, 0.31111907958984375, 0.2818171183268229, 0.3243682861328125, 0.4248291015625]}, 'weight': {'score': [0.012621854543685912, 0.00244036519714675, 0.006006786928457373, 0.0024094736580980007, 0.009813377114593005], 'topk_tokens': [' ', ' the', '.', '<|end_header_id|>', ' Anthony', '<|start_header_id|>', ' Miles', '.\n\n', 'Answer', '?\n', 'assistant', 'hall', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00209154486656189, 0.014436006546020508, 0.022849897543589272, 0.012534332275390626, 0.009514713287353517]}, 'saliency': {'score': [0.0009721517562866211, 6.907303081289012e-05, 0.0002950000412323896, 6.65856911917552e-05, 0.0007093280065255087], 'topk_tokens': [' Daniel', '<|end_header_id|>', ' Miles', ' the', '?\n', '<|end_header_id|>', 'Answer', ' Daniel', ' George', ' Min', 'assistant', '�', 'hall', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'way', ':'], 'evidence_proportions': [4.705190658569336e-05, 0.0019844993948936462, 0.0015715708335240681, 0.0010366201400756835, 0.0003036022186279297]}}, 31: {'grad': {'score': [1.107542724609375, 1.2153867152348863, 1.141835381002987, 1.2158139117108027, 0.7184814703269083], 'topk_tokens': ['\n', '\n', ' a', ' the', '\n', '\n', '\n', '\n', ' the', ' per', '\n', ' short', ' a', '\n', ' of', '\n', '\n', ' the', 'the', '.'], 'evidence_proportions': [1.2162841796875, 0.92657470703125, 1.1527099609375, 1.112451171875, 1.084466552734375]}, 'weight': {'score': [0.0041118812561035155, 0.0022976297826667108, 0.0021996287738575656, 0.002294175261290927, 0.0020199608607370345], 'topk_tokens': [' discarded', ' the', ':', ' ', ',', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'assistant', '<|start_header_id|>', ':', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0010573506355285645, 0.005403280258178711, 0.0037530163923899336, 0.004618167877197266, 0.006057643890380859]}, 'saliency': {'score': [0.0003020918369293213, 0.00010714014133935823, 0.00016239899046280806, 0.00010658505763529359, 0.00019681502561100193], 'topk_tokens': ['<|end_header_id|>', ' Miles', ' hallway', '<|eot_id|>', '.\n\n', '\n\n', 'CH', ' discarded', ' ', '<|start_header_id|>', ':', '?\n', '<|end_header_id|>', 'assistant', 'Answer', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'hall', 'way'], 'evidence_proportions': [6.743669509887696e-05, 0.00046402961015701294, 0.0003727575143178304, 0.0003417551517486572, 0.00028273463249206543]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:24:25.547 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:24:25.547 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-2_2-5-7-8-9.pkl | len: 10 |  size: 9.69 KB
Processing depth (2, 5, 7, 8, 9):   3%|▎         | 3/100 [00:55<29:58, 18.54s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (1, 2, 4, 5, 7):   3%|▎         | 3/100 [01:03<29:58, 18.54s/it]2025-01-22 03:24:32.978 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 03:24:32.985 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1484, 1489) --> . Daniel travelled to the
2025-01-22 03:24:32.985 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 03:24:32.992 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2473, 2477) -->  Daniel grabbed the apple
2025-01-22 03:24:32.992 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:24:33.007 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (4902, 4908) --> . Daniel went back to the
2025-01-22 03:24:33.007 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel put down the apple.
2025-01-22 03:24:33.023 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5992, 5997) --> . Daniel put down the
2025-01-22 03:24:33.023 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:24:33.047 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (8455, 8460) --> . Mary travelled to the
2025-01-22 03:24:33.047 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:24:33.076 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (10460, 10465) --> . John moved to the
2025-01-22 03:24:33.076 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:24:33.107 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10909, 10915) --> . Sandra journeyed to the
2025-01-22 03:24:33.108 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:24:33.108 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (67, 73) --> . John went back to the
2025-01-22 03:24:33.108 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:24:33.115 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2338, 2344) --> . Sandra went back to the
2025-01-22 03:24:33.115 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:24:33.121 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (2040, 2045) --> . Sandra moved to the
2025-01-22 03:24:33.121 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 03:24:33.127 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (2100, 2106) --> . John journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:24:35.180 | INFO     | test_jbb_retain:begin_test:632 - The garden.<|eot_id|>
2025-01-22 03:24:35.180 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12264])
your chose emoji: ['🇨🇭', '🧑🏾\u200d❤️\u200d🧑🏼', '🏊🏻', '🚣🏽\u200d♂️', '💆🏽\u200d♀️', '👨\u200d❤\u200d👨', '👩🏿\u200d🦯\u200d➡', '👨🏼\u200d🦼\u200d➡', '👨🏾\u200d❤️\u200d💋\u200d👨🏻', '🗄️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 225197.53it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:39,  5.58s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.48s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.67it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.71it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.91it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.14it/s][A100%|██████████| 8/8 [00:00<00:00, 21.22it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.72it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.56it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.02it/s][A100%|██████████| 8/8 [00:00<00:00, 20.95it/s]
2025-01-22 03:24:44.608 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3672643280029297, 0.31298809217883855, 0.3550677019007066, 0.3127597490738135, 0.6688096339886005], 'topk_tokens': ['hand', ' doctors', '�', 'dr', 'ra', '�', 'ir', ' type', 'work', '�', 'men', 'Ind', '�', 'Frank', ' hand', 'hand', ' hand', ' hand', ' Project', ' hand'], 'evidence_proportions': [0.3476348876953125, 0.4547271728515625, 0.25300057729085285, 0.447259521484375, 0.3740447998046875]}, 'weight': {'score': [0.016828579902648924, 0.0025406724969147566, 0.005610395003767575, 0.002502863826340023, 0.0007901518390728877], 'topk_tokens': ['<|start_header_id|>', '\n\n', '?\n', '.', 'Bridge', 'Answer', '.', ' Daniel', ' garden', ' Bridge', '<|start_header_id|>', ':', 'assistant', '\n\n', '<|eot_id|>', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0004361748695373535, 0.013744354248046875, 0.01515662670135498, 0.0493067741394043, 0.005216515064239502]}, 'saliency': {'score': [0.00046685099601745606, 3.543429321440817e-05, 0.00015900766148286706, 3.420666280064639e-05, 2.1460537727062518e-05], 'topk_tokens': [' Eighth', ' West', ' Daniel', ' THE', ' Market', '.', ' old', ' Miles', ' bedroom', ' Bridge', '<|eot_id|>', '\n\n', '.', ' garden', '<|end_header_id|>', 'way', 'Bridge', ' garden', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [7.3015689849853516e-06, 0.0006386414170265198, 0.0003398507833480835, 0.00128173828125, 0.0001264810562133789]}}, 25: {'grad': {'score': [0.6803903198242187, 0.8069985466597375, 0.6029566596536076, 0.8078260884216228, 0.5115104822012094], 'topk_tokens': ['.', ' had', ' per', 'ad', ' two', ' large', 'a', ' ST', '\n', ' two', ' had', ' great', ' George', ' great', ' state', ' had', ' post', ' too', 'st', ' post'], 'evidence_proportions': [0.769891357421875, 0.700042724609375, 0.5449778238932291, 0.92744140625, 0.49061126708984376]}, 'weight': {'score': [0.01690011262893677, 0.002546339034256749, 0.0023060949409709256, 0.002517613932627057, 0.0008961228797068963], 'topk_tokens': [' East', '<|start_header_id|>', ' Ot', ' garden', '.\n\n', '?\n', 'Answer', 'hall', '<|start_header_id|>', '.', ' Daniel', ':', '.', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00023552775382995605, 0.016116619110107422, 0.012198567390441895, 0.049170106649398804, 0.007563352584838867]}, 'saliency': {'score': [0.0010189032554626465, 3.7167402607037815e-05, 5.529470303479363e-05, 3.510648152775096e-05, 1.8823032195751483e-05], 'topk_tokens': [':', '<|start_header_id|>', ' garden', ' Ramsey', '.', ' Ramsey', '.', '.', ' Daniel', 'way', ' Market', 'assistant', ' Dan', ' East', ' Daniel', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.3715028762817382e-05, 0.0007183551788330078, 0.001017749309539795, 0.0025915443897247315, 0.0006932735443115235]}}, 26: {'grad': {'score': [0.41746414184570313, 0.40289595720479743, 0.46220914055319395, 0.40270093321643996, 0.44325962433448207], 'topk_tokens': [' City', 'ch', ' extra', 'ine', ' of', ' North', ' and', ' never', ' be', '3', 'istributed', 'ian', ' not', ' Out', 'asca', 'ucci', 'hall', 'outs', ' it', ' not'], 'evidence_proportions': [0.5133804321289063, 0.5077629089355469, 0.3597304026285807, 0.394989013671875, 0.341064453125]}, 'weight': {'score': [0.010674992799758911, 0.002486966957893972, 0.004650951308362624, 0.0024641723876152564, 0.0010946797063717474], 'topk_tokens': [' garden', '.', '<|start_header_id|>', ' Bridge', '.\n\n', ' garden', ' Daniel', '?\n', 'Answer', 'assistant', ' the', '\n\n', '<|start_header_id|>', 'hall', '<|eot_id|>', 'way', '<|end_header_id|>', '<|eot_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00021732449531555175, 0.00938272476196289, 0.006872877478599548, 0.030286538600921634, 0.007117468118667603]}, 'saliency': {'score': [0.0006158709526062012, 6.107008718594932e-05, 0.00018487870693206787, 5.958913086985542e-05, 2.4824761427365818e-05], 'topk_tokens': [' Arr', ' John', 'Answer', ' Ramsey', ' Bridge', ' Anthony', 'Bridge', ' garden', ' garden', '<|eot_id|>', '<|start_header_id|>', '?\n', ' the', ' Daniel', '\n\n', 'way', '<|end_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [1.5658140182495117e-05, 0.000637643039226532, 0.00023249288400014242, 0.002031320333480835, 0.0002432703971862793]}}, 27: {'grad': {'score': [0.32723846435546877, 0.3069981884196421, 0.3172748790067785, 0.30692811843452106, 0.37115967732209426], 'topk_tokens': [' majority', ' more', ' attempt', '240', '600', ' was', '202', '�', ' Bench', ' school', ' sco', ' heart', ' mailing', ' participate', ' hood', ' candidates', ' hand', ' EAR', ' business', 'hand'], 'evidence_proportions': [0.49540100097656253, 0.181793212890625, 0.2701988220214844, 0.303216552734375, 0.367901611328125]}, 'weight': {'score': [0.018053783178329466, 0.002529930580582607, 0.003913515630890341, 0.0024942868873769927, 0.0012573502384699308], 'topk_tokens': [' the', ' THE', ' bedroom', '?\n', ' Bridge', 'Answer', '.', ' garden', '\n\n', '.\n\n', 'assistant', ' Daniel', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.000388866662979126, 0.02077329158782959, 0.01154184341430664, 0.05193842053413391, 0.007472783327102661]}, 'saliency': {'score': [0.0009449887275695801, 4.518224245141225e-05, 0.00017238715115715475, 4.2985311830184405e-05, 3.4837482067254874e-05], 'topk_tokens': [' Robert', ' Daniel', ' East', 'Bridge', ' bedroom', ' hallway', ' Dan', '<|start_header_id|>', ':', ' garden', 'assistant', ' Daniel', '<|end_header_id|>', ' THE', ' Bridge', '.', '<|begin_of_text|>', 'way', '.\n\n', 'hall'], 'evidence_proportions': [2.6154518127441407e-05, 0.0007392019033432007, 0.0009839236736297607, 0.0022672176361083984, 0.0006595015525817871]}}, 28: {'grad': {'score': [0.37330123901367185, 0.4157190619077097, 0.38703262104707603, 0.4158858201450633, 0.3065125896380498], 'topk_tokens': [' vigorous', 'ial', ' host', ' hallway', 'urred', 'arp', ' law', ' locate', 'arch', '\n\n\n\n\n\n\n', 'ed', ' line', ' law', 'ched', ' line', 'burg', ' hall', 'ching', ' hall', ' line'], 'evidence_proportions': [0.4412925720214844, 0.317840576171875, 0.4155171712239583, 0.33289794921875, 0.339422607421875]}, 'weight': {'score': [0.008618783950805665, 0.0024463846833740694, 0.004562285016564762, 0.0024278517055714584, 0.0006967891867344196], 'topk_tokens': [' to', ' Bridge', ' ', ' discarded', '.\n\n', ' Daniel', 'Answer', '?\n', ' the', ' garden', 'assistant', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00030495524406433107, 0.0033178627490997314, 0.003380844990412394, 0.02276363968849182, 0.01331402063369751]}, 'saliency': {'score': [0.00033455729484558103, 4.766923617236021e-05, 8.044961620779599e-05, 4.6990440760496246e-05, 1.2980630764594445e-05], 'topk_tokens': [' to', 'Question', ' discarded', ' Bridge', '.\n\n', 'Bridge', '<|eot_id|>', 'assistant', ' apple', ' Daniel', '?\n', ' the', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', 'way', '<|begin_of_text|>', '\n\n', ':', 'hall'], 'evidence_proportions': [2.491474151611328e-06, 0.00010586529970169067, 0.0001423756281534831, 0.001125001907348633, 0.0002897500991821289]}}, 29: {'grad': {'score': [0.26896209716796876, 0.27601184317872646, 0.22681920668658087, 0.2761632843067574, 0.31373034990750825], 'topk_tokens': [' a', ' into', ' pur', ' by', '5', ' often', ' by', ' of', ' had', ' was', ' by', ' for', ' of', ' of', '\n', ' from', 'br', '000', ' George', 'A'], 'evidence_proportions': [0.3328826904296875, 0.22875595092773438, 0.22335942586263022, 0.270916748046875, 0.28997497558593754]}, 'weight': {'score': [0.003228987455368042, 0.00246956214895456, 0.0028933260370703306, 0.0024668267619250487, 0.0006350929347368387], 'topk_tokens': [' part', '.', ' garden', '<|start_header_id|>', ' Does', ' the', ' ', '.\n\n', 'Answer', '?\n', 'hall', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', 'way', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.196498870849609e-05, 0.002000093460083008, 0.0033749242623647055, 0.003718632459640503, 0.006714355945587158]}, 'saliency': {'score': [0.00015415310859680177, 2.323157233209222e-05, 0.00011983163216534783, 2.2694429438829735e-05, 2.1071388171269344e-05], 'topk_tokens': [' garden', ' PA', '.', '.\n\n', '?\n', ' the', 'Answer', 'assistant', 'IVE', ' ', ' Does', '<|eot_id|>', '<|eot_id|>', 'way', ':', 'hall', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [3.170967102050781e-06, 6.215274333953857e-05, 0.0002728402614593506, 0.00024113655090332033, 0.00014932751655578614]}}, 30: {'grad': {'score': [0.23142494201660158, 0.3127114910207518, 0.2070147850934197, 0.3131723242224747, 0.28810738829466015], 'topk_tokens': ['ar', 'doll', ' papers', ' M', '0', ' in', ' S', ' Star', ' account', 'deal', '2', ' boat', ' Bridge', ' boat', ' hour', ' Paul', ' Europe', ' Loan', ' Malta', ' an'], 'evidence_proportions': [0.2953224182128906, 0.2211923599243164, 0.2173121770222982, 0.14487762451171876, 0.27919616699218747]}, 'weight': {'score': [0.011634994745254517, 0.002459430904486384, 0.007235906579915215, 0.0024273380744582237, 0.0023540384494341337], 'topk_tokens': [' garden', 'Question', '<|end_header_id|>', '<|eot_id|>', ' the', '<|start_header_id|>', '.\n\n', 'Answer', ' Miles', '\n\n', 'hall', 'assistant', '?\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005088508129119872, 0.00599980354309082, 0.005910436312357585, 0.016660082340240478, 0.02911367416381836]}, 'saliency': {'score': [0.0003273355960845947, 4.8990963218050226e-05, 0.0003141375148997587, 4.76825098613299e-05, 0.00010410065834338848], 'topk_tokens': [' Broadway', 'IR', ' Sandra', '<|eot_id|>', ' the', ' Min', 'assistant', 'Gov', '.\n\n', '?\n', ' Robert', '<|end_header_id|>', ' Bridge', '<|eot_id|>', 'way', ' Miles', '<|start_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [6.22868537902832e-06, 0.00010627508163452148, 0.00017974277337392172, 0.0004273176193237305, 0.0009024202823638916]}}, 31: {'grad': {'score': [0.3849997711181641, 0.483157122991308, 0.34720577913172107, 0.4837367658065156, 0.2666403049459824], 'topk_tokens': ['f', 'u', ' ever', ' the', ' the', ' would', ' the', ' the', ' able', ' government', ' the', ' F', ' these', 'F', ' had', ' have', ' the', ' Independent', ' the', ' United'], 'evidence_proportions': [0.4711883544921875, 0.3347773551940918, 0.3716449737548828, 0.3988067626953125, 0.3412078857421875]}, 'weight': {'score': [0.0029686319828033446, 0.0022874182065716676, 0.002208900802275714, 0.002286241867068086, 0.0009068067257220929], 'topk_tokens': [' discarded', ' ', ' the', ',', ':', 'Question', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '\n\n', 'assistant', '<|start_header_id|>', ':', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00022394657135009767, 0.001802980899810791, 0.0010371307531992593, 0.004321455955505371, 0.007610815763473511]}, 'saliency': {'score': [5.520224571228027e-05, 3.396889210485874e-05, 6.900114171645221e-05, 3.382784276614808e-05, 1.376609389598553e-05], 'topk_tokens': [' Mess', '.\n\n', '<|eot_id|>', ' ', ' the', ' Ramsey', 'CH', '.', 'Question', '<|eot_id|>', ' Miles', '<|end_header_id|>', '?\n', 'Answer', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', ':', 'hall', 'way'], 'evidence_proportions': [1.3422966003417968e-05, 5.5149197578430176e-05, 4.5165419578552246e-05, 0.00012475848197937014, 3.9511919021606444e-05]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:24:44.609 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:24:44.609 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-3_1-2-4-5-7.pkl | len: 10 |  size: 9.54 KB
Processing depth (1, 2, 4, 5, 7):   4%|▍         | 4/100 [01:15<29:59, 18.75s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.27it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
Processing depth (0, 4, 6, 7, 8):   4%|▍         | 4/100 [01:22<29:59, 18.75s/it]2025-01-22 03:24:51.990 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 03:24:51.991 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 03:24:51.991 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 03:24:52.004 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4884, 4888) -->  Daniel grabbed the apple
2025-01-22 03:24:52.004 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 03:24:52.025 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7185, 7191) --> . Daniel went back to the
2025-01-22 03:24:52.025 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel put down the apple.
2025-01-22 03:24:52.049 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8388, 8393) --> . Daniel put down the
2025-01-22 03:24:52.049 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:24:52.076 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (9654, 9659) -->  Mary travelled to the bedroom
2025-01-22 03:24:52.076 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John moved to the garden.
2025-01-22 03:24:52.106 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (10465, 10470) --> . John moved to the
2025-01-22 03:24:52.106 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 03:24:52.137 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (10872, 10878) --> . Sandra journeyed to the
2025-01-22 03:24:52.137 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John went back to the office.
2025-01-22 03:24:52.138 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (73, 79) --> . John went back to the
2025-01-22 03:24:52.138 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:24:52.145 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2432, 2438) --> . Sandra went back to the
2025-01-22 03:24:52.145 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 03:24:52.151 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (2061, 2066) --> . Sandra moved to the
2025-01-22 03:24:52.151 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 03:24:52.157 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (2119, 2125) -->  John journeyed to the bedroom
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:24:54.225 | INFO     | test_jbb_retain:begin_test:632 - The garden.<|eot_id|>
2025-01-22 03:24:54.226 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12226])
your chose emoji: ['💼', '🧑🏽\u200d🦽', '🧙🏾\u200d♂️', '✋🏿', '👨🏽\u200d💼', '🇨🇾', '🇧🇻', '⛹\u200d♀️', '🏃\u200d♂\u200d➡', '🌽']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 188508.04it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.32s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.04s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.62it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.45it/s][A100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.16it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.56it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 19.55it/s][A100%|██████████| 8/8 [00:00<00:00, 20.00it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.30it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.92it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.27it/s][A100%|██████████| 8/8 [00:00<00:00, 20.25it/s]
2025-01-22 03:25:03.767 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.2627934265136719, 0.2402089407734688, 0.25633520238539753, 0.2401174941803455, 0.47272306500059186], 'topk_tokens': [' fear', ' short', ' CONNECT', 'Ind', ' clear', ' successful', 'le', 'ir', ' Wide', ' Aw', 'sur', 'hand', ' Project', ' Wide', ' hand', ' hand', ' hand', 'hand', ' hand', 'Sh'], 'evidence_proportions': [0.221484375, 0.33086395263671875, 0.20619583129882812, 0.302032470703125, 0.2783241271972656]}, 'weight': {'score': [0.017963713407516478, 0.0025460579884842845, 0.005218262181562536, 0.002506921067527796, 0.001102029825701858], 'topk_tokens': ['.\n\n', '\n\n', ' Daniel', '?\n', '\n\n', ' Daniel', 'Answer', ':', ' Bridge', '<|start_header_id|>', '.', ' garden', 'assistant', '<|eot_id|>', '\n\n', '<|eot_id|>', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.009407019615173338, 0.03036785125732422, 0.011806805928548178, 0.03386041522026062, 0.008088684082031249]}, 'saliency': {'score': [0.0010467016696929933, 3.1232039393656524e-05, 0.0002059261588489308, 2.865797687771875e-05, 2.2683631290089e-05], 'topk_tokens': [' Foster', ' old', '\n\n', ' bedroom', 'Answer', 'Bridge', ' hallway', 'assistant', 'Daniel', '\n\n', ' Bridge', '.', ' Daniel', ' Daniel', '<|end_header_id|>', ' garden', 'way', ' garden', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0002274155616760254, 0.0026807039976119995, 0.00026442110538482666, 0.002107459306716919, 0.00043676495552062987]}}, 25: {'grad': {'score': [0.4644420051574707, 0.57286590468865, 0.46361766142003674, 0.573393845342669, 0.4678494424530954], 'topk_tokens': [' get', ' g', ' daily', ' had', ' typ', 'g', ' possessed', 'st', ' great', ' rest', ' getting', 'ense', ' glo', 'ated', ' George', ' great', ' patron', ' post', ' genu', ' post'], 'evidence_proportions': [0.5200653076171875, 0.45811009407043457, 0.45646723111470544, 0.56702880859375, 0.32086715698242185]}, 'weight': {'score': [0.017793768644332887, 0.0025469574634671473, 0.002031288602772881, 0.0025170776328789766, 0.0010532818057320335], 'topk_tokens': [' THE', ' prior', ' to', '?\n', 'Answer', '.\n\n', ' garden', 'hall', ' Daniel', ' Daniel', '<|start_header_id|>', '.', ':', 'assistant', '<|eot_id|>', 'way', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011240482330322266, 0.04926109313964844, 0.0072820186614990234, 0.03530175089836121, 0.004395747184753418]}, 'saliency': {'score': [0.00038333892822265623, 2.428292935706967e-05, 3.372395739835851e-05, 2.3518969215324108e-05, 2.753147573182077e-05], 'topk_tokens': [' Marshall', ' Marshall', ' to', '.', 'Gov', ' the', ' Dan', ' THE', 'way', '<|start_header_id|>', ' Merch', 'assistant', 'hall', ' garden', ' Daniel', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [4.583597183227539e-05, 0.001571469008922577, 8.147458235422769e-05, 0.00040078163146972656, 0.00011513233184814454]}}, 26: {'grad': {'score': [0.34660369873046876, 0.36946755662768827, 0.3669562620275161, 0.3695215402319476, 0.353969111587062], 'topk_tokens': ['ine', 'ol', ' of', ':', ' two', ' of', ' and', ' extra', ' be', ' not', ' never', ',', 'outs', ' it', ' and', 'ian', 'asca', ' not', '3', 'ucci'], 'evidence_proportions': [0.3631439208984375, 0.3535499572753906, 0.35682932535807294, 0.3420562744140625, 0.31678314208984376]}, 'weight': {'score': [0.016203314065933228, 0.002485283378958556, 0.0038335244445239797, 0.0024533367098210383, 0.0008850458896521366], 'topk_tokens': ['<|start_header_id|>', ' discarded', ' Bridge', '.', ' Daniel', '.\n\n', ' the', '?\n', 'Answer', ' garden', 'assistant', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'hall', '<|eot_id|>', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004879093170166016, 0.026576519012451172, 0.011622309684753418, 0.028585988283157348, 0.012343502044677733]}, 'saliency': {'score': [0.0006088244915008544, 4.4171189587273575e-05, 7.543669027440688e-05, 4.2923912712072095e-05, 1.8528013518362336e-05], 'topk_tokens': [' Father', 'Answer', '<|eot_id|>', ' kitchen', 'assistant', ' Daniel', '<|eot_id|>', ' garden', '?\n', '185', ' the', ' Daniel', '<|start_header_id|>', 'way', ' garden', '\n\n', '<|end_header_id|>', ':', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [5.543828010559082e-05, 0.001016475260257721, 0.00020991265773773193, 0.001723027229309082, 0.00020058155059814452]}}, 27: {'grad': {'score': [0.26075836181640627, 0.22975977592646066, 0.22297502966488109, 0.2297150525678448, 0.29417066140608356], 'topk_tokens': [' majority', ' more', ' EAR', ' candidate', ' hood', ' Hoe', '�', ' New', ' participate', ' difficulty', ' majority', ' heart', '202', ' hand', '600', ' mailing', ' sco', ' candidates', ' business', 'hand'], 'evidence_proportions': [0.38233642578125, 0.21524810791015625, 0.20997492472330728, 0.24227294921874998, 0.2550140380859375]}, 'weight': {'score': [0.02232659935951233, 0.0025328745269962553, 0.0055849736227708705, 0.0024836868121179286, 0.001388717329863346], 'topk_tokens': [' Bridge', ' bedroom', ' THE', ' Daniel', '?\n', ' Daniel', 'Answer', '.', '\n\n', 'assistant', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' garden', '<|start_header_id|>', ':', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.01639147400856018, 0.04746246337890625, 0.00849453608194987, 0.03546030521392822, 0.0116178035736084]}, 'saliency': {'score': [0.0015855205059051513, 3.748228265508255e-05, 0.00016857858966378605, 3.393600245627009e-05, 4.107166420329701e-05], 'topk_tokens': [' bedroom', 'Daniel', '.', ' Dan', 'Gov', ' hallway', ' hallway', 'assistant', 'Answer', '<|start_header_id|>', ' THE', '<|end_header_id|>', ' Daniel', ' Daniel', '<|begin_of_text|>', ' garden', ':', 'way', '.\n\n', 'hall'], 'evidence_proportions': [0.0008744478225708008, 0.0037773028016090393, 0.0003888408342997233, 0.00314105749130249, 0.00042364597320556643]}}, 28: {'grad': {'score': [0.31256317138671874, 0.3397621468595398, 0.356321979971493, 0.33977175573883434, 0.24510398055567886], 'topk_tokens': [' hall', 'burg', ' of', ' hallway', ' house', ' *', ' *', '***', ' *', 'ching', ' host', ' houses', ' line', ' line', ' house', 'arp', ' line', ' hall', ' hall', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.5804931640625, 0.23059844970703125, 0.2516886393229167, 0.23520660400390625, 0.2606109619140625]}, 'weight': {'score': [0.0128242826461792, 0.0024309958447730788, 0.004448478712755091, 0.002404009272254875, 0.0006143527500557177], 'topk_tokens': [' the', ' the', ' to', ' garden', ' Daniel', ' garden', '.\n\n', ' the', '?\n', 'Answer', '<|eot_id|>', 'assistant', 'hall', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0007496535778045653, 0.01237177848815918, 0.012029985586802164, 0.025221139192581177, 0.013817214965820312]}, 'saliency': {'score': [0.00039517641067504883, 3.759882097028245e-05, 6.671074558706845e-05, 3.678294215519701e-05, 9.575124942895139e-06], 'topk_tokens': ['.', 'Right', ' discarded', 'Question', ':', 'Probably', '<|eot_id|>', ' Bridge', ' apple', '<|eot_id|>', ' the', ' Daniel', '?\n', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'way', '\n\n', ':', 'hall'], 'evidence_proportions': [3.180503845214844e-05, 0.0003074035048484802, 0.0003549059232076009, 0.0009866058826446533, 0.00028566122055053713]}}, 29: {'grad': {'score': [0.27181785583496093, 0.24204459717934826, 0.2232840201433967, 0.24203584846553003, 0.22490708936344495], 'topk_tokens': ['800', ' probably', ' of', '\n', '\n', ' and', ' very', ' was', ' hardly', '000', ' for', '25', ' of', ' Get', '\n', 'A', ' pur', '5', ' George', '\n'], 'evidence_proportions': [0.16749267578125, 0.2366790771484375, 0.3906351725260417, 0.228997802734375, 0.3044933319091797]}, 'weight': {'score': [0.004279241561889648, 0.0024702736728763977, 0.002457719515351688, 0.002466592706905199, 0.0007097915266499375], 'topk_tokens': [' Does', ' place', ' ', ' was', ' where', '<|start_header_id|>', ' to', '?\n', 'Answer', '.\n\n', 'hall', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010354936122894288, 0.0052835941314697266, 0.0051482270161310835, 0.004429781436920166, 0.005526185035705566]}, 'saliency': {'score': [0.00019896388053894043, 1.6880414113359717e-05, 8.295038167168113e-05, 1.6321789170251878e-05, 1.9050908811164625e-05], 'topk_tokens': [' the', ' the', ' the', ' Does', ' to', ' to', 'hall', 'Answer', ' ', '.\n\n', 'IVE', 'way', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.2907724380493165e-05, 0.00010389834642410278, 0.00041705866654713947, 0.00011093020439147949, 0.00026739239692687985]}}, 30: {'grad': {'score': [0.23048253059387208, 0.2699575527913924, 0.17283159143784466, 0.2703099897873627, 0.20517597957090897], 'topk_tokens': [' S', ' Press', ' Republican', ' capital', 'ar', ' preparations', ' Malta', ' Bridge', '0', ' paper', ' Paul', 'deal', '2', ' Paul', ' Star', ' Europe', ' hour', ' boat', ' boat', ' Loan'], 'evidence_proportions': [0.2733726024627685, 0.24675607681274414, 0.19134386380513507, 0.1921875, 0.25983505249023436]}, 'weight': {'score': [0.01652525782585144, 0.002440131668838225, 0.006433986565646003, 0.0024000397033150703, 0.0023971981171405678], 'topk_tokens': [' to', 'Question', ' garden', '.', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'Answer', '\n\n', '?\n', '.\n\n', 'assistant', 'hall', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0025044679641723633, 0.02180767059326172, 0.015477657318115234, 0.025301092863082887, 0.018801403045654294]}, 'saliency': {'score': [0.000515519380569458, 4.7838417874868956e-05, 0.00021917592076694265, 4.639901777908463e-05, 7.907188299930456e-05], 'topk_tokens': [' Sandra', ' the', ' hallway', ' Daniel', 'Question', '<|eot_id|>', ' Daniel', '.\n\n', ' Miles', 'assistant', '<|eot_id|>', ' Bridge', ' garden', 'Gov', '<|end_header_id|>', 'way', '<|start_header_id|>', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [0.00010182857513427734, 0.0008045509457588196, 0.0008097986380259196, 0.0006124556064605712, 0.00024791359901428224]}}, 31: {'grad': {'score': [0.271667891740799, 0.3563343671660985, 0.2514675364774816, 0.3568012639721006, 0.16309008905381867], 'topk_tokens': ['7', ' we', ' able', ' the', ' United', ' wield', ' was', ' the', ' these', ' had', ',', ' F', ' have', ' the', ' government', ' would', ' w', 'F', ' Independent', ' the'], 'evidence_proportions': [0.23787002563476564, 0.2552652359008789, 0.28856913248697913, 0.3237548828125, 0.246219402551651]}, 'weight': {'score': [0.004138554334640503, 0.002281437345962219, 0.0026655705536113065, 0.002276549223219653, 0.0009782413641611736], 'topk_tokens': [' to', ' was', ',', ' the', 'Question', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '\n\n', ':', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00146980881690979, 0.003885924816131592, 0.0025482972462972002, 0.0067762255668640135, 0.006280040740966797]}, 'saliency': {'score': [0.00019436240196228028, 1.741534496369449e-05, 5.055438069736256e-05, 1.6959272355649347e-05, 1.009576248400139e-05], 'topk_tokens': [' the', '<|eot_id|>', '<|end_header_id|>', '.', ' Where', ' Daniel', ':', 'assistant', ' hallway', 'Question', ' hallway', '<|eot_id|>', '<|begin_of_text|>', '?\n', '.\n\n', ':', 'Answer', '<|start_header_id|>', 'hall', 'way'], 'evidence_proportions': [0.0003494739532470703, 0.00011163204908370972, 0.00013746817906697592, 0.00028463602066040037, 8.34345817565918e-05]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:25:03.768 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:25:03.768 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-4_0-4-6-7-8.pkl | len: 10 |  size: 9.61 KB
Processing depth (0, 4, 6, 7, 8):   5%|▌         | 5/100 [01:34<29:55, 18.89s/it]Processing depth (0, 4, 6, 7, 8):   5%|▌         | 5/100 [01:34<29:58, 18.93s/it]
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:72 - Selected idx: 18
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bathroom? 
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:75 - Tag: 3-hop
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:76 - Needle: [' John went back to the office.', ' Sandra moved to the kitchen.', ' Daniel went to the hallway.', ' John moved to the garden.', ' John journeyed to the bedroom.', ' Sandra went back to the hallway.', ' Daniel travelled to the bathroom.', ' Sandra journeyed to the office.', ' Daniel dropped the apple there.', ' Mary travelled to the bedroom.']
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel went to the hallway.', ' Daniel travelled to the bathroom.', ' Daniel dropped the apple there.', ' Mary travelled to the bedroom.']
2025-01-22 03:25:04.209 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]
Processing depth (0, 1, 3, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:25:11.152 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:25:11.152 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 03:25:11.152 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 03:25:11.157 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (1536, 1541) --> . Daniel travelled to the
2025-01-22 03:25:11.157 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 03:25:11.167 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3757, 3762) --> . Daniel dropped the apple
2025-01-22 03:25:11.167 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:25:11.197 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10760, 10765) --> . Mary travelled to the
2025-01-22 03:25:11.197 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:25:11.210 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4508, 4514) --> . John went back to the
2025-01-22 03:25:11.210 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:25:11.218 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3175, 3180) --> . Sandra moved to the
2025-01-22 03:25:11.219 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:25:11.228 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3333, 3338) --> . John moved to the
2025-01-22 03:25:11.228 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 03:25:11.244 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5713, 5719) --> . John journeyed to the
2025-01-22 03:25:11.244 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 03:25:11.275 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10828, 10834) --> . Sandra went back to the
2025-01-22 03:25:11.275 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 03:25:11.301 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (9157, 9163) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:25:13.408 | INFO     | test_jbb_retain:begin_test:632 - Daniel's hand<|eot_id|>
2025-01-22 03:25:13.409 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12205])
your chose emoji: ['🇪🇬', '🧎🏿\u200d➡', '😣', '🎥', '🕑', '🤲🏾', '\U0001faf0🏽', '🧚🏻', '👩🏻\u200d🦯', '🕵🏻\u200d♂']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 246723.76it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:32,  4.68s/it][A
 38%|███▊      | 3/8 [00:04<00:06,  1.25s/it][A
 75%|███████▌  | 6/8 [00:04<00:01,  1.96it/s][A100%|██████████| 8/8 [00:04<00:00,  1.60it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.67it/s][A
 50%|█████     | 4/8 [00:00<00:00, 19.37it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 21.76it/s][A100%|██████████| 8/8 [00:00<00:00, 21.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.50it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.10it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.98it/s][A100%|██████████| 8/8 [00:00<00:00, 21.23it/s]
2025-01-22 03:25:21.933 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.4373464584350586, 0.4106614899041612, 0.46147582110236673, 0.41047542880235466, 0.7027681350708008], 'topk_tokens': [' way', '�', ' typ', ' turned', ' Aw', ' set', '\n', ' Wide', '�', 'respect', ' high', 'hand', ' Wide', ' Project', ' type', ' hand', 'hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.3877685546875, 0.35880584716796876, 0.5350624084472657, 0.46774902343749997]}, 'weight': {'score': [0.03641410320997238, 0.002556924973059138, 0.006811226115507238, 0.0024893102121918104, 0.0009046365817387898], 'topk_tokens': [' the', '.', ' Daniel', 'Bridge', ' bathroom', 'Answer', ' circus', ':', '<|start_header_id|>', ' bathroom', ' Bridge', 'assistant', '<|eot_id|>', ' hallway', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0662663459777832, 0.027965134382247927, 0.049419629573822024, 0.002005302906036377]}, 'saliency': {'score': [0.0018500000238418578, 4.322397453053751e-05, 0.0003423752153621, 3.9413980851542975e-05, 2.8644998868306477e-05], 'topk_tokens': [' Hotel', ' dropped', ' the', ' Dan', ' old', ' office', 'Daniel', ' Daniel', ' circus', ' Bridge', '\n\n', ' Daniel', '.', ' bathroom', ' garden', 'way', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0008017659187316895, 0.0033000230789184573, 0.0032353997230529785, 6.281137466430664e-05]}}, 25: {'grad': {'score': [0.5273113250732422, 0.8724652411584617, 0.5127599379595589, 0.8740394602328789, 0.7147914886474609], 'topk_tokens': ['be', ' cause', ' possessed', ' work', 'ad', ' of', ' Az', ' a', ' squ', ' which', '10', 'mand', ' per', ' case', 'which', 'ate', ' success', ' post', 'po', ' post'], 'evidence_proportions': [0.42672576904296877, 0.520660400390625, 0.591143798828125, 0.57071533203125]}, 'weight': {'score': [0.01834259182214737, 0.002549404944848577, 0.0035502332098343793, 0.002520616735324511, 0.000956579049428304], 'topk_tokens': [' the', ' Daniel', '.', ' apple', ' Daniel', ' \n', '.\n\n', 'Answer', ' Dan', "'s", '<|start_header_id|>', 'hall', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.010886335372924804, 0.019190007448196413, 0.041051679849624635, 0.002242344617843628]}, 'saliency': {'score': [0.0016716331243515014, 4.6206308653282773e-05, 5.016081473406623e-05, 4.352052705704195e-05, 3.8718183835347496e-05], 'topk_tokens': [' Merch', ' press', ' Merch', ' Daniel', ' there', 'Daniel', ' Marshall', ' Daniel', 'Print', 'way', 'door', 'assistant', '<|eot_id|>', ' Dan', '<|eot_id|>', 'hall', ' apple', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0006764233112335205, 0.0010869145393371583, 0.004748034477233886, 0.00017516016960144042]}}, 26: {'grad': {'score': [0.5534916400909424, 0.6509784383511324, 0.592338337617762, 0.6513028993837257, 0.5718376795450847], 'topk_tokens': [' be', ',', 'ian', 'ible', 'ian', ' true', 'ian', '3', ' true', ' they', ',', ' not', ' and', 'asca', 'ucci', 'could', 'outs', ' not', ' it', 'hall'], 'evidence_proportions': [0.7459716796875, 0.5450300216674804, 0.5436859130859375, 0.37927894592285155]}, 'weight': {'score': [0.016453446447849275, 0.002479212937161307, 0.0032737859908272236, 0.0024539948892726778, 0.0007031853000322978], 'topk_tokens': ['?', ' Bridge', ' hallway', '.\n\n', ' bathroom', ' circus', ' \n', ' hallway', 'Answer', 'assistant', '\n\n', ' the', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'hall', '<|end_header_id|>', 'way', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.035728049278259275, 0.011141270399093628, 0.011575937271118164, 0.007368528842926025]}, 'saliency': {'score': [0.0007836088538169861, 9.178360716621654e-05, 0.0002372422639061423, 9.023826413000025e-05, 2.2857884565989177e-05], 'topk_tokens': ['door', ' hallway', ' \n', ' bathroom', ' the', ' Dan', ':', '<|eot_id|>', ' kitchen', ' Bridge', 'Bridge', ' circus', ' hallway', '<|eot_id|>', '<|start_header_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.002146756649017334, 0.00021312832832336425, 0.00039725303649902345, 0.00037729740142822266]}}, 27: {'grad': {'score': [0.3859893798828125, 0.3696111576585282, 0.3128006318036248, 0.3697431301313422, 0.3967135270436605], 'topk_tokens': ['.', '.', '?', '.', '.', '.', '.', '.', '.', '.', ' majority', ' duration', '.', '.', '.', '.', '.', '.', '.', '.'], 'evidence_proportions': [0.3623046875, 0.40570831298828125, 0.424871826171875, 0.3510726928710937]}, 'weight': {'score': [0.046619608998298645, 0.0025379136692962045, 0.004298363538349376, 0.002460450348403671, 0.0012099539240201314], 'topk_tokens': [' apple', ' bathroom', ' \n', ' Bridge', ' Dan', ' hallway', '<|eot_id|>', ' bathroom', '<|eot_id|>', 'Answer', '\n\n', 'assistant', '.\n\n', '<|start_header_id|>', 'hall', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.12192177772521973, 0.027791613340377806, 0.032561129331588744, 0.004203915596008301]}, 'saliency': {'score': [0.004178434610366821, 6.146613767231433e-05, 0.00029677503249224494, 5.40332043270973e-05, 5.0016244252522786e-05], 'topk_tokens': [' hallway', ' the', ' THE', ' \n', '.', ' bathroom', 'assistant', '<|begin_of_text|>', ' hallway', ':', 'Daniel', ' the', '<|start_header_id|>', 'way', ' Daniel', ' Daniel', ' Dan', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.0024666190147399902, 0.00794852375984192, 0.006170064210891724, 0.00012853145599365235]}}, 28: {'grad': {'score': [0.5290239334106446, 0.48449360058785734, 0.4548494956072639, 0.48450325114840403, 0.33870677947998046], 'topk_tokens': [' Times', '.,', '\n\n\n\n', ' hallway', ' found', 'urred', ' line', ' line', 'ed', ' house', ' houses', ' huge', ' hall', 'burg', ' house', ' hall', ' the', ' line', '\n\n\n\n\n\n\n', '***'], 'evidence_proportions': [0.6939544677734375, 0.4660072326660156, 0.46910400390625, 0.487030029296875]}, 'weight': {'score': [0.00944315493106842, 0.002416002641030408, 0.008792106719578014, 0.0023866023954757444, 0.0006260385115941365], 'topk_tokens': [' Bridge', ' apple', ' hallway', 'Answer', ' \n', '?', ' bathroom', ' before', '.\n\n', ' the', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.008594155311584473, 0.006786340475082398, 0.007797378301620484, 0.014594745635986329]}, 'saliency': {'score': [0.0003387734293937683, 6.417320067182593e-05, 0.0002463521326289457, 6.321169925163662e-05, 1.7007191975911457e-05], 'topk_tokens': [':', 'Answer', ' \n', 'Bridge', '<|eot_id|>', 'assistant', ' apple', '.\n\n', ' Bridge', '?', '<|eot_id|>', '<|end_header_id|>', ' hallway', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', ':', 'way', '\n\n', 'hall'], 'evidence_proportions': [0.0005172967910766601, 0.0001354813575744629, 0.0004349112510681152, 0.00026740431785583497]}}, 29: {'grad': {'score': [0.392073917388916, 0.29138697552962195, 0.48912788839901195, 0.290668123310209, 0.26132628122965496], 'topk_tokens': ['\n', '\n', ' spr', ' FIRST', 'stage', '\n', '\n', '5', 'hall', ' marched', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.30365142822265623, 0.38839111328125, 0.3596912384033203, 0.5165618896484375]}, 'weight': {'score': [0.006063726544380188, 0.0024755601020503013, 0.0047832490766749666, 0.0024631999939390757, 0.0005931491653124491], 'topk_tokens': [' to', ' before', '<|start_header_id|>', '.', '?', ' \n', ' the', 'Answer', ' Does', '<|eot_id|>', '.\n\n', 'hall', 'assistant', '<|start_header_id|>', '<|eot_id|>', ':', '<|end_header_id|>', 'way', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006729358434677124, 0.008368265628814696, 0.004808086156845093, 0.0043491959571838375]}, 'saliency': {'score': [0.0005202651023864746, 3.3652063258978516e-05, 0.00035276921356425565, 3.1958608931766957e-05, 2.789547046025594e-05], 'topk_tokens': [' bathroom', ' the', ' to', ' the', '.', 'assistant', '?', ' the', '<|start_header_id|>', ' to', 'way', '<|eot_id|>', '<|eot_id|>', ':', '<|end_header_id|>', ' Does', '<|start_header_id|>', 'hall', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0001864314079284668, 0.0016340017318725586, 0.00022129416465759276, 3.933310508728027e-05]}}, 30: {'grad': {'score': [0.24590208530426025, 0.36553446118516036, 0.27469904282513785, 0.36598542808839046, 0.3559562683105469], 'topk_tokens': [' Pioneer', 'Square', ' boat', ' Republicans', ' papers', 'ire', '186', ' Europe', ' Paul', ' Paul', ' Republican', ' hour', ' OCC', ' preparations', ' Bridge', 'AM', ' Republicans', ' Press', ' Star', ' Loan'], 'evidence_proportions': [0.127410888671875, 0.21193962097167968, 0.28300323486328127, 0.3612545967102051]}, 'weight': {'score': [0.013370943069458009, 0.0024178751486782007, 0.01001494684640099, 0.0023785990423643806, 0.0027072086930274965], 'topk_tokens': ['<|end_header_id|>', 'Question', '<|eot_id|>', ' the', '?', '<|start_header_id|>', 'Answer', ' bathroom', ' \n', 'assistant', 'hall', '.\n\n', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.007346916198730469, 0.0157042920589447, 0.020503741502761838, 0.00992882251739502]}, 'saliency': {'score': [0.0006733670830726624, 4.873372038257544e-05, 0.0005111790755215813, 4.6412195836868026e-05, 0.00010958512624104818], 'topk_tokens': ['<|end_header_id|>', 'Answer', ' Daniel', '<|eot_id|>', '<|eot_id|>', ' the', 'Gov', ' the', '?', ' Dan', ' Bridge', ' garden', '.', '<|start_header_id|>', 'Question', ' bathroom', 'way', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [0.00036857128143310544, 0.0010260879993438722, 0.0010491609573364258, 0.00024964809417724607]}}, 31: {'grad': {'score': [0.2762865886092186, 0.32110104056595196, 0.259065915556515, 0.32134832403554675, 0.13826738248268763], 'topk_tokens': [' paid', ' I', 'Independent', ',', ' was', 'F', ' Democrats', ' the', ' able', ' the', ' would', ' F', ' determined', ' government', ' intense', ' had', ' United', ' these', ' the', ' Independent'], 'evidence_proportions': [0.20568165183067322, 0.3511531710624695, 0.3572218954563141, 0.1910896360874176]}, 'weight': {'score': [0.0035960346460342405, 0.0022761901428002673, 0.001853828044498668, 0.0022751998039223322, 0.0011169572671254476], 'topk_tokens': [' the', 'Question', ' before', ':', ' the', '?', '.\n\n', ' Where', '<|eot_id|>', 'Answer', ' \n', ':', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0023584246635437013, 0.0019190549850463868, 0.0037916660308837894, 0.006314992904663086]}, 'saliency': {'score': [0.00036521404981613157, 2.3610482229161543e-05, 6.25226427527035e-05, 2.293950273191456e-05, 8.397301038106282e-06], 'topk_tokens': [' the', ' Daniel', ' the', '?', '<|eot_id|>', ' PA', '<|start_header_id|>', ' apple', '<|end_header_id|>', 'Question', ' Where', '<|begin_of_text|>', ':', '.\n\n', ' hallway', ' the', 'assistant', ' hallway', 'way', 'hall'], 'evidence_proportions': [0.0008888959884643555, 0.00025977492332458497, 0.00016516447067260742, 0.0001470208168029785]}}, 'pred_res': "Daniel's hand<|eot_id|>", 'score': 0}
2025-01-22 03:25:21.935 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:25:21.935 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-0_0-1-3-9.pkl | len: 10 |  size: 9.05 KB
Processing depth (0, 1, 3, 9):   1%|          | 1/100 [00:17<29:05, 17.63s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]
Processing depth (3, 6, 7, 9):   1%|          | 1/100 [00:25<29:05, 17.63s/it]2025-01-22 03:25:29.438 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:25:29.449 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3802, 3807) --> . Daniel went to the
2025-01-22 03:25:29.449 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 03:25:29.470 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (7259, 7264) --> . Daniel travelled to the
2025-01-22 03:25:29.470 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 03:25:29.494 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8452, 8457) --> . Daniel dropped the apple
2025-01-22 03:25:29.494 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:25:29.524 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10776, 10781) --> . Mary travelled to the
2025-01-22 03:25:29.524 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:25:29.538 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4549, 4555) --> . John went back to the
2025-01-22 03:25:29.538 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:25:29.547 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3180, 3185) --> . Sandra moved to the
2025-01-22 03:25:29.547 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:25:29.557 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3416, 3421) --> . John moved to the
2025-01-22 03:25:29.557 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 03:25:29.574 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5750, 5756) --> . John journeyed to the
2025-01-22 03:25:29.574 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 03:25:29.605 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10852, 10858) --> . Sandra went back to the
2025-01-22 03:25:29.605 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 03:25:29.632 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (9174, 9180) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:25:31.852 | INFO     | test_jbb_retain:begin_test:632 - Daniel dropped the apple there.<|eot_id|>
2025-01-22 03:25:31.853 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12240])
your chose emoji: ['✝', '🤵🏼\u200d♀', '🧑🏻\u200d❤\u200d💋\u200d🧑🏾', '👩🏻\u200d❤\u200d👩🏽', '💑🏼', '🧙🏿', '🧚🏾\u200d♀', '🙋🏿\u200d♂️', '🇰🇭', '👩🏼\u200d🤝\u200d👨🏿']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 138654.68it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:37,  5.35s/it][A
 38%|███▊      | 3/8 [00:05<00:07,  1.42s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.73it/s][A100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 16.37it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.49it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.55it/s][A100%|██████████| 8/8 [00:00<00:00, 20.69it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 15.10it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 18.88it/s][A
100%|██████████| 8/8 [00:00<00:00, 21.22it/s][A100%|██████████| 8/8 [00:00<00:00, 20.18it/s]
2025-01-22 03:25:41.054 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.3487707138061523, 0.2722724927637221, 0.3097456763772404, 0.27204244496130126, 0.5549934989527652], 'topk_tokens': ['ugg', 'itter', ' lively', ' candidate', ' Project', ' hand', ' type', ' candidate', ' genu', ' high', '\n', ' type', 'ra', 'hand', ' hand', 'hand', ' hand', ' hand', ' hand', ' type'], 'evidence_proportions': [0.32524948120117186, 0.2542236328125, 0.3631591796875, 0.4524505615234375]}, 'weight': {'score': [0.028372929990291597, 0.002538134204029774, 0.005011311348746805, 0.0024888451779697513, 0.0007173265281476473], 'topk_tokens': ['\n\n', ' dropped', ' Market', 'Answer', ' Eighth', 'Bridge', ' Daniel', ' Third', ':', 'assistant', '.', '<|start_header_id|>', '<|eot_id|>', 'Third', '\n\n', '<|end_header_id|>', 'hall', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0030866324901580814, 0.03285543322563171, 0.07368257641792297, 0.0038670778274536135]}, 'saliency': {'score': [0.0016406893730163575, 4.046704073333179e-05, 0.00021355905953575582, 3.735853509013359e-05, 1.5463327106676602e-05], 'topk_tokens': [' Daniel', ' Bridge', '.', ' Bridge', ' dropped', ' Third', '<|eot_id|>', 'ighth', '<|eot_id|>', '\n\n', ' Anthony', ' Daniel', '<|end_header_id|>', ' Third', 'hall', 'Third', 'way', '<|begin_of_text|>', ' Eighth', 'Bridge'], 'evidence_proportions': [0.00011445283889770508, 0.002162414789199829, 0.004212117195129395, 7.377266883850097e-05]}}, 25: {'grad': {'score': [0.4739482879638672, 0.5136665108059197, 0.5140599643482882, 0.5137305839076016, 0.48748129794472145], 'topk_tokens': [' to', ' post', ' daily', ' her', ' daily', '10', 'antic', ' rest', ' large', ' two', 'ically', 'ad', ' a', ' daily', ' daily', ' daily', ' morning', ' daily', 'st', ' daily'], 'evidence_proportions': [0.5052810668945312, 0.5364715576171875, 0.409600830078125, 0.444439697265625]}, 'weight': {'score': [0.03140131533145905, 0.002536553315338157, 0.0024503039963105146, 0.0024894319137895907, 0.0018720463702553197], 'topk_tokens': [' dropped', ' apple', ' there', '.', ' Ramsey', 'Third', ' Market', '.', ' Anthony', '<|start_header_id|>', 'hall', ':', ' Daniel', 'assistant', 'way', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0026545822620391846, 0.021045881509780883, 0.0986569881439209, 0.0032478094100952147]}, 'saliency': {'score': [0.0006448999047279358, 3.963207912281488e-05, 5.791730740491082e-05, 3.858793651278176e-05, 6.227148206610428e-05], 'topk_tokens': [' Mess', ' Seventh', ' Fourth', '.', ' Eighth', ' Ramsey', '.', ' Seventh', '<|eot_id|>', ' apple', 'way', '<|eot_id|>', 'assistant', ' Market', ' there', '\n\n', ' Ramsey', '<|end_header_id|>', '<|begin_of_text|>', ' Anthony'], 'evidence_proportions': [6.498098373413086e-05, 0.00034937262535095215, 0.0020146608352661134, 0.00015058517456054688]}}, 26: {'grad': {'score': [0.6118927001953125, 0.5967801913593482, 0.6676657059613396, 0.5965576663225785, 0.5506579449302271], 'topk_tokens': [' Out', ',', 'ucci', ' as', ' This', ' several', ' the', '.', ',', ' before', '\n', ',', ' the', ' the', ' and', ',', ' This', '.', ' it', ' of'], 'evidence_proportions': [0.6786148071289062, 0.6374526977539063, 0.5638458251953125, 0.567657470703125]}, 'weight': {'score': [0.020526230335235596, 0.002482568400753077, 0.0029361388262580425, 0.0024516967432621573, 0.001940927693718358], 'topk_tokens': ['�', ' Jackson', ' Jackson', '.\n\n', ' \n', ' Daniel', 'assistant', 'Answer', ' the', ' Anthony', ' the', '\n\n', 'hall', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.001258927583694458, 0.015716087818145753, 0.058190065622329715, 0.006939840316772461]}, 'saliency': {'score': [0.0006083086133003235, 5.715275564429749e-05, 0.00012781076571520637, 5.605131258124626e-05, 0.00011296617357354416], 'topk_tokens': ['Answer', 'Third', ' Daniel', ' the', 'assistant', ' the', ' Jackson', 'Bridge', ' Ramsey', '<|start_header_id|>', '�', '<|eot_id|>', ':', '<|eot_id|>', 'way', ' Anthony', '\n\n', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [3.120303153991699e-05, 0.0004467129707336426, 0.0017856299877166749, 0.00016968846321105958]}}, 27: {'grad': {'score': [0.22459697723388672, 0.2942649008138579, 0.22014577248517206, 0.2945859615107793, 0.27029115777266655], 'topk_tokens': [' increased', '      ', '--', ' destination', ' hand', '--', ' Or', 'akes', '      ', '      ', ' Little', ' W', ' investor', ' and', 'BR', ' mailing', '      ', ' hood', ' New', 'hand'], 'evidence_proportions': [0.2232513427734375, 0.27839965820312496, 0.22297286987304688, 0.1737640380859375]}, 'weight': {'score': [0.026064553856849672, 0.0025345764060450042, 0.004127512959872975, 0.0024915245238687593, 0.0015079112429367868], 'topk_tokens': [' Bridge', ' hallway', 'Bridge', '.', ' bathroom', ' \n', ' Anthony', ' Daniel', 'Answer', 'assistant', '\n\n', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|end_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002812522649765015, 0.021749711036682128, 0.07499718070030212, 0.004698801040649414]}, 'saliency': {'score': [0.0026338934898376466, 5.815375146255923e-05, 0.0002634630483739516, 5.3354726861485386e-05, 7.779629606949655e-05], 'topk_tokens': ['.', ' hallway', 'assistant', '\n\n', ' Daniel', ' Ramsey', ' Anthony', ' the', ' the', ' Bridge', '<|start_header_id|>', ' Bridge', '<|begin_of_text|>', 'way', ' Daniel', ':', 'Bridge', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.0003550827503204346, 0.003044223785400391, 0.006917887926101685, 0.00021837949752807616]}}, 28: {'grad': {'score': [0.3191230773925781, 0.44727278390585434, 0.3996130999396829, 0.4476159969163651, 0.35049096157676296], 'topk_tokens': ['line', '\n', ' line', '\n', ' Brown', 'burg', ' law', ' line', '***', '\n', 'ed', 'urred', 'arch', 'ched', ' hall', ' hall', '\n\n\n\n\n\n\n', '\n', '\n', '\n\n\n\n'], 'evidence_proportions': [0.2913116455078125, 0.38112487792968747, 0.2869476318359375, 0.317108154296875]}, 'weight': {'score': [0.01726740151643753, 0.002435638946829829, 0.0046264152316486135, 0.0024051916874093687, 0.0010691137690293161], 'topk_tokens': [' bathroom', ' the', ' Daniel', ' Bridge', ' the', '.\n\n', 'Answer', '?', ' \n', 'assistant', ' before', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0027626216411590575, 0.01511332392692566, 0.03862742781639099, 0.012566232681274415]}, 'saliency': {'score': [0.0008421123027801513, 4.5810874883503266e-05, 9.390799438252168e-05, 4.437012251490046e-05, 5.147331639340049e-05], 'topk_tokens': [' apple', ' Market', '<|eot_id|>', ' Daniel', '�', ' before', ' dropped', ' Daniel', 'assistant', ' the', 'Bridge', ' Bridge', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', 'way', '<|begin_of_text|>', 'hall', ':', '\n\n'], 'evidence_proportions': [8.164644241333008e-05, 0.0007001042366027832, 0.002503514289855957, 8.318424224853515e-05]}}, 29: {'grad': {'score': [0.3251808166503906, 0.2743957963223883, 0.25630810681511373, 0.2743629208639165, 0.4237390618575247], 'topk_tokens': [' was', '\n', ' not', ' the', ' S', ' McC', '\u200d', ' was', '�', ' probably', ' closely', ' George', '\n', '25', '\u200d', ' West', ' was', ' Get', 'A', ' Far'], 'evidence_proportions': [0.31346435546875, 0.33926696777343746, 0.25203247070312496, 0.39595947265625]}, 'weight': {'score': [0.005938778817653656, 0.002462384670201172, 0.0022896098739960615, 0.002457162458380836, 0.0010194179258848492], 'topk_tokens': [' the', ' Does', ' the', ' the', ' bathroom', ' before', '?', ' \n', 'Answer', '.\n\n', 'hall', 'assistant', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011968076229095458, 0.006747049093246461, 0.01119895577430725, 0.0046123027801513675]}, 'saliency': {'score': [0.00017973631620407104, 2.6941691350091723e-05, 7.98728536157047e-05, 2.6543336110604446e-05, 3.3041364268252726e-05], 'topk_tokens': [' ', ' the', ' apple', 'assistant', ' bathroom', ' the', ' Does', '?', '.\n\n', 'E', '<|eot_id|>', 'way', ' before', '<|eot_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>', 'hall', '\n\n', '<|start_header_id|>'], 'evidence_proportions': [0.00011397004127502442, 0.0002019345760345459, 0.0003474593162536621, 5.558133125305176e-05]}}, 30: {'grad': {'score': [0.3782192230224609, 0.2945213152732684, 0.4473711743074305, 0.2939576223237118, 0.2636271476745605], 'topk_tokens': ['just', 'd', ' galaxy', ' county', ' congressional', ' adj', ' d', ' devil', ' count', ' theater', ' OCC', ' double', ' composing', ' river', ' an', 'district', ' Loan', ' loud', ' senate', 'district'], 'evidence_proportions': [0.436724853515625, 0.30393218994140625, 0.39986267089843747, 0.372357177734375]}, 'weight': {'score': [0.01757110804319382, 0.0024112444984107626, 0.005856211571132436, 0.002376760443002756, 0.0030310866079832376], 'topk_tokens': [' the', ' the', 'Question', ' Anthony', '<|start_header_id|>', ' bathroom', '?', '.\n\n', 'assistant', 'Answer', 'hall', ' \n', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0028544366359710693, 0.0197501003742218, 0.03877386450767517, 0.008906030654907226]}, 'saliency': {'score': [0.0004999548196792603, 4.2210348795746155e-05, 0.00030392583678750433, 4.072924156714741e-05, 0.00013710134907772667], 'topk_tokens': ['Bridge', ' the', ' bathroom', ' Bridge', ' Robert', '.\n\n', ' the', '<|start_header_id|>', ' Min', '<|start_header_id|>', 'Question', ' Bridge', '�', 'way', '<|eot_id|>', ':', ' Anthony', '<|eot_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0001361370086669922, 0.000904160737991333, 0.0008694708347320556, 9.005069732666015e-05]}}, 31: {'grad': {'score': [0.35398101806640625, 0.3832310766388191, 0.2859063989975873, 0.383550548310921, 0.19861745834350586], 'topk_tokens': [',', ' value', ' opposition', ' what', 'es', ' Democrats', 'ighth', ' effect', ' evening', ' intense', ' D', 'proved', ' debt', ' able', ' determined', ' Democratic', ',', ' United', ' had', ' Independent'], 'evidence_proportions': [0.332794189453125, 0.396270751953125, 0.40442504882812497, 0.28243408203125]}, 'weight': {'score': [0.0049073129892349245, 0.0022636796196798363, 0.002381576334728914, 0.0022590130222803146, 0.0011637960609636809], 'topk_tokens': [' the', ':', 'Question', ' bathroom', ' Where', ' before', '.\n\n', '<|eot_id|>', '?', 'Answer', ' \n', '<|start_header_id|>', 'assistant', '<|eot_id|>', ':', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0016842663288116455, 0.003978121280670166, 0.008496016263961792, 0.005470848083496094]}, 'saliency': {'score': [0.00023111850023269654, 2.674442349058209e-05, 6.953758351943073e-05, 2.6289714410606446e-05, 2.967558409038343e-05], 'topk_tokens': ['\n\n', ' the', ' Daniel', ' the', ' the', ' Where', ' apple', ' Market', ' hallway', '.\n\n', ' Anthony', ' Bridge', '<|eot_id|>', '<|end_header_id|>', ':', 'assistant', '<|begin_of_text|>', 'hall', '<|start_header_id|>', 'way'], 'evidence_proportions': [9.022951126098633e-05, 0.0002472758293151855, 0.00045351386070251464, 0.0001334547996520996]}}, 'pred_res': 'Daniel dropped the apple there.<|eot_id|>', 'score': 0}
2025-01-22 03:25:41.056 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:25:41.056 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-1_3-6-7-9.pkl | len: 10 |  size: 9.21 KB
Processing depth (3, 6, 7, 9):   2%|▏         | 2/100 [00:36<30:13, 18.51s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.14it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.23it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]
Processing depth (0, 4, 6, 9):   2%|▏         | 2/100 [00:44<30:13, 18.51s/it]2025-01-22 03:25:49.109 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:25:49.109 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 03:25:49.110 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 03:25:49.123 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4903, 4908) --> . Daniel travelled to the
2025-01-22 03:25:49.124 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 03:25:49.144 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7256, 7261) --> . Daniel dropped the apple
2025-01-22 03:25:49.144 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:25:49.175 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (10790, 10795) -->  Mary travelled to the bedroom
2025-01-22 03:25:49.175 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:25:49.188 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4538, 4544) --> . John went back to the
2025-01-22 03:25:49.188 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:25:49.197 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3187, 3192) --> . Sandra moved to the
2025-01-22 03:25:49.197 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:25:49.207 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3418, 3423) --> . John moved to the
2025-01-22 03:25:49.207 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 03:25:49.224 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5770, 5776) --> . John journeyed to the
2025-01-22 03:25:49.224 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 03:25:49.255 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10865, 10871) --> . Sandra went back to the
2025-01-22 03:25:49.255 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 03:25:49.281 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (9179, 9185) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:25:51.422 | INFO     | test_jbb_retain:begin_test:632 - Daniel dropped the apple there.<|eot_id|>
2025-01-22 03:25:51.422 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12242])
your chose emoji: ['👩🏾\u200d❤\u200d👩🏽', '🧜🏼\u200d♀️', '🤵🏿', '👩🏽\u200d❤️\u200d👩🏼', '👇🏼', '🤽🏼\u200d♂️', '🙅\u200d♂️', '🇨🇭', '👨🏾\u200d❤️\u200d👨🏽', '⛹🏿']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:38,  5.50s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.07s/it][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.57it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.39it/s][A100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.27it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.65it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.31it/s][A100%|██████████| 8/8 [00:00<00:00, 17.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 18.03it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 19.40it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 17.19it/s][A100%|██████████| 8/8 [00:00<00:00, 17.21it/s]
2025-01-22 03:26:01.390 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.37601375579833984, 0.35191259107671496, 0.3942422305836397, 0.35175499686478257, 0.6084740235633457], 'topk_tokens': ['in', 'dr', ' type', 'hom', ' high', ' type', 'Ind', 'itter', '\n', ' genu', ' typ', 'ir', ' Project', 'hand', ' hand', ' type', 'hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.33858642578125, 0.33952636718749996, 0.3933067321777344, 0.43263549804687496]}, 'weight': {'score': [0.029349231719970705, 0.002551624832371392, 0.0048964873832814835, 0.0025011222103975645, 0.000616815286813323], 'topk_tokens': ['\n\n', ' there', ' bathroom', 'Answer', '.', ':', ' hallway', ' Daniel', ' Bridge', '<|start_header_id|>', ' hallway', 'Bridge', 'assistant', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.04201040267944336, 0.01217726469039917, 0.05670334100723267, 0.006505918502807617]}, 'saliency': {'score': [0.0023136451840400697, 4.119454427075026e-05, 0.00016088520779329187, 3.7132654732963954e-05, 1.122595108661455e-05], 'topk_tokens': [':', ' Bridge', ' apple', ' Dan', '<|eot_id|>', ' old', ' Daniel', '\n\n', ' hallway', 'assistant', ' hallway', '\n\n', 'Daniel', ' Bridge', '<|end_header_id|>', ' Daniel', 'way', 'Bridge', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0011234760284423828, 0.0009139537811279297, 0.006735783815383911, 0.0004813671112060547]}}, 25: {'grad': {'score': [0.5986640930175782, 0.917566879976521, 0.7323756498448989, 0.9186065451035536, 0.7424756472872704], 'topk_tokens': [' great', ' mar', ' to', ' large', ' used', ' two', 'aud', ' genu', ' to', 'ated', ' daily', '10', ' o', ' per', ' a', 'po', 'st', ' post', 'ad', ' post'], 'evidence_proportions': [0.5657501220703125, 0.6626800537109375, 0.5841827392578125, 0.58204345703125]}, 'weight': {'score': [0.019611692428588866, 0.0025422947121523216, 0.0026210704270531147, 0.0025140717338375516, 0.0007172045633964932], 'topk_tokens': [' the', ' Dan', ' Daniel', '.\n\n', ' apple', 'Answer', ' Daniel', ' \n', '.', ' there', '<|start_header_id|>', ':', 'hall', '<|eot_id|>', 'assistant', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006284439563751221, 0.019501417875289917, 0.048493689298629766, 0.00416722297668457]}, 'saliency': {'score': [0.001070249080657959, 4.804024846274398e-05, 6.599373677197625e-05, 4.631318790606953e-05, 1.8890985508554988e-05], 'topk_tokens': [' East', ' apple', ' Ramsey', '.', ' Market', ' Merch', ' Daniel', ' Marshall', ' Merch', '<|eot_id|>', '<|eot_id|>', 'hall', '.', ' there', ' Dan', 'assistant', ' apple', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00026111602783203126, 0.0008146226406097412, 0.0029286742210388184, 0.00027658343315124513]}}, 26: {'grad': {'score': [0.5117572784423828, 0.5201136584766741, 0.5465224069707534, 0.5200537150882636, 0.4919435855039616], 'topk_tokens': ['ible', ',', ' will', ' could', ' City', ' to', ' be', ' Cl', '3', 'ian', ' and', 'could', 'asca', ' Out', ' not', 'ucci', 'outs', ' not', ' it', 'hall'], 'evidence_proportions': [0.6566497802734375, 0.49920501708984377, 0.51168212890625, 0.3794921875]}, 'weight': {'score': [0.020048139989376067, 0.002483949704090397, 0.0025002290220821604, 0.002455089290464162, 0.0009199795649223721], 'topk_tokens': [' Bridge', 'Bridge', '?', ' apple', ' hallway', '.\n\n', ' hallway', ' \n', 'Answer', 'assistant', '\n\n', '<|eot_id|>', ' the', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0201751172542572, 0.012082487344741821, 0.035174602270126344, 0.012760353088378907]}, 'saliency': {'score': [0.0006336092948913575, 8.063973724330967e-05, 9.7617506980896e-05, 7.968521051670488e-05, 2.447139356554169e-05], 'topk_tokens': [' hallway', ' Ramsey', ' Shak', ' Bridge', ' kitchen', ' apple', 'Answer', ' Bridge', ' hallway', ' Dan', '<|eot_id|>', '<|eot_id|>', 'Bridge', ':', ' the', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0004909753799438477, 0.0006710231304168701, 0.0011194705963134767, 0.0002529680728912354]}}, 27: {'grad': {'score': [0.3940326690673828, 0.3652857651560586, 0.28686949786017923, 0.3654573027665773, 0.37656486157289487], 'topk_tokens': [' majority', '.', '.', '.', '--', '.', '.', '--', '.', '.', '.', 'hand', '.', '.', '.', '.', '.', ' candidates', ' participate', ' duration'], 'evidence_proportions': [0.3849853515625, 0.38359603881835935, 0.4895072937011719, 0.3180419921875]}, 'weight': {'score': [0.04331733286380768, 0.002528330412335472, 0.0036597891765482284, 0.0024582582568918924, 0.0010317782151330378], 'topk_tokens': [' Daniel', ' apple', ' Daniel', ' Bridge', ' \n', ' bathroom', 'Answer', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', '.\n\n', ' hallway', '<|start_header_id|>', ' hallway', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.09403007030487061, 0.023109495639801025, 0.045449948310852056, 0.010679817199707032]}, 'saliency': {'score': [0.003543025255203247, 5.859136824804503e-05, 0.00023142204565160414, 5.239294967936116e-05, 3.2076516102269756e-05], 'topk_tokens': [' the', ' bathroom', ' Geo', ' \n', ' hallway', '.', ' Bridge', 'assistant', ' there', '<|begin_of_text|>', ' Dan', 'Bridge', 'Daniel', 'way', ' Daniel', ' Daniel', ':', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.0003593862056732178, 0.0062523365020751955, 0.0070686817169189455, 0.0004916965961456298]}}, 28: {'grad': {'score': [0.43062591552734375, 0.47290454769484996, 0.40998097027049346, 0.4731493983449835, 0.3632615718644919], 'topk_tokens': [' firm', ' huge', '\n', 'urred', '.,', ' home', 'ched', '\n\n\n\n', ' house', ' house', '***', ' line', 'ching', ' line', 'ed', 'burg', ' hall', ' hall', ' line', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.5898925781250001, 0.43185119628906254, 0.4062744140625, 0.2944854736328125]}, 'weight': {'score': [0.012868709862232208, 0.0024218745015503777, 0.005016407545875101, 0.0023974998948142054, 0.0008154852488606246], 'topk_tokens': [' Bridge', ' apple', ' hallway', ' bathroom', 'Answer', '.\n\n', '?', ' \n', ' before', ' the', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', '<|start_header_id|>', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00363505482673645, 0.006242018938064575, 0.022277849912643432, 0.019319915771484376]}, 'saliency': {'score': [0.000406135618686676, 5.830052727434089e-05, 7.975715048172894e-05, 5.767004355542547e-05, 1.3512434418668453e-05], 'topk_tokens': [' the', ' there', '?', '.', 'assistant', '<|eot_id|>', ' apple', '<|eot_id|>', ' hallway', ' the', '<|end_header_id|>', 'Bridge', ' Bridge', ' Bridge', '<|begin_of_text|>', ':', '<|start_header_id|>', 'way', '\n\n', 'hall'], 'evidence_proportions': [0.0002773106098175049, 0.00011864900588989257, 0.000906449556350708, 0.00032213330268859863]}}, 29: {'grad': {'score': [0.3621058464050293, 0.26721283619430125, 0.3881068790660185, 0.2667199924848555, 0.3513947516372523], 'topk_tokens': ['\n', ' pur', ' very', '25', '\n', 'in', '\n', ' was', '\n', ' Get', '.', ' the', ' was', 'A', '5', ' George', ' Far', '\n', '\n', '\n'], 'evidence_proportions': [0.2535736083984375, 0.33590354919433596, 0.37810668945312503, 0.48083953857421874]}, 'weight': {'score': [0.005634933710098267, 0.002469727300828503, 0.002212053712676553, 0.0024652532440498772, 0.0007679720514828397], 'topk_tokens': [' Where', '<|start_header_id|>', ' was', ' Does', ' before', '?', ' \n', 'Answer', ' the', '.\n\n', '<|eot_id|>', 'hall', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018646478652954103, 0.005307936668395997, 0.009218907356262207, 0.006148242950439453]}, 'saliency': {'score': [0.00022448897361755372, 2.7568148515233217e-05, 0.00011130115565131692, 2.7011562612134763e-05, 2.7842742880595098e-05], 'topk_tokens': [' bathroom', ' the', ' \n', 'Does', ' the', '      ', ' was', '.', '?', 'way', '<|start_header_id|>', '<|eot_id|>', ' Does', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', ':', 'hall', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.814697265625e-05, 0.0004737377166748047, 0.00027108192443847656, 0.0001149892807006836]}}, 30: {'grad': {'score': [0.21341118812561036, 0.3506648021992395, 0.2104622055502499, 0.3512809912376726, 0.3548726347303882], 'topk_tokens': [' boat', ' Press', ' States', ' THE', ' preparations', ' Europe', 'Star', ' People', ' Bridge', '      ', ' papers', ' hour', ' Republicans', ' Project', ' OF', ' OCC', ' an', 'AM', ' Star', ' Loan'], 'evidence_proportions': [0.13397979736328125, 0.2545158386230469, 0.27413349151611327, 0.19101562500000002]}, 'weight': {'score': [0.019331787526607514, 0.002421697084637942, 0.006595823694677914, 0.002382313759760512, 0.002826524763992152], 'topk_tokens': ['<|end_header_id|>', '<|eot_id|>', '.', ' bathroom', ' the', '<|start_header_id|>', '?', 'Answer', 'assistant', '.\n\n', 'hall', '<|eot_id|>', ' \n', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004743218421936035, 0.019375604391098023, 0.03752028942108154, 0.015688037872314452]}, 'saliency': {'score': [0.001222209632396698, 5.150163791869893e-05, 0.00037991387002608357, 4.866510475643077e-05, 0.00012262733941225662], 'topk_tokens': [' Robert', ' apple', ' Bridge', '?', 'Gov', ' apple', ' Where', ' Dan', '<|end_header_id|>', 'Question', ' Daniel', ' \n', 'assistant', ' bathroom', ' the', 'way', '<|begin_of_text|>', ':', '<|start_header_id|>', 'hall'], 'evidence_proportions': [0.00020214319229125977, 0.0021971821784973144, 0.001982074975967407, 0.0005074381828308105]}}, 31: {'grad': {'score': [0.2685241296887398, 0.3210443064357466, 0.22665675159762888, 0.321393710126948, 0.1813027677462273], 'topk_tokens': [',', ' Democrats', ' ever', ' intense', ' was', ' had', ' evening', ' determined', ' have', ' would', 'F', ' the', ' able', ' was', ' the', ' these', ' F', ' Independent', ' had', ' United'], 'evidence_proportions': [0.18004554510116577, 0.2853347957134247, 0.35380483269691465, 0.254911345243454]}, 'weight': {'score': [0.004735879600048065, 0.002278264778981456, 0.0021815238630070407, 0.0022745027327770264, 0.0009774596420760006], 'topk_tokens': [' was', 'Question', ':', ' before', ' the', '.\n\n', '?', ' Where', '<|eot_id|>', 'Answer', ' \n', ':', 'assistant', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018438100814819336, 0.0032740175724029538, 0.007824790477752686, 0.006000900268554687]}, 'saliency': {'score': [0.00033537447452545164, 2.4051647275844462e-05, 0.00012118588475620046, 2.3270003389426265e-05, 3.076521391721116e-05], 'topk_tokens': [' ', ' Daniel', '<|begin_of_text|>', ' apple', ' John', ' \n', '<|eot_id|>', '�', 'Question', ' Daniel', ' hallway', ' the', '<|end_header_id|>', ':', '.\n\n', ' hallway', 'assistant', '<|start_header_id|>', 'hall', 'way'], 'evidence_proportions': [0.0005421340465545654, 0.0002694129943847656, 0.0004736006259918213, 5.63502311706543e-05]}}, 'pred_res': 'Daniel dropped the apple there.<|eot_id|>', 'score': 0}
2025-01-22 03:26:01.392 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:26:01.392 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-2_0-4-6-9.pkl | len: 10 |  size: 9.04 KB
Processing depth (0, 4, 6, 9):   3%|▎         | 3/100 [00:57<31:16, 19.34s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]
Processing depth (1, 4, 6, 7):   3%|▎         | 3/100 [01:04<31:16, 19.34s/it]2025-01-22 03:26:08.976 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:26:08.980 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (1530, 1535) --> . Daniel went to the
2025-01-22 03:26:08.980 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 03:26:08.994 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4892, 4897) --> . Daniel travelled to the
2025-01-22 03:26:08.994 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 03:26:09.014 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (7244, 7249) --> . Daniel dropped the apple
2025-01-22 03:26:09.014 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:26:09.040 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (8421, 8426) --> . Mary travelled to the
2025-01-22 03:26:09.041 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:26:09.054 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4527, 4533) --> . John went back to the
2025-01-22 03:26:09.054 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:26:09.063 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3119, 3124) --> . Sandra moved to the
2025-01-22 03:26:09.063 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:26:09.072 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3323, 3328) --> . John moved to the
2025-01-22 03:26:09.072 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 03:26:09.088 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5759, 5765) --> . John journeyed to the
2025-01-22 03:26:09.088 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 03:26:09.119 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10827, 10833) --> . Sandra went back to the
2025-01-22 03:26:09.119 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 03:26:09.145 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (9182, 9188) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:26:11.408 | INFO     | test_jbb_retain:begin_test:632 - The apple was dropped by Daniel.<|eot_id|>
2025-01-22 03:26:11.409 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12201])
your chose emoji: ['🏊🏼\u200d♀️', '🤦🏻\u200d♂', '\U0001faf5🏽', '🧒🏼', '👘', '🌳', '♏', '🏇🏽', '🚶🏽\u200d♂', '🐽']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:05<00:35,  5.09s/it][A
 50%|█████     | 4/8 [00:05<00:04,  1.00s/it][A
 88%|████████▊ | 7/8 [00:05<00:00,  2.06it/s][A100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 19.17it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.83it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.68it/s][A100%|██████████| 8/8 [00:00<00:00, 22.05it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:00, 17.75it/s][A
 62%|██████▎   | 5/8 [00:00<00:00, 20.56it/s][A
100%|██████████| 8/8 [00:00<00:00, 22.48it/s][A100%|██████████| 8/8 [00:00<00:00, 21.70it/s]
2025-01-22 03:26:20.481 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.26534099578857423, 0.2779808982166452, 0.25306929560268626, 0.27807141612096087, 0.46923766817365375], 'topk_tokens': ['hom', ' high', 'in', 'announcement', ' proceed', 'ra', 'Ind', '\n', ' procession', ' typ', 'ir', ' genu', 'hand', ' type', ' Project', 'hand', ' hand', ' hand', ' hand', ' hand'], 'evidence_proportions': [0.256109619140625, 0.25032501220703124, 0.247760009765625, 0.30716934204101565]}, 'weight': {'score': [0.03230944275856018, 0.002553927003653547, 0.008733636316131143, 0.0024876537170920353, 0.001467075731073107], 'topk_tokens': ['\n\n', '.', ' Daniel', 'Answer', ' hallway', ' the', ' dropped', '.', 'Bridge', ':', ' there', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00040202140808105464, 0.016245704889297486, 0.09361175894737243, 0.018978285789489745]}, 'saliency': {'score': [0.001983635127544403, 3.769499030280449e-05, 0.000262639978352715, 3.3862320974530505e-05, 3.8836683545793805e-05], 'topk_tokens': [':', ' the', ' bedroom', ' bathroom', ' garden', '<|start_header_id|>', ' Bridge', '\n\n', ' there', '<|eot_id|>', '.', ' Daniel', ' bedroom', '<|eot_id|>', '<|end_header_id|>', ' dropped', '<|begin_of_text|>', 'way', 'hall', 'Bridge'], 'evidence_proportions': [1.845359802246094e-05, 0.0008125662803649903, 0.006590449810028076, 0.0005130708217620849]}}, 25: {'grad': {'score': [0.4848624229431152, 0.5473344740966076, 0.40737421372357535, 0.5478289670904968, 0.42500972747802734], 'topk_tokens': [' daily', 'ad', ' so', ' had', '10', 'ated', ' a', ' per', ' two', 'g', 'po', ' o', '10', ' to', ' extraordinary', '10', ' daily', ' post', 'st', ' post'], 'evidence_proportions': [0.430419921875, 0.580084228515625, 0.469698715209961, 0.45924682617187496]}, 'weight': {'score': [0.034168146550655365, 0.002552433038922225, 0.00462820687714745, 0.0024945819623185773, 0.0023959619658333914], 'topk_tokens': [' Anthony', '.', ' Daniel', ' \n', '.\n\n', '.', '<|start_header_id|>', 'Answer', '.', 'hall', ' apple', ':', ' there', 'assistant', 'way', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000873410701751709, 0.02419109344482422, 0.09259814620018006, 0.019009935855865478]}, 'saliency': {'score': [0.0011655062437057496, 3.861405397391405e-05, 0.00010126478531781365, 3.6583768487467196e-05, 6.170783724103655e-05], 'topk_tokens': [' Pills', ' East', ' Ramsey', 'way', ' apple', ' Market', '<|eot_id|>', '.', ' Ramsey', ' dropped', '<|eot_id|>', '.', 'hall', 'assistant', ' Mary', ' there', ' Anthony', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [4.652738571166992e-05, 0.0005503535270690918, 0.002065587043762207, 0.0019995570182800293]}}, 26: {'grad': {'score': [0.40117225646972654, 0.3950154502366027, 0.3840807465945973, 0.39503591474682215, 0.32247945240565706], 'topk_tokens': [',', 'could', ' accidentally', ' commander', ' City', ' they', ' be', ' not', 'ian', ' prepared', 'asca', 'istributed', ' Out', 'outs', '3', ' not', 'ucci', ' and', 'hall', ' it'], 'evidence_proportions': [0.51273193359375, 0.3719512939453125, 0.39849853515625, 0.3215072631835938]}, 'weight': {'score': [0.021174170076847076, 0.0025050419738979583, 0.005933237426421221, 0.0024647175946843967, 0.002138840300696237], 'topk_tokens': ['Bridge', '?', ' Anthony', '.\n\n', ' hallway', ' the', ' the', ' apple', ' \n', 'Answer', 'assistant', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'hall', '<|eot_id|>', ':', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002648293972015381, 0.010956645011901855, 0.0571574866771698, 0.016317719221115114]}, 'saliency': {'score': [0.0004632189869880676, 5.6974943916510845e-05, 0.00023530160679536706, 5.5807208328090086e-05, 4.4990330934524536e-05], 'topk_tokens': ['?', ' Sandra', ' Jackson', ' Ramsey', ' the', ' Bridge', ' Bridge', 'assistant', '<|start_header_id|>', 'Answer', '<|eot_id|>', '<|eot_id|>', 'Bridge', 'way', '\n\n', ' Anthony', ':', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [1.8221139907836913e-05, 0.00046184659004211426, 0.0008724868297576905, 0.0005003213882446289]}}, 27: {'grad': {'score': [0.3119840264320374, 0.3273227907954103, 0.2194806267233456, 0.3276498203316834, 0.2516912434782301], 'topk_tokens': [' heart', '      ', '      ', '.', '?', '.', '.', ' of', '.', '--', ' mailing', ' hood', ' duration', ' participate', '--', '.', '.', 'hand', '.', ' candidates'], 'evidence_proportions': [0.36893920898437504, 0.32019500732421874, 0.27025818824768066, 0.288543701171875]}, 'weight': {'score': [0.02634723633527756, 0.0025435732607371063, 0.006610408425331116, 0.0024930098321702746, 0.0024785250425338745], 'topk_tokens': [' Bridge', '.', ' lounge', ' Daniel', '?', ' bathroom', ' \n', 'Answer', ' hallway', 'assistant', '<|eot_id|>', '\n\n', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005954742431640625, 0.020797961950302125, 0.06804357767105103, 0.015951931476593018]}, 'saliency': {'score': [0.002390645444393158, 4.351036015669661e-05, 0.0003051319543053122, 3.891465349943059e-05, 5.699853811945234e-05], 'topk_tokens': ['<|begin_of_text|>', ' Dan', '.', ' Bridge', ' Bridge', ' bathroom', ' there', ' Geo', ':', ' bedroom', 'assistant', 'Bridge', ' hallway', '.', ' Daniel', ' Daniel', 'way', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [7.26938247680664e-05, 0.0030582845211029053, 0.005807298421859741, 0.000624305009841919]}}, 28: {'grad': {'score': [0.42470736503601075, 0.4662695640581469, 0.4114418310277602, 0.4664914066016429, 0.37043040139334543], 'topk_tokens': ['line', ' the', ' law', ' lodge', ' law', 'arch', 'ivery', ' line', '\n', 'ed', ' line', 'burg', 'urred', 'ching', 'arp', '\n\n\n\n\n\n\n', 'ched', ' hall', ' hall', ' line'], 'evidence_proportions': [0.457659912109375, 0.512935447692871, 0.3356925964355469, 0.39254150390625]}, 'weight': {'score': [0.012877465784549713, 0.0024578936253950583, 0.01086221810649423, 0.002417223792017242, 0.0024280399084091187], 'topk_tokens': [' bathroom', 'Question', ' apple', ' Bridge', ' the', ' before', '.\n\n', 'Answer', '?', 'assistant', ' \n', '<|start_header_id|>', '<|eot_id|>', 'hall', '<|eot_id|>', '<|end_header_id|>', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0001568436622619629, 0.005145442485809326, 0.027745378017425538, 0.018462198972702026]}, 'saliency': {'score': [0.0004280254244804382, 4.370966257716116e-05, 0.00019996920052696676, 4.2639774550135736e-05, 4.359654017857143e-05], 'topk_tokens': [' Far', ' most', ' the', ' Daniel', 'Bridge', '?', ' Bridge', ' \n', 'assistant', ' apple', '<|eot_id|>', '<|eot_id|>', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', 'way', ':', '\n\n', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [3.427267074584961e-06, 0.0001562356948852539, 0.0013589024543762206, 0.00019353628158569336]}}, 29: {'grad': {'score': [0.23643091917037964, 0.3088466127908882, 0.25076720293830423, 0.3091283423223613, 0.3164051089968], 'topk_tokens': ['5', ' often', ' by', ' his', ' in', ' hardly', ' for', ' not', ' George', ' no', ' very', ' pur', ' Far', ' in', ' pur', ' probably', '\n', ' was', ' was', 'A'], 'evidence_proportions': [0.3029825210571289, 0.213262939453125, 0.19393134117126465, 0.23554687500000002]}, 'weight': {'score': [0.00495026558637619, 0.0025062541499055045, 0.004021601641879362, 0.0024979906072341855, 0.0020294242671557833], 'topk_tokens': [' the', '<|start_header_id|>', ' Does', ' bathroom', ' hallway', ' before', '?', 'Answer', 'hall', '.\n\n', ' \n', '<|start_header_id|>', 'assistant', '<|eot_id|>', ':', 'way', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.4922361373901366e-05, 0.003501182794570923, 0.006394094228744507, 0.00987086296081543]}, 'saliency': {'score': [0.00015344321727752685, 1.8429703417077528e-05, 0.00012653364854700426, 1.7904945852334606e-05, 9.652014289583479e-05], 'topk_tokens': [':', ' the', '\u200d', ' ', ' the', '\n\n', 'hall', '<|start_header_id|>', ' before', '.\n\n', ' \n', '<|start_header_id|>', '?', ' Does', 'way', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', ':'], 'evidence_proportions': [4.3511390686035155e-07, 8.414983749389648e-05, 0.000215834379196167, 0.0003133535385131836]}}, 30: {'grad': {'score': [0.28982425928115846, 0.31613920368705956, 0.2570600194089553, 0.31634784493426726, 0.29700117451804026], 'topk_tokens': ['AM', ' S', '186', ' S', ' in', '      ', 'D', ' States', 'ire', ' People', 'ar', ' hour', ' Bridge', ' Europe', ' Star', ' OCC', ' M', ' d', ' an', ' Loan'], 'evidence_proportions': [0.4040743350982666, 0.32897377014160156, 0.31036987304687497, 0.11587905883789062]}, 'weight': {'score': [0.018342170119285583, 0.0024589579521886876, 0.011777352760819829, 0.0024067365804326876, 0.005616748439414161], 'topk_tokens': [' Anthony', '<|end_header_id|>', ' Miles', ' bathroom', 'Question', '.\n\n', '<|start_header_id|>', 'assistant', '?', 'hall', 'Answer', '\n\n', '<|end_header_id|>', '<|start_header_id|>', ' \n', '<|eot_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00030717849731445315, 0.014139723777770997, 0.02362571954727173, 0.035296058654785155]}, 'saliency': {'score': [0.0006466835737228393, 3.715934215393741e-05, 0.0005210576688542086, 3.4801891311205954e-05, 0.0001971402338572911], 'topk_tokens': [' Bridge', ' the', ' the', ' Bridge', 'Question', '.\n\n', ' Anthony', ' the', ' Sandra', '<|start_header_id|>', 'assistant', ' bathroom', ' Miles', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>', ':', 'hall'], 'evidence_proportions': [2.8860569000244143e-05, 0.0008136749267578125, 0.0007227897644042969, 0.001021409034729004]}}, 31: {'grad': {'score': [0.49427947998046873, 0.5570389239055842, 0.4533202227424173, 0.5574324732650945, 0.27735144751412527], 'topk_tokens': [' Democrats', ',', ' evening', 'F', ' the', ' paid', ' would', ' work', ' have', ' w', ' government', ' had', ' able', ' the', ' the', ' these', ' F', ' the', ' Independent', ' United'], 'evidence_proportions': [0.55655517578125, 0.5052001953125, 0.5510009765624999, 0.364361572265625]}, 'weight': {'score': [0.00384628027677536, 0.002278841975789428, 0.0033997174571542177, 0.0022731252241527103, 0.002285041447196688], 'topk_tokens': [' the', ':', ' Where', 'Question', ' apple', ' before', '.\n\n', '<|eot_id|>', '?', 'Answer', ' \n', '<|start_header_id|>', 'assistant', ':', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.00029682517051696775, 0.0017767608165740966, 0.0038205444812774657, 0.009490990638732912]}, 'saliency': {'score': [0.00016748756170272827, 3.542846068129388e-05, 8.371121743146111e-05, 3.5075967203933025e-05, 6.10649585723877e-05], 'topk_tokens': ['.', ' Where', ':', ' Bridge', ' Miles', ' the', 'Question', ' apple', ' the', '.\n\n', 'Answer', ' the', '<|eot_id|>', ' hallway', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', 'hall', 'way'], 'evidence_proportions': [3.1781196594238284e-05, 9.528398513793945e-05, 0.0002016782760620117, 0.00034120678901672363]}}, 'pred_res': 'The apple was dropped by Daniel.<|eot_id|>', 'score': 0}
2025-01-22 03:26:20.483 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:26:20.483 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-3_1-4-6-7.pkl | len: 10 |  size: 9.13 KB
Processing depth (1, 4, 6, 7):   4%|▍         | 4/100 [01:16<30:47, 19.24s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (0, 3, 7, 8):   4%|▍         | 4/100 [01:23<30:47, 19.24s/it]2025-01-22 03:26:28.008 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:26:28.008 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 03:26:28.009 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 03:26:28.019 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3797, 3802) --> . Daniel travelled to the
2025-01-22 03:26:28.020 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 03:26:28.043 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (8437, 8442) --> . Daniel dropped the apple
2025-01-22 03:26:28.043 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 03:26:28.071 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (9731, 9736) --> . Mary travelled to the
2025-01-22 03:26:28.071 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  John went back to the office.
2025-01-22 03:26:28.085 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (4532, 4538) --> . John went back to the
2025-01-22 03:26:28.085 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:26:28.094 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (3130, 3135) --> . Sandra moved to the
2025-01-22 03:26:28.094 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John moved to the garden.
2025-01-22 03:26:28.103 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3334, 3339) --> . John moved to the
2025-01-22 03:26:28.103 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 03:26:28.120 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (5746, 5752) --> . John journeyed to the
2025-01-22 03:26:28.120 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 03:26:28.152 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10865, 10871) --> . Sandra went back to the
2025-01-22 03:26:28.152 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 03:26:28.179 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (9198, 9204) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:26:30.321 | INFO     | test_jbb_retain:begin_test:632 - The hallway.<|eot_id|>
2025-01-22 03:26:30.322 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12225])
your chose emoji: ['🏃🏼\u200d♂️\u200d➡', '👨🏽\u200d🤝\u200d👨🏼', '👧🏻', '💆🏾\u200d♂️', '🛀🏽', '👨\u200d✈', '🇮🇴', '🇨🇮', '💑🏼', '🚵\u200d♂️']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 220752.84it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|█▎        | 1/8 [00:04<00:33,  4.80s/it][A
 50%|█████     | 4/8 [00:04<00:03,  1.06it/s][A
 75%|███████▌  | 6/8 [00:05<00:01,  1.78it/s][A
100%|██████████| 8/8 [00:05<00:00,  2.68it/s][A100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.91it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.48it/s][A
100%|██████████| 8/8 [00:00<00:00, 16.59it/s][A100%|██████████| 8/8 [00:00<00:00, 17.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 21.38it/s][A
 75%|███████▌  | 6/8 [00:00<00:00, 18.33it/s][A
100%|██████████| 8/8 [00:00<00:00, 16.69it/s][A100%|██████████| 8/8 [00:00<00:00, 17.41it/s]
2025-01-22 03:26:39.369 | INFO     | test_jbb_retain:begin_test:679 - {24: {'grad': {'score': [0.36396636962890627, 0.34911929683666587, 0.357729294720818, 0.3490708590689719, 0.5333881855010987], 'topk_tokens': [' division', 'system', 'ion', 'ured', ' Wide', ' appearance', ' procession', 'icing', ' disposed', ' high', ' proceed', ' parade', ' procession', 'hand', ' hand', 'hand', ' hand', ' hand', ' hand', ' Project'], 'evidence_proportions': [0.288873291015625, 0.41024169921875, 0.315863037109375, 0.440887451171875]}, 'weight': {'score': [0.02251437455415726, 0.0025514179013857155, 0.007057714111664716, 0.0025060364980503355, 0.0008444555103778839], 'topk_tokens': [' Seventh', ' Fort', ' top', ' Daniel', '.\n\n', 'Answer', ' hallway', '.', ':', '<|start_header_id|>', ' hallway', '<|eot_id|>', 'assistant', ' Bridge', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.039602112770080564, 0.011796677112579345, 0.03412747383117676, 0.004531234502792358]}, 'saliency': {'score': [0.0003958091139793396, 3.531069479855908e-05, 0.0002497241777532241, 3.411962967583243e-05, 2.212412655353546e-05], 'topk_tokens': [' dropped', ' Bench', ':', 'THE', '<|eot_id|>', ' Fort', '.', 'assistant', ' Seventh', '\n\n', ' old', '<|end_header_id|>', ' Bridge', ' bedroom', ' office', 'Bridge', ' garden', 'way', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.00011165738105773927, 0.00029355287551879883, 0.0010397672653198243, 0.0001382589340209961]}}, 25: {'grad': {'score': [0.41504392623901365, 0.6615498960505601, 0.4371165107278263, 0.6625816731572798, 0.5371601581573486], 'topk_tokens': [' not', ' bouncing', 'first', ' many', ' of', 'ad', ' which', ' of', 'which', ' success', ' per', ' post', ' possessed', ' of', ' foreign', 'po', '10', 'st', ' post', '10'], 'evidence_proportions': [0.3182018280029297, 0.416619873046875, 0.44484252929687496, 0.480511474609375]}, 'weight': {'score': [0.01824609935283661, 0.002541841928733499, 0.004226390929783092, 0.0025113375904291003, 0.0009234361350536346], 'topk_tokens': [' the', ' there', ' Seventh', ' Anthony', ' apple', ' \n', 'Answer', '.\n\n', ' Daniel', 'hall', '<|eot_id|>', '<|start_header_id|>', ':', 'assistant', '<|eot_id|>', '.', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0053997099399566645, 0.009926837682724, 0.04991907477378845, 0.007738775014877319]}, 'saliency': {'score': [0.0014184534549713135, 4.112676375825537e-05, 8.402940104989444e-05, 3.8744208846790204e-05, 3.220699727535248e-05], 'topk_tokens': [' Ramsey', 'assistant', ' Ramsey', '.', '<|eot_id|>', ' Fort', ' there', ' Daniel', ' top', ' Merch', 'way', ' Mary', '<|eot_id|>', 'hall', ' apple', ' Anthony', '.', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00032194852828979495, 0.0005366861820220946, 0.0033391177654266357, 0.0014760613441467283]}}, 26: {'grad': {'score': [0.39729843139648435, 0.46224864776281893, 0.392855447881362, 0.462549154837182, 0.4182397127151489], 'topk_tokens': ['ian', '3', ',', ' City', 'blue', ' Marshall', 'ian', 'cert', ' could', 'up', ' not', 'asca', ' Cl', 'outs', 'could', ' not', ' and', ' it', 'ucci', 'hall'], 'evidence_proportions': [0.5528564453125, 0.37747802734374997, 0.3803802490234375, 0.27847900390625]}, 'weight': {'score': [0.014164642989635467, 0.0024727930381557776, 0.004450863775085001, 0.0024480607066225766, 0.0011251211166381837], 'topk_tokens': [' Anthony', ' the', ' apple', ' hallway', ' hallway', ' Bridge', ' \n', '.\n\n', 'Answer', 'assistant', ' the', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'hall', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.019325536489486695, 0.006288576126098633, 0.023393547534942626, 0.007650911808013916]}, 'saliency': {'score': [0.0005097776651382446, 9.22760183139685e-05, 0.00018411436501671285, 9.13336381000389e-05, 2.915188670158386e-05], 'topk_tokens': [' Ramsey', ' apple', ' hallway', '.', ' Anthony', 'Answer', 'Bridge', ' Seventh', '<|start_header_id|>', '<|eot_id|>', ' hallway', '<|eot_id|>', ' the', ' Bridge', 'way', ':', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.0006498575210571289, 0.00017531514167785646, 0.0010113179683685302, 0.00020262002944946287]}}, 27: {'grad': {'score': [0.40503711700439454, 0.3256216202056755, 0.30785420361687155, 0.3255407743233069, 0.38928372859954835], 'topk_tokens': [' hand', '.', '.', ' majority', '?', '.', '.', ' hand', '.', ' business', '.', '.', ' attempt', ' hand', ' duration', ' majority', '.', '.', 'hand', ' EAR'], 'evidence_proportions': [0.4179443359375, 0.40267333984375, 0.3417182922363281, 0.4578125]}, 'weight': {'score': [0.034512351453304294, 0.002534822969995061, 0.00463031670626472, 0.0024764364613126763, 0.0016382962465286254], 'topk_tokens': [' apple', 'THE', '.', ' \n', '<|eot_id|>', ' hallway', ' bathroom', ' Bridge', '<|eot_id|>', 'Answer', 'assistant', '\n\n', '.\n\n', ' hallway', '<|start_header_id|>', ':', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.08511933088302613, 0.01334580183029175, 0.03530791401863098, 0.00427635908126831]}, 'saliency': {'score': [0.0020429104566574096, 5.2115017481789235e-05, 0.00021628421895644243, 4.838595048378934e-05, 7.524564862251281e-05], 'topk_tokens': [' Ramsey', ' top', ' THE', ' bedroom', ' bathroom', 'Immediately', ' hallway', ' Daniel', ':', '<|begin_of_text|>', ' hallway', 'assistant', 'THE', ' Bridge', '<|start_header_id|>', 'way', ' Daniel', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.001631045341491699, 0.0017884790897369385, 0.004564076662063599, 0.00018804073333740235]}}, 28: {'grad': {'score': [0.43797664642333983, 0.433841644177298, 0.40149352129767923, 0.43392519404857993, 0.3135249614715576], 'topk_tokens': [' found', ' *\n\n', 'ched', ' hallway', '\n', 'arp', ' line', 'urred', '***', ' house', 'ed', '\n\n\n\n', ' houses', ' line', ' house', 'burg', ' line', ' hall', '\n\n\n\n\n\n\n', ' hall'], 'evidence_proportions': [0.5962127685546875, 0.3937705993652344, 0.3952911376953125, 0.36663208007812503]}, 'weight': {'score': [0.014166529476642608, 0.0024207114861375723, 0.010848234681522144, 0.0023778782227522285, 0.0005856171250343323], 'topk_tokens': [' bathroom', ' Bridge', ' the', ' hallway', '?', 'Answer', ' \n', ' the', ' before', '.\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', '\n\n', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004636532068252564, 0.01642037630081177, 0.027873432636260985, 0.0077357769012451165]}, 'saliency': {'score': [0.0007418438792228699, 5.5481787889191714e-05, 0.00017771738417008345, 5.401281695932282e-05, 1.0991841554641723e-05], 'topk_tokens': [' to', ' Third', ' the', 'over', '<|eot_id|>', ' dropped', ' hallway', 'Bridge', '<|end_header_id|>', ' Daniel', '<|eot_id|>', ' Bridge', ' the', ' Bridge', ':', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'way', 'hall'], 'evidence_proportions': [0.0002470433712005615, 0.0008749306201934814, 0.0017543017864227294, 9.109973907470704e-05]}}, 29: {'grad': {'score': [0.3260774612426758, 0.24484303842231966, 0.34538830027860756, 0.24442877627680293, 0.2821913778781891], 'topk_tokens': [' the', '\n', ' Far', ' was', '\n', ' evil', ' George', '\n', '5', '\n', '\n', ' Get', '\n', '\n', '\n', 'A', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.23756256103515624, 0.37999496459960935, 0.29835205078125, 0.3884002685546875]}, 'weight': {'score': [0.004674340784549713, 0.002475085974441543, 0.004886429099475636, 0.002464738450008052, 0.0007702469825744629], 'topk_tokens': [' Where', ' the', '?', '<|start_header_id|>', ' before', ' the', ' \n', ' Does', 'Answer', '<|eot_id|>', 'assistant', '.\n\n', 'hall', ':', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'way', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.002064073085784912, 0.006194382905960083, 0.006601214408874512, 0.0038376927375793454]}, 'saliency': {'score': [0.0002803534269332886, 2.2690113186641264e-05, 0.00019036584040697883, 2.179852118726354e-05, 2.772025763988495e-05], 'topk_tokens': [' ', 'Right', ' to', ' was', ' the', 'Answer', 'assistant', ' the', '<|start_header_id|>', ' before', '\n\n', '<|eot_id|>', 'way', ' Does', '<|eot_id|>', '<|end_header_id|>', ':', 'hall', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.105303764343261e-05, 0.0008493423461914062, 0.00013344287872314451, 7.75754451751709e-05]}}, 30: {'grad': {'score': [0.21566152572631836, 0.2725967991223396, 0.21885277243221507, 0.27284043329150215, 0.2650700569152832], 'topk_tokens': [' an', 'Square', ' Pioneer', ' Pioneer', ' Pioneer', 'ISC', ' Republicans', ' compromised', ' Europe', ' Republican', ' preparations', ' Republicans', ' Fourth', ' hour', 'AM', ' Press', ' Bridge', 'ire', ' Star', ' Loan'], 'evidence_proportions': [0.12911529541015626, 0.23231048583984376, 0.21845970153808594, 0.2827606201171875]}, 'weight': {'score': [0.017231523990631104, 0.0024004932474692275, 0.011475615641650031, 0.0023507828173505003, 0.0026679553091526033], 'topk_tokens': ['.', ' bathroom', ' the', '<|end_header_id|>', '<|eot_id|>', '?', '<|start_header_id|>', 'Answer', 'assistant', 'hall', ' \n', '.\n\n', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0048919439315795905, 0.021275103092193604, 0.03193931579589844, 0.010819733142852783]}, 'saliency': {'score': [0.0008589074015617371, 3.672720200882108e-05, 0.00045380697530858656, 3.4211651139488866e-05, 0.00014975294470787048], 'topk_tokens': ['assistant', '�', ' Anthony', ' the', '<|end_header_id|>', '\n\n', '.', 'Question', '<|eot_id|>', ' Daniel', ' Sandra', '.', ' bathroom', ' the', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', 'way', 'hall', ':'], 'evidence_proportions': [0.00018584728240966797, 0.0012875199317932129, 0.0016873955726623537, 0.00027486681938171387]}}, 31: {'grad': {'score': [0.24773735255002977, 0.3159026272514501, 0.24266338436042562, 0.3162191575416031, 0.16497859843075274], 'topk_tokens': [',', ' I', 'ire', 'F', ' intense', 'proved', ' decided', ' would', ' the', ' determined', ' F', ' United', 'Independent', ' government', ' evening', ' these', ' the', ' had', ' the', ' Independent'], 'evidence_proportions': [0.19569790959358216, 0.2806479513645172, 0.329177188873291, 0.18542636036872864]}, 'weight': {'score': [0.004039990901947022, 0.0022779942962676434, 0.0029592347495696124, 0.0022731970145586033, 0.0012181472033262254], 'topk_tokens': ['Question', ' the', ':', ' before', ' the', '?', ' Where', '.\n\n', '<|eot_id|>', 'Answer', ' \n', 'assistant', ':', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018426001071929932, 0.003889906406402588, 0.006092393398284912, 0.004335063695907592]}, 'saliency': {'score': [0.0002464041113853455, 2.7821506204502154e-05, 6.132967331830193e-05, 2.736882591983926e-05, 1.2605264782905578e-05], 'topk_tokens': [' Where', '<|eot_id|>', ' How', ' ', ' \n', ' apple', 'Question', ' the', ' hallway', '<|begin_of_text|>', ' the', ' the', '<|end_header_id|>', 'assistant', ' hallway', '.\n\n', ':', '<|start_header_id|>', 'way', 'hall'], 'evidence_proportions': [0.0004931211471557617, 0.00017804503440856936, 0.0001836061477661133, 0.00013084411621093752]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 03:26:39.371 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:26:39.371 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-4_0-3-7-8.pkl | len: 10 |  size: 9.2 KB
Processing depth (0, 3, 7, 8):   5%|▌         | 5/100 [01:35<30:15, 19.12s/it]Processing depth (0, 3, 7, 8):   5%|▌         | 5/100 [01:35<30:15, 19.11s/it]
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:72 - Selected idx: 19
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:73 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:74 - Answer: hallway
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:75 - Tag: 4-hop
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:76 - Needle: [' Sandra journeyed to the office.', ' Sandra moved to the kitchen.', ' Daniel went to the hallway.', ' John journeyed to the bedroom.', ' Sandra went back to the hallway.', ' Daniel got the football.', ' John went back to the office.', ' Daniel travelled to the bathroom.', ' John moved to the garden.', ' Daniel dropped the football there.', ' Mary travelled to the bedroom.']
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:77 - Real Needle: [' Daniel went to the hallway.', ' Daniel got the football.', ' Daniel travelled to the bathroom.', ' Daniel dropped the football there.', ' Mary travelled to the bedroom.']
2025-01-22 03:26:39.868 | INFO     | __main__:<module>:78 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.27it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]
Processing depth (0, 2, 3, 6, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 03:26:46.968 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 03:26:46.969 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 03:26:46.969 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Daniel got the football.
2025-01-22 03:26:46.976 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (2508, 2512) -->  Daniel got the football
2025-01-22 03:26:46.976 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 03:26:46.987 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (3826, 3831) -->  Daniel travelled to the bathroom
2025-01-22 03:26:46.987 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 03:26:47.007 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (7184, 7189) --> . Daniel dropped the football
2025-01-22 03:26:47.007 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 03:26:47.037 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10750, 10755) --> . Mary travelled to the
2025-01-22 03:26:47.037 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 03:26:47.049 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 0 at --> (3829, 3835) -->  the bathroom. Sandra journeyed
2025-01-22 03:26:47.049 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 03:26:47.062 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 1 at --> (4861, 4866) -->  war. Sandra moved to
2025-01-22 03:26:47.062 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 03:26:47.065 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 2 at --> (874, 880) --> . John journeyed to the
2025-01-22 03:26:47.065 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 03:26:47.072 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 3 at --> (2365, 2371) -->  the senate. Sandra went back
2025-01-22 03:26:47.072 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 4 -->  John went back to the office.
2025-01-22 03:26:47.105 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 4 at --> (10924, 10930) --> . John went back to the
2025-01-22 03:26:47.105 | INFO     | test_jbb_retain:find_multi_needle_idx:481 - evidence 5 -->  John moved to the garden.
2025-01-22 03:26:47.135 | INFO     | test_jbb_retain:find_multi_needle_idx:489 - find evidence 5 at --> (10888, 10893) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:26:49.247 | INFO     | test_jbb_retain:begin_test:632 - The hallway.<|eot_id|>
2025-01-22 03:26:49.248 | INFO     | test_jbb_retain:begin_test:634 - torch.Size([1, 12218])
your chose emoji: ['👩🏻\u200d🌾', '🏊\u200d♀', '💃🏿', '👨🏽\u200d🦽\u200d➡', '⚒', '📩', '🤞🏻', '🧒🏼', '♎', '🕵🏽']
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 199728.76it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A