nohup: ÂøΩÁï•ËæìÂÖ•
2025-01-22 00:45:47.940 | INFO     | test_jbb_retain:<module>:7 - ['/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis', '/data/anaconda3/envs/zecheng/lib/python310.zip', '/data/anaconda3/envs/zecheng/lib/python3.10', '/data/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/root/.local/lib/python3.10/site-packages', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages', '__editable__.lm_eval-0.4.3.finder.__path_hook__', '__editable__.trl-0.8.7.dev0.finder.__path_hook__', '/data/zecheng/modelzipper/src', '/tmp/tmp1_th2eek', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 00:45:48.585 | INFO     | __main__:<module>:11 - ['/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/faiss_attn', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect', '/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis', '/data/anaconda3/envs/zecheng/lib/python310.zip', '/data/anaconda3/envs/zecheng/lib/python3.10', '/data/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/root/.local/lib/python3.10/site-packages', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages', '__editable__.lm_eval-0.4.3.finder.__path_hook__', '__editable__.trl-0.8.7.dev0.finder.__path_hook__', '/data/zecheng/modelzipper/src', '/tmp/tmp1_th2eek', '/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 00:45:48.930 | INFO     | __main__:<module>:70 - Selected idx: 0
2025-01-22 00:45:48.930 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 00:45:48.931 | INFO     | __main__:<module>:72 - Answer: office
2025-01-22 00:45:48.931 | INFO     | __main__:<module>:73 - Tag: 3-hop
2025-01-22 00:45:48.931 | INFO     | __main__:<module>:74 - Needle: [' John went back to the bedroom.', ' John took the milk.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 00:45:48.931 | INFO     | __main__:<module>:75 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 00:45:48.931 | INFO     | __main__:<module>:76 - =============================================
ModelZipper is ready for launchüöÄ | Current Versionü¶Ñ >>> 0.2.7 <<< | AOE Timeüïí 2025-01-22 04:45:43
Pid: 136324
begin to read data from /data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/haystack_for_detect/reasoning_needle_jbb_200.jsonl | file size: 247.16 KB | file type: jsonl
  0%|          | 0/100 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.36it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.37it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.64it/s]
Processing depth (3, 4, 5, 7):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-22 00:45:58.817 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:45:58.829 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (3754, 3760) --> . Mary journeyed to the
2025-01-22 00:45:58.829 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:45:58.840 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3754, 3760) --> . Mary journeyed to the
2025-01-22 00:45:58.841 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:45:58.857 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5918, 5922) -->  Mary dropped the football
2025-01-22 00:45:58.857 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:45:58.882 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8341, 8347) --> . Daniel went back to the
2025-01-22 00:45:58.882 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:45:58.908 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (8742, 8748) --> . John went back to the
2025-01-22 00:45:58.908 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John took the milk.
2025-01-22 00:45:58.920 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3989, 3993) -->  John took the milk
2025-01-22 00:45:58.920 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 00:45:58.945 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8247, 8253) --> . Sandra journeyed to the
2025-01-22 00:45:58.945 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:45:58.956 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3755, 3761) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-22 00:46:02.473 | INFO     | test_jbb_retain:begin_test:544 - Mary dropped the football.<|eot_id|>
2025-01-22 00:46:02.473 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12133])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:00<00:05,  1.40it/s][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:03,  1.59it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:01<00:02,  1.68it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:02<00:02,  1.73it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:02<00:01,  1.75it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:03<00:01,  1.76it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:04<00:00,  1.78it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.78it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.73it/s]
2025-01-22 00:46:15.766 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5648248845880682, 0.6106241711088792, 0.6384879025545988, 0.6106568054825169], 'topk_tokens': [' from', ' Minnesota', ' THE', 'Minnesota', 'fire', ' two', ' over', ' the', 'sur', 'cont', ' of', 'super', 'active', ' cont', ' cons', ' Minnesota', 'cont', 'ire', 'con', ' Minnesota'], 'evidence_proportions': [0.6566975911458334, 0.6566975911458334, 0.327239990234375, 0.5394694010416666]}, 'weight': {'score': [0.0459614558653398, 0.0025828948962035177, 0.01702064275741577, 0.002477696087646106], 'topk_tokens': [' Sandra', '.', ' football', ' bathroom', '.', 'Bridge', 'Answer', ' Mary', '<|eot_id|>', '<|eot_id|>', ' the', '<|start_header_id|>', 'assistant', ':', ' office', '<|end_header_id|>', ' bedroom', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0028045972188313804, 0.0028045972188313804, 0.2213287353515625, 0.015363653500874838]}, 'saliency': {'score': [0.009967224164442583, 9.35794101804679e-05, 0.0011316212740811434, 7.372521238048599e-05], 'topk_tokens': [' THE', ' top', ' random', ':', ' hand', ' Sandra', 'Bridge', 'Question', '<|eot_id|>', '.', '<|end_header_id|>', '<|eot_id|>', ' Dul', ' bedroom', '<|begin_of_text|>', 'office', ' office', ' football', ' Mary', ' bedroom'], 'evidence_proportions': [0.0004556576410929362, 0.0004556576410929362, 0.05278611183166504, 0.0004444320996602376]}}, 25: {'grad': {'score': [0.7503599687056108, 0.7088842642665842, 0.5230345292524858, 0.7091469578959492], 'topk_tokens': [' the', ' Republican', 'ation', 'ers', ' stere', ' the', 'ing', ' prisoners', ' construed', 'ylinder', ' printers', 'ing', 'ised', 'ervative', ' Gree', ' private', ' extraordinary', 'ation', ' private', 'erc'], 'evidence_proportions': [0.8402862548828125, 0.8402862548828125, 0.7843437194824219, 0.5478515625]}, 'weight': {'score': [0.022650057619268246, 0.00256633404895608, 0.008517251773314043, 0.002518963102923267], 'topk_tokens': [' football', ' random', '.\n\n', ' the', ' Miss', ' \n', ' Anthony', ' East', 'Answer', '<|start_header_id|>', ' THE', '<|eot_id|>', ' Mary', '<|eot_id|>', ':', '.', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.005304267009099324, 0.005304267009099324, 0.09658050537109375, 0.008054673671722412]}, 'saliency': {'score': [0.003570803187110207, 6.491183546967696e-05, 0.0009735118259083141, 5.687951312034758e-05], 'topk_tokens': [' directly', 'Mer', ' Sandra', ' Minneapolis', ' Met', ' East', 'assistant', '<|begin_of_text|>', ' Mary', ' random', ' return', ' THE', ':', ' Anthony', 'Answer', '<|eot_id|>', '.', '<|eot_id|>', ' Mary', '<|end_header_id|>'], 'evidence_proportions': [0.0018873810768127441, 0.0018873810768127441, 0.013426050543785095, 0.0003674825032552083]}}, 26: {'grad': {'score': [0.8639470880681818, 0.9884856561598682, 0.9423633922230114, 0.9887961795495487], 'topk_tokens': [' wh', ' some', 'field', ' Press', 'burn', ' Gutenberg', ' gold', ' Field', 'ol', ' Merch', ' generally', 'BO', 'Cut', ' generally', ' gang', ' Guards', 'str', ' worth', ' STR', ' str'], 'evidence_proportions': [0.9318033854166666, 0.9318033854166666, 0.51922607421875, 0.9580485026041666]}, 'weight': {'score': [0.008609162135557695, 0.002538560003619357, 0.0071391327814622355, 0.0025191434253367347], 'topk_tokens': ['.\n\n', ' Mary', ' before', ' kitchen', ' bathroom', '?', ' football', 'Bridge', ' bedroom', '<|eot_id|>', '<|eot_id|>', ' the', ' \n', 'Answer', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006417532761891683, 0.0006417532761891683, 0.03514289855957031, 0.006854822238286336]}, 'saliency': {'score': [0.000560638579455289, 0.00010865011674954068, 0.0007279163057153875, 0.00010670093121180393], 'topk_tokens': [' Sandra', ' office', '<|eot_id|>', ' Mary', ' kitchen', 'assistant', ' Bridge', ' bathroom', ' Dan', 'Answer', '.', ' East', '<|start_header_id|>', ' the', '<|begin_of_text|>', 'Bridge', ' bedroom', '<|end_header_id|>', 'office', ':'], 'evidence_proportions': [5.447864532470703e-05, 5.447864532470703e-05, 0.0022136420011520386, 0.00047095616658528644]}}, 27: {'grad': {'score': [0.5288807262073864, 0.4715110121806757, 0.35017533735795453, 0.47162740049739993], 'topk_tokens': [' plainly', 'ab', ' head', ' vigilant', ' STE', ' N', ' Temper', 'ad', '185', 'MIN', ' vain', 'fire', 'sur', ' N', ' carn', 'ad', 'ile', 'n', 'bread', 'na'], 'evidence_proportions': [0.5243021647135416, 0.5243021647135416, 0.681427001953125, 0.43634033203125]}, 'weight': {'score': [0.01993417198007757, 0.002563360879879996, 0.008742234923622826, 0.0025205113990126823], 'topk_tokens': ['?', ' before', '<|eot_id|>', 'Bridge', ' \n', '<|eot_id|>', ' bathroom', ' football', ' Mary', ' bedroom', 'Answer', '.', '.\n\n', '<|start_header_id|>', 'assistant', ' THE', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0029317637284596763, 0.0029317637284596763, 0.08436965942382812, 0.010981996854146322]}, 'saliency': {'score': [0.007996778596531261, 0.00016771099623407804, 0.0023822784423828125, 0.00014943624881684123], 'topk_tokens': [' Mary', ' the', ' office', 'THE', ' Dan', ' East', ' the', ' the', ' THE', ' Anthony', 'Bridge', ' the', ' John', ' bathroom', ' bedroom', '.', ' Mary', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0016094644864400227, 0.0016094644864400227, 0.03535640239715576, 0.002531657616297404]}}, 28: {'grad': {'score': [0.4348505193536932, 0.5088546495158632, 0.43855146928267047, 0.5091172217455132], 'topk_tokens': [' Hook', ' Gov', 'ib', ' been', ' ', 'nes', ' about', 'hom', ' ende', 'F', ' adj', ' ende', ' the', '.', ' Gov', ' firm', ' reached', ' escaping', ' ins', ' endeavor'], 'evidence_proportions': [0.3645426432291667, 0.3645426432291667, 0.593475341796875, 0.4697163899739583]}, 'weight': {'score': [0.005789334123784845, 0.002455762997545919, 0.006252827969464389, 0.002442788554227793], 'topk_tokens': [' the', ' bedroom', '.\n\n', ' the', '.', 'Question', ' football', ' bathroom', ' before', '?', '<|eot_id|>', ' \n', '<|eot_id|>', '<|start_header_id|>', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00018390019734700522, 0.00018390019734700522, 0.014469146728515625, 0.01121366024017334]}, 'saliency': {'score': [0.00048570199446244675, 5.806841180311784e-05, 0.0002480203455144709, 5.694469322234247e-05], 'topk_tokens': ['Bridge', ' Mary', ' nearly', ' football', ' Dul', '<|eot_id|>', 'Question', '<|start_header_id|>', ' during', ' kitchen', ' before', '<|eot_id|>', 'Answer', ' Bridge', ' Far', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', 'office', ':'], 'evidence_proportions': [1.5129645665486654e-05, 1.5129645665486654e-05, 0.002090662717819214, 0.0003568728764851888]}}, 29: {'grad': {'score': [0.8987870649857954, 0.982212318448702, 1.0378170013427734, 0.9822629389558988], 'topk_tokens': [' months', ' paper', '"The', ' Republican', ' the', '\n', ' house', ' papers', 'The', ' The', ' routes', ' place', 'paper', ' and', ' the', ' The', ' The', ' The', ' were', '\n'], 'evidence_proportions': [1.1751302083333333, 1.1751302083333333, 0.680816650390625, 0.4914143880208333]}, 'weight': {'score': [0.0045586390928788615, 0.0025427668586796973, 0.006808638572692871, 0.002531337004502154], 'topk_tokens': [' Does', ' the', 'Question', ' bathroom', '.\n\n', '?', ' football', ' Where', '<|eot_id|>', '<|eot_id|>', ' before', ' the', ' \n', 'Answer', '<|end_header_id|>', 'assistant', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00017315149307250977, 0.00017315149307250977, 0.013994216918945312, 0.007039229075113933]}, 'saliency': {'score': [0.00039531967856667257, 7.013514742322748e-05, 0.0007232237945903431, 6.835514494842537e-05], 'topk_tokens': ['Does', ' the', ' the', ' Do', ' to', ' Where', ' football', ':', '<|eot_id|>', '<|end_header_id|>', ' the', 'Answer', 'assistant', ' the', ' before', ' \n', ':', 'office', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.0646721522013344e-05, 3.0646721522013344e-05, 0.001071929931640625, 0.0006735920906066895]}}, 30: {'grad': {'score': [0.5287697531960227, 0.7451375476411207, 0.6160208962180398, 0.7457661687484816], 'topk_tokens': ['ob', ' an', ' Crew', ' Capt', ' Published', 'Country', ',', ' Loan', 'ire', 'office', ' o', ' Bridge', ' Was', ' States', ' o', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.4612223307291667, 0.4612223307291667, 0.638427734375, 0.59075927734375]}, 'weight': {'score': [0.01353720101443204, 0.0024749872170704897, 0.02396060119975697, 0.0024157652990189587], 'topk_tokens': [' the', ' Sandra', '.', ' before', ' Where', '<|eot_id|>', '.\n\n', ' bathroom', '?', ' the', '<|eot_id|>', 'Question', 'Answer', 'assistant', '<|end_header_id|>', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0017556150754292805, 0.0017556150754292805, 0.03615760803222656, 0.02202010154724121]}, 'saliency': {'score': [0.00339502908966758, 0.00012608892788944, 0.003322368318384344, 0.00011432523339353338], 'topk_tokens': ['.\n\n', ' to', ' East', ' office', ' bedroom', ' \n', '?', ' football', ' the', ' Sandra', ' Miles', ' bathroom', ' Mary', '<|begin_of_text|>', 'assistant', 'Question', '<|start_header_id|>', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.001058886448542277, 0.001058886448542277, 0.014165878295898438, 0.0008867482344309489]}}, 31: {'grad': {'score': [0.8458085493607954, 0.573938375566543, 0.6398204456676136, 0.5733238234727793], 'topk_tokens': ['IO', ' Mr', 'user', 'issippi', ' M', 'editary', ' membership', ' an', ' very', 'k', ' M', ' United', ' DAY', ' Mary', 'IR', ' marriage', ' most', ' Mr', ' Mr', 'Mr'], 'evidence_proportions': [0.9086812337239584, 0.9086812337239584, 1.111328125, 0.5430501302083334]}, 'weight': {'score': [0.0024328421462665906, 0.002312803641729115, 0.0019293752583590422, 0.002313282888882727], 'topk_tokens': [' was', ' the', ':', '.\n\n', ' before', 'Question', ' football', ' Where', '?', ' the', '<|eot_id|>', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0009714464346567789, 0.0009714464346567789, 0.008117437362670898, 0.0015659034252166748]}, 'saliency': {'score': [0.0012021552432667124, 0.00010658778130492409, 0.00030608068812977183, 0.00010423137338884553], 'topk_tokens': ['Answer', 'light', ' was', 'ot', ' Where', 'Question', ' the', ' the', '?', ' the', ' football', ' Mary', ' the', '<|eot_id|>', ' \n', ':', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0006817380587259928, 0.0006817380587259928, 0.004291892051696777, 0.0001831650733947754]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 00:46:15.767 | INFO     | modelzipper.tutils:auto_save_data:296 - /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label not exist! --> Create data dir /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label
2025-01-22 00:46:15.768 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:46:15.768 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-0_3-4-5-7.pkl | len: 10 |  size: 8.78 KB
Processing depth (3, 4, 5, 7):   1%|          | 1/100 [00:26<44:06, 26.74s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.32it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.29it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.66it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.52it/s]
Processing depth (1, 2, 6, 9):   1%|          | 1/100 [00:34<44:06, 26.74s/it]2025-01-22 00:46:23.826 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:46:23.831 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:46:23.832 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:46:23.836 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:46:23.836 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:46:23.857 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7155, 7159) -->  Mary dropped the football
2025-01-22 00:46:23.857 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:46:23.889 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10692, 10698) --> . Daniel went back to the
2025-01-22 00:46:23.889 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:46:23.915 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (8735, 8741) --> . John went back to the
2025-01-22 00:46:23.916 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John took the milk.
2025-01-22 00:46:23.927 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3996, 4000) -->  John took the milk
2025-01-22 00:46:23.927 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 00:46:23.952 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8247, 8253) --> . Sandra journeyed to the
2025-01-22 00:46:23.952 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:46:23.956 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1490, 1496) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:46:26.029 | INFO     | test_jbb_retain:begin_test:544 - the bedroom<|eot_id|>
2025-01-22 00:46:26.029 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12133])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:40,  5.78s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.50s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.45s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.05it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.41it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:00,  2.12it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.43it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.07it/s]
2025-01-22 00:46:36.243 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.6703990589488636, 0.6081193909018335, 0.6580144708806818, 0.6079152850002068], 'topk_tokens': [' a', 'cont', ' Minnesota', ' conn', ' the', 'Minnesota', ' Min', ' Ramsey', ' cont', 'irie', 'Minnesota', ' the', ' Minnesota', 'Minnesota', 'ire', ' Minnesota', ' Minnesota', ' cons', 'con', ' Minnesota'], 'evidence_proportions': [0.7930501302083334, 0.7930501302083334, 0.3941650390625, 0.6092529296875]}, 'weight': {'score': [0.049560974944721566, 0.002583072978280236, 0.006957189603285356, 0.0024896361733003484], 'topk_tokens': [' football', ' the', 'Answer', ' the', ' office', '.', ' bathroom', '<|start_header_id|>', 'assistant', '<|eot_id|>', ' bathroom', '<|eot_id|>', ' bedroom', 'Bridge', ':', '.', '<|end_header_id|>', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0008605817953745524, 0.0008605817953745524, 0.26513671875, 0.003244598706563314]}, 'saliency': {'score': [0.00623830340125344, 9.875499912382989e-05, 0.0005602376027540727, 8.674419090890005e-05], 'topk_tokens': ['<|end_header_id|>', ' folded', ' random', ' Market', 'Question', ' Mary', ' Mary', ' bathroom', 'Bridge', ' bedroom', '<|begin_of_text|>', '<|eot_id|>', '.', ' Dul', ' bathroom', '<|eot_id|>', ' bedroom', ' office', ' football', 'office'], 'evidence_proportions': [0.00011085470517476399, 0.00011085470517476399, 0.03380763530731201, 0.0001136461893717448]}}, 25: {'grad': {'score': [0.7188387784090909, 0.5888959975020601, 0.5228784734552557, 0.5887796832475796], 'topk_tokens': [' kinds', ' location', ' stere', '\n', 'ush', ' construed', ' safe', ' news', ' actions', ' extraordinary', 'ised', ' Key', 'ation', 'ing', ' private', 'ervative', ' private', ' Wood', 'ation', 'erc'], 'evidence_proportions': [0.6713358561197916, 0.6713358561197916, 1.12396240234375, 0.54376220703125]}, 'weight': {'score': [0.014541197906840931, 0.0025660695192693045, 0.004453008825128729, 0.0025408469992746403], 'topk_tokens': [' Dan', ' the', '.\n\n', ' dropped', ' East', '.', ' Mary', ' football', ' \n', '.', ' Mary', 'Answer', '<|start_header_id|>', '<|eot_id|>', ':', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016707181930541992, 0.0016707181930541992, 0.07269287109375, 0.0015143752098083496]}, 'saliency': {'score': [0.002483392303640192, 5.673064977286723e-05, 0.00029994411901994186, 5.1872718029544365e-05], 'topk_tokens': ['river', ' directly', ' East', ' Minnesota', 'assistant', ' cartoon', ' random', ' football', ' Anthony', 'Answer', ' dropped', ' Market', ' Mary', ':', ' Dan', '<|begin_of_text|>', ' Mary', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0006110072135925293, 0.0006110072135925293, 0.011702537536621094, 8.206566174825032e-05]}}, 26: {'grad': {'score': [0.6439056396484375, 0.818290327564895, 0.7464488636363636, 0.8187383447132358], 'topk_tokens': [' Col', 'Cut', ' Field', ' Col', ' Gutenberg', ' Merch', ' Capt', 'ol', ' some', ' Gen', 'burn', 'str', ' generally', ' black', 'BO', ' worth', ' STR', ' Guards', ' generally', ' str'], 'evidence_proportions': [0.6717529296875, 0.6717529296875, 0.40494537353515625, 0.7475179036458334]}, 'weight': {'score': [0.0071465047923001375, 0.002527353249806458, 0.0032221539454026656, 0.0025176843267034907], 'topk_tokens': [' the', ' before', ' kitchen', '?', ' football', ' football', ' bedroom', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'Bridge', ' the', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00021095077196756998, 0.00021095077196756998, 0.03437232971191406, 0.0028670628865559897]}, 'saliency': {'score': [0.00033743544058366257, 0.00010037453615670348, 0.00024419210173866964, 9.968151437685099e-05], 'topk_tokens': [' North', ' bathroom', ' Dul', ' \n', ' Ramsey', ' Seventh', 'assistant', ' kitchen', 'Answer', ' bathroom', ' Bridge', ' Bridge', '<|start_header_id|>', ' the', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', 'Bridge', ':', 'office'], 'evidence_proportions': [5.036592483520508e-06, 5.036592483520508e-06, 0.0016270279884338379, 0.00014250477155049643]}}, 27: {'grad': {'score': [0.4271933815696023, 0.4958830265631438, 0.2739247408780185, 0.49641187070134], 'topk_tokens': [' remin', 'use', 'APER', ' earnest', ' Sunday', 'remember', ' vain', 'sur', 'fire', ' const', ' very', ' product', ' vigorous', ' N', ' party', 'ly', ' plainly', 'bread', ' N', 'ile'], 'evidence_proportions': [0.4387003580729167, 0.4387003580729167, 0.6020889282226562, 0.2875823974609375]}, 'weight': {'score': [0.0190176476131786, 0.0025632359237890948, 0.004355213858864524, 0.002530035975750204], 'topk_tokens': [' the', ' Mary', ' Bridge', ' before', ' \n', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' football', 'Bridge', ' football', ' bedroom', 'Answer', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0012748738129933674, 0.0012748738129933674, 0.09812164306640625, 0.0017671982447306316]}, 'saliency': {'score': [0.004749457944523205, 0.00014559276613817815, 0.0009011246941306374, 0.00013584115449809012], 'topk_tokens': ['DW', ' football', ' THE', ' Jackson', '.', ' the', ' Anthony', ' Minnesota', '.', 'Minnesota', ' bathroom', ' the', ' bedroom', '<|begin_of_text|>', 'Bridge', ' Mary', ' Mary', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0008616348107655843, 0.0008616348107655843, 0.023235350847244263, 0.00020117561022440592]}}, 28: {'grad': {'score': [0.44087080522017047, 0.5292524126364854, 0.6045088334517046, 0.5292762943754006], 'topk_tokens': [' have', ' escaping', ' returns', ' take', ' out', ' been', ' balance', ' news', ' become', ' about', ' received', ' be', 'been', ' reached', 'hom', 'half', ' half', 'half', 'na', ' ins'], 'evidence_proportions': [0.46143341064453125, 0.46143341064453125, 0.5215682983398438, 0.345947265625]}, 'weight': {'score': [0.006936035372994163, 0.0024761358700419434, 0.0048836876045573845, 0.002463640300839703], 'topk_tokens': [' to', ' the', 'Question', ' Bridge', ' football', '.\n\n', ' the', '?', ' bathroom', ' before', '<|eot_id|>', ' \n', '<|eot_id|>', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [9.958942731221516e-05, 9.958942731221516e-05, 0.025890350341796875, 0.00797271728515625]}, 'saliency': {'score': [0.0007293874567205256, 5.471166092818e-05, 0.0002225935459136963, 5.317859757716578e-05], 'topk_tokens': [' during', 'Question', ' before', ' football', ' the', ' Nearly', 'Bridge', ' football', ' Dul', '.\n\n', 'assistant', 'office', '<|start_header_id|>', '<|eot_id|>', ' Bridge', ' Bridge', '<|end_header_id|>', '<|begin_of_text|>', ' Far', ':'], 'evidence_proportions': [1.6500552495320637e-05, 1.6500552495320637e-05, 0.003405451774597168, 0.00037111838658650714]}}, 29: {'grad': {'score': [0.7006724964488636, 0.9016872360424393, 0.8275201970880682, 0.9021879389725571], 'topk_tokens': [' business', '"The', ' The', 'boat', ' Newspaper', ' The', ' the', 'paper', 'The', '\n', ' were', ' The', ' the', ' The', ' The', 'APER', '\n', ' The', '\n', ' press'], 'evidence_proportions': [0.7200113932291666, 0.7200113932291666, 0.572235107421875, 0.74761962890625]}, 'weight': {'score': [0.0026345578106966886, 0.0025422763077869895, 0.003534168004989624, 0.0025403036164957423], 'topk_tokens': [' to', ' the', 'Question', '?', ' Where', ' football', ' bathroom', '.\n\n', '<|eot_id|>', ' before', '<|eot_id|>', ' the', ' \n', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.673632939656576e-05, 3.673632939656576e-05, 0.012294769287109375, 0.0013900597890218098]}, 'saliency': {'score': [0.00012551654468883169, 7.635065199762005e-05, 0.0002977929332039573, 7.585827090211546e-05], 'topk_tokens': [' dropped', 'ot', ' journey', ' the', 'assistant', 'Does', ' football', ' to', ' the', ':', ' Do', ' the', 'Answer', ' before', '<|end_header_id|>', ' \n', ':', 'office', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [5.821386973063151e-06, 5.821386973063151e-06, 0.00048363208770751953, 0.00012616316477457681]}}, 30: {'grad': {'score': [0.8284634676846591, 0.8369217076122786, 0.8262606534090909, 0.8369564958407855], 'topk_tokens': [' Was', ' Third', ',', 'Third', 'op', ' Fourth', 'Emp', ' Paul', ' its', ' States', ' Loan', 'ob', ' an', ' Bridge', ' o', ' o', ' o', ' o', 'ire', ' O'], 'evidence_proportions': [0.63800048828125, 0.63800048828125, 1.09130859375, 1.0341593424479167]}, 'weight': {'score': [0.008581730452450838, 0.0024758175856594592, 0.010995854030955921, 0.002449205156177536], 'topk_tokens': [' Wide', ' the', ' the', ' Min', ' before', ' the', '?', '<|eot_id|>', 'Question', '.\n\n', ' bathroom', '<|eot_id|>', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008823176225026449, 0.0008823176225026449, 0.038665771484375, 0.00392452875773112]}, 'saliency': {'score': [0.001829296350479126, 0.0001008459113977257, 0.0012375360185449774, 9.56326873453702e-05], 'topk_tokens': [' to', ' Miles', ' the', 'IR', ' bedroom', ' East', ' Sandra', ' Mary', '?', '.\n\n', ' football', ' office', ' bathroom', 'Question', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0005525648593902588, 0.0005525648593902588, 0.008290290832519531, 7.542967796325684e-05]}}, 31: {'grad': {'score': [1.0120350230823865, 0.886822643052122, 0.7510486949573864, 0.8868418601968923], 'topk_tokens': [' members', ' membership', ' M', ' the', ' the', ' very', ' an', ' H', ' the', ' an', ' the', ' an', ' B', 'Mr', ' marriage', ' an', ' most', ' Mr', ' the', ' an'], 'evidence_proportions': [1.1558736165364583, 1.1558736165364583, 1.155792236328125, 0.6285196940104166]}, 'weight': {'score': [0.0018594156612049449, 0.0023272411784960698, 0.0015011267228560014, 0.002329595547812461], 'topk_tokens': [' bathroom', ' the', ':', ' football', ' before', '?', 'Question', '.\n\n', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00045287609100341797, 0.00045287609100341797, 0.007192134857177734, 0.0011173486709594727]}, 'saliency': {'score': [0.0006425732916051692, 8.748691662575023e-05, 0.00017421624877236107, 8.63191103601996e-05], 'topk_tokens': ['ot', ' Where', ' Market', ' the', ' before', ' the', ' Mary', ' the', 'Question', ' football', ' the', '<|end_header_id|>', ' the', '<|eot_id|>', ' \n', ':', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.00025568405787150067, 0.00025568405787150067, 0.002644181251525879, 8.194645245869954e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 00:46:36.245 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:46:36.246 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-1_1-2-6-9.pkl | len: 10 |  size: 8.89 KB
Processing depth (1, 2, 6, 9):   2%|‚ñè         | 2/100 [00:47<37:39, 23.05s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (0, 1, 2, 4):   2%|‚ñè         | 2/100 [00:54<37:39, 23.05s/it]2025-01-22 00:46:43.584 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:46:43.584 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:46:43.584 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:46:43.589 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:46:43.589 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:46:43.596 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (2469, 2473) -->  Mary dropped the football
2025-01-22 00:46:43.597 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:46:43.611 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (4849, 4855) --> . Daniel went back to the
2025-01-22 00:46:43.611 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:46:43.637 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (8742, 8748) --> . John went back to the
2025-01-22 00:46:43.637 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John took the milk.
2025-01-22 00:46:43.649 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (4001, 4005) -->  John took the milk
2025-01-22 00:46:43.649 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 00:46:43.674 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8254, 8260) --> . Sandra journeyed to the
2025-01-22 00:46:43.674 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:46:43.674 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:46:45.736 | INFO     | test_jbb_retain:begin_test:544 - The office.<|eot_id|>
2025-01-22 00:46:45.737 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12133])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:06<00:42,  6.08s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:17,  2.83s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:07<00:08,  1.80s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:05,  1.31s/it][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:03,  1.04s/it][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:01,  1.12it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:09<00:00,  1.26it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.38it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.26s/it]
2025-01-22 00:46:58.356 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5781855149702593, 0.5910601276009477, 0.5069802024147727, 0.5912365397944777], 'topk_tokens': [' Ramsey', ' Fletcher', ' cont', ' Minneapolis', 'ire', 'cont', 'super', ' the', ' Minnesota', 'Minnesota', ' Minnesota', ' Minnesota', ' the', ' Min', ' Minnesota', ' scramble', 'Minnesota', ' Ramsey', ' Minnesota', ' Minnesota'], 'evidence_proportions': [0.4966227213541667, 0.7080891927083334, 0.2592439651489258, 0.7424723307291666]}, 'weight': {'score': [0.028297112746672196, 0.0025867127370185995, 0.011748576706106012, 0.002523261425503232], 'topk_tokens': ['.', ' bathroom', '<|start_header_id|>', '\n\n', ' office', 'Answer', ' office', ':', ' bathroom', ' Married', ' Mary', ' the', ' football', 'assistant', '<|eot_id|>', '<|eot_id|>', 'Bridge', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.008780479431152344, 0.012297024329503378, 0.1236724853515625, 0.00023025274276733398]}, 'saliency': {'score': [0.0059274787252599544, 7.463160688905595e-05, 0.0013235048814253373, 6.170977671421863e-05], 'topk_tokens': ['<|end_header_id|>', ' bedroom', ':', 'Bridge', 'Question', 'Mary', ' office', '<|begin_of_text|>', ' football', '<|eot_id|>', '.', '<|eot_id|>', ' office', ' Mary', 'office', ' office', ' office', ' the', ' Married', ' Mary'], 'evidence_proportions': [0.002129127581914266, 0.004655907551447551, 0.022397547960281372, 1.7354885737101238e-05]}}, 25: {'grad': {'score': [0.733945673162287, 0.5874386781520395, 0.4199163263494318, 0.5874769163332845], 'topk_tokens': [' the', ' news', 'irie', ' Wood', 'ers', ' THE', ' private', 'ylinder', 'ian', '\n', 'ation', ' almost', 'atter', ' printing', ' very', 'ing', ' Robert', 'ers', 'ible', 'erc'], 'evidence_proportions': [0.5424702962239584, 0.7907511393229166, 0.6458778381347656, 0.9273274739583334]}, 'weight': {'score': [0.018203808502717453, 0.0025632950225188793, 0.008967583829706366, 0.0025231837289680986], 'topk_tokens': ['.', ' the', "'s", '.\n\n', '<|start_header_id|>', 'Mary', ' the', ' Married', ' \n', ' Mary', 'Answer', ' Mary', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', ' Dan', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001691579818725586, 0.021993835767110188, 0.06402587890625, 0.00037796298662821454]}, 'saliency': {'score': [0.0007025138898329301, 6.424535877870639e-05, 0.0004503564401106401, 6.238146402455078e-05], 'topk_tokens': [' the', ' Ear', ' Press', ' THE', ' boat', 'E', ' Press', ' doctor', ' Mary', ' Mary', ' Met', '<|begin_of_text|>', 'Answer', 'office', ':', ' Dan', '<|eot_id|>', ' Married', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [1.9957621892293293e-05, 0.0011864701906840007, 0.0020083189010620117, 3.057718276977539e-05]}}, 26: {'grad': {'score': [1.4794644442471592, 1.3081701303049031, 1.4895241477272727, 1.307528474255774], 'topk_tokens': ['Cut', ' press', ' gang', ' some', ' generally', 'little', ' black', 'BO', ' Press', 'st', ' worth', 'str', ' STR', ' gold', ' gold', ' generally', ' Guards', 'ol', ' generally', ' str'], 'evidence_proportions': [1.7169596354166667, 1.0906778971354167, 1.18804931640625, 1.8250325520833333]}, 'weight': {'score': [0.0048230317505923185, 0.0025212612081369746, 0.0065693475983359595, 0.0025097074199872437], 'topk_tokens': ['\n\n', ' was', ' football', '.\n\n', ' before', ' Dan', '?', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'Bridge', '<|start_header_id|>', 'Answer', ' \n', 'assistant', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0017427603403727214, 0.006042977174123128, 0.014722824096679688, 8.34961732228597e-05]}, 'saliency': {'score': [0.0005170052701776677, 9.419447510243444e-05, 0.00042023983868685633, 9.28319049684081e-05], 'topk_tokens': ['<|eot_id|>', '?', ' Bridge', ' Father', ' Married', ' bedroom', 'assistant', ' bathroom', ' \n', ' bathroom', 'Answer', ' Bridge', '<|start_header_id|>', ' Dan', '<|end_header_id|>', ' the', '<|begin_of_text|>', 'Bridge', 'office', ':'], 'evidence_proportions': [0.00024106105168660483, 0.0006360312302907308, 0.0015212297439575195, 4.4405460357666016e-06]}}, 27: {'grad': {'score': [0.7038518732244318, 0.7753199680675731, 0.5688199129971591, 0.7758257408984472], 'topk_tokens': ['le', '\n\n\n\n\n\n\n', ' *', ' satisfaction', 'na', 'n', '\n', 'ile', ' print', ' vigilant', ' noble', ' sample', ' N', 'fire', ' wonderful', '185', ' state', 'ATION', ' notified', 'ATION'], 'evidence_proportions': [0.7824503580729166, 0.4869791666666667, 0.65655517578125, 0.8736572265625]}, 'weight': {'score': [0.014392207969318737, 0.0025626469797983943, 0.006729440255598588, 0.002533541050277507], 'topk_tokens': [' the', 'Bridge', ' Mary', ' Bridge', '<|eot_id|>', ' football', 'Mary', ' bathroom', '<|eot_id|>', '.', ' \n', '<|start_header_id|>', ' Dan', 'Answer', 'assistant', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0058193206787109375, 0.017546892166137695, 0.04367542266845703, 0.0002882679303487142]}, 'saliency': {'score': [0.0017069632356817071, 0.00013634263609582425, 0.0006594983014193448, 0.000132532936498768], 'topk_tokens': ['assistant', '<|eot_id|>', ' PA', ' Anthony', 'Answer', ' the', ' the', ' Father', 'Mary', ' Mary', 'Bridge', ' bathroom', ' ST', ' Mary', ' THE', '.\n\n', '<|begin_of_text|>', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00037040313084920246, 0.0024331112702687583, 0.005123496055603027, 3.9686759312947593e-05]}}, 28: {'grad': {'score': [0.7380787242542614, 0.5349521657395963, 0.5413873845880682, 0.5345708607108981], 'topk_tokens': [' back', 'e', ' of', ' lively', ' ', 't', '      ', ' become', 'e', 'ily', 'ole', 'ien', 'and', 'e', ' adj', ' reached', ' fel', ' endeavor', ' firm', ' ins'], 'evidence_proportions': [0.5486653645833334, 0.8665364583333334, 1.030029296875, 0.604400634765625]}, 'weight': {'score': [0.002694479443810203, 0.0024509554101844163, 0.01006945696744052, 0.0024366501779456103], 'topk_tokens': [' the', 'Question', '.', ' football', ':', '.\n\n', ' bathroom', '<|eot_id|>', ' before', ' the', '?', '<|eot_id|>', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009704033533732096, 0.0020133058230082193, 0.010271072387695312, 4.8667192459106445e-05]}, 'saliency': {'score': [0.0002630163322795521, 6.537853959267848e-05, 0.0008005201816558838, 6.368131458527614e-05], 'topk_tokens': [' the', '<|eot_id|>', ' during', ' the', 'Question', ' Far', ' the', ' the', ' the', '<|eot_id|>', ' Bridge', ' before', '<|start_header_id|>', ' \n', ' Bridge', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', 'office', ':'], 'evidence_proportions': [6.841619809468587e-05, 0.00028629104296366376, 0.0009090304374694824, 3.6656856536865234e-06]}}, 29: {'grad': {'score': [0.8419716574928977, 0.7931181113514627, 0.9713384454900568, 0.7927049424352307], 'topk_tokens': [' young', ' boat', 'mail', ' paper', ' mail', ' The', ' NEW', ' should', 'ants', 'ION', ' boat', ' house', ' months', ' The', ' book', ' were', 'ION', ' routes', '\n', 'boat'], 'evidence_proportions': [1.2059733072916667, 0.730194091796875, 0.71844482421875, 0.6720987955729166]}, 'weight': {'score': [0.005589113994078202, 0.0025271100962559875, 0.00903260978785428, 0.0025097016867805717], 'topk_tokens': ['Question', ':', ' bathroom', '?', '.\n\n', ' Where', '<|eot_id|>', '<|eot_id|>', ' Does', '.', ' before', ' \n', ' the', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00553441047668457, 0.0032441715399424234, 0.01735687255859375, 0.00014358758926391602]}, 'saliency': {'score': [0.00036311691457575017, 4.609869397811355e-05, 0.0006654750217090954, 4.439489114433391e-05], 'topk_tokens': [' the', ' journey', ' the', '.', ':', ' the', '.', '\n\n', ' Does', '<|start_header_id|>', ' Do', ':', ' from', ' the', '<|end_header_id|>', 'Answer', ' \n', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0005040268103281657, 0.0003034075101216634, 0.000776931643486023, 6.039937337239583e-06]}}, 30: {'grad': {'score': [0.7500700517134233, 0.7111106819118252, 0.6240456321022727, 0.7111982118895092], 'topk_tokens': [' Field', ' head', ' Loan', ' an', ' in', ' anx', ' Collection', ' o', ' Capt', ' O', ' Crew', ' Sons', ' States', ' an', ' Bridge', ' o', 'ire', ' o', ' Was', ' o'], 'evidence_proportions': [0.6397298177083334, 0.8145751953125, 0.7357406616210938, 0.8054580688476562]}, 'weight': {'score': [0.008815594694831154, 0.0024579906738683623, 0.03309079191901467, 0.002390685329740135], 'topk_tokens': ['<|eot_id|>', ' Where', ' the', ' Dan', ':', '<|eot_id|>', ' bathroom', '?', ' to', ' the', '.\n\n', 'Question', 'Answer', 'assistant', '<|end_header_id|>', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0049406687418619795, 0.00958980123202006, 0.02632904052734375, 0.00024068355560302734]}, 'saliency': {'score': [0.002066988836635243, 0.00010797774099154289, 0.0034371533177115702, 9.835569924214235e-05], 'topk_tokens': [' Dan', ' to', ' Mary', '?', 'Mary', ' Sandra', ' office', ' the', ' the', '.\n\n', ' the', 'Gov', '.', 'Question', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'office', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0006065169970194498, 0.0028260648250579834, 0.0061855316162109375, 2.26895014444987e-05]}}, 31: {'grad': {'score': [0.9050792347301136, 0.7973398228265348, 0.6261662569912997, 0.7974552435019543], 'topk_tokens': [' DAY', ' an', ' the', ' United', ' the', 'IR', ' Mr', ' W', ' the', ' H', ' an', ' marriage', ' members', 'Mr', ' membership', ' Mr', ' M', ' M', ' the', ' most'], 'evidence_proportions': [0.8507486979166666, 1.2029012044270833, 0.9783935546875, 0.6127115885416666]}, 'weight': {'score': [0.0021311694925481624, 0.0023207939157379866, 0.0024362937970594926, 0.0023209287879504673], 'topk_tokens': [' the', ' was', ' football', 'Question', ' before', '.\n\n', ':', '?', ' the', '<|eot_id|>', ' Where', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.001396457354227702, 0.0024522244930267334, 0.0057659149169921875, 0.00012166301409403484]}, 'saliency': {'score': [0.0008157464590939609, 0.0001066547389083504, 0.00047693198377435857, 0.00010469078909186412], 'topk_tokens': [' office', ' Mary', '?', ' the', ' football', ' Mary', 'Mary', '.', ' the', 'Question', ' the', '<|eot_id|>', ' the', '<|end_header_id|>', ' \n', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0009027520815531412, 0.0008805890878041586, 0.0017916858196258545, 1.3271967569986979e-05]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 100}
2025-01-22 00:46:58.357 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:46:58.358 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-2_0-1-2-4.pkl | len: 10 |  size: 8.7 KB
Processing depth (0, 1, 2, 4):   3%|‚ñé         | 3/100 [01:09<36:34, 22.62s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.26it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.32it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.77it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.53it/s]
Processing depth (1, 2, 5, 9):   3%|‚ñé         | 3/100 [01:16<36:34, 22.62s/it]2025-01-22 00:47:05.885 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:47:05.890 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:47:05.890 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:47:05.895 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:47:05.895 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:47:05.912 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5918, 5922) -->  Mary dropped the football
2025-01-22 00:47:05.912 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:47:05.943 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10692, 10698) --> . Daniel went back to the
2025-01-22 00:47:05.945 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:47:05.971 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (8735, 8741) --> . John went back to the
2025-01-22 00:47:05.971 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John took the milk.
2025-01-22 00:47:05.982 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3996, 4000) -->  John took the milk
2025-01-22 00:47:05.982 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 00:47:06.007 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8247, 8253) --> . Sandra journeyed to the
2025-01-22 00:47:06.007 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:47:06.011 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1490, 1496) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:47:08.100 | INFO     | test_jbb_retain:begin_test:544 - the bedroom<|eot_id|>
2025-01-22 00:47:08.101 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12133])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:41,  5.95s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:16,  2.79s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:07<00:08,  1.77s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:05,  1.29s/it][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:03,  1.03s/it][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:01,  1.15it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:09<00:00,  1.30it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.42it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.24s/it]
2025-01-22 00:47:20.444 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5509213534268466, 0.5645843795065925, 0.6292835582386364, 0.5644915174308047], 'topk_tokens': ['re', ' Min', ' over', ' rept', ' marched', ' a', ' whistle', 'sur', ' Minnesota', ' the', 'super', ' cont', 'ire', ' conn', ' Minnesota', ' cons', 'cont', 'cont', 'con', ' cont'], 'evidence_proportions': [0.6357625325520834, 0.6357625325520834, 0.276580810546875, 0.5641326904296875]}, 'weight': {'score': [0.0301465012810447, 0.0025830963977236878, 0.007180379195646806, 0.002524578894705958], 'topk_tokens': [' football', 'Bridge', ' hand', 'Answer', ' the', '.', ' bathroom', ' bathroom', '.', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ' office', 'assistant', ':', ' bedroom', ' football', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0005746781826019287, 0.0005746781826019287, 0.1613922119140625, 0.0017930070559183757]}, 'saliency': {'score': [0.005539151755246249, 9.46208582918586e-05, 0.000368001786145297, 8.421691650492846e-05], 'topk_tokens': [':', ' context', 'Question', ' top', ' Mary', ' THE', '.', ' hand', ' folded', ' random', '<|eot_id|>', ' bathroom', '<|begin_of_text|>', '<|eot_id|>', ' Dul', ' bedroom', ' football', ' office', ' bedroom', 'office'], 'evidence_proportions': [4.0590763092041016e-05, 4.0590763092041016e-05, 0.03021860122680664, 8.330742518107097e-05]}}, 25: {'grad': {'score': [0.5365350896661932, 0.7806366205706634, 0.4327565973455256, 0.7817137518411001], 'topk_tokens': [' both', 'ervative', ' construed', ' Republican', ' private', ' corner', ' private', 'ation', ' stere', ' compromised', ' Gree', '\n', 'ing', 'ised', 'ing', ' extraordinary', ' stere', 'ing', ' safe', 'erc'], 'evidence_proportions': [0.5816243489583334, 0.5816243489583334, 0.6981201171875, 0.3386332194010417]}, 'weight': {'score': [0.013635976748033003, 0.00256965835250647, 0.0048400244929573755, 0.0025453917856557958], 'topk_tokens': [' Mary', ' Dan', ' the', ' Minneapolis', ' \n', ' dropped', '.\n\n', '.', '<|start_header_id|>', ' Mary', ' THE', 'Answer', ' East', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016855696837107341, 0.0016855696837107341, 0.06855010986328125, 0.0009273687998453776]}, 'saliency': {'score': [0.0014347542415965688, 5.6555642480461204e-05, 0.0001860369335521351, 5.381236586280086e-05], 'topk_tokens': [' Rotary', ' dropped', 'Den', ' Min', ' football', ' Anthony', ' Min', ' random', 'assistant', ' Dan', ' Mary', ':', 'office', ' THE', ' Minneapolis', 'Answer', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00037154555320739746, 0.00037154555320739746, 0.006719350814819336, 3.810723622639974e-05]}}, 26: {'grad': {'score': [0.7971163662997159, 1.0319708938504326, 0.9375943270596591, 1.0325699414127114], 'topk_tokens': [' gold', ' broad', 'oth', 'Cut', ' Merch', ' Field', ' generally', ' some', 'ol', 'str', ' gang', ' black', ' worth', ' generally', ' went', 'BO', 'burn', ' STR', ' Guards', ' str'], 'evidence_proportions': [0.8243204752604166, 0.8243204752604166, 0.4166717529296875, 0.996337890625]}, 'weight': {'score': [0.0051836696538058195, 0.0025403304506733332, 0.003433633934367787, 0.002533895404845015], 'topk_tokens': ['.\n\n', ' East', ' kitchen', ' before', '?', ' football', ' bedroom', 'Bridge', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' \n', '<|start_header_id|>', ' the', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00022380550702412924, 0.00022380550702412924, 0.024057388305664062, 0.002520918846130371]}, 'saliency': {'score': [0.00017358498139814898, 9.643625810306457e-05, 0.00033656304532831365, 9.585896414628287e-05], 'topk_tokens': ['?', ' office', ' Ramsey', ' kitchen', ' Seventh', ' \n', ' North', ' Col', ' Az', ' bathroom', ' East', ' Bridge', '<|start_header_id|>', ' the', ' bedroom', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [3.914038340250651e-06, 3.914038340250651e-06, 0.0006528943777084351, 0.00019338726997375488]}}, 27: {'grad': {'score': [0.3583762428977273, 0.42942466908735066, 0.30067305131392047, 0.4297882122985976], 'topk_tokens': ['Minnesota', ' party', '      ', 'n', ' STE', 'use', ' vain', 'MIN', 'sur', ' N', ' plainly', 'ATION', 'bread', 'na', ' const', 'ab', 'ad', ' N', 'sur', 'ile'], 'evidence_proportions': [0.29241943359375, 0.29241943359375, 0.459503173828125, 0.4228719075520833]}, 'weight': {'score': [0.012608972462740812, 0.0025659794565773717, 0.004231377081437545, 0.0025446756277888085], 'topk_tokens': [' football', ' Bridge', 'Bridge', ' before', ' Mary', '<|eot_id|>', ' \n', '<|eot_id|>', ' bathroom', ' football', ' bedroom', 'Answer', '<|start_header_id|>', 'assistant', '.\n\n', ' THE', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0013403693834940593, 0.0013403693834940593, 0.06311988830566406, 0.0014722347259521484]}, 'saliency': {'score': [0.00341072678565979, 0.00013471514195338856, 0.0007829124277288264, 0.00012757490570753624], 'topk_tokens': [' THE', ' football', ' Hotel', ' the', '.', ' East', '.\n\n', ' the', 'Bridge', 'THE', ' Anthony', ' bathroom', 'THE', ' Mary', '<|begin_of_text|>', ' THE', ' Mary', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0005219777425130209, 0.0005219777425130209, 0.016981840133666992, 0.00014081597328186035]}}, 28: {'grad': {'score': [0.5874578302556818, 0.505322768592913, 0.5796286843039773, 0.5050381187291942], 'topk_tokens': ['ente', ' did', 't', ' door', '.', 'avored', 'extent', 'ugg', ' adj', ' the', ' dry', ' fel', ' ins', 'oggle', ' genu', ' reached', ' ende', ' ende', ' firm', ' endeavor'], 'evidence_proportions': [0.57171630859375, 0.57171630859375, 0.7845458984375, 0.487548828125]}, 'weight': {'score': [0.005703232505104758, 0.002466522581448612, 0.005232632160186768, 0.002455600240115995], 'topk_tokens': [' Bridge', '.', 'Question', ' to', '.\n\n', ' football', '?', ' the', ' bathroom', ' before', '<|eot_id|>', ' \n', '<|eot_id|>', '<|start_header_id|>', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010259946187337239, 0.00010259946187337239, 0.0187225341796875, 0.008224964141845703]}, 'saliency': {'score': [0.0005667101253162731, 5.325781351065115e-05, 0.00029574470086531204, 5.1882355535171354e-05], 'topk_tokens': [' nearly', ' to', ' Dul', ' Bridge', ' football', ' \n', ' the', ' Mary', ' to', ' before', ' during', '<|eot_id|>', '<|start_header_id|>', ' Bridge', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ' Far', ':'], 'evidence_proportions': [1.7444292704264324e-05, 1.7444292704264324e-05, 0.002479851245880127, 0.0003898143768310547]}}, 29: {'grad': {'score': [0.7317047119140625, 0.8595736119695097, 0.7473976828835227, 0.8600103819836616], 'topk_tokens': [' The', ' for', ' Printing', 'ION', '"The', ' Washington', ' to', ' message', ' Newspaper', ' Hotel', '.', '\n', ' Min', ' The', ' were', ' The', ' The', ' The', '\n', ' The'], 'evidence_proportions': [0.767578125, 0.767578125, 0.6466903686523438, 0.7166341145833334]}, 'weight': {'score': [0.0032837607643821025, 0.002542559698770505, 0.004735976457595825, 0.0025372200583654424], 'topk_tokens': [' to', 'Question', '?', ' Does', ' Where', ' bathroom', '.\n\n', ' football', '<|eot_id|>', ' before', '<|eot_id|>', ' \n', ' the', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.695487976074219e-05, 3.695487976074219e-05, 0.01560211181640625, 0.0015651384989420574]}, 'saliency': {'score': [0.0001532381231134588, 4.309989515036578e-05, 0.00027743523771112615, 4.247311336626812e-05], 'topk_tokens': [' the', "'s", ' to', ' bathroom', ' the', ' the', ' football', ' the', ':', 'assistant', ' before', ' Do', '<|end_header_id|>', ' the', 'Answer', ' \n', '<|begin_of_text|>', ':', '<|start_header_id|>', 'office'], 'evidence_proportions': [4.569689432779948e-06, 4.569689432779948e-06, 0.0006241798400878906, 0.0001366138458251953]}}, 30: {'grad': {'score': [0.7633001154119318, 0.779168759013185, 0.7546261874112216, 0.7792422886413771], 'topk_tokens': [' its', ' o', ',', ' States', ' o', ' Press', ' Third', 'deal', 'ob', ' men', ' LINE', ' Sons', ' Paul', ' Loan', ' Fourth', ' Bridge', ' O', ' States', ' Was', 'ire'], 'evidence_proportions': [0.6683349609375, 0.6683349609375, 0.905303955078125, 0.8585611979166666]}, 'weight': {'score': [0.00865218856117942, 0.002474924346081357, 0.013281280344182795, 0.0024440220514249686], 'topk_tokens': [' football', ' Min', '.', ' before', ' the', '?', 'Question', ' the', '<|eot_id|>', '.\n\n', ' bathroom', '<|eot_id|>', 'Answer', ' \n', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008045037587483724, 0.0008045037587483724, 0.03891944885253906, 0.004169384638468425]}, 'saliency': {'score': [0.0020309957590970125, 0.00011581594579544979, 0.0011565766551277854, 0.00011043750799064078], 'topk_tokens': ['IR', ' \n', ' the', ' Sandra', ' Wide', 'Den', ' office', '?', '.\n\n', ' East', 'Question', ' football', ' Mary', ' bathroom', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00033681591351826984, 0.00033681591351826984, 0.009971126914024353, 0.00012593468030293783]}}, 31: {'grad': {'score': [0.7698974609375, 0.5811059902271323, 0.4542422294616699, 0.5809933105795607], 'topk_tokens': [' C', ' B', 'k', ' the', ' Aw', ' into', ' the', 'IR', ' C', ' an', ' J', ' most', ' an', ' the', ' an', ' H', ' an', 'Mr', ' Mr', ' an'], 'evidence_proportions': [0.8529256184895834, 0.8529256184895834, 0.85205078125, 0.549072265625]}, 'weight': {'score': [0.0018859559839422052, 0.0023429168022165194, 0.001378533515063199, 0.002345502987835526], 'topk_tokens': [' the', ' the', ':', 'Question', ' before', ' football', '?', '.\n\n', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0003534555435180664, 0.0003534555435180664, 0.0077474117279052734, 0.0010433197021484375]}, 'saliency': {'score': [0.0004724860191345215, 7.728770479627436e-05, 0.00011736696416681463, 7.649570193285584e-05], 'topk_tokens': [' before', 'ot', '?', ' Where', ' the', ' the', ' Mary', 'Question', ' football', ' the', ' the', '<|end_header_id|>', ' the', '<|eot_id|>', ' \n', '<|start_header_id|>', 'assistant', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0001305540402730306, 0.0001305540402730306, 0.0021089911460876465, 6.534655888875325e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 00:47:20.445 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:47:20.446 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-3_1-2-5-9.pkl | len: 10 |  size: 8.74 KB
Processing depth (1, 2, 5, 9):   4%|‚ñç         | 4/100 [01:31<35:51, 22.41s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.20it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.23it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.67it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.47it/s]
Processing depth (2, 4, 5, 8):   4%|‚ñç         | 4/100 [01:39<35:51, 22.41s/it]2025-01-22 00:47:28.273 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:47:28.281 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2454, 2460) --> . Mary journeyed to the
2025-01-22 00:47:28.281 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:47:28.289 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2454, 2460) --> . Mary journeyed to the
2025-01-22 00:47:28.289 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:47:28.306 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5918, 5922) -->  Mary dropped the football
2025-01-22 00:47:28.306 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:47:28.334 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9590, 9596) -->  Daniel went back to the kitchen
2025-01-22 00:47:28.334 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:47:28.360 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (8735, 8741) --> . John went back to the
2025-01-22 00:47:28.360 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John took the milk.
2025-01-22 00:47:28.371 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3989, 3993) -->  John took the milk
2025-01-22 00:47:28.371 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 00:47:28.395 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8247, 8253) --> . Sandra journeyed to the
2025-01-22 00:47:28.395 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:47:28.405 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (2455, 2461) -->  Mary journeyed to the office
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:47:30.439 | INFO     | test_jbb_retain:begin_test:544 - bedroom<|eot_id|>
2025-01-22 00:47:30.440 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12133])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:41,  5.90s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:06<00:16,  2.76s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:07<00:08,  1.76s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:07<00:05,  1.29s/it][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:08<00:03,  1.03s/it][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:01,  1.15it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:09<00:00,  1.28it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.41it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.23s/it]
2025-01-22 00:47:42.758 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5534778941761364, 0.5679341860450144, 0.5478987260298296, 0.5679969450013828], 'topk_tokens': [' rep', ' two', 'Spring', ' conn', ' emb', 'cont', ' over', 'ir', 'enter', 'fire', ' Minnesota', ' cons', ' Minnesota', ' cont', 'super', 'sur', 'cont', 'con', 'active', 'ire'], 'evidence_proportions': [0.6118672688802084, 0.6118672688802084, 0.298370361328125, 0.6067708333333334]}, 'weight': {'score': [0.0342067534273321, 0.002580520887653868, 0.012050715359774504, 0.0025057444924624383], 'topk_tokens': [' Dul', ' bedroom', ' office', 'Bridge', ' football', ' THE', ' bathroom', 'Answer', '.', '<|eot_id|>', '<|eot_id|>', 'assistant', '.', ':', '<|start_header_id|>', ' football', ' bedroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003868331511815389, 0.003868331511815389, 0.14583587646484375, 0.020464181900024414]}, 'saliency': {'score': [0.007537259296937423, 0.00011297743851921419, 0.0010813420469110663, 9.770672292333119e-05], 'topk_tokens': ['.', ' top', 'Bridge', ' kitchen', ' Sandra', ':', '<|end_header_id|>', '<|eot_id|>', ' random', ' Mary', ' THE', '<|eot_id|>', ' Dul', '.', '<|begin_of_text|>', ' bedroom', ' office', ' football', ' bedroom', 'office'], 'evidence_proportions': [0.0009440481662750244, 0.0009440481662750244, 0.03569459915161133, 0.001952121655146281]}}, 25: {'grad': {'score': [0.675962274724787, 0.6513943477673053, 0.5292635830965909, 0.6515718667839037], 'topk_tokens': [' the', ' private', ' the', 'ing', ' construed', ' Key', ' printers', 'ers', 'le', 'ting', ' extraordinary', ' safe', ' stere', 'ing', 'ised', ' Wood', ' stere', 'ation', 'erc', 'ation'], 'evidence_proportions': [0.7343648274739584, 0.7343648274739584, 0.9849128723144531, 0.3531901041666667]}, 'weight': {'score': [0.01995694095438177, 0.0025635719692260086, 0.007158777930519797, 0.002523562983300784], 'topk_tokens': [' Do', ' directly', ' \n', ' dropped', ' Anthony', '.', ' Dan', '.', ' Mary', 'Answer', '<|start_header_id|>', ' East', ' THE', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.008382330338160196, 0.008382330338160196, 0.07578277587890625, 0.005888938903808594]}, 'saliency': {'score': [0.0029708472165194426, 6.591087022238323e-05, 0.0007931617173281583, 5.930197780199929e-05], 'topk_tokens': [' return', 'office', ' Empire', ' dropped', ' Minneapolis', ' random', ' Dan', ' Anthony', ' directly', ' Mary', ' Do', ':', 'assistant', ' Mary', ' THE', 'Answer', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0023123820622762046, 0.0023123820622762046, 0.008852958679199219, 0.00036636988321940106]}}, 26: {'grad': {'score': [0.9353790283203125, 1.1331617609188298, 1.1269087357954546, 1.133533011160322], 'topk_tokens': [' press', ' broad', ' press', ' hand', ' gang', ' wh', 'Cut', ' gold', ' some', 'burn', ' generally', ' went', ' generally', ' Guards', ' worth', 'str', ' Press', 'BO', ' STR', ' str'], 'evidence_proportions': [1.0006510416666667, 1.0006510416666667, 0.6296615600585938, 1.0086466471354167]}, 'weight': {'score': [0.011156943711367521, 0.002535347767607096, 0.00467743385921825, 0.0025157628693540006], 'topk_tokens': [' Bridge', ' before', ' East', '?', ' bathroom', ' football', ' bedroom', ' kitchen', 'Bridge', '<|eot_id|>', '<|eot_id|>', ' \n', ' the', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0007970233758290609, 0.0007970233758290609, 0.020750999450683594, 0.02548074722290039]}, 'saliency': {'score': [0.0009279928424141624, 8.653896946351029e-05, 0.0006664097309112549, 8.395281927272666e-05], 'topk_tokens': [' Sandra', ' Harper', ' the', 'Answer', ' Dul', ' Az', ' Ramsey', ' Anthony', ' bathroom', ' kitchen', ' Dan', ' Bridge', '<|begin_of_text|>', ' East', '<|start_header_id|>', ' bedroom', 'Bridge', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [1.9679466883341472e-05, 1.9679466883341472e-05, 0.001367807388305664, 0.002451409896214803]}}, 27: {'grad': {'score': [0.3840664950284091, 0.44950021084414915, 0.3111641623757102, 0.4498709771012207], 'topk_tokens': [' party', ' carn', ' N', 'ad', ' B', ' const', 'sur', 'bread', ' head', ' vain', 'MIN', ' STE', '185', 'na', ' plainly', 'ab', ' N', 'ile', 'ad', 'sur'], 'evidence_proportions': [0.22804768880208334, 0.22804768880208334, 0.547760009765625, 0.58697509765625]}, 'weight': {'score': [0.016886532306671143, 0.002558948436685192, 0.006078243255615234, 0.0025264754128529075], 'topk_tokens': ['.', '.', ' Mary', '<|eot_id|>', ' \n', 'Bridge', ' bathroom', '<|eot_id|>', ' Bridge', ' football', ' bedroom', 'Answer', 'assistant', '.\n\n', '<|start_header_id|>', ' THE', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.006390482187271118, 0.006390482187271118, 0.061603546142578125, 0.00806728998819987]}, 'saliency': {'score': [0.004744890061291781, 0.000150172642635916, 0.0016841780055652964, 0.00013902121585609008], 'topk_tokens': [' Bridge', ' the', ' West', 'THE', '.', '.\n\n', ' Hotel', ' bathroom', ' Mary', ' THE', ' Anthony', 'Bridge', ' East', '.', '<|begin_of_text|>', ' bedroom', ' Mary', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0027922391891479492, 0.0027922391891479492, 0.01617145538330078, 0.001032481590906779]}}, 28: {'grad': {'score': [0.4997156316583807, 0.5023715943036671, 0.49796641956676135, 0.5023844422916257], 'topk_tokens': [' out', ' adj', '600', ' balance', 'half', ' be', ' genu', ' ende', 'been', 'na', ' ende', ' firm', ' ', 'ib', ' fel', ' came', ' ins', ' escaping', ' endeavor', ' reached'], 'evidence_proportions': [0.4403889973958333, 0.4403889973958333, 0.5212860107421875, 0.6039886474609375]}, 'weight': {'score': [0.005315553058277477, 0.0024658152828208693, 0.004101100293072787, 0.0024576546260277514], 'topk_tokens': ['.', ' Bridge', ' Far', '.\n\n', 'Question', ' bathroom', ' football', ' the', '?', ' before', '<|eot_id|>', ' \n', '<|eot_id|>', 'assistant', 'Answer', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0001091758410135905, 0.0001091758410135905, 0.007877826690673828, 0.014020125071207682]}, 'saliency': {'score': [0.0005521096966483376, 5.924825467933683e-05, 0.00019449808380820534, 5.810538411741862e-05], 'topk_tokens': ['Question', '<|eot_id|>', '<|begin_of_text|>', ' \n', 'Bridge', ' kitchen', ' Bridge', ' Dul', ' football', ' during', '<|start_header_id|>', '<|eot_id|>', 'office', ' before', 'Answer', ' Bridge', 'assistant', '<|end_header_id|>', ' Far', ':'], 'evidence_proportions': [1.4543533325195312e-05, 1.4543533325195312e-05, 0.001252695918083191, 0.0011601845423380535]}}, 29: {'grad': {'score': [0.921722412109375, 1.070311534301607, 1.0683874650435015, 1.0705853983502305], 'topk_tokens': ['The', ' place', 'ION', '\n', ' the', ' regular', '\n', ' Newspaper', ' the', ' The', ' Press', ' Printing', ' Republican', 'paper', ' The', ' The', ' The', '\n', ' The', ' were'], 'evidence_proportions': [1.2006123860677083, 1.2006123860677083, 0.679168701171875, 0.5256449381510416]}, 'weight': {'score': [0.0034300143068486996, 0.002546774884236905, 0.004876583814620972, 0.0025409286296867515], 'topk_tokens': ['Does', ' Does', 'Question', ' bathroom', '.\n\n', '?', ' Where', ' football', '<|eot_id|>', '<|eot_id|>', ' \n', ' before', ' the', 'Answer', '<|end_header_id|>', 'assistant', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00043268005053202313, 0.00043268005053202313, 0.007438182830810547, 0.006752570470174153]}, 'saliency': {'score': [0.0002795539119026878, 5.701718921763542e-05, 0.0005358511751348323, 5.574102053107108e-05], 'topk_tokens': [':', ' the', ' to', "'s", ' the', ' Do', ' Where', 'Does', ' football', ' the', '<|end_header_id|>', 'assistant', ' before', 'Answer', ' the', ' \n', ':', 'office', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [7.341305414835612e-05, 7.341305414835612e-05, 0.00035165250301361084, 0.000643769900004069]}}, 30: {'grad': {'score': [0.607177734375, 0.7828207889112072, 0.7225841175426136, 0.7832499803734441], 'topk_tokens': [' Was', ' Paul', ' LINE', 'Third', 'Emp', ' Third', ' States', 'G', ' o', 'op', 'ob', ' Bridge', 'office', 'ire', ' o', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.6023152669270834, 0.6023152669270834, 0.809783935546875, 0.4818318684895833]}, 'weight': {'score': [0.010274713689630682, 0.002473343297882143, 0.013292044401168823, 0.0024394634225293373], 'topk_tokens': [' Min', '.', ' Sandra', ' Where', ' before', ' the', '.\n\n', ' bathroom', '?', '<|eot_id|>', 'Question', '<|eot_id|>', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0014602343241373699, 0.0014602343241373699, 0.01815032958984375, 0.02265326182047526]}, 'saliency': {'score': [0.0019242709333246405, 0.00011712334801446375, 0.0014150711623105135, 0.00011147351766202499], 'topk_tokens': [' Sandra', '.', ' Min', 'Den', 'IR', ' Wide', '.\n\n', ' the', ' Mary', '?', ' bathroom', ' East', ' \n', 'Question', 'assistant', '<|end_header_id|>', ':', '<|begin_of_text|>', 'office', '<|start_header_id|>'], 'evidence_proportions': [0.0005607207616170248, 0.0005607207616170248, 0.0061087459325790405, 0.0018617212772369385]}}, 31: {'grad': {'score': [0.8190155029296875, 0.5602817103162341, 0.5064558549360796, 0.5599088731134276], 'topk_tokens': ['IO', ' C', ' an', ' Mr', ' marriage', ' composing', ' an', ' an', 'editary', 'user', ' J', ' C', ' Aw', ' DAY', ' an', ' most', ' H', 'IR', ' Mr', 'Mr'], 'evidence_proportions': [0.8285319010416666, 0.8285319010416666, 1.0272216796875, 0.6611785888671875]}, 'weight': {'score': [0.0020245909690856934, 0.0023200704278305423, 0.0016978518529371781, 0.002321740211697885], 'topk_tokens': [' was', ':', ' the', 'Question', ' football', '.\n\n', ' before', ' Where', '?', ' the', '<|eot_id|>', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0009370744228363037, 0.0009370744228363037, 0.006107926368713379, 0.0014774004618326824]}, 'saliency': {'score': [0.0006611238826404918, 9.575621011967418e-05, 0.00024110620672052556, 9.446303430951151e-05], 'topk_tokens': ['ot', ' Where', '?', ' the', ' the', 'Answer', ' Mary', 'Question', ' the', ' football', ' the', '<|end_header_id|>', ' the', '<|eot_id|>', ' \n', ':', '<|begin_of_text|>', 'assistant', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.000459522008895874, 0.000459522008895874, 0.002046748995780945, 0.0001405775547027588]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 00:47:42.760 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:47:42.760 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-4_2-4-5-8.pkl | len: 10 |  size: 8.7 KB
Processing depth (2, 4, 5, 8):   5%|‚ñå         | 5/100 [01:53<35:25, 22.38s/it]Processing depth (2, 4, 5, 8):   5%|‚ñå         | 5/100 [01:53<36:04, 22.79s/it]
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:70 - Selected idx: 1
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:72 - Answer: office
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:73 - Tag: 4-hop
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:74 - Needle: [' Mary journeyed to the office.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary got the apple.', ' John took the milk.', ' Mary journeyed to the bathroom.', ' Sandra journeyed to the bedroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:75 - Real Needle: [' Mary journeyed to the office.', ' Mary got the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 00:47:42.969 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.27it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.36it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (0, 2, 5, 7, 8):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 00:47:49.863 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:47:49.864 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:47:49.864 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the apple.
2025-01-22 00:47:49.871 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2469, 2473) -->  Mary got the apple
2025-01-22 00:47:49.871 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:47:49.889 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5929, 5935) --> . Mary journeyed to the
2025-01-22 00:47:49.889 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:47:49.913 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8349, 8353) -->  Mary dropped the apple
2025-01-22 00:47:49.913 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 00:47:49.941 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9595, 9601) -->  Daniel went back to the kitchen
2025-01-22 00:47:49.942 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:47:49.960 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (5965, 5971) --> . John went back to the
2025-01-22 00:47:49.960 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John journeyed to the office.
2025-01-22 00:47:49.960 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:47:49.960 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John took the milk.
2025-01-22 00:47:49.969 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3230, 3234) -->  John took the milk
2025-01-22 00:47:49.969 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:47:49.975 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1889, 1895) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:47:52.073 | INFO     | test_jbb_retain:begin_test:544 - Mary journeyed to the bathroom.<|eot_id|>
2025-01-22 00:47:52.073 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12147])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.62s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.44s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.43s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.05it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.47it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.93it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.88it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.13it/s]
2025-01-22 00:48:01.540 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.7599158653846154, 0.7069572180580707, 0.8083149303089489, 0.7066591605008432], 'topk_tokens': [' officers', ' offices', ' offices', ' or', 'Official', ' office', ' office', ' offices', ' office', 'office', ' office', ' offices', ' office', ' office', ' office', ' office', ' office', ' office', 'office', ' office'], 'evidence_proportions': [1.3045654296875, 0.8035888671875, 0.5765380859375, 0.91131591796875, 0.2685953776041667]}, 'weight': {'score': [0.023111701011657715, 0.002581356719586376, 0.006157273595983332, 0.002530744445982989], 'topk_tokens': ['.', ' Mary', ' Mary', ' Fort', ' Mary', 'THE', '<|eot_id|>', 'Answer', 'Bridge', ' Floral', 'assistant', '<|eot_id|>', ':', ' bedroom', ' Bridge', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0027703444163004556, 0.02871394157409668, 0.03506783644358317, 0.05344390869140625, 0.0075406233469645185]}, 'saliency': {'score': [0.004627404304651113, 0.00011153131446207254, 0.0022565695372494783, 9.79288403982565e-05], 'topk_tokens': ['<|eot_id|>', ' Bridge', '<|end_header_id|>', '.', ' front', ' Mary', ' office', ' Mary', ' John', ' Mary', '<|start_header_id|>', ' bedroom', 'office', '<|eot_id|>', 'Bridge', ' top', ' Market', 'office', '<|begin_of_text|>', ' office'], 'evidence_proportions': [0.00344887375831604, 0.005173012614250183, 0.0055572787920633955, 0.009963274002075195, 0.0009550750255584717]}}, 25: {'grad': {'score': [1.135851970085731, 0.8856219358331961, 1.147110332142223, 0.8846089017360667], 'topk_tokens': ['ation', ' fifteen', ' principal', ' at', ' mentioned', 'ation', 'ENT', ' circulated', ' excited', 'ished', ' rest', ' at', ' Aw', ' Wood', 'op', 'op', 'erc', 'itable', 'ised', ' principles'], 'evidence_proportions': [1.3203938802083333, 0.78497314453125, 1.3193359375, 0.6662750244140625, 1.3147966066996257]}, 'weight': {'score': [0.019057234892478354, 0.002550578841516261, 0.0009619729085402055, 0.0025180010606056305], 'topk_tokens': ['.', ' Anthony', '?\n', ' Floral', ' Seventh', ' top', ' Mary', ' Mary', ' THE', 'THE', '<|eot_id|>', 'Answer', '<|eot_id|>', ' Mary', 'assistant', ':', '<|start_header_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000591278076171875, 0.0243985652923584, 0.018801838159561157, 0.06375503540039062, 0.004419167836507161]}, 'saliency': {'score': [0.005833277335533729, 9.203205403795712e-05, 7.261742245067249e-05, 7.973180980822654e-05], 'topk_tokens': [' Ramsey', '<|start_header_id|>', 'assistant', ' Min', 'Answer', ':', '.', ' counting', ' Floral', ' top', ' Bench', ' THE', '<|eot_id|>', 'Mary', '<|eot_id|>', ' Mary', ' Mary', '<|end_header_id|>', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.0001184542973836263, 0.01047690212726593, 0.006177246570587158, 0.01777958869934082, 0.00014417370160420737]}}, 26: {'grad': {'score': [1.080447270320012, 0.8311955169921392, 1.4363958185369319, 0.8295597057186487], 'topk_tokens': [' Press', ' B', ' steam', ' steam', ' well', ' some', ' week', ' Jul', 'ers', ' neighboring', ' journey', ' part', ' a', ' searched', 'BO', 'ols', ' Marshall', ' wh', ' Col', ' clear'], 'evidence_proportions': [1.2242635091145833, 0.89801025390625, 1.5416666666666667, 0.5924720764160156, 0.9223531087239584]}, 'weight': {'score': [0.012761226067176232, 0.002499051855094973, 0.00099629976532676, 0.0024797347752223003], 'topk_tokens': [' Anthony', ' Mary', 'Bridge', ' kitchen', ' Seventh', ' bedroom', '<|eot_id|>', '<|eot_id|>', ' Bridge', ' Floral', '?\n', ' bathroom', 'Answer', 'assistant', ' the', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003155072530110677, 0.007352650165557861, 0.009368578592936197, 0.028432369232177734, 0.021757880846659344]}, 'saliency': {'score': [0.0015421440968146692, 0.00010973762137299753, 0.00011319463903253728, 0.00010665369254480206], 'topk_tokens': ['THE', ' bathroom', ' Bench', ' apple', ' old', 'Answer', '<|start_header_id|>', ' old', 'Minnesota', ' Az', ' kitchen', '<|end_header_id|>', ' bedroom', ' Seventh', 'Bridge', ' Bridge', '<|begin_of_text|>', ':', 'office', ' the'], 'evidence_proportions': [4.032254219055176e-05, 0.00035762786865234375, 0.0011202096939086914, 0.0019146353006362915, 0.004007250070571899]}}, 27: {'grad': {'score': [0.5877955510066106, 0.6859332718999506, 0.5522641268643466, 0.686387143640634], 'topk_tokens': ['akes', ' attempts', ' N', 'APER', 'sp', 'roduced', ' West', 'ile', ' Moore', ' newspaper', ' excursion', '\n', 'bread', 'SP', ' accepted', ' newspaper', ' Newspaper', ' excessive', ' interruption', ' opposition'], 'evidence_proportions': [0.3989664713541667, 0.8403091430664062, 0.4093831380208333, 0.7718505859375, 0.6639912923177084]}, 'weight': {'score': [0.015080142479676466, 0.002555768041809318, 0.0029547133228995585, 0.002528133091675566], 'topk_tokens': ['<|eot_id|>', ' Mary', ' Mary', '<|eot_id|>', '?\n', ' Floral', 'THE', ' Bridge', 'Answer', 'assistant', 'THE', ' THE', ' bedroom', '.\n\n', ' bathroom', ':', '<|end_header_id|>', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007960796356201172, 0.022339046001434326, 0.02001473307609558, 0.03662729263305664, 0.005225578943888347]}, 'saliency': {'score': [0.004932676370327289, 0.00012215742584864036, 0.0006126436320218173, 0.00011092990836312225], 'topk_tokens': [' illumination', ' The', ' prior', 'Answer', '<|end_header_id|>', '.', 'Mary', ' Mary', 'THE', ' Floral', ' Bridge', ' bathroom', ' Mary', '.\n\n', ' Mary', ':', '<|start_header_id|>', ' THE', ' bedroom', 'office'], 'evidence_proportions': [0.0003058413664499919, 0.006937891244888306, 0.006731977065404256, 0.013443052768707275, 0.0007498164971669515]}}, 28: {'grad': {'score': [0.883328364445613, 0.8778140979967487, 0.8417393077503551, 0.8778678352463769], 'topk_tokens': [' first', ' spoken', ' came', ' ', 'CH', '600', 'being', ' become', ' acted', 'hum', ' taken', 'been', ' out', 'hom', ' kept', ' be', ' ', ' acted', 'na', ' ins'], 'evidence_proportions': [0.7579930623372396, 0.807861328125, 0.9013671875, 0.98345947265625, 0.97418212890625]}, 'weight': {'score': [0.012738906420194186, 0.0024538186952718266, 0.0018554275686090643, 0.002432808227040986], 'topk_tokens': ['During', ' the', ' to', ' Mary', ' Mary', ' the', '<|eot_id|>', ' Bridge', ' Floral', '?\n', '<|eot_id|>', ' to', 'Answer', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00027484695116678876, 0.007253885269165039, 0.02244572838147481, 0.026737213134765625, 0.00982062021891276]}, 'saliency': {'score': [0.0018187371584085317, 0.00010011865137162861, 0.00041889331557533956, 9.584651487088974e-05], 'topk_tokens': ['<|eot_id|>', ' Bridge', ' parade', ' Mary', '?\n', 'SSION', ' to', ' the', 'Bridge', 'Answer', ' bathroom', ' the', 'office', 'assistant', '<|end_header_id|>', ' Floral', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [7.197260856628418e-05, 0.0012317150831222534, 0.0036743183930714927, 0.002583026885986328, 0.0015917420387268066]}}, 29: {'grad': {'score': [1.5466590294471154, 1.2284930376214092, 1.8467823375355115, 1.2266853621908185], 'topk_tokens': [' regular', ' express', 'ION', 'The', ' paper', ' THE', ' mail', ' regular', 'mail', ' Papers', ' printed', ' printed', 'aper', 'ATING', 'paper', '\n', ' Press', ' the', '\n', 'APER'], 'evidence_proportions': [2.017578125, 1.321044921875, 2.47412109375, 0.8507614135742188, 0.7626190185546875]}, 'weight': {'score': [0.005139201879501343, 0.0025261315271897376, 0.0015887022018432617, 0.002522221405381418], 'topk_tokens': ['Does', ':', 'THE', ' Does', ' Where', ' the', '<|eot_id|>', '<|eot_id|>', '?\n', ' the', ' where', 'Answer', ' was', ' in', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006201664606730143, 0.003605365753173828, 0.009307553370793661, 0.006619453430175781, 0.005525608857472737]}, 'saliency': {'score': [0.0008982717990875244, 0.00010349223796051023, 0.0002744522961703214, 0.0001014737775134465], 'topk_tokens': [' United', ' the', ' the', '<|eot_id|>', 'NEW', ' where', ' in', 'office', 'IVE', ' the', ' the', ' was', '?\n', ' the', 'Does', ' Does', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.64320182800293e-05, 0.0001109391450881958, 0.0022439658641815186, 0.0007674992084503174, 0.001016487677892049]}}, 30: {'grad': {'score': [1.0505887545072115, 1.140621623950531, 0.9845997203480114, 1.1410987197678006], 'topk_tokens': [' o', ' office', ' office', 'office', ' office', ' offices', ' office', ' office', ' o', 'op', ' o', 'ob', ' office', ' office', 'op', ' o', ' o', ' o', 'office', ' O'], 'evidence_proportions': [1.3921305338541667, 0.85687255859375, 1.1736857096354167, 0.846435546875, 0.8511962890625]}, 'weight': {'score': [0.014421288783733662, 0.002463124496807865, 0.0038471872156316585, 0.0024349151214443253], 'topk_tokens': [':', ' Mary', '<|eot_id|>', ' Where', ' Broadway', 'Question', '<|eot_id|>', ' the', ' Floral', ' bathroom', ' the', '.\n\n', 'assistant', 'Answer', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015901724497477214, 0.006810903549194336, 0.021691441535949707, 0.030406951904296875, 0.014398733774820963]}, 'saliency': {'score': [0.002933025360107422, 0.000231056274607482, 0.0007116388190876354, 0.0002243771561708601], 'topk_tokens': [' Congress', ':', ' Where', '.', ' Third', 'assistant', ' nearly', ' Seventh', ' the', ' Mary', '?\n', ' office', ' bathroom', ' the', 'office', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0015366673469543457, 0.001330941915512085, 0.004583974679311116, 0.005234420299530029, 0.002212226390838623]}}, 31: {'grad': {'score': [1.7841796875, 2.3183175762922876, 1.2645818536931819, 2.321380940560181], 'topk_tokens': [' be', ' B', ' most', ' H', ' C', ' C', ' the', ' and', ' were', ' their', ' U', ' its', ' L', ' S', ' was', ' S', ' its', 'editary', ' S', ' W'], 'evidence_proportions': [1.3514811197916667, 1.7442626953125, 1.9786783854166667, 2.36279296875, 1.6632486979166667]}, 'weight': {'score': [0.0032959534571721004, 0.0023188206754856046, 0.0019427239894866943, 0.0023174049804825575], 'topk_tokens': [' the', ' where', ' was', ',', ' the', ':', 'Question', '.\n\n', ' Where', '<|eot_id|>', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0011121431986490886, 0.004918694496154785, 0.0038088162740071616, 0.006386756896972656, 0.0018245379130045574]}, 'saliency': {'score': [0.0010214516749748816, 0.0001805206023770523, 0.0005300993269140071, 0.00017807824721406112], 'topk_tokens': [' Ramsey', ' Mary', ' Mary', ' open', ' office', ' Where', 'Question', '<|eot_id|>', 'light', '.\n\n', ' the', ' Market', '?\n', '<|eot_id|>', '<|end_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0012650092442830403, 0.0011232197284698486, 0.0011598467826843262, 0.001657038927078247, 0.00014792879422505698]}}, 'pred_res': 'Mary journeyed to the bathroom.<|eot_id|>', 'score': 0}
2025-01-22 00:48:01.544 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:48:01.544 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-0_0-2-5-7-8.pkl | len: 10 |  size: 9.3 KB
Processing depth (0, 2, 5, 7, 8):   1%|          | 1/100 [00:18<30:30, 18.49s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.82it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (1, 5, 6, 7, 8):   1%|          | 1/100 [00:25<30:30, 18.49s/it]2025-01-22 00:48:08.664 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:48:08.669 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:48:08.669 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the apple.
2025-01-22 00:48:08.686 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5925, 5929) -->  Mary got the apple
2025-01-22 00:48:08.686 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:48:08.691 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:48:08.691 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:48:08.714 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8349, 8353) -->  Mary dropped the apple
2025-01-22 00:48:08.714 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 00:48:08.743 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9595, 9601) -->  Daniel went back to the kitchen
2025-01-22 00:48:08.743 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:48:08.761 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (5958, 5964) --> . John went back to the
2025-01-22 00:48:08.761 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John journeyed to the office.
2025-01-22 00:48:08.765 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1490, 1496) -->  Mary journeyed to the office
2025-01-22 00:48:08.765 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John took the milk.
2025-01-22 00:48:08.775 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3225, 3229) -->  John took the milk
2025-01-22 00:48:08.775 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:48:08.780 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1889, 1895) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:48:10.902 | INFO     | test_jbb_retain:begin_test:544 - the bathroom<|eot_id|>
2025-01-22 00:48:10.902 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12147])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:40,  5.72s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.49s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.45s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.03it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.45it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.91it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.39it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.86it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.11it/s]
2025-01-22 00:48:20.657 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.7815011831430289, 0.6889989659642769, 0.8831065784801136, 0.6884473235279497], 'topk_tokens': [' office', ' office', ' office', ' conn', ' office', ' offices', 'office', ' office', 'Official', 'enter', ' office', ' offices', ' Or', ' office', ' office', ' or', ' office', ' office', ' office', 'office'], 'evidence_proportions': [0.85552978515625, 0.846282958984375, 0.85552978515625, 1.26611328125, 0.267181396484375]}, 'weight': {'score': [0.024583073762746956, 0.0025743179453165993, 0.011368922211907127, 0.002511041443778034], 'topk_tokens': [' front', '?\n', ' Mary', ' Broadway', ' bedroom', ' Fort', ' Bridge', 'Answer', 'THE', '<|eot_id|>', '<|eot_id|>', ' Mary', ':', 'assistant', 'Bridge', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005105992158253987, 0.0717153549194336, 0.005105992158253987, 0.053882598876953125, 0.012582699457804361]}, 'saliency': {'score': [0.004500765066880446, 0.00013258494666715994, 0.0018371045589447021, 0.00012010067969784738], 'topk_tokens': [' kitchen', ' office', ' bathroom', 'THE', ' front', 'Answer', 'Bridge', '<|eot_id|>', ' top', ' bedroom', ' John', ' Mary', ' dropped', '<|eot_id|>', '<|start_header_id|>', ' Mary', ' office', ' Market', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.000929713249206543, 0.013929784297943115, 0.000929713249206543, 0.010233879089355469, 0.0015347798665364583]}}, 25: {'grad': {'score': [0.8681077223557693, 0.784306691265742, 0.8528823852539062, 0.7840019650384814], 'topk_tokens': [' Aw', 'ation', ' regular', 'op', ' the', ' fifteen', 'ation', ' at', 'THE', ' of', ' circulated', ' exc', 'op', ' excited', 'ised', ' Wood', 'ished', ' principles', 'erc', 'itable'], 'evidence_proportions': [0.8387451171875, 0.81591796875, 0.8387451171875, 0.70343017578125, 1.0714111328125]}, 'weight': {'score': [0.024671747134282038, 0.0025464696191016397, 0.0033728426153009586, 0.002497429257039738], 'topk_tokens': ['THE', ' the', '?\n', '.', ' Anthony', '.', ' top', 'Answer', 'THE', '<|eot_id|>', ' THE', ' Mary', '<|eot_id|>', ':', ' Mary', 'assistant', '<|start_header_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006720324357350667, 0.07071685791015625, 0.006720324357350667, 0.0605010986328125, 0.005991617838541667]}, 'saliency': {'score': [0.007213851580253014, 8.223552607304667e-05, 0.0010093613104386764, 6.522711481242995e-05], 'topk_tokens': [' Ramsey', 'Answer', '.', ' Peter', ':', ' Mary', 'assistant', ' Ramsey', '<|start_header_id|>', ' Bench', ' top', ' Mary', '.', ' THE', '<|eot_id|>', '<|eot_id|>', ' Mary', '<|end_header_id|>', '<|begin_of_text|>', ' Mary'], 'evidence_proportions': [0.003253936767578125, 0.022022336721420288, 0.003253936767578125, 0.014833211898803711, 0.00018178423245747885]}}, 26: {'grad': {'score': [0.7969853327824519, 0.7895493745627212, 1.2710737748579546, 0.788657971230748], 'topk_tokens': ['str', ' two', 'ers', ' and', ' B', ' Col', 'leading', 'ols', ' hand', ' Marshall', ' part', ' week', 'ers', ' Col', 'BO', ' Press', ' Jul', ' a', ' clear', ' wh'], 'evidence_proportions': [0.7800496419270834, 0.8295135498046875, 0.7800496419270834, 0.8411865234375, 0.7797037760416666]}, 'weight': {'score': [0.018716463675865762, 0.002493324628317049, 0.0017487731846896086, 0.0024598214894461738], 'topk_tokens': [' Floral', ' Mary', ' Anthony', ' Seventh', ' Bridge', '<|eot_id|>', ' kitchen', ' Mary', '<|eot_id|>', 'Bridge', '?\n', ' bathroom', 'Answer', 'assistant', ' the', 'office', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.001770615577697754, 0.04215216636657715, 0.001770615577697754, 0.03103160858154297, 0.028774261474609375]}, 'saliency': {'score': [0.002155794547154353, 0.00010081884527178946, 0.0002486895431171764, 9.613472630628702e-05], 'topk_tokens': [' apple', 'Minnesota', ' Bench', 'assistant', ' Anthony', ' Mary', ' Mary', ' bathroom', ' bedroom', ' Az', ' kitchen', '<|end_header_id|>', ' Bridge', ' Seventh', ':', 'Bridge', '<|start_header_id|>', ' the', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00012896458307902017, 0.0033343881368637085, 0.00012896458307902017, 0.0036587119102478027, 0.004421780506769816]}}, 27: {'grad': {'score': [0.6310330904447116, 0.6154550239474442, 0.5795725042169745, 0.6154867887359031], 'topk_tokens': [' excursion', ' attempts', ' newspaper', ' river', '186', ' exchanged', 'SP', ' EX', ' intended', '186', ' newspaper', 'sp', ' accepted', ' N', 'bread', ' excessive', ' opposition', ' interruption', 'ile', ' Newspaper'], 'evidence_proportions': [0.445648193359375, 0.80206298828125, 0.445648193359375, 0.84967041015625, 0.7420247395833334]}, 'weight': {'score': [0.02123194932937622, 0.002546279810516812, 0.0051232224160974674, 0.002501447140091796], 'topk_tokens': ['Bridge', '<|eot_id|>', '<|eot_id|>', ' Mary', ' Bridge', '?\n', ' bedroom', 'Answer', 'THE', ' Mary', 'assistant', ' bathroom', ' THE', '.\n\n', 'THE', ':', '<|end_header_id|>', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005870193243026733, 0.06971073150634766, 0.005870193243026733, 0.03901481628417969, 0.007781028747558594]}, 'saliency': {'score': [0.006263219393216646, 0.0001256004538847317, 0.0012986524538560348, 0.00011028062606703081], 'topk_tokens': [' Broadway', ' Dan', ' top', ' Mary', ' Floral', ' Mary', 'THE', ' Bridge', ' the', 'Answer', ' bathroom', ':', '<|end_header_id|>', ' Mary', 'THE', ' bedroom', '<|start_header_id|>', ' Mary', ' THE', 'office'], 'evidence_proportions': [0.0018753806749979656, 0.0219191312789917, 0.0018753806749979656, 0.01145423948764801, 0.0011409421761830647]}}, 28: {'grad': {'score': [1.0212777944711537, 0.934670950927031, 1.0093439275568181, 0.9343491111271796], 'topk_tokens': [' first', ' spoken', '600', ' been', ' out', ' ', ' came', ' being', 'being', ' kept', ' acted', 'CH', 'been', 'hom', ' be', ' ins', ' acted', 'PA', 'na', ' taken'], 'evidence_proportions': [0.8212483723958334, 1.0714111328125, 0.8212483723958334, 1.16064453125, 1.2950032552083333]}, 'weight': {'score': [0.015303212862748366, 0.0024411269534967673, 0.001962759278037331, 0.0024143613849668577], 'topk_tokens': ['.\n\n', ' bathroom', ' Mary', ' Broadway', ' Floral', ' Mary', ' Bridge', ' Mary', '<|eot_id|>', '<|eot_id|>', ' to', 'Answer', '?\n', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0007795393466949463, 0.039681434631347656, 0.0007795393466949463, 0.03637504577636719, 0.014050523440043131]}, 'saliency': {'score': [0.0006571045288672814, 8.493800724138496e-05, 0.0002993670376864347, 8.331882136979879e-05], 'topk_tokens': [' Far', ' Broadway', 'Question', ' to', 'During', 'office', ' Bridge', ' the', '?\n', ' to', 'Answer', 'Bridge', 'assistant', ' bathroom', ' Floral', '<|end_header_id|>', ' Bridge', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.0431817372639976e-05, 0.0005283057689666748, 4.0431817372639976e-05, 0.0009195506572723389, 0.0018013517061869304]}}, 29: {'grad': {'score': [1.0768667367788463, 1.3100120926876697, 1.3398104580965908, 1.3104588504113812], 'topk_tokens': ['\n', 'LY', ' express', ' regular', ' The', ' Printing', ' press', ' mail', ' printed', 'mail', ' printed', ' the', ' regular', 'aper', 'paper', 'ATING', '\n', '\n', ' Press', 'APER'], 'evidence_proportions': [1.1387736002604167, 1.4661865234375, 1.1387736002604167, 0.864044189453125, 0.83538818359375]}, 'weight': {'score': [0.004208862781524658, 0.0025028499107477018, 0.0015516687523234975, 0.0025009136782747766], 'topk_tokens': [' to', 'Question', ' Where', '.\n\n', 'THE', 'Does', 'THE', '<|eot_id|>', '<|eot_id|>', ' Does', '?\n', ' the', 'Answer', ' in', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.220738093058269e-05, 0.011861801147460938, 6.220738093058269e-05, 0.006485462188720703, 0.005882481733957927]}, 'saliency': {'score': [0.0005035698413848877, 9.88566754766821e-05, 0.00019272890957919034, 9.781645306003319e-05], 'topk_tokens': [' the', ' to', ' the', '.\n\n', ' Where', 'THE', ' in', 'IVE', ' Mary', '<|eot_id|>', ' the', '?\n', 'Does', ' the', 'office', ' Does', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.7948290506998697e-06, 0.001522853970527649, 3.7948290506998697e-06, 0.0007189661264419556, 0.0006799995899200439]}}, 30: {'grad': {'score': [0.6545442434457632, 1.0662363221767224, 1.1898054643110796, 1.0668962240790485], 'topk_tokens': [' head', ' o', ' offices', ' office', ' o', ' office', ' office', ' office', 'op', 'office', 'ob', ' office', ' o', ' office', ' o', 'op', ' o', ' o', 'office', ' O'], 'evidence_proportions': [0.6503092447916666, 0.47780418395996094, 0.6503092447916666, 0.558258056640625, 0.84503173828125]}, 'weight': {'score': [0.012094456415909987, 0.002439358232639309, 0.0039573934945193205, 0.002415853619092792], 'topk_tokens': [' Miles', ':', ' Floral', ' Fort', ' Anthony', 'Question', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' Broadway', ' the', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0018981993198394775, 0.022363662719726562, 0.0018981993198394775, 0.022064208984375, 0.01899433135986328]}, 'saliency': {'score': [0.0011662474045386682, 0.00022032641807164918, 0.0005256669087843461, 0.00021773890989514955], 'topk_tokens': [',', ' artist', ' the', '<|eot_id|>', 'office', 'Question', 'assistant', ':', ' nearly', ' nearly', ' Seventh', ' Third', '<|begin_of_text|>', ' the', ' bathroom', '?\n', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0005367596944173177, 0.0014785826206207275, 0.0005367596944173177, 0.0016453266143798828, 0.0018976132074991863]}}, 31: {'grad': {'score': [1.5754746657151442, 2.351274410445304, 1.1550847833806819, 2.3551159909062913], 'topk_tokens': [' the', ' two', ' B', ' be', ' a', ' C', ' U', ' be', ' the', ' and', ' S', ' was', ' L', ' their', ' its', ' were', ' its', ' S', 'editary', ' W'], 'evidence_proportions': [1.5943196614583333, 1.1598968505859375, 1.5943196614583333, 1.9395751953125, 1.5721028645833333]}, 'weight': {'score': [0.0028513486568744364, 0.0023018997660859916, 0.0016228583726015959, 0.002301953748359866], 'topk_tokens': [' where', ' was', ' Market', ' the', ',', ':', 'Question', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0010177095731099446, 0.0076427459716796875, 0.0010177095731099446, 0.004872322082519531, 0.001977046330769857]}, 'saliency': {'score': [0.0005117356777191162, 0.00016884880700183215, 0.00016865134239196777, 0.00016811244517907104], 'topk_tokens': [' Miles', ' Ramsey', ' Mary', ' open', 'light', ' Where', '<|eot_id|>', ' the', ' Peter', 'Question', ' Market', '?\n', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0001643796761830648, 0.0016696155071258545, 0.0001643796761830648, 0.00093117356300354, 0.00015490253766377768]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-22 00:48:20.658 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:48:20.658 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-1_1-5-6-7-8.pkl | len: 10 |  size: 9.28 KB
Processing depth (1, 5, 6, 7, 8):   2%|‚ñè         | 2/100 [00:37<30:47, 18.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.34it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.80it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.57it/s]
Processing depth (1, 2, 4, 6, 9):   2%|‚ñè         | 2/100 [00:44<30:47, 18.86s/it]2025-01-22 00:48:27.999 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:48:28.004 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:48:28.005 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the apple.
2025-01-22 00:48:28.012 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2469, 2473) -->  Mary got the apple
2025-01-22 00:48:28.012 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:48:28.017 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1488, 1494) -->  tragedy. Mary journeyed to
2025-01-22 00:48:28.017 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:48:28.037 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (7174, 7178) -->  Mary dropped the apple
2025-01-22 00:48:28.037 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 00:48:28.068 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10697, 10703) --> . Daniel went back to the
2025-01-22 00:48:28.069 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:48:28.086 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (5965, 5971) --> . John went back to the
2025-01-22 00:48:28.086 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John journeyed to the office.
2025-01-22 00:48:28.091 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1490, 1496) -->  Mary journeyed to the office
2025-01-22 00:48:28.091 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John took the milk.
2025-01-22 00:48:28.100 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3230, 3234) -->  John took the milk
2025-01-22 00:48:28.100 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:48:28.106 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1889, 1895) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:48:30.084 | INFO     | test_jbb_retain:begin_test:544 - Mary's hand<|eot_id|>
2025-01-22 00:48:30.084 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12147])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:38,  5.50s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.38s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.39s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.09it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.45it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.81it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.16it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.47it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.11it/s]
2025-01-22 00:48:40.018 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.7752450796274039, 0.6579226717476747, 0.8417247425426136, 0.6573364368776341], 'topk_tokens': [' below', ' offices', ' editor', ' editor', 'ors', ' election', ' Minnesota', ' office', ' office', ' offices', ' effect', ' or', 'active', ' office', 'editor', ' office', ' office', ' office', 'office', ' office'], 'evidence_proportions': [0.861328125, 0.717071533203125, 0.861328125, 0.88006591796875, 0.5719807942708334]}, 'weight': {'score': [0.03450870972413283, 0.002576896296341102, 0.020587937398390335, 0.002475543511168995], 'topk_tokens': ['\n\n', '.', ' Bench', ' Bridge', '?\n', '.', ' Mary', ' bathroom', '<|eot_id|>', 'Answer', '<|eot_id|>', ' bedroom', 'assistant', ':', '<|start_header_id|>', ' Mary', '<|end_header_id|>', 'Bridge', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.013526052236557007, 0.10333061218261719, 0.013526052236557007, 0.07613372802734375, 0.0028427441914876304]}, 'saliency': {'score': [0.008869659442168016, 0.00014624814259504544, 0.0024408183314583516, 0.00012333357041556523], 'topk_tokens': [' Bridge', '.', ' Newspaper', ' bathroom', '<|eot_id|>', ' front', ' office', ' Market', '<|start_header_id|>', ' Mary', '.', '<|eot_id|>', ' Bench', ' Mary', ' bedroom', '<|begin_of_text|>', ' office', 'Bridge', ' Mary', 'office'], 'evidence_proportions': [0.004299908876419067, 0.031191647052764893, 0.004299908876419067, 0.01349020004272461, 4.747509956359863e-05]}}, 25: {'grad': {'score': [1.1044029822716346, 0.7515914054140258, 1.100677490234375, 0.750198711019815], 'topk_tokens': [' the', ' com', ' principles', 'ation', ' of', ' the', 'itable', ' the', 'ian', ' the', ' the', 'le', ' the', ' at', ' the', ' Aw', 'op', 'op', ' the', ' Wood'], 'evidence_proportions': [0.9158121744791666, 1.0767822265625, 0.9158121744791666, 1.046234130859375, 1.5387776692708333]}, 'weight': {'score': [0.025468647480010986, 0.002548153561931136, 0.007392907684499567, 0.002490099067875555], 'topk_tokens': [' Dan', '.', ' Mary', ' Peter', ' the', ' apple', ' Paul', '?\n', ' Mary', 'Answer', '<|eot_id|>', ' Mary', '<|eot_id|>', '.', '<|start_header_id|>', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.012571791807810465, 0.04938793182373047, 0.012571791807810465, 0.075103759765625, 0.002226094404856364]}, 'saliency': {'score': [0.012154038135822002, 9.879613375995412e-05, 0.0029879781332882967, 6.764182452573904e-05], 'topk_tokens': [' Emily', ' Ramsey', 'office', 'Answer', ' April', ':', ' directly', ' Paul', ' Senator', ' apple', ' Dan', ' Bench', ' Paul', '<|eot_id|>', '<|eot_id|>', ' Mary', ' Mary', ' Mary', '<|end_header_id|>', '.'], 'evidence_proportions': [0.009767303864161173, 0.022028043866157532, 0.009767303864161173, 0.027195215225219727, 0.0003173847993214925]}}, 26: {'grad': {'score': [0.83721923828125, 0.6557816133272286, 0.7848732688210227, 0.655157086869112], 'topk_tokens': [' scene', 'ian', 'CE', ' date', ' office', ' rumors', ' office', ' by', ' a', 'str', ' court', ' not', ' effect', '\n', 'istributed', ' str', 'ers', ' part', ' wh', ' Jul'], 'evidence_proportions': [0.8099365234375, 0.82452392578125, 0.8099365234375, 1.23046875, 0.6380818684895834]}, 'weight': {'score': [0.016077211269965537, 0.0024876958382493704, 0.004545054652474143, 0.0024547572963819573], 'topk_tokens': [' discarded', ' bathroom', ' kitchen', ' bedroom', '.', ' Bridge', ' Mary', ' Mary', '<|eot_id|>', '<|eot_id|>', '?\n', 'assistant', 'Answer', 'Bridge', ' the', 'office', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.007568031549453735, 0.037035584449768066, 0.007568031549453735, 0.041573524475097656, 0.0021257797876993814]}, 'saliency': {'score': [0.0014841808722569393, 0.00010132300960683011, 0.0005618089979345149, 9.751464697794721e-05], 'topk_tokens': [' Dan', ' Anthony', 'Answer', ' kitchen', '<|end_header_id|>', ' Seventh', 'assistant', ' apple', ' Mary', ' Mary', ' Bridge', ' Bench', ' Az', ' bedroom', '<|start_header_id|>', ':', ' the', 'Bridge', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00043378273646036786, 0.004138514399528503, 0.00043378273646036786, 0.0038399696350097656, 0.00024489561716715497]}}, 27: {'grad': {'score': [0.6162085899939904, 0.6225067874670754, 0.6620094992897727, 0.6224485024884953], 'topk_tokens': [' material', ' producing', ' Press', ' wood', 'tele', ' time', 'ly', ' remin', ' river', 'bread', ' earnest', ' news', ' told', 'sp', ' newspaper', 'ile', ' N', ' talk', ' newspaper', ' Newspaper'], 'evidence_proportions': [0.5325113932291666, 1.0218505859375, 0.5325113932291666, 0.666717529296875, 0.4795023600260417]}, 'weight': {'score': [0.028205243440774772, 0.0025531791720294944, 0.011373555118387396, 0.0024820278670293085], 'topk_tokens': [' Geo', '<|eot_id|>', ' Bridge', '<|eot_id|>', ' bathroom', '?\n', ' Mary', ' THE', 'Bridge', '.', ' bedroom', 'Answer', ' Mary', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.01453348994255066, 0.0788583755493164, 0.01453348994255066, 0.057186126708984375, 0.002459406852722168]}, 'saliency': {'score': [0.006855852328813993, 0.00012886480045216556, 0.002658304843035611, 0.00010981270916431799], 'topk_tokens': ['Answer', ' Bridge', ' upper', ' Joseph', '<|end_header_id|>', ' the', '<|begin_of_text|>', ' prior', ' the', 'THE', '<|start_header_id|>', ' Mary', ' Peter', ' Paul', ' Mary', ' bedroom', ' Mary', ':', '.\n\n', 'office'], 'evidence_proportions': [0.003473818302154541, 0.01974228024482727, 0.003473818302154541, 0.013609170913696289, 0.0005267560482025146]}}, 28: {'grad': {'score': [1.2737379807692308, 1.1748357953843938, 1.2246981534090908, 1.17453264451285], 'topk_tokens': [' hands', ' ', ' first', ' been', ' ', ' on', ' Father', ' acted', ' kept', 'hum', 'being', ' being', 'hom', ' be', ' acted', ' taken', 'na', ' ins', 'CH', 'PA'], 'evidence_proportions': [1.0841064453125, 1.439697265625, 1.0841064453125, 1.5447998046875, 1.3616536458333333]}, 'weight': {'score': [0.01624997304036067, 0.0024514778797219677, 0.0037131255323236637, 0.002419536955621989], 'topk_tokens': [' Bridge', '.\n\n', ' discarded', ' Bridge', 'Question', ' was', ' to', ' to', '<|eot_id|>', '<|eot_id|>', ' the', ' Mary', 'Answer', '?\n', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015053252379099529, 0.013640642166137695, 0.0015053252379099529, 0.07433319091796875, 0.008756677309672037]}, 'saliency': {'score': [0.0047892538400796745, 0.00012398572682019505, 0.0004716569727117365, 0.0001133299348728882], 'topk_tokens': [' to', ' the', ' Third', 'Question', ' the', 'Bridge', ' apple', ' Far', ' Bridge', ' Mary', ' to', 'Answer', 'office', 'assistant', ' Bridge', '<|end_header_id|>', ' Mary', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0004629194736480713, 0.004510238766670227, 0.0004629194736480713, 0.024198710918426514, 0.0006882945696512858]}}, 29: {'grad': {'score': [1.0304764967698317, 1.184090591946868, 1.1249556107954546, 1.1845281538062131], 'topk_tokens': ['ER', 'mail', ' railway', ' The', ' express', ' press', ' newspaper', '\n', 'LY', ' regular', ' printed', ' the', ' regular', 'aper', 'ATING', ' Press', '\n', 'paper', '\n', 'APER'], 'evidence_proportions': [1.17547607421875, 0.9847640991210938, 1.17547607421875, 0.613983154296875, 1.048614501953125]}, 'weight': {'score': [0.003101486426133376, 0.0025295838768839966, 0.0030199614438143644, 0.0025274635750285337], 'topk_tokens': [' discarded', ':', ' was', 'Question', '.\n\n', ' in', 'Does', ' Where', '<|eot_id|>', '<|eot_id|>', ' Does', ' the', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00015185276667277017, 0.006625175476074219, 0.00015185276667277017, 0.011294364929199219, 0.0011897087097167969]}, 'saliency': {'score': [0.0002535856687105619, 0.00011250056085375387, 0.0003545419736341997, 0.00011175738889396156], 'topk_tokens': [' to', '.\n\n', ' on', '      ', ' the', '<|eot_id|>', ' Where', 'Answer', ' the', ':', ' the', '?\n', 'assistant', 'Does', ' Does', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.4865070978800457e-05, 0.0004023909568786621, 2.4865070978800457e-05, 0.0010199546813964844, 0.00010091066360473633]}}, 30: {'grad': {'score': [1.27069091796875, 1.3063976298203557, 1.471339832652699, 1.3061744785143339], 'topk_tokens': [' an', ' o', ' office', ' office', ' office', ' office', ' office', ' head', 'office', 'op', ' o', 'ob', ' office', ' o', 'op', ' o', ' o', 'office', ' o', ' O'], 'evidence_proportions': [1.0411783854166667, 1.568359375, 1.0411783854166667, 1.1177978515625, 1.6331990559895833]}, 'weight': {'score': [0.0070519584875840405, 0.0024650439341594795, 0.006178506396033547, 0.0024484373766394175], 'topk_tokens': [' Geo', 'Bridge', ',', ' prior', ' Anthony', ':', '<|eot_id|>', ' Where', '<|eot_id|>', ' the', 'Question', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0030066470305124917, 0.009229183197021484, 0.0030066470305124917, 0.02309417724609375, 0.0029962857564290366]}, 'saliency': {'score': [0.0014281914784358097, 0.0002268493612152685, 0.00090214881029996, 0.0002230404625351927], 'topk_tokens': [' Third', ' was', ' office', ',', ' Where', '<|eot_id|>', 'Christmas', '.\n\n', ':', ' the', ' Geo', ' the', '?\n', 'assistant', 'Question', ':', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0013460417588551838, 0.0016764551401138306, 0.0013460417588551838, 0.0033562183380126953, 0.00014163057009379068]}}, 31: {'grad': {'score': [1.3597288865309496, 2.1638539887542185, 0.9702009721235796, 2.167751816952192], 'topk_tokens': ['7', ' D', ' was', ' its', ' their', ' were', ' L', ' and', 'UX', '186', ' was', ' U', ' most', ' S', ' S', 'G', ' its', ' S', ' W', 'editary'], 'evidence_proportions': [1.572021484375, 0.8567314147949219, 1.572021484375, 1.73052978515625, 1.0232747395833333]}, 'weight': {'score': [0.002482336301069993, 0.002288799833805927, 0.0018850646235726097, 0.002289118008045764], 'topk_tokens': [' to', ' where', ' was', ' the', ',', ':', 'Question', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0012637078762054443, 0.0034789443016052246, 0.0012637078762054443, 0.006293058395385742, 0.0017147064208984375]}, 'saliency': {'score': [0.0008582495726071871, 0.00017529599940750332, 0.00037226622754877263, 0.00017347051904040146], 'topk_tokens': [' the', ' the', ' the', ' Market', '.\n\n', ' Mary', ' Miles', ' Where', 'Question', '<|eot_id|>', ' Peter', '?\n', '<|eot_id|>', 'Answer', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0005967319011688232, 0.0012350380420684814, 0.0005967319011688232, 0.0022492408752441406, 0.00020276506741841635]}}, 'pred_res': "Mary's hand<|eot_id|>", 'score': 0}
2025-01-22 00:48:40.019 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:48:40.020 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-2_1-2-4-6-9.pkl | len: 10 |  size: 9.16 KB
Processing depth (1, 2, 4, 6, 9):   3%|‚ñé         | 3/100 [00:56<30:51, 19.09s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.12it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.17it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.19it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.57it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.40it/s]
Processing depth (0, 1, 5, 7, 8):   3%|‚ñé         | 3/100 [01:05<30:51, 19.09s/it]2025-01-22 00:48:48.166 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:48:48.166 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:48:48.167 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the apple.
2025-01-22 00:48:48.171 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1497, 1501) -->  Mary got the apple
2025-01-22 00:48:48.171 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:48:48.189 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5929, 5935) --> . Mary journeyed to the
2025-01-22 00:48:48.189 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:48:48.213 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8349, 8353) -->  Mary dropped the apple
2025-01-22 00:48:48.213 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 00:48:48.242 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9595, 9601) -->  Daniel went back to the kitchen
2025-01-22 00:48:48.242 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:48:48.260 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (5965, 5971) --> . John went back to the
2025-01-22 00:48:48.260 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John journeyed to the office.
2025-01-22 00:48:48.260 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:48:48.260 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John took the milk.
2025-01-22 00:48:48.269 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3230, 3234) -->  John took the milk
2025-01-22 00:48:48.270 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:48:48.275 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1894, 1900) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:48:50.479 | INFO     | test_jbb_retain:begin_test:544 - Mary journeyed to the bathroom.<|eot_id|>
2025-01-22 00:48:50.479 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12147])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.70s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.47s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.92it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.32it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.61it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.10it/s]
2025-01-22 00:49:00.291 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.7297867995042068, 0.7377555508683842, 0.7946527654474432, 0.7376692314579826], 'topk_tokens': [' office', ' offices', ' editor', ' offices', 'active', ' offices', 'editor', 'Official', ' office', ' or', ' offices', ' office', ' office', ' office', ' office', ' office', 'office', ' office', ' office', 'office'], 'evidence_proportions': [1.2243245442708333, 0.88238525390625, 0.3839314778645833, 1.1612548828125, 0.1917266845703125]}, 'weight': {'score': [0.022836969448969915, 0.0025812840302772153, 0.005408037792552601, 0.0025326238861853164], 'topk_tokens': [' front', ' Broadway', ' Mary', '<|eot_id|>', ' Fort', 'Answer', ':', '<|eot_id|>', ' Mary', 'Bridge', 'THE', 'assistant', ' bedroom', ' Floral', '<|start_header_id|>', ' Bridge', ' bathroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.006249745686848958, 0.03833818435668945, 0.020435770352681477, 0.06308937072753906, 0.004656314849853516]}, 'saliency': {'score': [0.012938673679645244, 0.00015313913423992085, 0.0045646727085113525, 0.00011764804781611207], 'topk_tokens': [' Market', 'Mary', ' front', ' office', ' Floral', ' bedroom', ' Mary', ' Bridge', '<|eot_id|>', ' office', '<|start_header_id|>', 'Bridge', 'office', '<|begin_of_text|>', 'office', ' top', ' Mary', ' office', ' office', ' Mary'], 'evidence_proportions': [0.014782965183258057, 0.02223806083202362, 0.00533988078435262, 0.030965328216552734, 0.0004758139451344808]}}, 25: {'grad': {'score': [1.6534329927884615, 1.1372043384486377, 1.7357510653409092, 1.1350070016166018], 'topk_tokens': [' at', 'op', ' the', ' the', ' her', ' at', ' of', ' at', ' good', ' the', ' the', 'ished', ' the', ' of', ' principles', ' excited', ' at', 'ised', 'itable', 'erc'], 'evidence_proportions': [1.8309733072916667, 1.50274658203125, 1.7890218098958333, 0.771484375, 2.0287272135416665]}, 'weight': {'score': [0.020673577602093037, 0.002549269962962326, 0.001209126277403398, 0.0025127648115256594], 'topk_tokens': [' Seventh', ' Ramsey', ' THE', ' April', ' top', ' Floral', 'THE', '<|eot_id|>', 'Answer', ' Mary', '<|eot_id|>', ' Anthony', 'THE', '<|start_header_id|>', ' Mary', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015432437260945637, 0.0413365364074707, 0.011101325352986654, 0.06933975219726562, 0.003156741460164388]}, 'saliency': {'score': [0.004571983447441688, 0.00011142999294393101, 0.0001583695411682129, 0.00010176078710326695], 'topk_tokens': [' Min', 'assistant', ' Floral', ' April', 'Answer', ' Dan', ' Ramsey', ' top', 'MIN', 'Mary', ':', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' Anthony', ' Bench', ' Mary', ' Mary', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004378954569498698, 0.011791974306106567, 0.0018940567970275879, 0.014208555221557617, 0.00014628966649373373]}}, 26: {'grad': {'score': [1.2423612154447115, 0.9899435524528768, 1.5073797052556819, 0.9884604969533768], 'topk_tokens': ['British', 'leading', ' a', 'ols', ' searched', ' Jul', ' Marshall', ' some', ' neighboring', ' Col', ' Press', ' July', 'press', ' wh', 'ers', ' several', ' B', ' well', ' James', ' Col'], 'evidence_proportions': [1.386962890625, 1.3994140625, 1.5214640299479167, 0.803466796875, 1.0065511067708333]}, 'weight': {'score': [0.011592284991190983, 0.0024975636867115318, 0.0009932653470472856, 0.0024807577871624156], 'topk_tokens': ['THE', '<|eot_id|>', 'Bridge', ' bedroom', ' Seventh', ' Mary', '<|eot_id|>', '?\n', ' bathroom', ' Anthony', ' Bridge', 'Answer', ' Floral', 'assistant', '<|end_header_id|>', ' the', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0006386041641235352, 0.010924100875854492, 0.007665306329727173, 0.030271053314208984, 0.014465888341267904]}, 'saliency': {'score': [0.0003207096686730018, 0.00015130572107478182, 0.00013257427649064496, 0.00015097579703076045], 'topk_tokens': [' Bench', ' apple', 'Minnesota', ' Az', 'THE', 'Answer', ' Fort', ' bathroom', ' Floral', ' Anthony', ' Seventh', ' bedroom', '<|end_header_id|>', 'Bridge', '<|start_header_id|>', ' Bridge', ':', '<|begin_of_text|>', ' the', 'office'], 'evidence_proportions': [9.13540522257487e-05, 0.0005063861608505249, 0.00038353602091471356, 0.0002987384796142578, 0.00037810206413269043]}}, 27: {'grad': {'score': [1.24609375, 1.064353611922792, 1.1904463334517046, 1.0637338876468112], 'topk_tokens': [' newspaper', ' wood', ' possessed', ' Papers', 'paper', 'bread', ' paper', 'apers', 'news', 'urnished', ' paper', 'SP', 'papers', ' newspaper', ' newspaper', 'APER', ' news', ' paper', ' newspaper', ' Newspaper'], 'evidence_proportions': [1.5537109375, 1.628173828125, 1.0824381510416667, 1.03753662109375, 0.9864501953125]}, 'weight': {'score': [0.016110427104509793, 0.0025564180066903652, 0.0027732144702564587, 0.0025269019535756015], 'topk_tokens': ['<|eot_id|>', '?\n', ' Anthony', ' Mary', ' Mary', ' Floral', ' THE', 'Answer', ' bedroom', ' bathroom', 'THE', ' Bridge', 'assistant', '.\n\n', ':', 'THE', '<|end_header_id|>', 'office', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001749118169148763, 0.03508949279785156, 0.014277289311091105, 0.041516780853271484, 0.0027145942052205405]}, 'saliency': {'score': [0.005146019733869112, 0.00015510473833014242, 0.0008148767731406472, 0.0001431818580186108], 'topk_tokens': ['assistant', 'looking', '<|end_header_id|>', ' Dan', ' Mary', ' illumination', 'RE', 'Mary', 'THE', ' Mary', ' Floral', '.\n\n', ' bedroom', ' Mary', ' Bridge', ' THE', 'THE', '<|begin_of_text|>', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0007125735282897949, 0.009214550256729126, 0.0044236282507578535, 0.0158827006816864, 0.0004317164421081543]}}, 28: {'grad': {'score': [0.9468900240384616, 0.9051514978084616, 0.8253978382457386, 0.9052068138012225], 'topk_tokens': [' insisted', ' exc', ' become', '600', '\n', ' being', ' taken', 'sur', ' kept', ' be', 'been', ' OUT', ' ', ' acted', 'hom', ' hands', 'CH', 'hum', 'na', ' ins'], 'evidence_proportions': [0.7674153645833334, 1.145263671875, 0.8675537109375, 0.9112548828125, 1.0972086588541667]}, 'weight': {'score': [0.010715872049331665, 0.0024423163578298143, 0.001503399827263572, 0.002426246918584579], 'topk_tokens': [' the', ' to', ' Mary', ' Mary', 'During', ' the', '<|eot_id|>', '?\n', ' Floral', '<|eot_id|>', 'Answer', ' Bridge', ' to', 'assistant', ' the', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0003761152426401774, 0.0058699846267700195, 0.020696520805358887, 0.022136688232421875, 0.006691694259643555]}, 'saliency': {'score': [0.0013415263249323918, 0.00010187575449932945, 0.0002889606085690585, 9.887213648257102e-05], 'topk_tokens': [' bedroom', 'Question', ' procession', ' Bridge', '<|eot_id|>', ' parade', ' bathroom', 'Bridge', 'Answer', ' the', ' the', 'SSION', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ' Floral', 'office', ' Bridge', '<|begin_of_text|>', ':'], 'evidence_proportions': [9.690721829732259e-05, 0.00045342743396759033, 0.0033363799254099527, 0.0016121864318847656, 0.0010029176870981853]}}, 29: {'grad': {'score': [1.6857440655048077, 1.3267687926989875, 1.950998999855735, 1.3248626343938559], 'topk_tokens': [' the', ' Paul', ' mail', 'engers', 'ION', 'The', 'mail', 'ants', 'published', ' Papers', ' printed', 'aper', 'paper', ' Press', ' printed', 'ATING', '\n', '\n', ' the', 'APER'], 'evidence_proportions': [2.1575113932291665, 1.21588134765625, 2.7224934895833335, 1.0933837890625, 0.8853759765625]}, 'weight': {'score': [0.005132324420488798, 0.002527835879544484, 0.0017125281420621004, 0.0025237221755663054], 'topk_tokens': ['Does', ':', ' Where', '<|eot_id|>', 'THE', '<|eot_id|>', '?\n', ' Does', 'THE', 'Answer', ' was', ' the', ' where', ' in', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001504361629486084, 0.0013156980276107788, 0.010071376959482828, 0.00809335708618164, 0.004391630490620931]}, 'saliency': {'score': [0.0009680940554692195, 0.00014951046783449778, 0.00027359615672718394, 0.00014752608154955092], 'topk_tokens': [' the', ' place', '.', 'NEW', '?\n', ' the', 'THE', ':', 'Does', ' in', ' the', ' the', 'THE', ' Does', 'office', ' where', '<|end_header_id|>', ':', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [1.4821688334147135e-05, 3.425776958465576e-05, 0.002550711234410604, 0.0012968480587005615, 0.0007421374320983887]}}, 30: {'grad': {'score': [1.0290386493389423, 1.0385511088875627, 0.9566206498579546, 1.0387204993550379], 'topk_tokens': [' office', ' office', ' o', ' office', ' office', ' offices', ' office', ' an', 'ob', 'op', ' o', ' o', ' office', ' office', 'op', 'office', ' o', ' o', ' o', ' O'], 'evidence_proportions': [1.34033203125, 1.094970703125, 1.2545267740885417, 0.7579193115234375, 0.6290486653645834]}, 'weight': {'score': [0.011828349186823918, 0.002448148301171907, 0.0034684322097084737, 0.0024261392548935207], 'topk_tokens': [':', ' Fort', ' Where', '<|eot_id|>', '<|eot_id|>', 'Question', ' the', ' Anthony', ' Floral', ' the', '.\n\n', ' bathroom', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003284613291422526, 0.008372187614440918, 0.018550952275594074, 0.02197265625, 0.009190718332926432]}, 'saliency': {'score': [0.0038701250002934383, 0.00027165129131932877, 0.001828499815680764, 0.0002610892729762763], 'topk_tokens': [' the', ' office', 'NEW', ' Seventh', ' Third', '?\n', ' Mary', ' Floral', ' bathroom', ' Congress', ' the', 'assistant', ' office', ' office', 'office', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.006134917338689168, 0.0026604384183883667, 0.004302601019541423, 0.005209863185882568, 0.0010861555735270183]}}, 31: {'grad': {'score': [1.4540311373197115, 1.8578565815602106, 0.8952054110440341, 1.8604743641650872], 'topk_tokens': [' miles', ' its', ' C', ' H', ' were', ' their', ' be', ' C', ' and', ' was', ' S', ' most', ' L', ' its', 'G', ' U', ' S', ' S', ' W', 'editary'], 'evidence_proportions': [1.1080729166666667, 1.462738037109375, 1.6469319661458333, 2.09246826171875, 1.1756591796875]}, 'weight': {'score': [0.003322271200326773, 0.002316124420203695, 0.0020478557456623425, 0.002314450351495052], 'topk_tokens': [' to', ' the', ' where', ' was', ',', 'Question', ':', '.\n\n', ' Where', '<|eot_id|>', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0016001065572102864, 0.004786700010299683, 0.003940761089324951, 0.006086111068725586, 0.0016071001688639324]}, 'saliency': {'score': [0.001320552367430467, 0.00016578504289361281, 0.0006938468326221813, 0.00016234389672288422], 'topk_tokens': ['<|eot_id|>', ' Miles', ' Where', ' open', ' Mary', ' the', ' Ramsey', ' Mary', ' Market', ' office', 'light', '.\n\n', '?\n', '<|eot_id|>', 'Answer', '<|start_header_id|>', 'assistant', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0018761952718098958, 0.001372992992401123, 0.0014232099056243896, 0.0020415037870407104, 0.00014665722846984863]}}, 'pred_res': 'Mary journeyed to the bathroom.<|eot_id|>', 'score': 0}
2025-01-22 00:49:00.292 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:49:00.293 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-3_0-1-5-7-8.pkl | len: 10 |  size: 9.31 KB
Processing depth (0, 1, 5, 7, 8):   4%|‚ñç         | 4/100 [01:17<31:17, 19.55s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:03,  1.02s/it][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.13it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.20it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.53it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.35it/s]
Processing depth (0, 3, 7, 8, 9):   4%|‚ñç         | 4/100 [01:25<31:17, 19.55s/it]2025-01-22 00:49:08.351 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:49:08.351 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:49:08.351 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the apple.
2025-01-22 00:49:08.362 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3781, 3785) -->  Mary got the apple
2025-01-22 00:49:08.362 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:49:08.388 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8341, 8347) --> . Mary journeyed to the
2025-01-22 00:49:08.388 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:49:08.414 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9591, 9595) -->  dropped the apple.
2025-01-22 00:49:08.415 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 00:49:08.447 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10697, 10703) --> . Daniel went back to the
2025-01-22 00:49:08.447 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:49:08.465 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (5958, 5964) --> . John went back to the
2025-01-22 00:49:08.465 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John journeyed to the office.
2025-01-22 00:49:08.465 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:49:08.465 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John took the milk.
2025-01-22 00:49:08.474 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3225, 3229) -->  John took the milk
2025-01-22 00:49:08.474 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:49:08.480 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1889, 1895) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:49:10.450 | INFO     | test_jbb_retain:begin_test:544 - Mary's bathroom<|eot_id|>
2025-01-22 00:49:10.451 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12147])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.71s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.48s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.45s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.93it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.90it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:49:20.232 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.78155517578125, 0.6664445440468352, 0.7651256214488636, 0.6660178140139503], 'topk_tokens': [' office', ' office', 'Official', ' or', 'office', 'active', ' offices', ' office', ' office', ' office', ' office', ' offices', ' office', 'office', ' office', ' office', ' office', ' office', ' office', 'office'], 'evidence_proportions': [1.2621866861979167, 0.73358154296875, 0.6466878255208334, 0.7900390625, 0.4621175130208333]}, 'weight': {'score': [0.024416863918304443, 0.002582420840833656, 0.004475932229648937, 0.002532065269346326], 'topk_tokens': ['\n\n', 'Mary', ' Floral', 'THE', '?\n', ' Bridge', '.', ' Mary', '<|eot_id|>', 'Bridge', 'Answer', ' Mary', ':', '<|eot_id|>', 'assistant', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003105004628499349, 0.049204349517822266, 0.052923341592152916, 0.021711349487304688, 0.00250093142191569]}, 'saliency': {'score': [0.007111219259408804, 0.00016116941894599073, 0.0014856132594021883, 0.0001438287809539182], 'topk_tokens': [' Bench', ' location', '\n\n', ' bathroom', '<|eot_id|>', 'el', '<|end_header_id|>', ' office', 'Bridge', '<|start_header_id|>', ' Mary', 'Mary', '.', '<|eot_id|>', ' office', ' Mary', '<|begin_of_text|>', ' Market', ' Mary', 'office'], 'evidence_proportions': [0.003064523140589396, 0.016070112586021423, 0.015377223491668701, 0.0023429691791534424, 9.814898173014323e-05]}}, 25: {'grad': {'score': [1.5082820012019231, 1.0786197520114824, 1.6317194158380681, 1.0766910344606075], 'topk_tokens': [' the', 'ation', ' the', ' circulated', 'ised', ' subscribers', ' by', ' at', ' revisit', ' at', ' the', ' at', ' exc', 'ian', 'erc', ' at', ' principles', ' at', 'itable', ' at'], 'evidence_proportions': [1.4644775390625, 1.123291015625, 1.8143717447916667, 0.6201171875, 2.0947672526041665]}, 'weight': {'score': [0.02194117124264057, 0.0025473374949138206, 0.0007557706399397416, 0.002508925363136987], 'topk_tokens': ['.\n\n', ' prior', ' Mary', 'Mary', ' to', '?\n', 'THE', ' Anthony', ' Mary', 'Answer', '<|eot_id|>', '.', '<|start_header_id|>', '<|eot_id|>', ' Mary', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006008942921956381, 0.042125701904296875, 0.04806028803189596, 0.024618148803710938, 0.001921325922012329]}, 'saliency': {'score': [0.005630692610373864, 8.506749979173493e-05, 4.454634406349876e-05, 7.322593401625239e-05], 'topk_tokens': [' Ramsey', ' Floral', ' directly', 'office', '<|start_header_id|>', ' Paul', ':', ' top', 'Answer', ' Bench', ' Paul', ' Mary', '<|end_header_id|>', '<|eot_id|>', 'Mary', '.', '<|eot_id|>', ' Mary', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [8.215506871541341e-05, 0.015636295080184937, 0.012182543675104776, 0.002214789390563965, 0.00023424625396728516]}}, 26: {'grad': {'score': [0.9426586444561298, 0.6502098857802082, 0.9258395108309659, 0.6490804320593017], 'topk_tokens': [' Marshall', 'graph', 'ente', ' B', 'BO', ' Press', ' some', ' Jul', ' part', ' Col', ' not', 'ers', ' wh', ' neighboring', 'ols', ' searched', ' week', ' a', 'prob', ' clear'], 'evidence_proportions': [0.8291829427083334, 1.047607421875, 1.1116943359375, 0.8603515625, 0.8720041910807291]}, 'weight': {'score': [0.010749782507236186, 0.002493508000051605, 0.0008910650556737727, 0.002478682002827369], 'topk_tokens': ['.\n\n', ' the', ' Seventh', ' Bridge', 'Bridge', ' Anthony', ' Floral', '<|eot_id|>', ' Mary', '<|eot_id|>', ' bathroom', '?\n', 'assistant', 'Answer', ' the', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003236830234527588, 0.013893365859985352, 0.029674232006072998, 0.008122920989990234, 0.0019069512685139973]}, 'saliency': {'score': [0.002535629730958205, 8.828431571478785e-05, 0.00010810115120627664, 8.298996391104091e-05], 'topk_tokens': [' Ramsey', 'assistant', ' Mary', 'Mary', ' apple', ':', ' the', 'Minnesota', ' bathroom', ' Bridge', ' bedroom', ' Anthony', ' Az', 'Bridge', ' Seventh', ' Mary', '<|start_header_id|>', 'office', '<|begin_of_text|>', ' the'], 'evidence_proportions': [4.729628562927246e-05, 0.002251669764518738, 0.008955279986063639, 0.00029858946800231934, 0.00028497974077860516]}}, 27: {'grad': {'score': [0.5352654090294471, 0.5958261096592312, 0.5610129616477273, 0.596019520738698], 'topk_tokens': ['\n', 'paper', ' ', ' ', ' earnest', ' Cl', ' newspaper', ' EX', ' told', ' July', 'bread', ' excessive', ' paper', ' Moore', ' interruption', ' excursion', ' Newspaper', ' newspaper', ' opposition', 'ile'], 'evidence_proportions': [0.4974161783854167, 0.7401580810546875, 0.4809977213541667, 0.5359725952148438, 0.4903157552083333]}, 'weight': {'score': [0.01738142967224121, 0.0025402530347067532, 0.0031683661720969462, 0.002507223608907358], 'topk_tokens': [' Bridge', '<|eot_id|>', '.', ' Mary', 'Mary', '?\n', 'THE', ' Mary', ' bedroom', ' THE', 'THE', 'Answer', 'assistant', '.\n\n', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000958720842997233, 0.04073691368103027, 0.03971552848815918, 0.0078277587890625, 0.002268830935160319]}, 'saliency': {'score': [0.008743785894834079, 0.00013189384874291593, 0.0008046654137698087, 0.00011216736600355869], 'topk_tokens': [' Anthony', 'Bridge', '?\n', ' THE', ' the', ' Floral', '.', 'Answer', ' Mary', ' bathroom', ' prior', 'THE', ' bedroom', 'THE', 'Mary', ':', ' Mary', '.\n\n', ' Mary', 'office'], 'evidence_proportions': [0.0003181000550587972, 0.02188393473625183, 0.0217189093430837, 0.0012809783220291138, 0.0004094541072845459]}}, 28: {'grad': {'score': [0.7504366361177884, 0.8756189424026669, 0.8294428045099432, 0.8759718566243879], 'topk_tokens': [' ', 'been', ' be', ' out', ' executed', ' balance', 'being', ' contents', 'character', ' PA', 'hum', ' being', ' taken', ' acted', 'na', 'hom', ' kept', 'CH', ' acted', ' ins'], 'evidence_proportions': [0.7124074300130209, 0.820648193359375, 0.55865478515625, 0.7896957397460938, 0.9072672526041666]}, 'weight': {'score': [0.017822593450546265, 0.0024434566223059126, 0.0019337697462602096, 0.002411339900856343], 'topk_tokens': ['.\n\n', ' to', ' Mary', ' the', ' to', ' Bridge', ' the', '<|eot_id|>', ' the', ' Mary', '<|eot_id|>', '?\n', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002164145310719808, 0.028992176055908203, 0.04835428794225057, 0.006779193878173828, 0.004812955856323242]}, 'saliency': {'score': [0.003497641820173997, 0.00010684656305993689, 0.0003444335677407005, 9.912921814728982e-05], 'topk_tokens': [' during', ' Mary', ' the', 'Question', ' bathroom', 'Bridge', ' Floral', ' Bridge', 'Answer', ' Mary', 'office', ' Mary', 'assistant', ' the', '<|end_header_id|>', ' Bridge', ' the', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [4.033247629801432e-05, 0.004626750946044922, 0.011359373728434244, 0.0004824995994567871, 0.00035057465235392254]}}, 29: {'grad': {'score': [1.370199937086839, 1.1247641588299448, 1.732330322265625, 1.1231322452831087], 'topk_tokens': [' the', ' newspaper', ' printing', 'LY', '\n', '\n', ' Printing', ' printed', ' regular', 'AILY', ' THE', 'paper', ' regular', ' the', 'ATING', 'aper', '\n', ' Press', '\n', 'APER'], 'evidence_proportions': [1.8284505208333333, 1.4200286865234375, 1.6031417846679688, 0.83740234375, 1.0009867350260417]}, 'weight': {'score': [0.006417322617310744, 0.0025258833159634247, 0.001690542156046087, 0.002519040913160611], 'topk_tokens': ['Does', ' the', ' in', ':', ' Where', '<|eot_id|>', ' the', '<|eot_id|>', ' Does', ' where', '?\n', ' the', 'Answer', ' was', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0007163683573404948, 0.006665706634521484, 0.01937076449394226, 0.0032727718353271484, 0.0010956128438313801]}, 'saliency': {'score': [0.0016129865096165584, 9.360658022511378e-05, 0.000202864408493042, 9.014343251946367e-05], 'topk_tokens': ['Answer', ' in', 'assistant', ' the', '?\n', '<|start_header_id|>', ':', ' where', ' in', '      ', ' the', ' was', ' the', 'Does', ' Does', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [3.112355868021647e-05, 0.0004582703113555908, 0.006239384412765503, 0.0004812479019165039, 9.275476137797038e-05]}}, 30: {'grad': {'score': [1.1320706881009615, 1.1467147501852004, 1.0724820223721592, 1.1468811716070728], 'topk_tokens': [' head', ' o', ' office', ' offices', ' office', ' office', 'op', ' office', ' o', 'office', ' o', ' office', 'ob', ' office', 'op', ' o', ' o', 'office', ' o', ' O'], 'evidence_proportions': [1.321044921875, 1.01318359375, 0.7945760091145834, 1.30712890625, 1.2431437174479167]}, 'weight': {'score': [0.014091411462196937, 0.0024719527154530466, 0.0033764486963098698, 0.0024453429444428664], 'topk_tokens': [' prior', ' Anthony', '<|eot_id|>', ':', 'Question', ' the', '<|eot_id|>', ' Where', ' the', ' bathroom', ' the', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015790462493896484, 0.012484073638916016, 0.04273812969525655, 0.009564399719238281, 0.0020466248194376626]}, 'saliency': {'score': [0.004462102284798255, 0.00022572091071436322, 0.0006440254774960605, 0.00021585820381448895], 'topk_tokens': [' office', ' Seventh', ' to', '<|eot_id|>', ',', ' bathroom', ' the', 'Question', ' Third', ' the', ':', ' office', '?\n', ' the', '<|begin_of_text|>', ' the', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0017522374788920085, 0.0018711388111114502, 0.01571520169576009, 0.0007690191268920898, 0.0001082321008046468]}}, 31: {'grad': {'score': [1.6604966383713942, 2.4157283392666065, 1.2279496626420454, 2.4195104362097526], 'topk_tokens': ['UX', ' be', ' be', ' U', ' the', ' L', '186', ' most', ' S', 'G', ' their', ' and', ' was', ' were', ' S', ' its', ' W', ' its', ' S', 'editary'], 'evidence_proportions': [1.3768717447916667, 1.3871612548828125, 1.7877197265625, 2.567138671875, 1.3946940104166667]}, 'weight': {'score': [0.0028406014809241663, 0.0023046517550714693, 0.0016784478317607534, 0.0023046386812296932], 'topk_tokens': [' to', ' where', ' was', ' the', ',', 'Question', ':', '.\n\n', ' the', '<|eot_id|>', ' Where', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0008869965871175131, 0.0061577558517456055, 0.004341403643290202, 0.0019161701202392578, 0.0016982555389404297]}, 'saliency': {'score': [0.0009118479031782884, 0.000191867768553532, 0.00038869272578846324, 0.00018996299766191874], 'topk_tokens': [' Miles', ' office', ' Mary', ' Market', 'light', '<|eot_id|>', '.\n\n', ' the', ' Mary', 'Question', ' Where', '?\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ':', 'Answer', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.000952442487080892, 0.0017155259847640991, 0.0014925102392832439, 0.0002759099006652832, 0.00017876426378885904]}}, 'pred_res': "Mary's bathroom<|eot_id|>", 'score': 0}
2025-01-22 00:49:20.233 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:49:20.233 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-4_0-3-7-8-9.pkl | len: 10 |  size: 9.18 KB
Processing depth (0, 3, 7, 8, 9):   5%|‚ñå         | 5/100 [01:37<31:10, 19.69s/it]Processing depth (0, 3, 7, 8, 9):   5%|‚ñå         | 5/100 [01:37<30:53, 19.52s/it]
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:70 - Selected idx: 2
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:72 - Answer: office
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:73 - Tag: 3-hop
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:74 - Needle: [' Daniel went back to the kitchen.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' John moved to the bedroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:75 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 00:49:20.638 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.16it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.26it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.31it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.73it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.52it/s]
Processing depth (0, 5, 6, 9):   0%|          | 0/100 [00:07<?, ?it/s]2025-01-22 00:49:28.022 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:49:28.023 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:49:28.023 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:49:28.041 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5924, 5930) --> . Mary journeyed to the
2025-01-22 00:49:28.041 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:49:28.061 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7169, 7173) -->  Mary dropped the football
2025-01-22 00:49:28.061 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 00:49:28.079 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:49:28.079 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 00:49:28.098 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:49:28.098 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 00:49:28.100 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (501, 507) --> . Sandra journeyed to the
2025-01-22 00:49:28.100 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:49:28.125 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8417, 8423) -->  affair. John went back to
2025-01-22 00:49:28.125 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:49:28.125 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:49:28.125 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John took the milk.
2025-01-22 00:49:28.136 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (3724, 3728) -->  John took the milk
2025-01-22 00:49:28.136 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John moved to the bedroom.
2025-01-22 00:49:28.168 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (11182, 11187) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:49:30.800 | INFO     | test_jbb_retain:begin_test:544 - Mary's location before the bathroom was not specified, but it was mentioned that Mary journeyed to the bathroom.<|eot_id|>
2025-01-22 00:49:30.800 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:04<00:32,  4.64s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  2.01s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:05,  1.17s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.28it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:01,  1.80it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:05<00:00,  2.35it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:05<00:00,  2.96it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  3.48it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.37it/s]
2025-01-22 00:49:39.763 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.36434173583984375, 0.6269145384013829, 0.3955891927083333, 0.6280234723354129], 'topk_tokens': [' Minnesota', ' the', ' Minnesota', 'ire', 'eff', ' evil', 'con', ' Fletcher', 'super', ' up', ' Minnesota', ' marched', ' Minnesota', ' Min', ' of', 'Minnesota', ' effect', ' Minnesota', 'active', ' Minnesota'], 'evidence_proportions': [0.3506266276041667, 0.47157033284505206, 0.4673309326171875, 0.20216878255208334]}, 'weight': {'score': [0.05543430285020308, 0.0025776664324406538, 0.004275403239510276, 0.0024768749567254406], 'topk_tokens': [' the', '\n\n', ' \n', ' hallway', 'Answer', ' bathroom', ' Bridge', '<|start_header_id|>', ' football', '<|eot_id|>', ' Mary', '<|eot_id|>', 'assistant', 'Bridge', ' bathroom', ':', '<|end_header_id|>', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0030466715494791665, 0.04238283634185791, 0.22737884521484375, 0.006243705749511719]}, 'saliency': {'score': [0.012082349170338024, 0.00011001509155220682, 0.0002577756390427098, 8.783139455390412e-05], 'topk_tokens': ['.', ' hand', ' kitchen', ' random', ' \n', ' bedroom', '<|end_header_id|>', ' Bridge', '<|eot_id|>', ' bedroom', ' office', 'Mary', '<|eot_id|>', ' hallway', ' Mary', '<|begin_of_text|>', ' football', 'Bridge', ' Mary', 'office'], 'evidence_proportions': [0.0002732276916503906, 0.0328377882639567, 0.01654183864593506, 0.0001630385716756185]}}, 25: {'grad': {'score': [1.0370150479403408, 0.9375084408184475, 0.8831523548473011, 0.9374757446702928], 'topk_tokens': [' stere', ' storm', ' stere', 'ian', ' for', 'ing', ' Gree', ' the', ' exc', 'don', ' the', ' private', 'ex', 'irie', ' extraordinary', 'ation', 'ation', ' private', 'ised', 'erc'], 'evidence_proportions': [1.212158203125, 1.0192667643229167, 0.87652587890625, 0.9866129557291666]}, 'weight': {'score': [0.02068870717828924, 0.00256729628406232, 0.0022305195981805973, 0.0025352482059147227], 'topk_tokens': [' the', ' bathroom', ' the', ' random', ' football', '.\n\n', '.', ' football', ' \n', 'Answer', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' Mary', '.', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00039275487263997394, 0.03656428058942159, 0.0520172119140625, 0.004223416248957316]}, 'saliency': {'score': [0.002191286195408214, 7.902073024094085e-05, 0.00023712533893007222, 7.474658054107966e-05], 'topk_tokens': [' Market', ' football', '.', ' Minnesota', ' directly', 'nes', 'Mary', ' Min', ' football', 'Answer', ' random', 'office', 'assistant', ' Mary', '<|eot_id|>', '.', '<|eot_id|>', ':', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [1.5109777450561523e-05, 0.0035028954346974692, 0.005982875823974609, 0.0005281269550323486]}}, 26: {'grad': {'score': [0.6270113858309659, 0.6494622153595242, 0.6780617453835227, 0.64942501481035], 'topk_tokens': [' by', ' gentleman', ' Col', 'BO', ' Field', ' steam', ' proprietor', ' generally', ' Guards', 'str', ' some', 'cery', 'io', ' Gen', ' Merch', ' STR', ' generally', ' generally', ' worth', ' str'], 'evidence_proportions': [0.843017578125, 0.7889912923177084, 0.30841064453125, 0.46142578125]}, 'weight': {'score': [0.011863066391511396, 0.002525519571062527, 0.0029588287526910954, 0.0025073499495423405], 'topk_tokens': [' before', ' kitchen', ' Mary', ' football', ' the', 'Bridge', ' football', ' Bridge', '?', '<|eot_id|>', ' bathroom', '<|eot_id|>', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00037531057993570965, 0.017366111278533936, 0.03336524963378906, 0.003512988487879435]}, 'saliency': {'score': [0.00046841664747758347, 0.00011144991096870892, 0.0001737132216944839, 0.00011063059752645756], 'topk_tokens': [' the', ' North', ' Seventh', ' bedroom', ' football', ' Gal', ' Gray', ' hallway', ' bathroom', ' Bridge', ' bathroom', 'Answer', 'assistant', ' Bridge', '<|end_header_id|>', 'Bridge', '<|start_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [2.645452817281087e-05, 0.0005144675572713217, 0.001419365406036377, 0.0002303620179494222]}}, 27: {'grad': {'score': [0.508392333984375, 0.7046031882820217, 0.5177390358664773, 0.7054700662961011], 'topk_tokens': [' time', ' newspaper', 'na', ' print', 'urnished', 'bread', 'ile', ' B', ' Temper', 'ly', 'remember', ' talk', ' print', ' Newspaper', ' told', ' product', ' tell', 'APER', ' remember', 'remember'], 'evidence_proportions': [0.7544962565104166, 0.23077392578125, 0.6668243408203125, 0.4342854817708333]}, 'weight': {'score': [0.028116022998636418, 0.0025606138124487925, 0.0029105255098053904, 0.0025131676793049163], 'topk_tokens': ['?', ' bathroom', ' before', '<|eot_id|>', '<|eot_id|>', ' \n', '.', ' Bridge', ' bathroom', ' football', 'Answer', ' Mary', ' football', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0010638634363810222, 0.05388874808947245, 0.0645284652709961, 0.005120495955149333]}, 'saliency': {'score': [0.0028732256455854936, 0.00013331829971650105, 0.0004181320017034357, 0.00012755655211254117], 'topk_tokens': [' Dan', 'Question', ':', ' ST', ' \n', '<|eot_id|>', ' the', '<|start_header_id|>', ' Minnesota', ' upper', ' bathroom', ' Bridge', ' kitchen', ' the', ' football', ' football', '.\n\n', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00020329157511393228, 0.0018679896990458171, 0.011706441640853882, 0.0006595849990844727]}}, 28: {'grad': {'score': [0.6968598799272017, 0.6763306266978103, 0.631131952459162, 0.6764166196423922], 'topk_tokens': [' next', ' executed', ' on', ' ', ' about', 'sur', '600', 'hom', ' balance', 'been', ' out', ' returns', ' had', 'being', ' be', ' acted', ' kept', 'hum', 'na', ' ins'], 'evidence_proportions': [0.49331919352213544, 0.6985270182291666, 0.557281494140625, 0.9917856852213541]}, 'weight': {'score': [0.011705279350280762, 0.002480600474498599, 0.004617364117593476, 0.002457987712116284], 'topk_tokens': [' the', ':', ' the', ' was', '.\n\n', ' football', ' the', '<|eot_id|>', ' bathroom', ' before', '<|eot_id|>', '?', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00037616491317749023, 0.017065465450286865, 0.024190902709960938, 0.009350458780924479]}, 'saliency': {'score': [0.001611980524930087, 8.292438797861249e-05, 0.00018618504206339517, 7.986089367631115e-05], 'topk_tokens': ['Question', ' bedroom', ' during', '<|eot_id|>', ' the', ' football', ' Mary', ' bathroom', ' kitchen', ' before', ' Far', ' \n', ' Bridge', ' Bridge', 'assistant', '<|start_header_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [3.567337989807129e-05, 0.0029726624488830566, 0.0038770437240600586, 0.00031756361325581867]}}, 29: {'grad': {'score': [1.2103160511363635, 0.980712890625, 1.2608531605113635, 0.9795307275192777], 'topk_tokens': ['ATING', ' The', '\n', ' THE', 'The', ' The', ' the', ' o', '\n', '\n', ' paper', 'ION', ' printed', ' the', 'paper', '\n', ' The', '      ', '\n', ' the'], 'evidence_proportions': [1.6807454427083333, 1.6698811848958333, 0.38885498046875, 0.8279622395833334]}, 'weight': {'score': [0.005222006277604537, 0.0025404175528186226, 0.0022704276171597567, 0.0025362758771328084], 'topk_tokens': [' Does', ':', ' football', ' was', '.\n\n', ' bathroom', '<|eot_id|>', '?', '<|eot_id|>', ' Where', ' \n', ' before', ' the', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008662740389506022, 0.00776288906733195, 0.009829521179199219, 0.0039651791254679365]}, 'saliency': {'score': [0.0004080263051119718, 7.05663648372345e-05, 0.00018138777125965464, 6.965003099964381e-05], 'topk_tokens': ['.\n\n', ' the', 'Answer', ' was', ' Do', '      ', 'ot', ' to', 'assistant', ':', 'Does', ' Does', ' before', ' the', ' \n', '<|end_header_id|>', '<|start_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [3.085533777872721e-05, 0.0005899866422017416, 0.0007275938987731934, 0.00039019187291463214]}}, 30: {'grad': {'score': [0.9285195090553977, 0.9882849478823674, 1.0209239612926135, 0.9883046081992165], 'topk_tokens': ['doll', 'Emp', ' anx', 'G', ' its', ' head', ' an', ' dre', ' o', ' Paul', 'ob', 'office', ' an', 'op', ' o', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.9981486002604166, 1.0373331705729167, 0.6854248046875, 0.912139892578125]}, 'weight': {'score': [0.016115819866007023, 0.0024562293383334865, 0.006305945642066725, 0.0024208740394306492], 'topk_tokens': ['.', ' bathroom', ' before', ':', ' the', '<|eot_id|>', ' Where', '<|eot_id|>', 'Question', '.\n\n', '?', ' bathroom', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0013146400451660156, 0.032054136196772255, 0.0272369384765625, 0.00756460428237915]}, 'saliency': {'score': [0.0046593384309248495, 0.00019391963934874826, 0.0006430564504681212, 0.00018457036884667155], 'topk_tokens': ['Mary', 'IR', '<|eot_id|>', ' office', ' the', ' football', ' Where', '.', ' football', '?', ' bathroom', 'Question', '<|begin_of_text|>', ' \n', ' Mary', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00023731589317321777, 0.011346926291783651, 0.007327675819396973, 0.0006148815155029297]}}, 31: {'grad': {'score': [0.9103282581676136, 0.9962882907680276, 0.5474576083096591, 0.9976694644419163], 'topk_tokens': [' S', ' the', 'Mr', ' many', ' United', ' M', ' may', ' answer', 'G', 'issippi', 'Mary', 'Mexico', ' membership', ' M', ' very', ' marriage', ' Mr', ' Mary', ' most', 'editary'], 'evidence_proportions': [0.7167561848958334, 1.17974853515625, 1.421112060546875, 0.49395751953125]}, 'weight': {'score': [0.003406968983736905, 0.0023280389614312477, 0.0020375179521965256, 0.0023268689236254116], 'topk_tokens': [' bathroom', ' was', '.\n\n', 'Question', ':', ' before', ' football', '?', '<|eot_id|>', ' the', ' Where', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007387797037760416, 0.0060929059982299805, 0.005642890930175781, 0.0018986066182454426]}, 'saliency': {'score': [0.0014902976426211271, 0.00012492129666890838, 0.00019887180039376923, 0.00012223555729622425], 'topk_tokens': ['Mary', ' the', ' before', ' was', ' the', '.\n\n', 'Question', 'ot', ' Where', '?', ' football', '<|eot_id|>', ' Mary', '<|end_header_id|>', ' \n', '<|begin_of_text|>', '<|start_header_id|>', ':', 'assistant', 'office'], 'evidence_proportions': [0.0003759761651357015, 0.0034448603789011636, 0.0021609365940093994, 0.00020296374956766763]}}, 'pred_res': "Mary's location before the bathroom was not specified, but it was mentioned that Mary journeyed to the bathroom.<|eot_id|>", 'score': 0}
2025-01-22 00:49:39.764 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:49:39.764 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-0_0-5-6-9.pkl | len: 10 |  size: 8.95 KB
Processing depth (0, 5, 6, 9):   1%|          | 1/100 [00:19<31:24, 19.04s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.25it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.30it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.71it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.51it/s]
Processing depth (1, 2, 4, 7):   1%|          | 1/100 [00:26<31:24, 19.04s/it]2025-01-22 00:49:47.020 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:49:47.025 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:49:47.025 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:49:47.030 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:49:47.030 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:49:47.044 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4852, 4856) -->  Mary dropped the football
2025-01-22 00:49:47.044 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 00:49:47.063 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (6175, 6181) --> . Daniel went back to the
2025-01-22 00:49:47.063 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 00:49:47.082 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (6175, 6181) --> . Daniel went back to the
2025-01-22 00:49:47.082 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 00:49:47.084 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (494, 500) --> . Sandra journeyed to the
2025-01-22 00:49:47.084 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:49:47.109 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8424, 8430) -->  affair. John went back to
2025-01-22 00:49:47.109 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:49:47.114 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1497, 1503) -->  Mary journeyed to the office
2025-01-22 00:49:47.114 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John took the milk.
2025-01-22 00:49:47.124 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (3731, 3735) -->  John took the milk
2025-01-22 00:49:47.124 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John moved to the bedroom.
2025-01-22 00:49:47.157 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (11182, 11187) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:49:49.098 | INFO     | test_jbb_retain:begin_test:544 - the office<|eot_id|>
2025-01-22 00:49:49.099 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:37,  5.33s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:13,  2.31s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.35s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.13it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:01,  1.61it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:00,  2.16it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.75it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  3.37it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.23it/s]
2025-01-22 00:49:58.184 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.6089949174360796, 0.6080033268079108, 0.6199237939083215, 0.6079689936889053], 'topk_tokens': [' Bu', ' cont', 'u', 'enter', 'human', ' effect', 'Spring', ' super', 'oub', ' Minnesota', ' up', ' cons', 'active', 'sur', 'ire', ' over', 'sur', ' conn', 'con', 'super'], 'evidence_proportions': [0.69384765625, 0.69384765625, 0.2651519775390625, 0.66851806640625]}, 'weight': {'score': [0.016400432044809513, 0.002578033520117927, 0.009051485495133833, 0.0025352222505637435], 'topk_tokens': [' the', 'Answer', ' bedroom', '.', ' office', '.', ' Bridge', ' bedroom', '<|eot_id|>', ' football', 'assistant', ' hallway', ':', '<|eot_id|>', '<|start_header_id|>', 'Bridge', ' bathroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019421080748240154, 0.0019421080748240154, 0.06894874572753906, 0.010284870862960815]}, 'saliency': {'score': [0.0022089671004902234, 9.606875280506484e-05, 0.0008704517826889501, 9.011171951016352e-05], 'topk_tokens': [':', ' office', ' office', 'Question', '<|end_header_id|>', 'CH', 'IR', 'Bridge', '.', ' office', '<|eot_id|>', ' bedroom', ' top', '<|eot_id|>', ' bedroom', ' football', '<|begin_of_text|>', ' hallway', ' office', 'office'], 'evidence_proportions': [0.00010323524475097656, 0.00010323524475097656, 0.011008620262145996, 0.0005539953708648682]}}, 25: {'grad': {'score': [0.6118691184303977, 1.0278953775827298, 0.6494201891350023, 1.0296850293581474], 'topk_tokens': ['antic', ' corner', ' very', '\n', ' her', ' the', 'aud', '\n', ' safe', ' Gree', ' extraordinary', 'ting', ' both', ' the', ' be', '\n', 'ation', ' the', 'erc', 'ation'], 'evidence_proportions': [0.5387369791666666, 0.5387369791666666, 1.0610198974609375, 0.4586995442708333]}, 'weight': {'score': [0.011006921529769897, 0.002568325950866237, 0.004160259709213719, 0.002548630017949562], 'topk_tokens': [' Press', ' Merch', ' Dan', ' top', ' Ot', ' Capt', ' Daniel', ' Empire', ' \n', '.', 'Answer', '<|start_header_id|>', '.', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0052346885204315186, 0.0052346885204315186, 0.03890228271484375, 0.003954480091730754]}, 'saliency': {'score': [0.0022893927314064717, 7.717825587658573e-05, 0.0004924933115641276, 7.202038642323699e-05], 'topk_tokens': ['.', ' Mary', ' football', ' Press', ' Daniel', ' Mary', 'Answer', 'office', 'assistant', ' Mary', ' post', ' Empire', '<|eot_id|>', ' post', '<|eot_id|>', ':', ' Capt', ' Ot', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0015308062235514324, 0.0015308062235514324, 0.007652968168258667, 0.0002308487892150879]}}, 26: {'grad': {'score': [0.8953843550248579, 1.0894762126481725, 1.0825079715613164, 1.0898483265011105], 'topk_tokens': [' Gal', ' gang', ' black', ' Col', ' Press', ' Gen', ' went', ' gold', ' ne', ' Guards', 'ol', ' generally', ' Col', ' some', ' generally', ' Col', 'str', 'BO', ' STR', ' str'], 'evidence_proportions': [0.7962824503580729, 0.7962824503580729, 0.6273193359375, 1.2722981770833333]}, 'weight': {'score': [0.002284475348212502, 0.002532480420114808, 0.00376398093772657, 0.0025295710175265887], 'topk_tokens': [' before', ' bedroom', ' football', '?', ' Bridge', ' hallway', ' the', ' the', 'Bridge', '<|eot_id|>', '<|eot_id|>', ' bathroom', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0007439156373341879, 0.0007439156373341879, 0.003941535949707031, 0.004260887702306111]}, 'saliency': {'score': [0.00017603148113597524, 0.00012040365908072895, 0.0002882191629120798, 0.00011984451546775863], 'topk_tokens': [' Daniel', ' Dan', ' North', ' office', 'assistant', 'Answer', ' Father', ' Bridge', ' Ramsey', ' bedroom', ' hallway', ' bedroom', ' Bridge', ' bathroom', '<|end_header_id|>', ':', 'Bridge', '<|start_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [4.722674687703451e-05, 4.722674687703451e-05, 0.00040659308433532715, 0.00027993321418762207]}}, 27: {'grad': {'score': [0.45846973765980115, 0.6011697607759507, 0.4061501242897727, 0.6019615451563857], 'topk_tokens': ['urnished', ' possessed', ' navy', ' printer', ' tell', ' print', ' plainly', ' N', 'ly', '      ', ' product', ' noble', ' very', 'sur', 'na', 'bread', ' remember', ' party', 'remember', 'ile'], 'evidence_proportions': [0.3285115559895833, 0.3285115559895833, 0.8140029907226562, 0.4813639322916667]}, 'weight': {'score': [0.0067285570231350985, 0.002562238387383243, 0.004544778303666549, 0.0025492488209213323], 'topk_tokens': [' before', '<|eot_id|>', ' bedroom', '<|eot_id|>', ' ST', ' \n', 'Bridge', '.', ' football', ' THE', ' bathroom', 'Answer', ' Bridge', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0027518669764200845, 0.0027518669764200845, 0.01672840118408203, 0.008015374342600504]}, 'saliency': {'score': [0.0018226395953785288, 0.0001467640438400133, 0.0011699181614500103, 0.00014092319806022522], 'topk_tokens': [' the', ' the', ' Mary', ' office', '.', ' the', '.', ' Daniel', '.\n\n', ' Mary', '.', ' Bridge', ' bathroom', 'Bridge', ' ST', '<|start_header_id|>', ':', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0014107823371887207, 0.0014107823371887207, 0.003058195114135742, 0.00182265043258667]}}, 28: {'grad': {'score': [0.6354217529296875, 0.539126007110224, 0.5406346176609849, 0.5389467058155738], 'topk_tokens': [' exact', ' take', ' balance', 'half', 'hum', ' have', ' returns', ' came', '600', ' escaping', ' out', 'na', ' endeavor', ' firm', ' reached', 'hom', ' be', ' become', 'been', ' ins'], 'evidence_proportions': [0.5060145060221354, 0.5060145060221354, 0.81842041015625, 0.7722371419270834]}, 'weight': {'score': [0.003916049545461481, 0.002465464466686582, 0.0057994217583627414, 0.002453727638574586], 'topk_tokens': ['Question', ' football', '.\n\n', ' Bridge', ' the', ' bathroom', ' Bridge', ' the', '?', '<|eot_id|>', ' \n', ' before', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009889801343282063, 0.0009889801343282063, 0.0020017623901367188, 0.011046379804611206]}, 'saliency': {'score': [0.00023869492790915749, 7.598201204845691e-05, 0.00019832993998672023, 7.535212982147451e-05], 'topk_tokens': ['Question', 'Super', ' bedroom', ' bedroom', ' Daniel', ' the', ' the', ' \n', '<|eot_id|>', 'Bridge', ' before', '<|start_header_id|>', ' Bridge', 'assistant', ' Bridge', '<|begin_of_text|>', '<|end_header_id|>', ' Far', 'office', ':'], 'evidence_proportions': [7.6750914255778e-05, 7.6750914255778e-05, 0.00028936564922332764, 0.0005288024743398031]}}, 29: {'grad': {'score': [0.4926064231178977, 0.7713972335981232, 0.6595144560842803, 0.7722097308683226], 'topk_tokens': [' The', ' The', ' printed', 'AILY', ' NEW', ' Paul', ' were', 'ants', '\n', 'ATING', 'paper', '\n', ' The', 'boat', 'ION', ' The', 'APER', '\n', 'ION', '\n'], 'evidence_proportions': [0.5341796875, 0.5341796875, 0.2741241455078125, 0.55511474609375]}, 'weight': {'score': [0.0021493380719965153, 0.0025392775084966887, 0.002694951765465014, 0.002539562087767593], 'topk_tokens': ['Question', ' football', ' the', ' Where', ' Does', ' bathroom', '.\n\n', '?', '<|eot_id|>', '<|eot_id|>', ' \n', ' before', ' the', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015265246232350667, 0.0015265246232350667, 0.0015797615051269531, 0.0037746826807657876]}, 'saliency': {'score': [0.00020267475735057485, 7.924806985330692e-05, 0.00020318681543523616, 7.868531737425758e-05], 'topk_tokens': [' Does', ' bathroom', ' the', 'nes', '<|eot_id|>', 'IVE', 'ot', '?', ' Do', '<|eot_id|>', 'assistant', ' the', 'Answer', '<|end_header_id|>', ' before', ' \n', '<|begin_of_text|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.00020709633827209473, 0.00020709633827209473, 0.00013612210750579834, 0.00023820002873738608]}}, 30: {'grad': {'score': [0.598480224609375, 0.8272619464109319, 0.8336607037168561, 0.827660692701227], 'topk_tokens': [' Was', ' Sons', 'office', ' an', ' Capt', '.', 'Country', ' anx', 'ob', 'op', ' an', ' States', ' Bridge', ' Paul', 'ire', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.5058492024739584, 0.5058492024739584, 0.4349212646484375, 0.8927815755208334]}, 'weight': {'score': [0.005030054937709461, 0.002468074274801054, 0.007641618902033026, 0.002449295597278302], 'topk_tokens': [' Where', ':', ' the', 'IR', ' before', ' the', '<|eot_id|>', 'Question', '.\n\n', '?', '<|eot_id|>', ' bathroom', 'Answer', 'assistant', ' \n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0026794870694478354, 0.0026794870694478354, 0.007544517517089844, 0.008054882287979126]}, 'saliency': {'score': [0.0014072737910530784, 0.00012411163910698412, 0.0010519551508354418, 0.00011924531948159313], 'topk_tokens': [' football', ' Daniel', '.', ' football', ' bedroom', ' \n', ' Bridge', '.', 'Question', '?', ' Miles', 'IR', ' bathroom', ' office', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0008879601955413818, 0.0008879601955413818, 0.0038505494594573975, 0.0008170505364735922]}}, 31: {'grad': {'score': [0.94537353515625, 0.8309890098936039, 0.6928317908084753, 0.8311579281664916], 'topk_tokens': [' the', ' members', ' United', 'IO', ' may', ' H', ' the', ' an', ' membership', ' very', 'Mr', ' Mr', ' M', ' an', ' M', 'editary', ' B', ' an', ' marriage', ' most'], 'evidence_proportions': [1.1491902669270833, 1.1491902669270833, 0.94189453125, 0.5400594075520834]}, 'weight': {'score': [0.0014300779862837358, 0.002318790958317872, 0.002080220164674701, 0.0023210587596554206], 'topk_tokens': ['Question', ' was', ' the', ' football', ':', '.\n\n', ' before', ' the', '<|eot_id|>', ' Where', '?', 'Answer', ' \n', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0012184778849283855, 0.0012184778849283855, 0.001171708106994629, 0.0020255247751871743]}, 'saliency': {'score': [0.0005554313009435481, 0.00011323192695037369, 0.000312517989765514, 0.00011188364064418421], 'topk_tokens': [' Miles', ' Mary', ':', ' was', ' Mary', 'ot', 'Question', '?', ' Where', ' the', ' football', '<|end_header_id|>', ' the', '<|eot_id|>', ' \n', '<|begin_of_text|>', 'assistant', ':', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0007098913192749023, 0.0007098913192749023, 0.0005038678646087646, 0.0002808868885040283]}}, 'pred_res': 'the office<|eot_id|>', 'score': 100}
2025-01-22 00:49:58.186 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:49:58.186 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-1_1-2-4-7.pkl | len: 10 |  size: 8.74 KB
Processing depth (1, 2, 4, 7):   2%|‚ñè         | 2/100 [00:37<30:30, 18.68s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.13it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.28it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.75it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.54it/s]
Processing depth (0, 4, 7, 8):   2%|‚ñè         | 2/100 [00:44<30:30, 18.68s/it]2025-01-22 00:50:05.549 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:50:05.550 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:50:05.550 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:50:05.565 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (4844, 4850) --> . Mary journeyed to the
2025-01-22 00:50:05.565 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:50:05.589 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8344, 8348) -->  Mary dropped the football
2025-01-22 00:50:05.590 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 00:50:05.609 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:50:05.609 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 00:50:05.628 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:50:05.628 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 00:50:05.630 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (501, 507) --> . Sandra journeyed to the
2025-01-22 00:50:05.630 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:50:05.655 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8417, 8423) -->  affair. John went back to
2025-01-22 00:50:05.655 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:50:05.656 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:50:05.656 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John took the milk.
2025-01-22 00:50:05.666 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (3724, 3728) -->  John took the milk
2025-01-22 00:50:05.667 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John moved to the bedroom.
2025-01-22 00:50:05.699 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (11182, 11187) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:50:07.713 | INFO     | test_jbb_retain:begin_test:544 - The bedroom.<|eot_id|>
2025-01-22 00:50:07.714 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:35,  5.12s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:13,  2.21s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.28s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.19it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:01,  1.68it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:05<00:00,  2.25it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.87it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  3.49it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.28it/s]
2025-01-22 00:50:16.460 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.46788163618607953, 0.5480736766082894, 0.44593672318892047, 0.5484982812847243], 'topk_tokens': [' over', ' the', 'eff', ' cons', 'editor', 'edit', ' conn', ' Minnesota', 'eff', ' the', 're', 'ir', 'super', 'enter', 'cont', ' cont', 'active', 'ire', ' Minnesota', 'con'], 'evidence_proportions': [0.3735860188802083, 0.615478515625, 0.32098388671875, 0.51251220703125]}, 'weight': {'score': [0.013368308544158936, 0.002567539491424209, 0.002734245675982851, 0.002547435445839937], 'topk_tokens': [' bathroom', ' \n', ' Fort', 'Bridge', 'THE', '.', 'Answer', ' football', '<|eot_id|>', '<|start_header_id|>', 'assistant', ' Bridge', '<|eot_id|>', ' bedroom', '.', ':', ' hallway', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0012522538503011067, 0.00398525595664978, 0.06163787841796875, 0.0026877025763193765]}, 'saliency': {'score': [0.002166219732978127, 9.246641057943646e-05, 0.00012458273858735055, 8.860612678575149e-05], 'topk_tokens': ['Right', ' bedroom', 'Question', ' random', ' Market', ' \n', ' top', ' Bridge', '<|eot_id|>', 'THE', '<|eot_id|>', ' office', ' Mary', ' football', ' bedroom', '.', '<|begin_of_text|>', ' bedroom', ' hallway', 'office'], 'evidence_proportions': [6.41942024230957e-05, 0.00018896659215291342, 0.01140660047531128, 8.524457613627116e-05]}}, 25: {'grad': {'score': [0.6971858631480824, 0.6780162179473987, 0.5809145840731534, 0.678246319801648], 'topk_tokens': ['ervative', ' stere', ' stere', 'ated', '\n', ' const', '\n', ' remin', ' the', ' for', ' extraordinary', ' the', 'ation', ' be', 'antic', ' the', ' per', 'ting', 'ation', 'erc'], 'evidence_proportions': [0.7343546549479166, 0.7519327799479166, 0.96783447265625, 0.42483774820963544]}, 'weight': {'score': [0.011787674643776634, 0.0025585233910051584, 0.001853231227759159, 0.0025436580402920304], 'topk_tokens': [' directly', ' April', '.\n\n', '.', 'Right', 'THE', ' \n', '.', '<|start_header_id|>', ' Mary', '<|eot_id|>', '<|eot_id|>', 'Answer', ' Anthony', '.', 'assistant', ':', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00019854307174682617, 0.006418069203694661, 0.05131721496582031, 0.0023933847745259604]}, 'saliency': {'score': [0.0009537772698835893, 6.868786175156017e-05, 8.929137027624881e-05, 6.70214528571403e-05], 'topk_tokens': ['<|start_header_id|>', ' April', '.', 'Mer', ' Continental', ' Min', ' directly', ' Market', '.', '<|eot_id|>', ' Mary', 'assistant', '<|eot_id|>', 'Right', ':', '<|begin_of_text|>', 'office', 'Answer', ' Anthony', '<|end_header_id|>'], 'evidence_proportions': [6.179014841715495e-06, 0.00043966372807820636, 0.0043256282806396484, 0.0001675883928934733]}}, 26: {'grad': {'score': [0.8744146173650568, 0.880964201926243, 0.9922041459517046, 0.8806725598777444], 'topk_tokens': [' mar', 'oth', ' wh', ' broad', ' black', ' Col', ' sm', ' some', ' went', ' Col', 'ol', 'str', ' generally', ' worth', ' generally', ' Guards', 'BO', ' Merch', ' STR', ' str'], 'evidence_proportions': [1.1069742838541667, 0.9981689453125, 0.3199615478515625, 0.8877360026041666]}, 'weight': {'score': [0.00416846979748119, 0.0025114248487405335, 0.00261806177370476, 0.002508119299464414], 'topk_tokens': [' before', ' Bridge', ' football', '?', ' bathroom', ' bedroom', ' the', '<|eot_id|>', '<|eot_id|>', ' Bridge', ' Anthony', ' hallway', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00013689200083414713, 0.0027042031288146973, 0.015583992004394531, 0.0020539661248524985]}, 'saliency': {'score': [0.00019506432793357155, 0.00013536517071339264, 0.00011443911176739317, 0.00013531366806610696], 'topk_tokens': [' North', 'Answer', ' Father', ' colon', ' bathroom', ' Floral', ' hallway', ' Az', 'assistant', '<|start_header_id|>', ' bedroom', '<|end_header_id|>', 'Bridge', ' Seventh', ' Bridge', ' bedroom', ' Bridge', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [9.506940841674805e-06, 0.00011015931765238444, 0.0007053017616271973, 0.00012536843617757162]}}, 27: {'grad': {'score': [0.9109080921519886, 1.0064380132017616, 0.8159336899266099, 1.007131662497319], 'topk_tokens': [' N', ' print', ' plainly', '      ', ' Newspaper', ' very', ' possessed', 'ile', 'ATION', ' tell', ' print', 'ly', '<|eot_id|>', 'APER', ' notified', ' product', 'urnished', 'remember', ' remember', ' print'], 'evidence_proportions': [1.2979329427083333, 0.8381144205729166, 0.81524658203125, 0.6604512532552084]}, 'weight': {'score': [0.0082637911493128, 0.0025624052882390782, 0.002056354826146906, 0.00255341406821968], 'topk_tokens': [' before', '.', ' Bridge', '<|eot_id|>', ' \n', ' bathroom', ' football', 'THE', '.', ' Anthony', 'THE', 'Answer', ' Bridge', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0003477334976196289, 0.0029090046882629395, 0.036538124084472656, 0.0026850799719492593]}, 'saliency': {'score': [0.001334537159312855, 0.0001344832300080019, 0.00028558933373653525, 0.00013188770467369707], 'topk_tokens': [' \n', 'Bridge', ' Floral', 'Minnesota', ' ST', '<|eot_id|>', 'assistant', ' Jackson', ' bathroom', ' Mary', '<|start_header_id|>', '.', ' Anthony', 'THE', '.\n\n', 'THE', ':', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [7.799267768859863e-05, 0.0005368888378143311, 0.00600123405456543, 0.00027759869893391925]}}, 28: {'grad': {'score': [0.6468783291903409, 0.5118749051412784, 0.5266746752189867, 0.5115889159126632], 'topk_tokens': ['ib', 'hum', '      ', ' exact', ',', ' returns', '      ', 'half', '      ', '      ', 'avored', ' reached', '      ', ' balance', '      ', ' firm', ' endeavor', ' become', 'na', ' ins'], 'evidence_proportions': [0.57122802734375, 0.6951904296875, 0.767730712890625, 0.5936482747395834]}, 'weight': {'score': [0.005774590102109042, 0.002453970916969116, 0.003750474163980195, 0.0024443919680545005], 'topk_tokens': [' football', 'Question', ' Bridge', ' bathroom', ' the', '.\n\n', '<|eot_id|>', ' to', ' Bridge', '?', ' before', '<|eot_id|>', 'assistant', ' \n', 'Answer', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00019675493240356445, 0.005629837512969971, 0.012922286987304688, 0.006732046604156494]}, 'saliency': {'score': [0.0006368837573311546, 5.9146997094115105e-05, 0.00016857096643158883, 5.779735683004899e-05], 'topk_tokens': [' bedroom', '.\n\n', ' during', ' to', ' nearly', '.', 'Bridge', ' \n', ' Far', '<|eot_id|>', ' Floral', ' before', '<|start_header_id|>', ' to', 'assistant', 'office', ' Bridge', ' Bridge', '<|end_header_id|>', ':'], 'evidence_proportions': [1.3768672943115234e-05, 0.001119166612625122, 0.001138448715209961, 0.00044333934783935547]}}, 29: {'grad': {'score': [0.7362088290127841, 0.8026592115214438, 0.918663602886778, 0.8024635415884359], 'topk_tokens': ['ATING', ' mail', ' The', '\n', 'The', ' The', ' The', 'ants', ' months', 'ION', '      ', ' The', 'mail', ' Paul', ' the', '\n', '\n', ' The', '\n', 'boat'], 'evidence_proportions': [1.1867268880208333, 0.7242024739583334, 0.3764801025390625, 0.5375162760416666]}, 'weight': {'score': [0.0035245879129929976, 0.002520035865238555, 0.0014233336304173324, 0.0025212010871602042], 'topk_tokens': [':', 'Does', '?', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' Does', ' Where', ' \n', ' the', ' before', 'Answer', ' in', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002401272455851237, 0.0046160221099853516, 0.008754730224609375, 0.0022308528423309326]}, 'saliency': {'score': [0.000302843072197654, 5.019203216023927e-05, 6.603291540434866e-05, 4.968917331397459e-05], 'topk_tokens': [' Where', 'Question', ' the', ' the', '      ', ' Do', ':', ' bedroom', ' the', ' Does', 'Does', ' \n', '<|end_header_id|>', 'Answer', ' before', ' in', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [1.389781634012858e-05, 0.0005959769090016683, 0.0005879998207092285, 0.0001085499922434489]}}, 30: {'grad': {'score': [0.6966552734375, 0.6693306961536879, 0.6257042162346117, 0.6694000365272168], 'topk_tokens': [' Fourth', 'ch', ' Paul', 'ob', ' Collection', ' Crew', ' Sons', ' anx', ' Loan', 'Country', ' Third', ' o', ' o', ' o', ' O', ' States', ' o', ' Bridge', ' Was', 'ire'], 'evidence_proportions': [0.7065938313802084, 0.8408203125, 0.6718292236328125, 0.5591023763020834]}, 'weight': {'score': [0.0067774219946427775, 0.0024318619703908952, 0.004842913512027625, 0.002417376977303361], 'topk_tokens': ['IR', ' Third', ' the', '<|eot_id|>', ':', ' before', ' Where', '<|eot_id|>', ' bathroom', '?', 'Question', '.\n\n', 'assistant', 'Answer', '<|end_header_id|>', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.000723580519358317, 0.006340225537618001, 0.0204010009765625, 0.004186073939005534]}, 'saliency': {'score': [0.001121664589101618, 0.00017159315642456416, 0.00048656355250965464, 0.00016900524651058886], 'topk_tokens': ['.', ' John', ' Seventh', ':', ' Floral', 'Answer', ' Bridge', ' Mary', '?', 'IR', '.', ' Third', 'Question', ' bathroom', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [6.719430287679036e-05, 0.001002560059229533, 0.004093170166015625, 0.0003142356872558594]}}, 31: {'grad': {'score': [0.6684431596235796, 0.6176972450454807, 0.4151010224313447, 0.6181577813247785], 'topk_tokens': [' an', ' Mr', ' Aw', ' Aw', ' the', 'IR', ' Mr', ' an', ' C', ' J', ' marriage', ' M', 'Mr', ' an', ' M', ' an', ' C', ' H', ' most', ' B'], 'evidence_proportions': [0.6196492513020834, 0.7461344401041666, 0.9706878662109375, 0.43804931640625]}, 'weight': {'score': [0.0019238753752274947, 0.0023118020046059536, 0.0017328370701182973, 0.00231408764329647], 'topk_tokens': [' was', ' the', 'Question', ' football', ':', '.\n\n', '<|eot_id|>', ' before', '?', ' Where', ' the', 'Answer', ' \n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0005889733632405599, 0.000969847043355306, 0.006000518798828125, 0.001495043436686198]}, 'saliency': {'score': [0.00047057054259560323, 9.513851212246229e-05, 0.00013456561348655006, 9.434792260659163e-05], 'topk_tokens': ['light', ' was', ' Where', '.', ' hallway', ' Mary', ' the', ' football', ' the', 'Question', 'Answer', '<|end_header_id|>', ' the', '<|eot_id|>', ' \n', 'assistant', '<|start_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00025523702303568524, 0.00025070707003275555, 0.001645803451538086, 0.00012227892875671387]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 00:50:16.462 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:50:16.462 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-2_0-4-7-8.pkl | len: 10 |  size: 8.72 KB
Processing depth (0, 4, 7, 8):   3%|‚ñé         | 3/100 [00:55<29:53, 18.49s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.32it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (1, 4, 6, 8):   3%|‚ñé         | 3/100 [01:02<29:53, 18.49s/it]2025-01-22 00:50:23.699 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:50:23.704 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:50:23.705 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:50:23.709 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:50:23.710 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:50:23.730 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7169, 7173) -->  Mary dropped the football
2025-01-22 00:50:23.730 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 00:50:23.748 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:50:23.748 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 00:50:23.767 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (6170, 6176) --> . Daniel went back to the
2025-01-22 00:50:23.767 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 00:50:23.769 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (494, 500) --> . Sandra journeyed to the
2025-01-22 00:50:23.769 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:50:23.794 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8417, 8423) -->  affair. John went back to
2025-01-22 00:50:23.794 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:50:23.799 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1497, 1503) -->  Mary journeyed to the office
2025-01-22 00:50:23.799 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John took the milk.
2025-01-22 00:50:23.809 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (3724, 3728) -->  John took the milk
2025-01-22 00:50:23.810 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John moved to the bedroom.
2025-01-22 00:50:23.842 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (11182, 11187) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:50:25.986 | INFO     | test_jbb_retain:begin_test:544 - Mary dropped the football.<|eot_id|>
2025-01-22 00:50:25.987 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:36,  5.28s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:13,  2.30s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.33s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.14it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:01,  1.62it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:00,  2.17it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.77it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  3.36it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.23it/s]
2025-01-22 00:50:35.039 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.7720614346590909, 0.6470761165675419, 0.6948642008232347, 0.6467183324132004], 'topk_tokens': [' Minnesota', ' the', ' of', ' Wood', ' THE', ' Ramsey', 'Minnesota', ' from', 'ire', ' Phil', 'con', ' Min', ' Minnesota', 'Minnesota', 'active', ' Minnesota', ' marched', 'irie', ' Minnesota', ' Minnesota'], 'evidence_proportions': [0.923828125, 0.923828125, 0.555084228515625, 0.6131795247395834]}, 'weight': {'score': [0.05161162398078225, 0.002578931927170689, 0.008280082182450728, 0.0024741723816812575], 'topk_tokens': [' bedroom', ' \n', ' the', ' football', 'Answer', ' the', ' bathroom', '<|eot_id|>', ' bathroom', '.', '<|start_header_id|>', 'assistant', '<|eot_id|>', ' hallway', ':', 'Bridge', '<|end_header_id|>', ' football', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005654066801071167, 0.005654066801071167, 0.259735107421875, 0.004777749379475911]}, 'saliency': {'score': [0.007829170335422863, 0.0001148395056311614, 0.0012283704497597435, 9.776667015512893e-05], 'topk_tokens': ['.', '.', ' bathroom', ' Mary', ' Broadway', ' Third', '<|eot_id|>', ' Pills', ' bedroom', ' office', ' bathroom', '<|begin_of_text|>', ' office', '<|eot_id|>', ' Market', 'Bridge', ' Mary', ' hallway', ' football', 'office'], 'evidence_proportions': [0.0020185609658559165, 0.0020185609658559165, 0.03678321838378906, 0.0001476903756459554]}}, 25: {'grad': {'score': [0.7351226806640625, 0.5885675786394879, 0.7314841530539773, 0.5879109624813619], 'topk_tokens': ['186', ' Key', ' private', ' moved', ' opinion', 'ervative', ',', 'ing', ' news', 'irie', ' Mary', 'ush', 'ised', 'ush', ' Wood', 'ation', 'op', 'op', 'ation', 'erc'], 'evidence_proportions': [0.7111485799153646, 0.7111485799153646, 1.3603515625, 0.3662516276041667]}, 'weight': {'score': [0.023209241303530605, 0.002566582205894553, 0.005229897571332527, 0.0025217605812185052], 'topk_tokens': [' Market', '.', '.', '.', ' the', ' Miss', ' Mary', ' football', ' \n', 'Answer', ' Mary', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', '.', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.014178723096847534, 0.014178723096847534, 0.08071136474609375, 0.0029355287551879883]}, 'saliency': {'score': [0.002406502311879938, 6.783458014479741e-05, 0.0003456527536565607, 6.28218711541366e-05], 'topk_tokens': [' football', ' Miss', 'Minnesota', ' Mary', ' random', 'Mer', ' Min', ' Minnesota', 'office', 'Answer', ' football', ' Mary', ' Market', 'assistant', '<|begin_of_text|>', ':', '<|eot_id|>', '<|eot_id|>', '.', '<|end_header_id|>'], 'evidence_proportions': [0.001107354958852132, 0.001107354958852132, 0.009454727172851562, 0.0003059804439544678]}}, 26: {'grad': {'score': [0.5879551280628551, 0.6939860856725387, 0.6950600363991477, 0.6941760501721198], 'topk_tokens': [' steam', ' black', ' by', 'ols', ' esp', 'ol', ' gang', ' generally', ' Jul', ' some', ' Merch', 'cery', ' Gen', ' generally', ' generally', 'BO', 'str', ' worth', ' STR', ' str'], 'evidence_proportions': [0.6438395182291666, 0.6438395182291666, 0.3296394348144531, 0.6483968098958334]}, 'weight': {'score': [0.008598013357682661, 0.0025248485134352107, 0.0029832359516259394, 0.0025125491326336113], 'topk_tokens': [' Bridge', ' the', ' football', ' before', '?', ' the', ' hallway', ' football', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'Bridge', ' \n', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002177069584528605, 0.002177069584528605, 0.03670501708984375, 0.0027018984158833823]}, 'saliency': {'score': [0.0002350129864432595, 0.00011719507648240668, 0.000159122727134011, 0.00011686632377500225], 'topk_tokens': [' Jackson', ' the', ' Ramsey', 'assistant', ' football', ' Merch', ':', ' bedroom', ' hallway', '.', ' Bridge', 'Answer', ' Seventh', ' Bridge', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>', 'office'], 'evidence_proportions': [4.169344902038574e-05, 4.169344902038574e-05, 0.0009507536888122559, 0.00014449159304300943]}}, 27: {'grad': {'score': [0.4803244850852273, 0.5816589295177189, 0.45623293789950287, 0.5821855495045639], 'topk_tokens': ['urnished', ' const', ' too', 'tele', ' B', '      ', ' print', 'use', ' plainly', 'ly', ' product', 'APER', ' Newspaper', ' Temper', 'na', ' N', ' remember', 'bread', 'remember', 'ile'], 'evidence_proportions': [0.3943990071614583, 0.3943990071614583, 0.7705535888671875, 0.4586893717447917]}, 'weight': {'score': [0.02261210029775446, 0.0025615139466035435, 0.004670155770850904, 0.002519283062627236], 'topk_tokens': ['?', ' THE', '<|eot_id|>', ' Bridge', ' before', ' \n', '<|eot_id|>', ' bathroom', 'Bridge', ' football', 'Answer', ' football', '.', 'assistant', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00982914368311564, 0.00982914368311564, 0.08862495422363281, 0.0041694442431132]}, 'saliency': {'score': [0.005326070568778298, 0.00014246092998239607, 0.0012712912126020951, 0.00012995031959787943], 'topk_tokens': ['<|start_header_id|>', ' upper', 'Christmas', '.', ' football', ' Jackson', ' ST', ' Minnesota', ' Bridge', ' the', ' bathroom', ' football', ' Mary', 'Minnesota', ':', ' Mary', '.\n\n', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0036452611287434897, 0.0036452611287434897, 0.01736605167388916, 0.0006610353787740072]}}, 28: {'grad': {'score': [0.71661376953125, 0.7033187368805565, 0.7506917317708334, 0.7031652762380612], 'topk_tokens': [' being', ' executed', ' returns', 'hom', ' office', ' reached', ' news', ' escaping', ' acted', ' kept', ' been', ' been', ' been', 'hum', 'being', ' out', 'been', ' ins', ' be', 'na'], 'evidence_proportions': [0.5699615478515625, 0.5699615478515625, 0.7077484130859375, 1.0158284505208333]}, 'weight': {'score': [0.007165762511166659, 0.0024763448951979944, 0.004415405519080885, 0.0024625223376738524], 'topk_tokens': [' to', 'Question', ' Bridge', '.\n\n', ' Bridge', ' the', ' football', ' bathroom', '?', '<|eot_id|>', ' before', ' \n', '<|eot_id|>', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0014887948830922444, 0.0014887948830922444, 0.0243072509765625, 0.007092038790384929]}, 'saliency': {'score': [0.0007802275094118985, 6.512653329162798e-05, 0.00019782781600952148, 6.346347500961885e-05], 'topk_tokens': [' Nearly', 'Answer', 'Question', ' nearly', 'Bridge', ' Thanksgiving', ' during', ' football', ' kitchen', ' football', ' before', ':', ' Far', ' Bridge', ' Bridge', '<|start_header_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.803970654805501e-05, 9.803970654805501e-05, 0.0034122467041015625, 0.0003899236520131429]}}, 29: {'grad': {'score': [0.6099267439408735, 0.9568318055183981, 0.9022861827503551, 0.9576117539932224], 'topk_tokens': ['APER', ' The', ' The', ' paper', 'The', '\n', 'The', '\n', ' The', ' were', 'nes', ' printed', '\n', 'paper', '\n', ' The', '\n', ' The', '\n', ' the'], 'evidence_proportions': [0.7225748697916666, 0.7225748697916666, 0.2930908203125, 0.5958544413248698]}, 'weight': {'score': [0.0032517693259499288, 0.0025392748393390694, 0.0016953240741382945, 0.002540281661181969], 'topk_tokens': [' Does', '?', '.\n\n', ' Where', ' bathroom', ' football', 'aha', ' in', '<|eot_id|>', '<|eot_id|>', ' the', ' \n', ' before', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0015945136547088623, 0.0015945136547088623, 0.010114669799804688, 0.001991013685862223]}, 'saliency': {'score': [0.000279832970012318, 8.565015725332693e-05, 0.00013446807861328125, 8.516367637301796e-05], 'topk_tokens': ['ot', ' bedroom', '<|eot_id|>', '.\n\n', ' the', ':', ' football', ' the', ' Do', 'aha', 'assistant', 'Does', ' Does', 'Answer', ' \n', '<|end_header_id|>', '<|start_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00020523866017659506, 0.00020523866017659506, 0.0006875991821289062, 0.00015717744827270508]}}, 30: {'grad': {'score': [0.8011696555397727, 0.929293193195382, 0.9841303969874526, 0.9293766373451616], 'topk_tokens': ['Country', ' office', ' head', ' anx', ' o', 'Emp', 'ire', ' Paul', ' an', ' Bridge', ' an', 'ob', 'office', 'op', ' o', ' o', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.6553548177083334, 0.6553548177083334, 0.971923828125, 0.9789632161458334]}, 'weight': {'score': [0.009050884030082008, 0.0024664605335063513, 0.005599567384430857, 0.002445932141626324], 'topk_tokens': [' football', ':', ' the', ' Where', ' the', ' before', '<|eot_id|>', '?', 'Question', '.\n\n', '<|eot_id|>', ' bathroom', 'Answer', 'assistant', '<|end_header_id|>', ' \n', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0031094650427500405, 0.0031094650427500405, 0.0321502685546875, 0.005534132321675618]}, 'saliency': {'score': [0.0022774149071086536, 0.00016202448149986632, 0.0008327635851773349, 0.00015634574340470797], 'topk_tokens': [' office', ' Wide', ':', ' kitchen', ' football', ' the', ' bathroom', ' Mary', ' football', '.\n\n', 'IR', ' \n', '?', '.', 'Question', 'assistant', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0011109511057535808, 0.0011109511057535808, 0.008395671844482422, 0.0005315045515696207]}}, 31: {'grad': {'score': [1.0486034046519885, 0.8965820473277494, 0.6550237482244318, 0.8969646615681592], 'topk_tokens': ['IO', 'user', ' soon', 'Mexico', ' an', ' DAY', ' the', ' Mary', ' M', ' B', ' may', ' an', ' membership', 'Mr', ' Mr', ' very', ' marriage', ' United', 'editary', ' most'], 'evidence_proportions': [1.1951548258463542, 1.1951548258463542, 1.38726806640625, 0.52972412109375]}, 'weight': {'score': [0.001950925046747381, 0.0023160902418350757, 0.0016705140923008773, 0.0023185162442518918], 'topk_tokens': [' was', ' the', 'Question', ':', ' football', '.\n\n', ' before', '?', ' the', ' Where', '<|eot_id|>', 'Answer', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00102005402247111, 0.00102005402247111, 0.005611896514892578, 0.0013720194498697917]}, 'saliency': {'score': [0.0008354214104739102, 0.00011365195619145388, 0.00027159669182517314, 0.00011190788075358679], 'topk_tokens': [' the', ' hallway', ':', ' Market', ' Mary', 'Question', ' was', ' before', 'ot', ' Where', ' the', ' football', ' the', '<|eot_id|>', ' \n', '<|begin_of_text|>', ':', 'assistant', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0006922781467437744, 0.0006922781467437744, 0.0021779537200927734, 0.0002266863981882731]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 00:50:35.041 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:50:35.041 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-3_1-4-6-8.pkl | len: 10 |  size: 8.82 KB
Processing depth (1, 4, 6, 8):   4%|‚ñç         | 4/100 [01:14<29:38, 18.53s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (1, 2, 5, 8):   4%|‚ñç         | 4/100 [01:21<29:38, 18.53s/it]2025-01-22 00:50:42.117 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:50:42.121 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:50:42.122 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 00:50:42.126 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1495, 1501) -->  tragedy. Mary journeyed to
2025-01-22 00:50:42.126 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary dropped the football.
2025-01-22 00:50:42.143 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5932, 5936) -->  Mary dropped the football
2025-01-22 00:50:42.143 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 00:50:42.162 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (6175, 6181) --> . Daniel went back to the
2025-01-22 00:50:42.162 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 00:50:42.180 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (6175, 6181) --> . Daniel went back to the
2025-01-22 00:50:42.180 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 00:50:42.182 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (494, 500) --> . Sandra journeyed to the
2025-01-22 00:50:42.182 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:50:42.207 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8417, 8423) -->  affair. John went back to
2025-01-22 00:50:42.207 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John journeyed to the office.
2025-01-22 00:50:42.212 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1497, 1503) -->  Mary journeyed to the office
2025-01-22 00:50:42.212 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John took the milk.
2025-01-22 00:50:42.222 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (3731, 3735) -->  John took the milk
2025-01-22 00:50:42.222 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John moved to the bedroom.
2025-01-22 00:50:42.254 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (11182, 11187) --> . John moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:50:44.347 | INFO     | test_jbb_retain:begin_test:544 - The hallway.<|eot_id|>
2025-01-22 00:50:44.348 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:36,  5.25s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:13,  2.26s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.31s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.16it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:01,  1.64it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:00,  2.19it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.78it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  3.38it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.25it/s]
2025-01-22 00:50:53.257 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.6378118341619318, 0.6673657540078614, 0.6646299651174834, 0.6674269850729397], 'topk_tokens': [' Minnesota', 'enter', ' whistle', ' the', ' emb', ' res', 'ent', ' cont', ' rep', 'super', 'sur', 'active', ' conn', ' cons', 'ire', 'cont', 'con', 'cont', ' Minnesota', ' cont'], 'evidence_proportions': [0.7470703125, 0.7470703125, 0.334625244140625, 0.6214192708333334]}, 'weight': {'score': [0.032387901436198845, 0.0025807135113768568, 0.008445752389503248, 0.0025104824341152794], 'topk_tokens': [' the', ' hand', ' the', ' football', 'Bridge', 'Answer', '<|start_header_id|>', '<|eot_id|>', ' bedroom', ' bathroom', '<|eot_id|>', 'assistant', '.', ':', ' hallway', ' bathroom', ' football', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0024790167808532715, 0.0024790167808532715, 0.160430908203125, 0.006843666235605876]}, 'saliency': {'score': [0.007109967145052823, 0.00010704776351295649, 0.0013906486106641364, 9.080505662881973e-05], 'topk_tokens': [' THE', 'assistant', ' top', ' dropped', ':', ' Mary', '<|eot_id|>', ' random', ' kitchen', '<|begin_of_text|>', ' folded', '<|eot_id|>', ' bathroom', ' bedroom', ' office', ' bedroom', ' office', ' hallway', ' football', 'office'], 'evidence_proportions': [0.0001843571662902832, 0.0001843571662902832, 0.038008689880371094, 0.00036203861236572266]}}, 25: {'grad': {'score': [0.6871462735262784, 0.9919701288277906, 0.6459782918294271, 0.9934688351403334], 'topk_tokens': [' extraordinary', ' corner', ' down', 'ting', ' neither', '\n', ' for', ' safe', 'ing', ' hate', 'le', '\n', ' private', 'ing', ' kinds', 'ised', 'ing', 'ation', 'erc', 'ation'], 'evidence_proportions': [0.738525390625, 0.738525390625, 0.8325653076171875, 0.4874420166015625]}, 'weight': {'score': [0.014471270821311256, 0.002564034416442409, 0.0034036925344756155, 0.0025400810617080822], 'topk_tokens': [' football', ' football', ' dropped', ' the', '.\n\n', ' \n', '.', ' East', '<|start_header_id|>', ' THE', ' Mary', '.', 'Answer', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0041304826736450195, 0.0041304826736450195, 0.06255340576171875, 0.003098090489705404]}, 'saliency': {'score': [0.0016270577907562256, 6.603397819059319e-05, 0.00026636954509850705, 6.264742418543276e-05], 'topk_tokens': [' Mary', ' Rotary', ' Min', ' directly', ' football', ' Min', ' random', ' Minneapolis', '.', ' East', ' Mary', ' THE', 'Answer', 'office', ':', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0007086495558420817, 0.0007086495558420817, 0.006407737731933594, 0.00027675429979960126]}}, 26: {'grad': {'score': [0.7293229536576704, 0.8840803109308116, 0.9327651515151515, 0.8842289971228836], 'topk_tokens': [' wh', ' Col', ' black', ' gang', ' Merch', ' Gen', ' Col', 'oth', 'ol', ' generally', ' gold', ' worth', ' went', 'str', ' generally', ' Guards', ' some', 'BO', ' STR', ' str'], 'evidence_proportions': [0.7331136067708334, 0.7331136067708334, 0.2709808349609375, 1.0273030598958333]}, 'weight': {'score': [0.006472240794788708, 0.0025372687318743472, 0.0037625612634601016, 0.0025267664546125886], 'topk_tokens': ['.\n\n', ' before', ' the', ' kitchen', 'Bridge', '?', ' bedroom', ' football', ' bathroom', ' hallway', '<|eot_id|>', '<|eot_id|>', ' \n', '<|start_header_id|>', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009209116299947103, 0.0009209116299947103, 0.028047561645507812, 0.003191351890563965]}, 'saliency': {'score': [0.0004115619442679665, 9.701764815176323e-05, 0.00027827060583866006, 9.595080600190594e-05], 'topk_tokens': [' the', '.', ' \n', ' North', ' bedroom', 'assistant', ' bathroom', ' East', ' kitchen', ':', ' Bridge', 'Answer', ' hallway', ' bathroom', ' bedroom', '<|start_header_id|>', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [6.068746248881022e-05, 6.068746248881022e-05, 0.0016963183879852295, 0.0002568066120147705]}}, 27: {'grad': {'score': [0.5821117054332386, 0.5192513363825321, 0.6002770626183712, 0.5189158714784637], 'topk_tokens': [' plainly', ' party', ' Temper', 'papers', ' N', ' B', ' Newspaper', ' earnest', ' very', ' Press', '\n', ' per', 'remember', ' product', 'sur', ' N', 'bread', 'ab', 'sur', 'ile'], 'evidence_proportions': [0.5049031575520834, 0.5049031575520834, 0.7586822509765625, 0.6188151041666666]}, 'weight': {'score': [0.014739936048334295, 0.0025621143500585875, 0.003760028969157826, 0.002536691108531064], 'topk_tokens': [' football', ' before', ' hallway', ' Mary', '<|eot_id|>', ' \n', '<|eot_id|>', ' bedroom', '.', ' bathroom', ' football', '<|start_header_id|>', 'Answer', ' THE', 'assistant', '.\n\n', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003407349189122518, 0.003407349189122518, 0.06357574462890625, 0.004847904046376546]}, 'saliency': {'score': [0.0038509585640647197, 0.00013883669477644271, 0.000916058366948908, 0.0001299625530162481], 'topk_tokens': ['.', ' the', ' ST', ' Mary', ' kitchen', '.', ' Bridge', ' THE', ' upper', ' East', ' football', ' Mary', ' hallway', '.\n\n', '<|begin_of_text|>', ' bathroom', ' Mary', ':', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.001375118891398112, 0.001375118891398112, 0.015780866146087646, 0.0008493661880493164]}}, 28: {'grad': {'score': [0.7102827592329546, 0.6269998308620761, 0.6668053829308712, 0.6267396962683086], 'topk_tokens': [' endeavor', ' ', ' about', 'half', ' came', ' escaping', ' balance', ' have', 'hom', ' next', 'na', ' executed', '600', ' ', ' out', ' be', 'been', ' ', ' ins', ' reached'], 'evidence_proportions': [0.6132609049479166, 0.6132609049479166, 0.6982421875, 0.912353515625]}, 'weight': {'score': [0.007103765552694147, 0.0024807257678974293, 0.006368788805874911, 0.0024617054292288784], 'topk_tokens': ['.', ' bedroom', 'Question', ' bedroom', '.\n\n', ' the', ' football', '?', ' bathroom', ' before', '<|eot_id|>', ' \n', '<|eot_id|>', '<|start_header_id|>', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012004772822062175, 0.0012004772822062175, 0.02028656005859375, 0.010121812423070272]}, 'saliency': {'score': [0.0007364831187508323, 5.8819156828231235e-05, 0.00030462308363481003, 5.691556493648276e-05], 'topk_tokens': [' hallway', ' football', ' the', ' bedroom', ' kitchen', ' Bridge', '<|eot_id|>', ' before', '<|start_header_id|>', ' during', 'Answer', ' bedroom', '<|eot_id|>', ' Bridge', 'assistant', ' Far', '<|end_header_id|>', 'office', '<|begin_of_text|>', ':'], 'evidence_proportions': [9.321173032124837e-05, 9.321173032124837e-05, 0.0032968521118164062, 0.0003161132335662842]}}, 29: {'grad': {'score': [0.5648554021661932, 0.8487164971857507, 0.8304221413352273, 0.8492828295956985], 'topk_tokens': ['\n', '.', '\n', ' the', ' Printing', '\n', '.', 'paper', 'nes', ' The', '\n', ' The', ' The', ' The', ' were', ' The', ' Press', '\n', ' The', '\n'], 'evidence_proportions': [0.6249491373697916, 0.6249491373697916, 0.354644775390625, 0.584808349609375]}, 'weight': {'score': [0.004366189241409302, 0.0025443129528499096, 0.002994208624868682, 0.0025397708346389675], 'topk_tokens': [' Do', 'Question', ' Where', ' Does', '?', ' football', ' bathroom', '.\n\n', '<|eot_id|>', '<|eot_id|>', ' before', ' \n', ' the', 'Answer', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0019175807634989421, 0.0019175807634989421, 0.013475418090820312, 0.003190586964289347]}, 'saliency': {'score': [0.00030715357173572886, 4.265384529623736e-05, 0.0002758755828395034, 4.1536230864698733e-05], 'topk_tokens': [' football', ' bedroom', ' the', ':', 'assistant', 'Does', ' before', ' the', '<|eot_id|>', ' the', ' the', ' Does', '<|begin_of_text|>', ' Do', 'Answer', '<|end_header_id|>', ' \n', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.00014441212018330893, 0.00014441212018330893, 0.0008461326360702515, 0.0002733170986175537]}}, 30: {'grad': {'score': [0.8099587180397727, 0.9364142695814126, 0.9665961988044508, 0.9365619598542609], 'topk_tokens': [' o', ' Was', 'op', ' office', ' States', ' Third', ' Paul', 'Emp', 'ire', ' office', ' Bridge', ' o', 'ob', 'op', ' o', 'office', ' o', ' o', ' o', ' O'], 'evidence_proportions': [0.6705067952473959, 0.6705067952473959, 0.8023529052734375, 1.09393310546875]}, 'weight': {'score': [0.010543021288785067, 0.0024666299465105337, 0.007629956259871974, 0.002437847148373515], 'topk_tokens': ['.', ' the', ' before', ' the', ' the', ' the', 'Question', '?', '<|eot_id|>', '.\n\n', ' bathroom', '<|eot_id|>', 'Answer', 'assistant', '<|start_header_id|>', ' \n', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0026045143604278564, 0.0026045143604278564, 0.037872314453125, 0.008200506369272867]}, 'saliency': {'score': [0.002739239822734486, 0.00011251559177432596, 0.000952218518112645, 0.00010544554053391513], 'topk_tokens': [':', ' football', ' kitchen', ' bedroom', ' East', '.\n\n', 'Question', '.', ' Mary', '?', ' football', ' office', ' \n', ' bathroom', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.0006527801354726156, 0.0006527801354726156, 0.011595353484153748, 0.001008083422978719]}}, 31: {'grad': {'score': [1.0039117986505681, 0.868910070073263, 0.7051618171460701, 0.8691113149519447], 'topk_tokens': [' United', ' the', ' an', ' C', ' DAY', ' an', ' the', ' W', 'IR', ' an', ' were', ' the', ' an', 'editary', ' the', ' C', ' H', 'Mr', ' most', ' an'], 'evidence_proportions': [1.143218994140625, 1.143218994140625, 1.24273681640625, 0.5660807291666666]}, 'weight': {'score': [0.002034382386641069, 0.002339912159567724, 0.001490005941101999, 0.002342787257575973], 'topk_tokens': [' bathroom', ' was', 'Question', ':', ' before', '.\n\n', ' football', '?', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007831255594889323, 0.0007831255594889323, 0.007057905197143555, 0.0011878808339436848]}, 'saliency': {'score': [0.0005340061404488304, 0.00010011334108372131, 0.00019417387066465435, 9.906731122659708e-05], 'topk_tokens': ['?', ' hallway', ' the', ' Mary', ' the', 'ot', ' before', 'Question', '<|end_header_id|>', ' Where', 'Answer', ' football', '<|eot_id|>', ' the', ' \n', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0002644658088684082, 0.0002644658088684082, 0.0018969178199768066, 0.00016447901725769043]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 00:50:53.259 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:50:53.259 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-4_1-2-5-8.pkl | len: 10 |  size: 8.75 KB
Processing depth (1, 2, 5, 8):   5%|‚ñå         | 5/100 [01:32<29:09, 18.42s/it]Processing depth (1, 2, 5, 8):   5%|‚ñå         | 5/100 [01:32<29:22, 18.55s/it]
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:70 - Selected idx: 3
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:72 - Answer: office
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:73 - Tag: 4-hop
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:74 - Needle: [' Sandra journeyed to the bedroom.', ' Mary journeyed to the office.', ' John went back to the bedroom.', ' Mary picked up the apple.', ' John journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:75 - Real Needle: [' Mary journeyed to the office.', ' Mary picked up the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 00:50:53.468 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.30it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.58it/s]
Processing depth (1, 2, 5, 6, 8):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 00:51:00.567 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:51:00.572 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 00:51:00.572 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary picked up the apple.
2025-01-22 00:51:00.579 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2467, 2472) --> . Mary picked up the
2025-01-22 00:51:00.579 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:51:00.584 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 00:51:00.584 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:51:00.605 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (7162, 7166) -->  Mary dropped the apple
2025-01-22 00:51:00.605 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 00:51:00.633 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9595, 9601) -->  Daniel went back to the hallway
2025-01-22 00:51:00.634 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:51:00.664 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (10000, 10006) --> . Sandra journeyed to the
2025-01-22 00:51:00.664 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John went back to the bedroom.
2025-01-22 00:51:00.689 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8462, 8468) --> . John went back to the
2025-01-22 00:51:00.689 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John journeyed to the office.
2025-01-22 00:51:00.694 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1496, 1502) -->  Mary journeyed to the office
2025-01-22 00:51:00.694 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:51:00.723 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9596, 9602) -->  went back to the hallway.
2025-01-22 00:51:00.723 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John moved to the bedroom.
2025-01-22 00:51:00.727 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1255, 1260) --> . John moved to the
2025-01-22 00:51:00.727 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John took the milk.
2025-01-22 00:51:00.754 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (9532, 9536) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:51:02.687 | INFO     | test_jbb_retain:begin_test:544 - Mary<|eot_id|>
2025-01-22 00:51:02.688 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12161])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:38,  5.56s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.42s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:07,  1.41s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.06it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.49it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.89it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.23it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.67it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:51:12.437 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5191606592248987, 0.5954903976840623, 0.5568969032981179, 0.5957659069175691], 'topk_tokens': [' sure', ' up', 'ly', 'office', ' office', ' election', ' office', ' or', 'ent', ' rept', ' or', ' or', 'enter', ' election', 'Minnesota', 'editor', ' Minnesota', ' Minnesota', ' effect', 'active'], 'evidence_proportions': [0.6245524088541666, 0.47410888671875, 0.6245524088541666, 0.5906753540039062, 0.29824384053548175]}, 'weight': {'score': [0.03488252118781761, 0.002573327721022838, 0.013361567800695246, 0.002471835516921983], 'topk_tokens': ['\n\n', ' Bridge', ' apple', ' bedroom', '?\n', ' hallway', ' Mary', ' Mary', 'Answer', '<|eot_id|>', ' bathroom', '<|eot_id|>', 'assistant', '<|start_header_id|>', ' Mary', ':', '<|end_header_id|>', 'Bridge', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005428840716679891, 0.07179239988327027, 0.005428840716679891, 0.095123291015625, 0.02287113666534424]}, 'saliency': {'score': [0.013005417806130869, 0.00017988580843144455, 0.0026697895743630147, 0.00014448498316385584], 'topk_tokens': [' office', ' apple', '.', ' Bridge', ' front', ' Bench', ' bedroom', '<|start_header_id|>', ' Market', ' office', '<|eot_id|>', ' bathroom', '<|eot_id|>', ' hallway', ' Mary', '<|begin_of_text|>', ' Mary', 'Bridge', ' Mary', 'office'], 'evidence_proportions': [0.0022624234358469644, 0.03762305974960327, 0.0022624234358469644, 0.022234439849853516, 0.007824023564656576]}}, 25: {'grad': {'score': [0.8807553891782407, 0.6544297944457577, 0.7990241773200758, 0.6535306450040795], 'topk_tokens': [' the', ' regular', ' Tribune', 'ian', 'graph', 'graph', ' location', 'graph', 'THE', ' Aw', ' subscribers', ' the', 'ation', ' the', 'graph', 'op', ' location', 'op', 'ation', ' Wood'], 'evidence_proportions': [0.7142333984375, 0.931884765625, 0.7142333984375, 0.916107177734375, 1.1476236979166667]}, 'weight': {'score': [0.020919905768500432, 0.0025474983032798626, 0.007555916453852798, 0.002492856247547411], 'topk_tokens': [' ', '.', ' THE', '.', '?\n', ' Paul', ' Mary', ' apple', ' Daniel', ' Mary', 'Answer', ' Mary', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00543945034344991, 0.030016303062438965, 0.00543945034344991, 0.07878875732421875, 0.005721251169840495]}, 'saliency': {'score': [0.005048272786317048, 8.058138292772563e-05, 0.000787283435012355, 6.75723078545781e-05], 'topk_tokens': [' THE', ' counting', ' Bench', ' Dan', ':', '<|start_header_id|>', 'Answer', ' Paul', ' apple', ' Mary', '<|begin_of_text|>', ' Paul', '.', ' Daniel', ' Mary', ' Mary', '<|eot_id|>', ' Mary', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.002743363380432129, 0.008769237995147705, 0.002743363380432129, 0.014749526977539062, 8.978446324666341e-05]}}, 26: {'grad': {'score': [0.7346372251157407, 0.6648146931909275, 0.8202405409379439, 0.66423514586071], 'topk_tokens': [' hand', ' Merch', ' worth', 'British', 'leading', ' a', 'ers', ' Press', 'ers', ' some', ' clear', ' STR', 'BO', ' by', ' str', 'str', 'istributed', ' part', ' Jul', ' wh'], 'evidence_proportions': [0.74072265625, 0.704150390625, 0.74072265625, 0.66522216796875, 0.7941487630208334]}, 'weight': {'score': [0.012714357287795455, 0.002490003260109345, 0.006179253260294597, 0.0024571351440427795], 'topk_tokens': [' apple', ' Mary', ' hallway', ' Bridge', ' bathroom', ' discarded', ' Mary', ' Mary', '<|eot_id|>', '<|eot_id|>', '?\n', 'assistant', 'Answer', ' the', 'Bridge', '<|start_header_id|>', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0022375782330830893, 0.018741905689239502, 0.0022375782330830893, 0.03647422790527344, 0.012805044651031494]}, 'saliency': {'score': [0.0013160065368369774, 0.00010403562197802462, 0.0007260766896334561, 9.96358392849881e-05], 'topk_tokens': ['?\n', ' Az', ' kitchen', '<|end_header_id|>', ' Mary', ' Bench', ' bathroom', ' Mary', ' Mary', ':', ' hallway', ' Seventh', ' apple', ' Bridge', 'assistant', '<|start_header_id|>', ' the', 'Bridge', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0001739164193471273, 0.0022316932678222655, 0.0001739164193471273, 0.0025255680084228516, 0.002030740181605021]}}, 27: {'grad': {'score': [0.6316799587673612, 0.5960665157650251, 0.5048587683475378, 0.5962357541938208], 'topk_tokens': [' EX', 'tele', ' remin', ' paper', 'na', ' earnest', ' material', ' tell', ' newspaper', ' producing', ' news', 'ly', 'APER', 'bread', ' told', ' newspaper', ' N', ' talk', ' Newspaper', 'ile'], 'evidence_proportions': [0.7229817708333334, 0.502587890625, 0.7229817708333334, 0.679443359375, 0.524810791015625]}, 'weight': {'score': [0.01901173150097882, 0.0025499717563741116, 0.007963653766747677, 0.002498487081545833], 'topk_tokens': [' bathroom', 'THE', ' Bridge', ' to', ' Mary', '<|eot_id|>', '?\n', '<|eot_id|>', ' Mary', 'Bridge', ' Mary', 'Answer', ' THE', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.006618529558181763, 0.04081859588623047, 0.006618529558181763, 0.04447746276855469, 0.00864859422047933]}, 'saliency': {'score': [0.0027385552724202475, 0.00013294736874257422, 0.001360774040222168, 0.00012378685535266066], 'topk_tokens': [' Paul', 'Answer', '.', ' Mary', ' prior', ' Paul', ' hallway', 'Bridge', ' the', ' upper', ' the', ' Daniel', ' Mary', '<|end_header_id|>', ' Mary', 'THE', '<|start_header_id|>', '<|begin_of_text|>', ':', 'office'], 'evidence_proportions': [0.0006267428398132324, 0.004806828498840332, 0.0006267428398132324, 0.006998181343078613, 0.0023988684018452964]}}, 28: {'grad': {'score': [0.8746654369212963, 0.8987996864209077, 0.8244591915246212, 0.8990562229050907], 'topk_tokens': [' spoken', ' acted', ' been', ' hands', ' kept', 'PA', ' become', ' balance', 'been', ' been', 'na', 'hum', ' acted', ' be', 'being', ' being', ' taken', ' ins', 'hom', 'CH'], 'evidence_proportions': [0.7819417317708334, 1.02080078125, 0.7819417317708334, 1.1060791015625, 0.7840576171875]}, 'weight': {'score': [0.008728755844963921, 0.0024647885596762948, 0.00784537647709702, 0.0024361438833169088], 'topk_tokens': [' to', ' ', ' Bridge', 'Question', ' the', ' discarded', ' to', ' was', ' Mary', '<|eot_id|>', '<|eot_id|>', ' the', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008353690306345621, 0.005601644515991211, 0.0008353690306345621, 0.033199310302734375, 0.01080775260925293]}, 'saliency': {'score': [0.0008066980927078812, 9.911016032833572e-05, 0.00042782588438554245, 9.663536126462131e-05], 'topk_tokens': [' ', ' during', '.\n\n', ' apple', 'Question', ' Bridge', ' to', ' Mary', 'Bridge', 'Answer', '?\n', ' the', ' Far', ' Mary', 'assistant', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', ':'], 'evidence_proportions': [5.9932470321655273e-05, 0.0008389592170715332, 5.9932470321655273e-05, 0.0034196972846984863, 0.0005313456058502197]}}, 29: {'grad': {'score': [0.7309027777777778, 1.0603037580674999, 0.9758485736268939, 1.0612688780835589], 'topk_tokens': [' press', ' mail', ' were', ' express', ' THE', ' newspaper', 'ION', '\n', ' regular', ' printed', ' the', '\n', ' regular', ' The', 'aper', 'ATING', ' Press', 'paper', '\n', 'APER'], 'evidence_proportions': [1.0348307291666667, 0.636083984375, 1.0348307291666667, 0.31385040283203125, 0.4800974527994792]}, 'weight': {'score': [0.002988115504935936, 0.0025347747526738846, 0.0031525582978219695, 0.0025320789700331377], 'topk_tokens': [' the', ' discarded', 'Question', '.\n\n', 'Does', ' Where', ' in', '<|eot_id|>', '<|eot_id|>', ' ', ' Does', ' the', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00020150343577067056, 0.0030913233757019045, 0.00020150343577067056, 0.010408401489257812, 0.0035284757614135742]}, 'saliency': {'score': [0.00030504774164270474, 9.904423895355002e-05, 0.0003150538964705034, 9.799570443560681e-05], 'topk_tokens': [' the', ' the', '<|eot_id|>', ' discarded', ' to', ' from', ' was', ' the', ' Where', ' on', 'Answer', ' the', 'Does', '?\n', ' Does', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.276897430419922e-05, 0.00026203393936157225, 2.276897430419922e-05, 0.0010622739791870117, 0.0004006326198577881]}}, 30: {'grad': {'score': [0.6732697663483797, 1.1485635546842885, 1.081019314852628, 1.1498080306158356], 'topk_tokens': [' an', ' o', ' an', ' its', ' M', 'G', ' head', ' office', ' Paul', 'op', ' o', ' office', 'ob', ' o', ' o', ' o', 'office', ' o', 'op', ' O'], 'evidence_proportions': [0.6359914143880209, 0.71767578125, 0.6359914143880209, 0.641357421875, 0.7320963541666666]}, 'weight': {'score': [0.007705322018376103, 0.002471431270198858, 0.011646095550421513, 0.0024347396258587663], 'topk_tokens': [',', ' was', ' discarded', ':', ' ', ' Where', ' the', '<|eot_id|>', '<|eot_id|>', 'Question', '.\n\n', ' the', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0026310384273529053, 0.005211353302001953, 0.0026310384273529053, 0.021785736083984375, 0.01054525375366211]}, 'saliency': {'score': [0.0018135529977303963, 0.00018177039919446038, 0.001806445194013191, 0.00017370030100479763], 'topk_tokens': [' Sandra', ',', ' Nearly', ':', ' Geo', ' Daniel', ' nearly', '.\n\n', ' ', ' Mary', 'Christmas', ' Mary', ' the', 'assistant', ':', 'Question', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.001563022534052531, 0.001475226879119873, 0.001563022534052531, 0.004518866539001465, 0.0007930099964141846]}}, 31: {'grad': {'score': [1.1476598668981481, 1.6049815526597058, 0.6862085515802557, 1.608506893034091], 'topk_tokens': [' miles', 'UX', ' their', ' the', ' were', ' B', ' S', '7', ' United', ' L', ' was', ' M', 'G', ' was', ' its', ' S', ' and', ' W', ' most', 'editary'], 'evidence_proportions': [1.228271484375, 1.014501953125, 1.228271484375, 1.6357421875, 0.7720133463541666]}, 'weight': {'score': [0.0027653000972889087, 0.0023030584740381947, 0.003129381122011127, 0.0022997742328408986], 'topk_tokens': [' to', ' where', ' was', ' the', ',', ':', 'Question', ' the', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0011693934599558513, 0.0021495461463928224, 0.0011693934599558513, 0.0065555572509765625, 0.003943403561909993]}, 'saliency': {'score': [0.0010466619774147316, 0.00014872439429231314, 0.00048374045978892934, 0.00014580777486690674], 'topk_tokens': [' Market', ' Daniel', ' ', ' Emily', ' Mary', ' Miles', ' the', '<|eot_id|>', ' Where', ' Mary', ' Mary', 'Question', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.000721663236618042, 0.0011508584022521973, 0.000721663236618042, 0.002472400665283203, 0.0006593366463979086]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-22 00:51:12.438 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:51:12.438 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-0_1-2-5-6-8.pkl | len: 10 |  size: 9.18 KB
Processing depth (1, 2, 5, 6, 8):   1%|          | 1/100 [00:18<31:09, 18.88s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.23it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.60it/s]
Processing depth (0, 5, 6, 7, 8):   1%|          | 1/100 [00:25<31:09, 18.88s/it]2025-01-22 00:51:19.623 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:51:19.624 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:51:19.624 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary picked up the apple.
2025-01-22 00:51:19.641 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5918, 5923) --> . Mary picked up the
2025-01-22 00:51:19.641 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:51:19.663 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7154, 7160) --> . Mary journeyed to the
2025-01-22 00:51:19.663 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:51:19.687 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8337, 8341) -->  Mary dropped the apple
2025-01-22 00:51:19.687 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 00:51:19.715 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9595, 9601) -->  Daniel went back to the hallway
2025-01-22 00:51:19.715 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:51:19.745 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (10000, 10006) --> . Sandra journeyed to the
2025-01-22 00:51:19.745 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John went back to the bedroom.
2025-01-22 00:51:19.770 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8462, 8468) --> . John went back to the
2025-01-22 00:51:19.770 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John journeyed to the office.
2025-01-22 00:51:19.771 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:51:19.771 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:51:19.800 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9596, 9602) -->  went back to the hallway.
2025-01-22 00:51:19.800 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John moved to the bedroom.
2025-01-22 00:51:19.804 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1262, 1267) --> . John moved to the
2025-01-22 00:51:19.804 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John took the milk.
2025-01-22 00:51:19.830 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (9532, 9536) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:51:21.881 | INFO     | test_jbb_retain:begin_test:544 - Mary's bathroom.<|eot_id|>
2025-01-22 00:51:21.882 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12161])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.61s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.44s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.43s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.05it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.47it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.93it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.88it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.13it/s]
2025-01-22 00:51:31.884 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.710881268536603, 0.7049470923600263, 0.6728867039535985, 0.7050212698416957], 'topk_tokens': [' Or', ' officers', ' office', 'office', ' offices', ' office', ' office', ' or', 'active', 'Official', 'enter', ' office', ' office', ' offices', ' office', ' office', ' office', ' office', ' office', 'office'], 'evidence_proportions': [1.362152099609375, 0.37789459228515626, 0.5770975748697916, 0.96649169921875, 0.30047607421875]}, 'weight': {'score': [0.026536806865974708, 0.0025723179851150292, 0.013089479822101015, 0.0024901807017634844], 'topk_tokens': [' THE', ' Mary', ' Floral', ' top', 'Bridge', 'Answer', '<|eot_id|>', '<|eot_id|>', 'THE', ' Mary', ' hallway', ':', 'assistant', ' Bridge', '<|start_header_id|>', ' bedroom', ' bathroom', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019299983978271484, 0.0488701343536377, 0.007825523614883423, 0.04099845886230469, 0.04160269101460775]}, 'saliency': {'score': [0.007886464949007388, 0.00012255211875380472, 0.003481489239316998, 9.607351250779694e-05], 'topk_tokens': [' bedroom', ' Market', ' office', 'Bridge', '<|eot_id|>', '.', '<|end_header_id|>', '<|start_header_id|>', ' office', ' Mary', '<|eot_id|>', ' bedroom', 'office', ' bathroom', ' office', ' Mary', ' top', '<|begin_of_text|>', 'office', ' hallway'], 'evidence_proportions': [0.002576082944869995, 0.013466262817382812, 0.0007938345273335775, 0.00904548168182373, 0.014866967995961508]}}, 25: {'grad': {'score': [1.1602251971209492, 1.0871868062977883, 1.0407654733368845, 1.0871504407219381], 'topk_tokens': [' of', 'ation', ' the', ' at', ' thirst', ' distinguished', 'ation', ' the', ' principal', ' fifteen', ' excited', ' good', ' rest', 'THE', 'ished', 'itable', ' her', 'ised', 'erc', ' principles'], 'evidence_proportions': [1.5096842447916667, 0.487384033203125, 1.3683268229166667, 0.775146484375, 1.4200846354166667]}, 'weight': {'score': [0.02114714295775802, 0.002545139796049944, 0.004433440439628832, 0.0024984928484663516], 'topk_tokens': [' April', ' Press', '?\n', 'THE', '.', 'Answer', ' top', 't', '<|eot_id|>', ' Mary', '<|eot_id|>', ' THE', 'THE', ':', '<|start_header_id|>', 'assistant', ' Mary', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00033843517303466797, 0.05531487464904785, 0.007957488298416138, 0.05291175842285156, 0.005495985349019368]}, 'saliency': {'score': [0.008578821464821144, 9.676635456469404e-05, 0.00025724822824651545, 7.740657683946688e-05], 'topk_tokens': ['Mary', ' April', ' counting', 'Print', 'count', 'assistant', ' counting', ':', 'Answer', ' Min', ' THE', '<|start_header_id|>', ' Mary', ' top', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', ' Mary', '<|begin_of_text|>', ' Mary'], 'evidence_proportions': [8.066495259602864e-05, 0.026553094387054443, 0.0027211010456085205, 0.02027738094329834, 0.00015709797541300455]}}, 26: {'grad': {'score': [1.0139397515190973, 0.9125311363099153, 1.1688713304924243, 0.9116059723820734], 'topk_tokens': [' Merch', 'graph', 'leading', 'BO', 'ols', 'ers', ' well', ' steam', ' a', ' Married', ' neighboring', ' Col', ' wh', ' part', ' some', ' hand', ' Jul', ' Marshall', ' Press', ' clear'], 'evidence_proportions': [1.3793538411458333, 0.44700927734375, 1.1674143473307292, 0.939666748046875, 1.0170084635416667]}, 'weight': {'score': [0.01506584882736206, 0.0024916386904218994, 0.008945540948347612, 0.002445990301740669], 'topk_tokens': [' kitchen', ' bedroom', 'THE', ' Mary', 'Bridge', '<|eot_id|>', ' Floral', '<|eot_id|>', ' hallway', ' Bridge', ' bathroom', '?\n', 'Answer', ' the', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00026290615399678546, 0.023045527935028075, 0.004364291826883952, 0.020294189453125, 0.03043505549430847]}, 'saliency': {'score': [0.002620116428092674, 0.00011785473565193596, 0.0017950245828339548, 0.00010769959472382669], 'topk_tokens': ['assistant', ' apple', ' Mary', ' kitchen', ' Daily', ' old', 'THE', ' bedroom', ' bedroom', ' bedroom', ' Seventh', ' old', ' hallway', ':', 'Bridge', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', ' the', 'office'], 'evidence_proportions': [0.00010703007380167644, 0.0019434809684753418, 0.0011581877867380779, 0.0015898942947387695, 0.007845809062321981]}}, 27: {'grad': {'score': [0.5255085980450666, 0.6376132966656047, 0.46839060927882337, 0.6383247876634167], 'topk_tokens': [' paper', ' EX', ' Moore', '186', 'ile', ' paper', ' interruption', ' Papers', 'sp', ' news', ' opposition', ' excessive', 'APER', ' newspaper', 'sp', ' N', 'bread', ' newspaper', 'SP', ' Newspaper'], 'evidence_proportions': [0.4416707356770833, 0.3145660400390625, 0.354461669921875, 0.7399444580078125, 0.8132216135660807]}, 'weight': {'score': [0.01920242883540966, 0.002551038886806266, 0.00649674010999275, 0.002503133766672626], 'topk_tokens': ['<|eot_id|>', ' bedroom', '<|eot_id|>', ' Mary', '?\n', ' Floral', ' Bridge', ' bathroom', 'Answer', ' Mary', 'THE', 'assistant', '.\n\n', ' THE', 'THE', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.000573436419169108, 0.05433752536773682, 0.005648483832677205, 0.034366607666015625, 0.01199666659037272]}, 'saliency': {'score': [0.004380936975832339, 0.00013507918916348023, 0.0007740475914695046, 0.00012386510029987964], 'topk_tokens': [' Dan', ' prior', 'RE', ' top', '.', ' old', 'Answer', '<|end_header_id|>', ' bedroom', ' bedroom', '.\n\n', ' Bridge', ' Floral', '<|begin_of_text|>', ' Mary', ':', ' Mary', '<|start_header_id|>', ' THE', 'office'], 'evidence_proportions': [0.0002645552158355713, 0.01274559497833252, 0.00149458646774292, 0.010020390152931213, 0.0006534854571024576]}}, 28: {'grad': {'score': [0.9252522786458334, 0.8714304846152265, 0.8639729817708334, 0.8713307497689106], 'topk_tokens': [' spoken', ' balance', ' being', ' been', ' ', ' kept', ' become', ' taken', ' out', 'being', '600', 'CH', ' came', ' ', 'hom', 'been', ' be', ' acted', 'na', ' ins'], 'evidence_proportions': [0.8090006510416666, 1.156884765625, 0.777099609375, 0.9683837890625, 0.9678751627604166]}, 'weight': {'score': [0.014450909914793792, 0.0024363448683940923, 0.0074268091808665886, 0.00239593517009083], 'topk_tokens': ['.\n\n', 'Question', ' Mary', ' the', ' the', ' Floral', 'During', ' the', ' Mary', ' Bridge', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0002146462599436442, 0.026024723052978517, 0.013655980428059896, 0.024660110473632812, 0.01303112506866455]}, 'saliency': {'score': [0.001832233534918891, 9.585535370440093e-05, 0.0005974281917918812, 9.061416436707314e-05], 'topk_tokens': [' Mary', ' the', '?\n', ' parade', ' the', ' Mary', 'Bridge', 'During', 'SSION', ' bedroom', 'Answer', ' bathroom', 'office', ' Floral', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', ':'], 'evidence_proportions': [6.426374117533366e-05, 0.0028429031372070312, 0.0028060277303059897, 0.002560853958129883, 0.001298437515894572]}}, 29: {'grad': {'score': [1.3258158365885417, 1.0317872177659706, 1.236423376834754, 1.03057332319789], 'topk_tokens': [' o', ' express', ' printed', ' THE', ' regular', ' Papers', 'AILY', 'ION', ' mail', ' ox', 'mail', ' printed', 'aper', ' the', 'ATING', '\n', 'paper', ' Press', '\n', 'APER'], 'evidence_proportions': [1.7753855387369792, 1.7305419921875, 1.6880696614583333, 0.6191253662109375, 0.6478474934895834]}, 'weight': {'score': [0.004745463530222575, 0.0025146683217231883, 0.0038241888537551417, 0.002506121213718021], 'topk_tokens': [' the', ' where', ' ', ' Where', ' Does', ' was', 'THE', '<|eot_id|>', '<|eot_id|>', ' the', '?\n', 'THE', ' in', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00038356582323710126, 0.008785176277160644, 0.0043761034806569414, 0.005333423614501953, 0.005718320608139038]}, 'saliency': {'score': [0.000620583693186442, 0.00010124944909411042, 0.0006156762440999349, 9.868825692475694e-05], 'topk_tokens': [' the', ' was', 'THE', '<|eot_id|>', ' in', ' the', 'nes', ' the', 'NEW', ' the', 'IVE', '?\n', 'THE', 'Does', ' Does', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.760059674580892e-05, 0.0004674077033996582, 0.0011151631673177083, 0.0006724148988723755, 0.0008020798365275065]}}, 30: {'grad': {'score': [1.011720445421007, 1.1439717117230535, 1.0147709702000474, 1.1446190230227655], 'topk_tokens': [' office', ' o', ' o', 'office', ' office', ' office', ' office', ' offices', 'op', ' o', ' office', 'ob', ' office', ' o', 'op', ' o', 'office', ' o', ' o', ' O'], 'evidence_proportions': [1.3462727864583333, 1.083984375, 1.2293294270833333, 0.972503662109375, 0.42548370361328125]}, 'weight': {'score': [0.014172549600954409, 0.0024399745232041785, 0.011714714946168842, 0.002388512409591265], 'topk_tokens': [' Mary', ' the', ' Where', ' the', '<|eot_id|>', ' Mary', 'Question', '<|eot_id|>', ' the', ' Floral', '.\n\n', ' bathroom', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0011128584543863933, 0.022118139266967773, 0.008680601914723715, 0.025443077087402344, 0.018589178721110027]}, 'saliency': {'score': [0.003222472137875027, 0.00022569138253194804, 0.00185762932806304, 0.0002145563719893727], 'topk_tokens': [' office', ' office', '<|eot_id|>', ' hallway', ' Floral', ' Where', 'assistant', ' the', ' Mary', ' nearly', ' the', ' Mary', '?\n', ' bathroom', 'office', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', ':', 'office'], 'evidence_proportions': [0.0009318689505259196, 0.004257655143737793, 0.0026753743489583335, 0.007716774940490723, 0.0022013187408447266]}}, 31: {'grad': {'score': [1.5325068721064814, 1.9785061508262765, 1.0355881199692234, 1.9820720663467026], 'topk_tokens': [' C', ' S', ' be', '7', ' S', ' the', 'G', ' B', ' most', ' C', ' their', ' U', ' and', ' L', ' its', ' was', ' were', ' S', 'editary', ' W'], 'evidence_proportions': [1.1129150390625, 1.3787109375, 1.7721354166666667, 2.481689453125, 1.2078450520833333]}, 'weight': {'score': [0.0049169328477647566, 0.0022944486681238693, 0.0034270286560058594, 0.0022855102055567037], 'topk_tokens': ['.', ' where', ' the', ' was', ',', ':', 'Question', '.\n\n', '<|eot_id|>', ' Where', ' the', 'Answer', '<|eot_id|>', '?\n', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0007665356000264486, 0.006879878044128418, 0.003662268320719401, 0.0062143802642822266, 0.007821242014567057]}, 'saliency': {'score': [0.001135603145316795, 0.00017050644936436523, 0.0005075949611085834, 0.00016743435718240417], 'topk_tokens': [' ', ' office', ' Mary', ' open', ' Market', ' the', '<|eot_id|>', ' Where', ' Mary', 'Question', 'light', '<|eot_id|>', '<|end_header_id|>', '?\n', '<|start_header_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0008578201134999593, 0.001959824562072754, 0.0008891423543294271, 0.0016276389360427856, 0.0006449719270070394]}}, 'pred_res': "Mary's bathroom.<|eot_id|>", 'score': 0}
2025-01-22 00:51:31.885 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:51:31.885 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-1_0-5-6-7-8.pkl | len: 10 |  size: 9.28 KB
Processing depth (0, 5, 6, 7, 8):   2%|‚ñè         | 2/100 [00:38<31:23, 19.22s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (2, 3, 4, 6, 7):   2%|‚ñè         | 2/100 [00:45<31:23, 19.22s/it]2025-01-22 00:51:39.047 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:51:39.055 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2460, 2466) --> . Mary journeyed to the
2025-01-22 00:51:39.055 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary picked up the apple.
2025-01-22 00:51:39.066 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3774, 3779) --> . Mary picked up the
2025-01-22 00:51:39.066 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:51:39.074 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (2460, 2466) --> . Mary journeyed to the
2025-01-22 00:51:39.074 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:51:39.094 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (7162, 7166) -->  Mary dropped the apple
2025-01-22 00:51:39.094 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 00:51:39.119 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (8341, 8347) --> . Daniel went back to the
2025-01-22 00:51:39.120 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:51:39.150 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (10000, 10006) --> . Sandra journeyed to the
2025-01-22 00:51:39.150 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John went back to the bedroom.
2025-01-22 00:51:39.175 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8469, 8475) --> . John went back to the
2025-01-22 00:51:39.175 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John journeyed to the office.
2025-01-22 00:51:39.183 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (2461, 2467) -->  Mary journeyed to the office
2025-01-22 00:51:39.183 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:51:39.208 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8341, 8347) --> . Daniel went back to the
2025-01-22 00:51:39.208 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John moved to the bedroom.
2025-01-22 00:51:39.212 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1255, 1260) --> . John moved to the
2025-01-22 00:51:39.212 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John took the milk.
2025-01-22 00:51:39.239 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (9539, 9543) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:51:41.321 | INFO     | test_jbb_retain:begin_test:544 - Mary picked up the apple.<|eot_id|>
2025-01-22 00:51:41.321 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12161])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.58s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.43s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.42s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.06it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.48it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.94it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.42it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.90it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.14it/s]
2025-01-22 00:51:50.845 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.6378529866536459, 0.6282449045003289, 0.7003233938506155, 0.6280269413203984], 'topk_tokens': [' editor', ' rept', 'ot', 'consider', ' day', ' of', 'Minnesota', ' Minnesota', ' day', ' or', 'active', 'issippi', ' election', ' effect', 'Minnesota', ' or', ' or', 'editor', ' Minnesota', ' Minnesota'], 'evidence_proportions': [0.85833740234375, 0.50606689453125, 0.85833740234375, 0.5399169921875, 0.37199656168619794]}, 'weight': {'score': [0.032507490228723596, 0.0025710372717808044, 0.01590837131847035, 0.0024678879490196534], 'topk_tokens': ['\n\n', ' bedroom', '.', '?\n', '.', ' hallway', '.', ' Sandra', ' Mary', ' Mary', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', 'Bridge', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.013204614321390787, 0.059314823150634764, 0.013204614321390787, 0.08740234375, 0.012177228927612305]}, 'saliency': {'score': [0.00804652770360311, 0.0002086924495130917, 0.0027126904689904413, 0.0001843800074323528], 'topk_tokens': ['<|end_header_id|>', ' bedroom', ' office', ' Market', '.', ' office', ' office', ' Bridge', '<|eot_id|>', ' Mary', '<|eot_id|>', ' Sandra', '<|start_header_id|>', '.', ' hallway', '<|begin_of_text|>', ' Mary', ' Mary', 'Bridge', 'office'], 'evidence_proportions': [0.004631201426188151, 0.015406525135040284, 0.004631201426188151, 0.020291805267333984, 0.0005803306897481283]}}, 25: {'grad': {'score': [1.2552591959635417, 0.7248444629717175, 1.1027217055812026, 0.722630867433678], 'topk_tokens': [' of', ' location', ' location', ' the', ' utter', ' the', ' the', ' of', ' the', ' the', 'Russia', ' Aw', 'occ', ' the', ' the', 'op', ' Wood', 'op', ' the', ' opinion'], 'evidence_proportions': [1.1743392944335938, 1.18857421875, 1.1743392944335938, 1.313751220703125, 1.4336751302083333]}, 'weight': {'score': [0.02296587935200444, 0.0025445815323705216, 0.0067465269204342, 0.0024875676317726355], 'topk_tokens': ['Bridge', ' Geo', ' Miss', ' apple', ' Paul', ' Dan', '?\n', '.', ' Mary', '.', 'Answer', ' Mary', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.005233019590377808, 0.04641867876052856, 0.005233019590377808, 0.06662750244140625, 0.009779850641886393]}, 'saliency': {'score': [0.0075160883091114185, 8.965880328040957e-05, 0.0008734554955453584, 7.095444175829269e-05], 'topk_tokens': [' apple', ' Anthony', 'office', '<|start_header_id|>', ' Miss', ' Paul', ':', ' Daniel', ' Mary', ' apple', ' Dan', ' apple', '<|begin_of_text|>', '.', '<|eot_id|>', '.', '<|eot_id|>', ' Mary', '<|end_header_id|>', ' Mary'], 'evidence_proportions': [0.0032780965169270835, 0.015800178050994873, 0.0032780965169270835, 0.02066946029663086, 0.000319749116897583]}}, 26: {'grad': {'score': [0.6339446173773872, 0.65572735479785, 0.7443132111520478, 0.6555344109534035], 'topk_tokens': ['BO', ' a', ' a', ' got', 'ioneer', 'CE', ' for', 'ers', ' and', 'str', ' some', 'istributed', 'ers', ' worth', ' and', ' wh', ' by', ' str', ' part', ' Jul'], 'evidence_proportions': [0.78887939453125, 0.431439208984375, 0.78887939453125, 0.5375518798828125, 0.5570913950602213]}, 'weight': {'score': [0.011004865169525146, 0.0025043155099670716, 0.005511419339613481, 0.0024771528835780443], 'topk_tokens': [' Ramsey', ' the', '.', ' apple', '.', ' Mary', ' Mary', ' discarded', '<|eot_id|>', '<|eot_id|>', '?\n', 'assistant', 'Answer', ' the', 'Bridge', 'office', '<|start_header_id|>', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.002326279878616333, 0.018826520442962645, 0.002326279878616333, 0.03685283660888672, 0.0046120087305704755]}, 'saliency': {'score': [0.0005584270865828903, 9.716941976511957e-05, 0.0005750493569807572, 9.483743637817454e-05], 'topk_tokens': [' Ramsey', 'Answer', ' hallway', ' Gal', ' Sandra', ' Az', ' Dan', ' Bench', 'Mer', ' carrier', ' Father', ' apple', 'assistant', '<|end_header_id|>', ':', '<|start_header_id|>', ' the', 'office', 'Bridge', '<|begin_of_text|>'], 'evidence_proportions': [0.00012194116910298665, 0.0011937856674194337, 0.00012194116910298665, 0.0013961195945739746, 0.0003434717655181885]}}, 27: {'grad': {'score': [0.4809525101273148, 0.5818849623345391, 0.4156438654119318, 0.5825634001109616], 'topk_tokens': [' remember', ' material', 'ly', 'sp', ' told', ' producing', 'apers', 'na', ' earnest', ' *', ' newspaper', ' news', 'bread', ' talk', 'ile', 'news', ' noble', ' newspaper', ' N', ' Newspaper'], 'evidence_proportions': [0.3895670572916667, 0.5572021484375, 0.3895670572916667, 0.693115234375, 0.458740234375]}, 'weight': {'score': [0.02674723996056451, 0.0025458009307893948, 0.00915213425954183, 0.002473798298908641], 'topk_tokens': [' to', ' Mary', ' Geo', '.', '.', '<|eot_id|>', '<|eot_id|>', '?\n', ' Mary', ' THE', 'Bridge', 'Answer', ' Mary', 'assistant', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.015952706336975098, 0.051333284378051756, 0.015952706336975098, 0.05546092987060547, 0.008705476919809977]}, 'saliency': {'score': [0.004360702302720811, 0.00014491062811955235, 0.001739277984156753, 0.0001311586246523573], 'topk_tokens': ['assistant', ' Daniel', '.', ' the', ' prior', ' upper', ' Mary', ' hallway', ' Mary', ' Paul', '<|end_header_id|>', 'THE', ' the', '.\n\n', '.', ' Mary', '<|start_header_id|>', 'Bridge', ':', 'office'], 'evidence_proportions': [0.0023520986239115396, 0.008394980430603027, 0.0023520986239115396, 0.00842738151550293, 0.00230489174524943]}}, 28: {'grad': {'score': [1.3119891131365742, 1.132886848176848, 1.1498209635416667, 1.1324411250535766], 'topk_tokens': [' on', ' containing', ' kept', ' within', ' suffered', ' acted', 'PA', 'been', 'CH', ' been', ' ins', ' acted', ' hands', ' been', ' be', 'being', 'hum', 'hom', ' taken', ' being'], 'evidence_proportions': [0.927459716796875, 1.6177734375, 0.927459716796875, 1.52337646484375, 1.685302734375]}, 'weight': {'score': [0.012262070620501483, 0.0024454806339659404, 0.0048406268611098785, 0.002417050678158933], 'topk_tokens': ['.\n\n', ' the', ' Mary', ' was', ' the', ' discarded', ' to', 'Question', '<|eot_id|>', ' Mary', ' the', '<|eot_id|>', 'Answer', 'assistant', '?\n', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0009361704190572103, 0.01584515571594238, 0.0009361704190572103, 0.051845550537109375, 0.0055389801661173505]}, 'saliency': {'score': [0.0009602970547146267, 8.420084472119313e-05, 0.000495040055477258, 8.112621102502087e-05], 'topk_tokens': [' the', ' to', ' during', ' subscribers', ' Mary', ' Bridge', ' apple', ' Nearly', 'Question', ' the', '?\n', ' Far', ' Mary', 'Answer', 'office', 'assistant', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.271301587422689e-05, 0.0013529181480407715, 3.271301587422689e-05, 0.0039806067943573, 0.00047474106152852374]}}, 29: {'grad': {'score': [1.0514594184027777, 1.0547424180917537, 1.1415201822916667, 1.0545131340111955], 'topk_tokens': ['prising', ' county', 'ATING', ' printed', '\n', ' business', ' regular', ' press', ' newspaper', ' regular', ' the', 'aper', '\n', ' the', ' The', ' press', ' Press', '\n', 'paper', 'APER'], 'evidence_proportions': [1.5684407552083333, 0.71396484375, 1.5684407552083333, 0.533782958984375, 0.64385986328125]}, 'weight': {'score': [0.004282441404130723, 0.0025249319053799817, 0.0033773508938876066, 0.0025186869592437326], 'topk_tokens': [' discarded', ' ', '.\n\n', 'Bridge', '.', 'Question', 'Does', ' Where', ' Does', ' the', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0004986325899759928, 0.0042585253715515135, 0.0004986325899759928, 0.01290130615234375, 0.00612407922744751]}, 'saliency': {'score': [0.00037990013758341473, 0.00011914407384637683, 0.00034072904875784207, 0.000117958192833985], 'topk_tokens': [' to', ' apple', ' the', ' discarded', 'Question', ':', ' Where', ' ', 'assistant', 'Bridge', '.', 'Answer', 'Does', ' Does', '?\n', 'office', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [8.376439412434895e-05, 0.00043184757232666015, 8.376439412434895e-05, 0.00104483962059021, 0.00048558910687764484]}}, 30: {'grad': {'score': [0.9930058232060185, 1.227877483196991, 1.3281166770241477, 1.2281281352190896], 'topk_tokens': [' o', ' office', 'op', ' M', ' an', ' dre', ' it', ' an', ' office', 'ob', ' head', 'office', ' o', ' o', ' o', ' o', ' o', 'op', 'office', ' O'], 'evidence_proportions': [0.6805775960286459, 1.35311279296875, 0.6805775960286459, 1.0579833984375, 1.2744547526041667]}, 'weight': {'score': [0.008343575177369294, 0.0024668665237257397, 0.010930814526297829, 0.002430678684534361], 'topk_tokens': [' prior', ' Sandra', ',', ' the', ' discarded', ':', '<|eot_id|>', ' Where', '<|eot_id|>', '.\n\n', ' the', 'Question', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.003041227658589681, 0.00941934585571289, 0.003041227658589681, 0.024078369140625, 0.007561932007471721]}, 'saliency': {'score': [0.002053254180484348, 0.00020852873398463192, 0.0017091144214976919, 0.00020032193279951888], 'topk_tokens': [' Mary', ' Geo', ' Mary', ':', 'Christmas', ',', '.\n\n', '<|eot_id|>', ' the', ' Nearly', ' Central', ' Miles', ' Mary', 'assistant', '<|start_header_id|>', 'Question', '<|end_header_id|>', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0016369521617889404, 0.002385115623474121, 0.0016369521617889404, 0.005555689334869385, 0.00027435024579366046]}}, 31: {'grad': {'score': [0.9776961715133102, 1.7002758429766505, 0.6567137747099905, 1.7047331840806998], 'topk_tokens': [' was', ' M', ' miles', ' Mr', ' was', ' S', ' United', ' U', ' S', ' was', ' L', 'UX', ' B', ' and', ' its', 'G', ' S', ' most', ' W', 'editary'], 'evidence_proportions': [1.1254475911458333, 0.980859375, 1.1254475911458333, 1.528076171875, 0.3126373291015625]}, 'weight': {'score': [0.003212138458534523, 0.002278880011356162, 0.0029933831908486104, 0.0022748498879985595], 'topk_tokens': [' apple', ' where', ' the', ' was', ',', ':', 'Question', '.\n\n', ' the', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019149482250213623, 0.004304838180541992, 0.0019149482250213623, 0.0072519779205322266, 0.002202709515889486]}, 'saliency': {'score': [0.0020755948843779386, 0.0001534068603063996, 0.0007664940573952415, 0.00014744710205192853], 'topk_tokens': [' Ramsey', ' Daniel', ' Emily', '.\n\n', ' Where', ' Miles', ' the', 'Question', ' Mary', '?\n', ' Mary', ' Mary', '<|eot_id|>', '<|end_header_id|>', 'Answer', '<|start_header_id|>', ':', '<|begin_of_text|>', 'office', 'assistant'], 'evidence_proportions': [0.0019021332263946533, 0.0028703689575195314, 0.0019021332263946533, 0.004059121012687683, 0.00043785572052001953]}}, 'pred_res': 'Mary picked up the apple.<|eot_id|>', 'score': 0}
2025-01-22 00:51:50.846 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:51:50.847 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-2_2-3-4-6-7.pkl | len: 10 |  size: 9.14 KB
Processing depth (2, 3, 4, 6, 7):   3%|‚ñé         | 3/100 [00:57<30:52, 19.10s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.30it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.35it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.81it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.58it/s]
Processing depth (1, 5, 6, 7, 9):   3%|‚ñé         | 3/100 [01:04<30:52, 19.10s/it]2025-01-22 00:51:57.917 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:51:57.922 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 00:51:57.922 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary picked up the apple.
2025-01-22 00:51:57.940 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5918, 5923) --> . Mary picked up the
2025-01-22 00:51:57.940 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:51:57.945 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1494, 1500) -->  tragedy. Mary journeyed to
2025-01-22 00:51:57.945 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:51:57.969 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8337, 8341) -->  Mary dropped the apple
2025-01-22 00:51:57.969 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 00:51:58.001 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10704, 10710) --> . Daniel went back to the
2025-01-22 00:51:58.001 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:51:58.031 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (9993, 9999) --> . Sandra journeyed to the
2025-01-22 00:51:58.031 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John went back to the bedroom.
2025-01-22 00:51:58.057 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8462, 8468) --> . John went back to the
2025-01-22 00:51:58.057 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John journeyed to the office.
2025-01-22 00:51:58.061 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1496, 1502) -->  Mary journeyed to the office
2025-01-22 00:51:58.061 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:51:58.094 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10704, 10710) --> . Daniel went back to the
2025-01-22 00:51:58.094 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John moved to the bedroom.
2025-01-22 00:51:58.097 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1255, 1260) --> . John moved to the
2025-01-22 00:51:58.098 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John took the milk.
2025-01-22 00:51:58.124 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (9532, 9536) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:52:00.113 | INFO     | test_jbb_retain:begin_test:544 - the bathroom<|eot_id|>
2025-01-22 00:52:00.113 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12161])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.66s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.46s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.05it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.93it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.42it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.90it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:52:09.887 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.6509054678457754, 0.6078545556965798, 0.608001708984375, 0.6077581142616855], 'topk_tokens': [' election', 'ly', ' office', 'Official', 'editor', ' or', ' rept', ' election', ' or', 'ent', 'active', ' effect', ' office', ' sure', ' conn', 'office', ' office', ' Or', ' or', 'enter'], 'evidence_proportions': [0.7605794270833334, 0.3881744384765625, 0.7605794270833334, 0.889678955078125, 0.4913177490234375]}, 'weight': {'score': [0.020111958185831707, 0.002569782746658507, 0.010223800485784357, 0.00250977933244312], 'topk_tokens': [' Mary', ' Broadway', ' bedroom', ' Bridge', ' bedroom', ' Fort', 'Answer', '<|eot_id|>', 'Bridge', '.', '<|eot_id|>', ' Mary', 'assistant', ':', 'THE', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00371094544728597, 0.05504508018493652, 0.00371094544728597, 0.055248260498046875, 0.0003788471221923828]}, 'saliency': {'score': [0.0054522863140812625, 0.00016048067662606728, 0.0012497053001866195, 0.00014570556592716715], 'topk_tokens': [' Bench', 'Bridge', ' front', ' John', ' dropped', 'THE', '<|eot_id|>', ' bedroom', ' top', ' hallway', ' office', '<|eot_id|>', '<|start_header_id|>', ' Market', ' Mary', ' bedroom', '.', '<|begin_of_text|>', ' Mary', 'office'], 'evidence_proportions': [0.0010750691095987956, 0.015736210346221923, 0.0010750691095987956, 0.013898611068725586, 5.900859832763672e-06]}}, 25: {'grad': {'score': [0.7923764829282407, 0.7829070972159418, 0.7137515906131628, 0.7830745315961499], 'topk_tokens': [' distinguished', ' Wood', ' the', ' trib', ' excited', 'ing', ' circulated', ' of', ' exc', ' her', ' principles', 'ised', ' regular', ' the', 'ation', 'ation', 'itable', 'THE', 'ished', 'erc'], 'evidence_proportions': [0.7119140625, 0.6895263671875, 0.7119140625, 0.7523193359375, 1.0657145182291667]}, 'weight': {'score': [0.02185895045598348, 0.0025457868173817686, 0.0048357880476749306, 0.002496458100630391], 'topk_tokens': [' Mary', 'During', '?\n', ' Seventh', 'THE', ' Anthony', ' top', ' THE', 'Answer', '<|eot_id|>', ' Mary', '<|eot_id|>', 'THE', ':', ' Mary', '<|start_header_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0058580438296000166, 0.05296295881271362, 0.0058580438296000166, 0.06319808959960938, 0.0003813306490580241]}, 'saliency': {'score': [0.006565842363569472, 7.794816899944526e-05, 0.000701776056578665, 6.177371113416351e-05], 'topk_tokens': ['THE', ' Ramsey', 'assistant', ':', ' Paul', ' THE', '.', ' Mary', ' Ramsey', '<|start_header_id|>', ' top', ' Bench', ' Mary', 'Answer', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', ' Mary', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.002907305955886841, 0.017220830917358397, 0.002907305955886841, 0.014040470123291016, 2.067287762959798e-05]}}, 26: {'grad': {'score': [0.7154959219473379, 0.8668556630508509, 1.0752397017045454, 0.8666251449755161], 'topk_tokens': [' B', ' some', 'graph', ' steam', 'ols', ' Col', 'ers', 'hand', 'BO', ' well', 'ers', ' part', ' Col', 'leading', ' a', ' Jul', ' hand', ' clear', ' Press', ' wh'], 'evidence_proportions': [0.73614501953125, 0.410162353515625, 0.73614501953125, 0.74066162109375, 0.911865234375]}, 'weight': {'score': [0.011160241232977973, 0.0024861551611123386, 0.004641364921223034, 0.002460928172264613], 'topk_tokens': [' Anthony', ' Mary', ' Floral', 'THE', ' Seventh', ' Bridge', ' Mary', '<|eot_id|>', '<|eot_id|>', 'Bridge', ' bathroom', '?\n', ' the', 'Answer', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012109676996866863, 0.02996826171875, 0.0012109676996866863, 0.03312873840332031, 0.0007397731145222982]}, 'saliency': {'score': [0.0007239756760773835, 0.00011436945904166666, 0.0005870100223656857, 0.00011172081767591791], 'topk_tokens': ['Minnesota', ' Ramsey', ' Ramsey', ' Mary', '?\n', ' bedroom', 'THE', ' bathroom', ' Az', 'assistant', '<|end_header_id|>', ' bedroom', ' Bridge', ' Seventh', ':', ' the', '<|start_header_id|>', 'Bridge', '<|begin_of_text|>', 'office'], 'evidence_proportions': [3.4421682357788086e-05, 0.0016860604286193849, 3.4421682357788086e-05, 0.0025850534439086914, 6.062785784403483e-05]}}, 27: {'grad': {'score': [0.6984772858796297, 0.6035003298297089, 0.553881211714311, 0.603423741635415], 'topk_tokens': [' News', 'sp', '185', ' Moore', ' EX', 'papers', 'paper', ' newspaper', ' river', ' news', ' paper', 'SP', 'sp', '186', ' newspaper', ' N', 'APER', 'bread', 'ile', ' Newspaper'], 'evidence_proportions': [0.8153889973958334, 0.441015625, 0.8153889973958334, 0.81390380859375, 0.6022542317708334]}, 'weight': {'score': [0.018714611177091247, 0.002543524908586267, 0.005903711824706106, 0.002498287736192514], 'topk_tokens': [' Floral', 'Bridge', '<|eot_id|>', '<|eot_id|>', '?\n', ' Mary', ' Bridge', ' bathroom', 'Answer', ' Mary', ' THE', 'assistant', 'THE', '.\n\n', 'THE', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004745225111643474, 0.05294698476791382, 0.004745225111643474, 0.044361114501953125, 0.0010287364323933919]}, 'saliency': {'score': [0.0036048668402212636, 0.00013240830517810965, 0.0012961824735005696, 0.00012148862179375814], 'topk_tokens': [' Mary', ' the', '.', ' top', '.', '<|end_header_id|>', ' Dan', 'Answer', ' John', ' Floral', ' Broadway', 'THE', ' Mary', '.\n\n', ':', ' Mary', '<|start_header_id|>', '<|begin_of_text|>', ' THE', 'office'], 'evidence_proportions': [0.0007759332656860352, 0.010903584957122802, 0.0007759332656860352, 0.008268028497695923, 7.169445355733235e-05]}}, 28: {'grad': {'score': [0.9382109465422453, 0.867018570665132, 0.8711307410037878, 0.8668485387912281], 'topk_tokens': [' out', '600', ' came', ' become', ' balance', ' been', 'hum', ' kept', ' been', ' being', 'being', 'CH', ' acted', 'PA', 'been', ' be', 'hom', 'na', ' taken', ' ins'], 'evidence_proportions': [0.6761194864908854, 1.46806640625, 0.6761194864908854, 1.10400390625, 0.9103190104166666]}, 'weight': {'score': [0.01085244063977842, 0.002437777065636835, 0.007178330060207482, 0.0024060796215880326], 'topk_tokens': ['Question', ' Broadway', ' Mary', ' Floral', ' the', ' Mary', ' the', ' Bridge', ' Mary', ' to', '<|eot_id|>', '<|eot_id|>', '?\n', 'Answer', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0007618168989817301, 0.02274552583694458, 0.0007618168989817301, 0.03940773010253906, 0.0020859241485595703]}, 'saliency': {'score': [0.000329611478028474, 7.807529257834178e-05, 0.0007187525431315104, 7.576728412792386e-05], 'topk_tokens': [' bedroom', ' to', ' to', 'During', ' Far', 'Question', ' Bridge', ' bathroom', '?\n', 'office', ' the', 'assistant', 'Answer', 'Bridge', ' Floral', ' Bridge', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.7527411778767903e-05, 0.0008807778358459473, 2.7527411778767903e-05, 0.0008687227964401245, 0.0001150667667388916]}}, 29: {'grad': {'score': [0.9764585141782407, 1.1258766822278632, 1.0723525538589016, 1.1263559507376142], 'topk_tokens': ['LY', '\n', 'AILY', ' regular', ' press', ' printed', ' THE', ' mail', ' express', 'mail', ' The', ' the', ' regular', 'aper', 'paper', '\n', 'ATING', ' Press', '\n', 'APER'], 'evidence_proportions': [1.036376953125, 1.357763671875, 1.036376953125, 0.74444580078125, 0.69354248046875]}, 'weight': {'score': [0.0026285692497535987, 0.0025064714113897392, 0.0030425805034059467, 0.0025047372759132163], 'topk_tokens': [' to', 'Question', 'Does', '.\n\n', ' Where', '<|eot_id|>', ' Does', ' the', '<|eot_id|>', 'THE', 'THE', '?\n', ' in', 'Answer', 'assistant', '<|end_header_id|>', 'office', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010910630226135254, 0.007877922058105469, 0.00010910630226135254, 0.006766796112060547, 0.000534216562906901]}, 'saliency': {'score': [0.0002097045933758771, 9.260010603647782e-05, 0.0003988020347826409, 9.150397410168585e-05], 'topk_tokens': [' Where', ' the', ' the', ' the', 'IVE', 'assistant', '<|eot_id|>', ' in', ' to', 'THE', ' the', 'Does', '?\n', 'THE', ' Does', 'office', '<|end_header_id|>', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.675820668538412e-06, 0.000634610652923584, 9.675820668538412e-06, 0.0005615204572677612, 2.1129846572875977e-05]}}, 30: {'grad': {'score': [0.7310542353877315, 1.0358382299288826, 0.9779991381096117, 1.0366758617460063], 'topk_tokens': [' o', 'G', ' o', ' office', ' offices', ' Paul', ' office', 'office', ' o', 'op', ' office', ' office', 'ob', ' o', ' o', 'office', 'op', ' o', ' o', ' O'], 'evidence_proportions': [0.4865010579427083, 0.891748046875, 0.4865010579427083, 0.5901031494140625, 1.1802164713541667]}, 'weight': {'score': [0.006556727268077709, 0.0024450443728546616, 0.009466611977779505, 0.002416726834299465], 'topk_tokens': ['THE', ' the', ' Floral', ' bathroom', 'Question', ' Fort', ' the', '<|eot_id|>', ' Anthony', '<|eot_id|>', ' Broadway', '.\n\n', 'Answer', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0019152462482452393, 0.013778781890869141, 0.0019152462482452393, 0.02007293701171875, 0.0008105039596557617]}, 'saliency': {'score': [0.0010479357507493761, 0.00020556974046414379, 0.0013541940486792362, 0.000200558711425988], 'topk_tokens': [' John', 'Question', ' the', ' artist', '<|eot_id|>', ' the', ':', '<|begin_of_text|>', '.\n\n', ' the', ' nearly', ' Third', ' nearly', ' Seventh', '?\n', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.0007150173187255859, 0.0016829848289489746, 0.0007150173187255859, 0.0027507543563842773, 4.935264587402344e-05]}}, 31: {'grad': {'score': [1.2673520688657407, 1.8047143167290143, 0.8896863532788826, 1.8084080045325475], 'topk_tokens': [' United', ' its', ' be', ' the', '7', ' be', ' B', ' two', ' C', ' the', ' and', ' most', ' their', ' its', ' L', ' was', ' S', ' were', ' W', 'editary'], 'evidence_proportions': [1.2330729166666667, 1.11904296875, 1.2330729166666667, 1.6455078125, 1.2073974609375]}, 'weight': {'score': [0.0026344899778012877, 0.0022850497659076194, 0.002668853962060177, 0.0022832237373036236], 'topk_tokens': [' where', ' the', ' Market', ' was', ',', ':', 'Question', ' the', ' Where', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0010595520337422688, 0.005819463729858398, 0.0010595520337422688, 0.0055425167083740234, 0.001191536585489909]}, 'saliency': {'score': [0.0006613532702128092, 0.00013654892415011644, 0.000280205047491825, 0.00013498647108773777], 'topk_tokens': [' Mary', ' Where', ' Mary', ' Ramsey', ' open', ' the', 'light', '<|eot_id|>', ' Mary', ' Market', 'Question', '<|end_header_id|>', '<|start_header_id|>', '?\n', '<|eot_id|>', 'Answer', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0003039439519246419, 0.001629936695098877, 0.0003039439519246419, 0.001396089792251587, 7.919470469156901e-05]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-22 00:52:09.888 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:52:09.889 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-3_1-5-6-7-9.pkl | len: 10 |  size: 9.21 KB
Processing depth (1, 5, 6, 7, 9):   4%|‚ñç         | 4/100 [01:16<30:31, 19.08s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.39it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.39it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.40it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.66it/s]
Processing depth (0, 4, 6, 7, 9):   4%|‚ñç         | 4/100 [01:23<30:31, 19.08s/it]2025-01-22 00:52:16.663 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 00:52:16.664 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:52:16.664 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary picked up the apple.
2025-01-22 00:52:16.678 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (4845, 4850) --> . Mary picked up the
2025-01-22 00:52:16.679 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 00:52:16.700 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7154, 7160) --> . Mary journeyed to the
2025-01-22 00:52:16.701 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary dropped the apple.
2025-01-22 00:52:16.724 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8337, 8341) -->  Mary dropped the apple
2025-01-22 00:52:16.724 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 00:52:16.757 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10704, 10710) --> . Daniel went back to the
2025-01-22 00:52:16.757 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:52:16.787 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (9993, 9999) --> . Sandra journeyed to the
2025-01-22 00:52:16.788 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  John went back to the bedroom.
2025-01-22 00:52:16.814 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8462, 8468) --> . John went back to the
2025-01-22 00:52:16.814 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John journeyed to the office.
2025-01-22 00:52:16.814 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 00:52:16.814 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 00:52:16.847 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10704, 10710) --> . Daniel went back to the
2025-01-22 00:52:16.847 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John moved to the bedroom.
2025-01-22 00:52:16.851 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1262, 1267) --> . John moved to the
2025-01-22 00:52:16.851 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  John took the milk.
2025-01-22 00:52:16.878 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (9532, 9536) -->  John took the milk
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:52:18.916 | INFO     | test_jbb_retain:begin_test:544 - Mary's hand<|eot_id|>
2025-01-22 00:52:18.917 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12161])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:38,  5.52s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.39s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.39s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.09it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.45it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.89it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.37it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.83it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.14it/s]
2025-01-22 00:52:28.847 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.5267579820421007, 0.5909853879336924, 0.4575616085168087, 0.5914924625994636], 'topk_tokens': [' Minnesota', ' Ramsey', '185', '\n', ' from', ' or', 'issippi', ' office', 'enter', 'editor', ' office', ' or', 'ly', 'office', ' effect', ' or', 'Minnesota', ' Minnesota', ' Minnesota', 'active'], 'evidence_proportions': [0.671478271484375, 0.384326171875, 0.4815317789713542, 0.9197235107421875, 0.2839800516764323]}, 'weight': {'score': [0.014818884708263256, 0.0025692124081747826, 0.007404345454591693, 0.002528701746137755], 'topk_tokens': [' Father', ' bedroom', '?\n', ' top', ' Fort', ' bedroom', 'Bridge', ' bathroom', 'Answer', '<|eot_id|>', ':', ' Mary', '<|eot_id|>', 'assistant', '<|start_header_id|>', '.', '<|end_header_id|>', ' Bridge', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0027332305908203125, 0.014140892028808593, 0.008769869804382324, 0.06396675109863281, 0.0007533033688863119]}, 'saliency': {'score': [0.00785041959197433, 0.0001283623636489262, 0.0008230119040518096, 0.00010924156880483129], 'topk_tokens': [' office', ' office', 'office', '.', '<|eot_id|>', '<|start_header_id|>', ' bedroom', ' hallway', 'Mary', ' office', '<|eot_id|>', ' office', ' Mary', '.', ' Bridge', ' top', '<|begin_of_text|>', ' office', 'office', ' Mary'], 'evidence_proportions': [0.0027098655700683594, 0.008448171615600585, 0.0024373034636179605, 0.034668684005737305, 2.7120113372802734e-05]}}, 25: {'grad': {'score': [1.2231207953559027, 0.9635732472457453, 1.1224531693892046, 0.9625610336433568], 'topk_tokens': [' the', 'MIN', ' the', ' THE', ' good', ' the', 'ation', ' her', 'erc', 'ation', ' the', ' of', ' the', ' the', ' the', ' the', 'THE', ' the', ' the', ' the'], 'evidence_proportions': [1.3873697916666667, 0.927655029296875, 1.2755533854166667, 0.98968505859375, 1.4082845052083333]}, 'weight': {'score': [0.0169880919986301, 0.002546345081061191, 0.003228050289732037, 0.002512268947981748], 'topk_tokens': ['?\n', 'door', ' Anthony', ' THE', ' prior', ' top', ' to', ' Dan', '<|eot_id|>', 'Answer', ' Mary', '<|eot_id|>', '<|start_header_id|>', ' Mary', 't', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004931886990865072, 0.033123779296875, 0.008581658204396566, 0.05840492248535156, 0.0008318026860555013]}, 'saliency': {'score': [0.005431482085475215, 0.00010473473550535724, 0.00015251203016801313, 9.272128188451499e-05], 'topk_tokens': ['door', ' Bench', ':', ' Mary', '<|start_header_id|>', ' Capt', ' fifteen', 'office', ' post', ' Dan', 'Print', 'Mary', '<|eot_id|>', '<|eot_id|>', 'Answer', ' Ot', '<|end_header_id|>', ' Mary', ' Mary', '<|begin_of_text|>'], 'evidence_proportions': [7.917483647664388e-05, 0.011398780345916747, 0.002400994300842285, 0.01859945058822632, 6.288290023803711e-05]}}, 26: {'grad': {'score': [0.9433277271412037, 0.9320607002281509, 1.1365744850852273, 0.9314779385470028], 'topk_tokens': [' the', ' and', ' str', ' a', 'istributed', 'chants', ' several', ' generally', 'str', ' some', ' STR', ' proprietor', 'press', ' clear', ' generally', ' Col', ' wh', ' James', ' Press', ' Jul'], 'evidence_proportions': [1.3129069010416667, 0.700537109375, 1.1223551432291667, 0.49212646484375, 0.8978474934895834]}, 'weight': {'score': [0.007724909870712845, 0.0025074491569069294, 0.004696399876565644, 0.0024898413891616187], 'topk_tokens': [' bedroom', ' Father', '.', ' the', ' Anthony', ' bathroom', 'Bridge', '<|eot_id|>', ' Mary', '<|eot_id|>', '?\n', 'Answer', 'assistant', ' Bridge', '<|start_header_id|>', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0003319183985392253, 0.0071394920349121095, 0.005169222752253215, 0.03351879119873047, 0.0009655157725016276]}, 'saliency': {'score': [0.0004088812404208713, 0.00015255301874699114, 0.0004249251250064734, 0.00015123853956895613], 'topk_tokens': ['.', '?\n', ' Az', 'assistant', '.', ' Seventh', ' marshal', ' bedroom', '<|end_header_id|>', ' bedroom', ' Dan', ' bedroom', ' old', ' Father', 'Bridge', '<|start_header_id|>', ' Bridge', '<|begin_of_text|>', ' the', 'office'], 'evidence_proportions': [7.063150405883789e-05, 0.0005477428436279297, 0.00020080804824829102, 0.0015918910503387451, 5.081295967102051e-05]}}, 27: {'grad': {'score': [1.0824799714265045, 1.1543744347611609, 0.8926276004675663, 1.1552484987156948], 'topk_tokens': ['remember', ' print', ' newspapers', ' print', ' talk', ' news', ' product', ' told', 'apers', ' producing', ' possessed', ' remember', ' tell', ' print', 'urnished', ' newspaper', ' news', 'APER', ' newspaper', ' Newspaper'], 'evidence_proportions': [1.646240234375, 1.298046875, 0.8738199869791666, 0.9964065551757812, 0.6051228841145834]}, 'weight': {'score': [0.013454640353167499, 0.002546087119332919, 0.003996827385642312, 0.0025177961694773667], 'topk_tokens': ['THE', ' Mary', '<|eot_id|>', ' the', '<|eot_id|>', ' to', '?\n', '.', ' Mary', 'Answer', '.\n\n', 'assistant', ' THE', 'THE', ' Bridge', '<|start_header_id|>', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0008885065714518229, 0.019293546676635742, 0.007584333419799805, 0.05214071273803711, 0.001234610875447591]}, 'saliency': {'score': [0.0027685915982281722, 0.00015522439278123763, 0.0009780797091397371, 0.00014715076310372813], 'topk_tokens': [' PA', ' bedroom', '.\n\n', 'RE', 'Bridge', '.', ' Father', ' Mary', '<|end_header_id|>', ' prior', '<|start_header_id|>', ' Dan', ' Mary', 'THE', ' THE', '.', ' Bridge', ':', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0003193020820617676, 0.004184067249298096, 0.0011981626351674397, 0.011020034551620483, 0.00010778506596883138]}}, 28: {'grad': {'score': [0.8542977792245371, 0.8013599774932171, 0.7582212505918561, 0.8013595030109399], 'topk_tokens': [' came', ' ', ' hands', 'being', ' returns', ' first', ' being', 'been', ' taken', ' balance', ' kept', ' become', 'hom', ' be', '600', 'hum', 'CH', ' acted', 'na', ' ins'], 'evidence_proportions': [0.6617838541666666, 1.21572265625, 0.6949055989583334, 0.99969482421875, 0.8080851236979166]}, 'weight': {'score': [0.01105170559000086, 0.0024210841975439087, 0.00555117383147731, 0.0023932960676987347], 'topk_tokens': ['.\n\n', 'Question', ' where', ' Mary', ' ', ' to', '<|eot_id|>', ' the', ' Mary', '<|eot_id|>', 'Answer', '?\n', ' Bridge', 'assistant', ' the', '<|end_header_id|>', '<|start_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00020807981491088867, 0.007970201969146728, 0.01596999168395996, 0.03667736053466797, 0.0024611949920654297]}, 'saliency': {'score': [0.0014281184585006148, 8.999331064309708e-05, 0.0003093372691761364, 8.641008915885819e-05], 'topk_tokens': [' apple', '<|start_header_id|>', ' to', 'SSION', ' Far', ' bedroom', 'Question', ' the', ' Mary', ' Bridge', 'Answer', ' to', 'Bridge', 'office', ' Mary', 'assistant', '<|end_header_id|>', ' Bridge', ':', '<|begin_of_text|>'], 'evidence_proportions': [4.244844118754069e-05, 0.00042848587036132814, 0.0022687514623006186, 0.005425944924354553, 0.00014096498489379883]}}, 29: {'grad': {'score': [0.8797675238715278, 0.8767435691235715, 1.0359982577237217, 0.8763025998182753], 'topk_tokens': ['ants', 'boat', ' ox', 'AILY', '\n', 'AM', ' mail', ' routes', ' o', 'mail', ' express', 'aper', ' the', ' printed', 'paper', 'ION', '\n', 'ATING', '\n', 'APER'], 'evidence_proportions': [1.471435546875, 0.43912353515625, 1.0803426106770833, 0.424468994140625, 0.7582600911458334]}, 'weight': {'score': [0.002831278023896394, 0.0025225326260835887, 0.0037336493983413234, 0.0025185416338316304], 'topk_tokens': [':', ' to', ' the', ' ', ' Where', '<|eot_id|>', '?\n', '<|eot_id|>', ' in', ' Does', ' was', ' where', 'Answer', ' the', 'assistant', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0008647839228312174, 0.0027068376541137694, 0.004839857419331868, 0.006195068359375, 0.0006503661473592123]}, 'saliency': {'score': [0.00032261565879539205, 9.914994249608107e-05, 0.00029511885209517044, 9.811709540388481e-05], 'topk_tokens': ['THE', '.', 'THE', 'Question', 'IVE', 'Answer', '?\n', ' to', ' the', 'Does', 'assistant', ' Does', ' the', ' where', ' was', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [6.16908073425293e-05, 0.000354456901550293, 0.0007064739863077799, 0.0005349218845367432, 3.1610329945882164e-05]}}, 30: {'grad': {'score': [1.0394742047345196, 1.050227091126778, 0.9987339828953599, 1.0503914801629035], 'topk_tokens': [' o', ' head', '2', ' Capt', ' o', 'ire', 'G', ' office', 'op', ' Paul', ' office', ' o', 'ob', 'office', ' o', 'op', ' o', ' o', ' o', ' O'], 'evidence_proportions': [1.0482177734375, 1.00771484375, 1.1663818359375, 0.5706501007080078, 1.2428385416666667]}, 'weight': {'score': [0.008416542300471553, 0.002440998842966534, 0.010523790662938898, 0.0024056297771636983], 'topk_tokens': ['<|eot_id|>', ':', ' prior', ' to', ' the', ' bathroom', ' the', '<|eot_id|>', 'Question', ' Where', '.\n\n', 'Answer', ' the', 'assistant', '?\n', '<|end_header_id|>', 'office', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0016341209411621094, 0.006534004211425781, 0.014062662919362387, 0.023041725158691406, 0.0013715028762817383]}, 'saliency': {'score': [0.001897399072293882, 0.00022986499159428024, 0.0014913605921196215, 0.00022270538034118386], 'topk_tokens': [':', ' office', ' the', ' office', ' ', '.', ' Congress', '.', 'Question', ' the', 'Gov', '?\n', ' Bridge', 'assistant', ' the', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', ':', 'office'], 'evidence_proportions': [0.001408924659093221, 0.0016086339950561524, 0.0035448471705118814, 0.003196895122528076, 0.0001127322514851888]}}, 31: {'grad': {'score': [1.1189959490740742, 1.3087595480401628, 0.7071454597241951, 1.3108232415117411], 'topk_tokens': ['ARCH', ' H', ' its', ' the', ' C', ' S', ' and', '7', ' was', ' S', ' United', ' Mr', ' U', ' B', ' M', 'G', ' L', ' most', ' W', 'editary'], 'evidence_proportions': [0.8920491536458334, 0.906201171875, 1.33154296875, 1.730712890625, 0.9029134114583334]}, 'weight': {'score': [0.0029011964797973633, 0.002303934132573558, 0.0027065620277867174, 0.0023015039248633147], 'topk_tokens': ['.', ' the', ' was', '.', ' where', 'Question', ':', '.\n\n', '<|eot_id|>', ' Where', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '<|start_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0009677608807881674, 0.002726316452026367, 0.004163503646850586, 0.00623011589050293, 0.0014987786610921223]}, 'saliency': {'score': [0.000976688332027859, 0.00014352854327485915, 0.00040282083280158764, 0.00014096290171072522], 'topk_tokens': ['.', 'ot', ' Ramsey', ' office', ' Mary', ' Ramsey', ' Where', 'light', ' Mary', 'Question', ' the', '<|end_header_id|>', '<|eot_id|>', '?\n', 'Answer', '<|start_header_id|>', ':', '<|begin_of_text|>', 'assistant', 'office'], 'evidence_proportions': [0.0009805162747701008, 0.0009425401687622071, 0.0011754234631856282, 0.001961946487426758, 0.00014574329058329263]}}, 'pred_res': "Mary's hand<|eot_id|>", 'score': 0}
2025-01-22 00:52:28.849 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:52:28.849 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-4_0-4-6-7-9.pkl | len: 10 |  size: 9.09 KB
Processing depth (0, 4, 6, 7, 9):   5%|‚ñå         | 5/100 [01:35<30:08, 19.03s/it]Processing depth (0, 4, 6, 7, 9):   5%|‚ñå         | 5/100 [01:35<30:14, 19.10s/it]
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:70 - Selected idx: 4
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:72 - Answer: bathroom
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:73 - Tag: 3-hop
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:74 - Needle: [' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:75 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 00:52:29.048 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.37it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.38it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.65it/s]
Processing depth (3, 5, 7, 9):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 00:52:35.865 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:52:35.876 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (3761, 3767) --> . Daniel journeyed to the
2025-01-22 00:52:35.877 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:52:35.894 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5925, 5930) --> . Daniel went to the
2025-01-22 00:52:35.895 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:52:35.918 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (8343, 8347) -->  Daniel left the apple
2025-01-22 00:52:35.919 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 00:52:35.951 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10691, 10697) --> . Mary journeyed to the
2025-01-22 00:52:35.952 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary got the football there.
2025-01-22 00:52:35.964 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4233, 4238) --> . Mary got the football
2025-01-22 00:52:35.964 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 00:52:35.984 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (6642, 6647) --> . Mary moved to the
2025-01-22 00:52:35.984 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:52:35.996 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3853, 3859) --> . John went back to the
2025-01-22 00:52:35.996 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:52:36.005 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3049, 3055) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:52:38.103 | INFO     | test_jbb_retain:begin_test:544 - The apple was left on the table.<|eot_id|>
2025-01-22 00:52:38.103 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12132])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.59s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.43s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.42s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.06it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.48it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.95it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.44it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.91it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.14it/s]
2025-01-22 00:52:47.588 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.18754827408563524, 0.244173317914349, 0.2236091440374201, 0.24430907194558413], 'topk_tokens': [' whistle', ' hundred', ' entrance', ' announced', ' appearance', ' examined', ' ', 'itter', ' comparison', 'announcement', 'ols', 'remark', ' and', 'consider', ' appearance', ' appearance', ' compl', ' absence', 'est', ' Arr'], 'evidence_proportions': [0.1719799041748047, 0.19912204742431638, 0.08639049530029297, 0.2609103520711263]}, 'weight': {'score': [0.027597942522593906, 0.002587870427694741, 0.012624878774989735, 0.0025261746207452395], 'topk_tokens': [' lounge', 'Answer', '.', ' top', 'Bridge', ' garden', ' bedroom', ' bathroom', ':', 'assistant', ' the', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'b', '<|eot_id|>', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.007431810100873312, 0.030186259746551515, 0.09062004089355469, 0.003592411677042643]}, 'saliency': {'score': [0.0011341671148935954, 2.9402305719338673e-05, 0.0003485517068342729, 2.6903021248846785e-05], 'topk_tokens': [':', '<|eot_id|>', 'THE', ' bathroom', '.', '.', ' Miles', '<|eot_id|>', 'Bridge', ' Bench', ' lounge', ' Daniel', ' Daniel', ' bedroom', 'b', ' top', ' bathroom', 'athroom', '<|begin_of_text|>', ' garden'], 'evidence_proportions': [0.00010930995146433513, 0.0018603265285491945, 0.00341719388961792, 3.187358379364014e-05]}}, 25: {'grad': {'score': [0.3859739757719494, 0.47350884096878865, 0.41826629638671875, 0.4737613689335537], 'topk_tokens': [' with', ' bogus', ' of', ' immense', ' You', ' with', ' self', ' a', ' free', ' incorporated', 'light', ' im', ' with', ' black', ' a', ' a', ' inverted', ' of', ' no', ' for'], 'evidence_proportions': [0.39337158203125, 0.42525939941406254, 0.47667694091796875, 0.285369873046875]}, 'weight': {'score': [0.02769976286661057, 0.0025112223615359514, 0.006687180562452836, 0.0024598801161649013], 'topk_tokens': [' Bench', 'b', ' Daniel', '.', ' Daniel', ' the', '.', ' bathroom', 'Answer', ' bathroom', ' apple', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0028970887263615923, 0.029991847276687623, 0.09813594818115234, 0.003634909788767497]}, 'saliency': {'score': [0.0014541688419523694, 2.5021727801253387e-05, 0.00010995295914736661, 2.2385226272408867e-05], 'topk_tokens': [' Dan', ' Ramsey', 'Den', 'THE', ':', 'assistant', ' Anthony', ' Daniel', '<|eot_id|>', ' Daniel', ' bathroom', ' top', ' Bench', ' bathroom', ' THE', 'athroom', '\n\n', '<|end_header_id|>', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [3.47594420115153e-05, 0.0008702516555786133, 0.006446145474910736, 3.219147523244222e-05]}}, 26: {'grad': {'score': [0.3521699451264881, 0.40290691311289656, 0.3892333290793679, 0.4030199047748592], 'topk_tokens': ['bec', '\n', ' I', 'issippi', ' when', ' and', ' Empire', ',\n', 'watch', ',\n', '\n', ' Eagle', ',', 'RI', ' it', ' and', 'hue', ' Press', 'itter', ' bitter'], 'evidence_proportions': [0.3753458658854167, 0.408544921875, 0.2672119140625, 0.338653564453125]}, 'weight': {'score': [0.025394690888268606, 0.0024940281676617384, 0.0033663416450673885, 0.0024526698469839623], 'topk_tokens': [' garden', 'Bridge', ' apple', ' Daniel', ' apple', ' \n', 'b', '<|eot_id|>', 'Answer', 'assistant', '<|eot_id|>', ' bathroom', ' bathroom', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0012907832860946655, 0.018523383140563964, 0.09994316101074219, 0.005525708198547363]}, 'saliency': {'score': [0.0017914871374766033, 3.2135528647757185e-05, 0.00012708793986927378, 2.890733340856771e-05], 'topk_tokens': [' garden', ' \n', ' Bridge', 'Answer', ' bathroom', ' Anthony', '<|start_header_id|>', 'athroom', 'assistant', 'Bridge', ' bathroom', ' the', ' apple', ':', ' Daniel', '<|end_header_id|>', ' Daniel', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [2.6861826578776043e-05, 0.002227020263671875, 0.006354600191116333, 0.00015109280745188397]}}, 27: {'grad': {'score': [0.20712457384381974, 0.20087913761524, 0.19221982088955966, 0.20088404588576306], 'topk_tokens': ['ically', 'ottle', 'est', ' was', ' would', ' be', 'command', '-n', 'str', ' would', ' business', ' was', ' was', ' step', 'ly', ' medicine', ' would', ' dre', ' Bottle', '.'], 'evidence_proportions': [0.17790603637695312, 0.2101470947265625, 0.22322845458984375, 0.22308842341105145]}, 'weight': {'score': [0.02863949111529759, 0.002539182112056825, 0.004173288291150873, 0.00249088101835784], 'topk_tokens': ['THE', ' Daniel', 'THE', '<|eot_id|>', ' \n', ' THE', 'Answer', ' apple', ' lounge', 'assistant', '.\n\n', ' bathroom', '<|start_header_id|>', 'b', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0016567111015319824, 0.02871193289756775, 0.10735654830932617, 0.0030838648478190107]}, 'saliency': {'score': [0.002169993661698841, 3.846543877518712e-05, 0.00020478665828704834, 3.446104252058427e-05], 'topk_tokens': [' garden', ' Bridge', '<|begin_of_text|>', ' \n', ' apple', ' lounge', ' top', ' Daniel', ' THE', ' bathroom', ' the', ' Daniel', 'THE', ' bathroom', 'THE', '<|end_header_id|>', '.\n\n', ':', 'athroom', 'b'], 'evidence_proportions': [0.00013148784637451172, 0.002270418405532837, 0.008269920945167542, 5.819400151570638e-05]}}, 28: {'grad': {'score': [0.20285324823288692, 0.26792631962685415, 0.23265907981178977, 0.2681034957577841], 'topk_tokens': ['\n', 'ew', 'E', ' RID', 'ib', 'nes', ' ', ' Cedar', 'ot', 'arp', ' half', 'inen', ' a', 'dent', '.', 'S', ' half', ' ', 'half', '.'], 'evidence_proportions': [0.24240938822428384, 0.15744171142578126, 0.16153430938720703, 0.2286860148111979]}, 'weight': {'score': [0.0203055739402771, 0.0024108584938002165, 0.0033385577526959505, 0.0023780931606814833], 'topk_tokens': ['.\n\n', ' bathroom', ' Daniel', ' bathroom', ' the', ' apple', ' garden', ' \n', '<|eot_id|>', ' before', 'Answer', '<|eot_id|>', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.006088381012280783, 0.01467939019203186, 0.05386161804199219, 0.016840557257334392]}, 'saliency': {'score': [0.0008761371885027204, 2.4026764927452664e-05, 7.527524774724787e-05, 2.2453676479130125e-05], 'topk_tokens': ['?', ' the', '<|eot_id|>', ' Daniel', ' before', ' Bridge', ' the', ' garden', '<|start_header_id|>', ' apple', 'assistant', ':', 'b', ' Bridge', 'athroom', '<|end_header_id|>', 'Bridge', ' Daniel', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [9.136398633321127e-05, 0.0006863653659820556, 0.0034818723797798157, 8.189678192138672e-05]}}, 29: {'grad': {'score': [0.231819334484282, 0.24417738692476565, 0.24945155057040128, 0.24418925324142512], 'topk_tokens': ['d', 'd', 'adv', 'pend', 'eff', ' awake', 'super', 's', ' S', 'ION', 'st', ' extra', 'ys', 'assistant', ' a', ' THE', 'Emp', 'tal', ' M', ' ga'], 'evidence_proportions': [0.17043368021647137, 0.23677520751953124, 0.23343658447265625, 0.28799692789713544]}, 'weight': {'score': [0.011657300449552991, 0.002489081556825516, 0.002527598630298268, 0.002473089167447111], 'topk_tokens': [' apple', 'THE', ' Does', '?', ' \n', '<|eot_id|>', ' before', '.\n\n', ' the', '<|eot_id|>', ' the', 'Answer', 'b', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0018585970004399617, 0.009111136198043823, 0.04180908203125, 0.0034766197204589844]}, 'saliency': {'score': [0.0002371526899791899, 2.7665440614792265e-05, 8.231401443481445e-05, 2.72022003848309e-05], 'topk_tokens': ['Question', ' the', ' number', 'Answer', '<|eot_id|>', ' before', ' the', 'THE', 'Does', 'b', '<|eot_id|>', ' Does', 'THE', 'athroom', 'assistant', '<|end_header_id|>', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010793407758076985, 0.00011733174324035645, 0.000825345516204834, 7.409354050954184e-05]}}, 30: {'grad': {'score': [0.27591505504789804, 0.28854383445837195, 0.2785960110751065, 0.2885838655931761], 'topk_tokens': ['Emp', ' Loan', ' Republicans', ' an', ' the', ' forb', ' Million', '186', ' account', ' Europe', ' the', ' LINE', ' its', 'ar', ' an', ' of', '2', 'ire', ' account', 'deal'], 'evidence_proportions': [0.2865130106608073, 0.25315093994140625, 0.4277458190917969, 0.18306668599446613]}, 'weight': {'score': [0.024139598721549624, 0.0024834003770700147, 0.006346793337301774, 0.002438761375220927], 'topk_tokens': [' Anthony', ' Miles', ' the', '?', ' garden', '.\n\n', 'Question', ' the', 'b', '<|eot_id|>', '<|eot_id|>', 'Answer', ' \n', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0031241973241170245, 0.02610495686531067, 0.07845306396484375, 0.00730822483698527]}, 'saliency': {'score': [0.002105209089460827, 4.4568291307723614e-05, 0.000425078652121804, 4.0297311759334186e-05], 'topk_tokens': [' the', ' Daniel', 'b', ' Anthony', ' Bench', ' the', ' Broadway', ' bathroom', '.\n\n', 'Bridge', 'assistant', ' apple', ' bathroom', '<|start_header_id|>', ' the', '<|begin_of_text|>', ' Bridge', '<|end_header_id|>', 'athroom', ':'], 'evidence_proportions': [0.0001423110564549764, 0.0013924717903137207, 0.008664906024932861, 0.00028892358144124347]}}, 31: {'grad': {'score': [0.2811686723005204, 0.27419718713685715, 0.2782418511130593, 0.2741777210604502], 'topk_tokens': ['did', ' the', ' the', ' the', ' the', ' the', ' the', ' location', ' the', ' the', ' department', ' population', ' the', ' apple', ' the', ' the', ' the', ' location', ' the', ' the'], 'evidence_proportions': [0.2787730097770691, 0.2793514788150787, 0.344818115234375, 0.24264570077260336]}, 'weight': {'score': [0.0036740530104864212, 0.0022910210444792634, 0.001438883217898282, 0.0022901695195783893], 'topk_tokens': [' apple', ':', ' Where', '?', ' the', 'Question', ' before', '.\n\n', '<|eot_id|>', ' \n', 'Answer', '<|start_header_id|>', 'b', 'assistant', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010698139667510986, 0.003782403469085693, 0.009210824966430664, 0.002496818701426188]}, 'saliency': {'score': [0.00019655199277968634, 1.6926456784632258e-05, 2.2148544138128106e-05, 1.6605002750752582e-05], 'topk_tokens': [' before', ' garden', ' the', '<|eot_id|>', '<|begin_of_text|>', ' Market', ' the', ' Daniel', 'Question', ' apple', ' the', ' the', ' \n', 'Answer', '<|start_header_id|>', ':', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [4.1867295900980636e-05, 0.00023424625396728516, 0.0006036087870597839, 4.845360914866129e-05]}}, 'pred_res': 'The apple was left on the table.<|eot_id|>', 'score': 0}
2025-01-22 00:52:47.589 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:52:47.589 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-0_3-5-7-9.pkl | len: 10 |  size: 8.69 KB
Processing depth (3, 5, 7, 9):   1%|          | 1/100 [00:18<30:26, 18.45s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.43it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.41it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.41it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.88it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.68it/s]
Processing depth (2, 7, 8, 9):   1%|          | 1/100 [00:25<30:26, 18.45s/it]2025-01-22 00:52:54.663 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:52:54.671 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2454, 2460) --> . Daniel journeyed to the
2025-01-22 00:52:54.671 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:52:54.696 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (8336, 8341) --> . Daniel went to the
2025-01-22 00:52:54.696 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:52:54.723 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (9585, 9589) -->  left the apple.
2025-01-22 00:52:54.723 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 00:52:54.755 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (10691, 10697) --> . Mary journeyed to the
2025-01-22 00:52:54.755 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary got the football there.
2025-01-22 00:52:54.768 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4233, 4238) --> . Mary got the football
2025-01-22 00:52:54.768 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 00:52:54.787 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (6636, 6641) --> . Mary moved to the
2025-01-22 00:52:54.787 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:52:54.799 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3853, 3859) --> . John went back to the
2025-01-22 00:52:54.799 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:52:54.809 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3056, 3062) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:52:57.002 | INFO     | test_jbb_retain:begin_test:544 - The apple was in Daniel's hand.<|eot_id|>
2025-01-22 00:52:57.002 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12132])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:40,  5.73s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.49s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.45s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.45it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.91it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.40it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.88it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.11it/s]
2025-01-22 00:53:06.791 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.15317040398007348, 0.2408431623322904, 0.16218809647993607, 0.24113852615747633], 'topk_tokens': [' Arr', 'est', ' whistle', 'adv', ' whistle', ' in', 'ad', 'com', ' in', ' announced', 'itter', 'remark', ' emb', 'G', ' ', ' Do', 'vent', ' STE', ' compl', 'consider'], 'evidence_proportions': [0.16779581705729166, 0.15223140716552735, 0.13299942016601562, 0.15277481079101562]}, 'weight': {'score': [0.013136408158711024, 0.0025928381003123228, 0.004925837570970709, 0.0025702826124210845], 'topk_tokens': [' Father', '\n\n', ' Anthony', ' \n', ' Broadway', 'Bridge', 'Answer', '.', ' bedroom', ' bathroom', ':', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'b', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003597189982732137, 0.01641799211502075, 0.04273796081542969, 0.003444075584411621]}, 'saliency': {'score': [0.000531041905993507, 3.5061340642456756e-05, 0.0001538450067693537, 3.398386524325367e-05], 'topk_tokens': [' Eighth', ' Anthony', ' Miles', '.', 'Bridge', '\n\n', 'assistant', ' Market', '<|eot_id|>', ':', '\n\n', '<|eot_id|>', ' Daniel', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom', '.', 'athroom', 'b'], 'evidence_proportions': [1.5060106913248699e-05, 0.0013848662376403807, 0.0009912103414535522, 2.8724471728006996e-05]}}, 25: {'grad': {'score': [0.4086229233514695, 0.4615425301619798, 0.38094295154918323, 0.4617810765126664], 'topk_tokens': [' with', ' free', ' for', ' old', ' with', ' the', ' black', ' incorporated', ' the', ' printing', ' old', ' with', 'posit', ' with', ' of', ' a', ' self', ' of', ' for', ' no'], 'evidence_proportions': [0.4586893717447917, 0.43473129272460936, 0.48842620849609375, 0.28359731038411456]}, 'weight': {'score': [0.015169258628572737, 0.0025152453189530785, 0.0024586225097829647, 0.0024933722973106477], 'topk_tokens': [' Daniel', ' Ramsey', '.\n\n', ' \n', 'b', ' Anthony', ' apple', ' bathroom', 'Answer', ' Bench', '.', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0004201829433441162, 0.023895543813705445, 0.043593406677246094, 0.0036969979604085284]}, 'saliency': {'score': [0.0011319390365055629, 3.178959822134074e-05, 5.060705271634188e-05, 2.9844748551901546e-05], 'topk_tokens': ['Den', ' Daniel', ' Market', ' Ramsey', ':', '<|start_header_id|>', ' bathroom', '<|eot_id|>', ' Dan', '<|eot_id|>', ' Daniel', 'athroom', ' Bench', ' Ramsey', '.', ' apple', ' Anthony', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.114443143208821e-05, 0.0017624855041503905, 0.003593139350414276, 3.647804260253906e-05]}}, 26: {'grad': {'score': [0.35706147693452384, 0.37072771007159044, 0.36768507957458496, 0.37075697973474064], 'topk_tokens': [' B', ' bonds', 'pro', ' Bench', ' branches', ' bonds', ' bouncing', ' began', 'b', ' bend', ' bonds', 'RI', 'b', 'ree', 'bec', 'b', 'b', ' broad', 'itter', ' bitter'], 'evidence_proportions': [0.37978108723958337, 0.34543457031249997, 0.336090087890625, 0.3580118815104167]}, 'weight': {'score': [0.014419528700056531, 0.002506271585496699, 0.001432789997621016, 0.002487535081653456], 'topk_tokens': [' Ramsey', ' Bench', ' the', ' apple', ' apple', ' the', ' Anthony', 'b', '<|eot_id|>', ' \n', 'Answer', 'assistant', '<|eot_id|>', ' bathroom', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [8.42660665512085e-05, 0.01907958984375, 0.04610013961791992, 0.0037509997685750323]}, 'saliency': {'score': [0.0011026632218133835, 3.737758647084089e-05, 4.515187306837602e-05, 3.551337603027364e-05], 'topk_tokens': [' Bridge', ' Bridge', 'Answer', ' Charles', '.', ' Bench', ' bathroom', 'Bridge', 'assistant', '<|start_header_id|>', ' the', 'athroom', ' apple', ' Daniel', '\n\n', ':', '<|end_header_id|>', ' Anthony', 'b', '<|begin_of_text|>'], 'evidence_proportions': [7.500251134236654e-06, 0.002360290288925171, 0.0025752410292625427, 0.00016808509826660156]}}, 27: {'grad': {'score': [0.22147841680617558, 0.19127422595250052, 0.20915291526100852, 0.19118924256077752], 'topk_tokens': [' concerned', ' ', ' A', ' heard', ' designated', ' department', 'city', ' Met', '185', ' medicine', ' Met', ' was', 'imm', ' step', ' heard', 'ly', ' STR', ' would', 'direction', 'str'], 'evidence_proportions': [0.22584788004557294, 0.2481231689453125, 0.24945735931396484, 0.1762523651123047]}, 'weight': {'score': [0.010711891310555595, 0.002547321940431096, 0.002210420641032132, 0.002533755604987345], 'topk_tokens': [' bedroom', ' Broadway', ' garden', ' apple', ' \n', '<|eot_id|>', 'Answer', '<|eot_id|>', ' Anthony', '.', 'assistant', '.\n\n', '<|start_header_id|>', 'b', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00024051964282989502, 0.016572600603103636, 0.03144252300262451, 0.0024789174397786455]}, 'saliency': {'score': [0.0006274836403982979, 4.433697764157365e-05, 0.00010434063998135653, 4.321506559316425e-05], 'topk_tokens': ['THE', ' \n', 'Lu', '<|eot_id|>', ' Broadway', ' bedroom', ' garden', ' Daniel', ' Ramsey', 'assistant', ' Daniel', '<|begin_of_text|>', '\n\n', ' bathroom', '.', '.\n\n', '<|end_header_id|>', 'athroom', ':', 'b'], 'evidence_proportions': [8.414189020792643e-06, 0.001988428831100464, 0.0007023438811302185, 6.252527236938477e-05]}}, 28: {'grad': {'score': [0.2748877207438151, 0.3650035932027709, 0.2831296053799716, 0.3653090564060243], 'topk_tokens': ['\n', ' the', 'en', '185', 'G', ',', 'F', ' James', 'E', 'arp', ' Central', ' RID', 'ib', ' ', '.', 'inen', '.', '.', 'S', ' Cedar'], 'evidence_proportions': [0.3015543619791667, 0.18657951354980468, 0.20456886291503906, 0.36869049072265625]}, 'weight': {'score': [0.010087536914008004, 0.002438752858492392, 0.0024663453752344303, 0.0024254191254015752], 'topk_tokens': [' bathroom', ' the', '?', ' to', ' garden', 'Answer', '<|eot_id|>', ' apple', ' garden', ' before', ' \n', 'assistant', '<|eot_id|>', 'b', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [5.1905711491902665e-05, 0.018465036153793336, 0.014380455017089844, 0.010279973347981771]}, 'saliency': {'score': [0.0002743956588563465, 2.858661603279255e-05, 2.4760311300104316e-05, 2.816668457437579e-05], 'topk_tokens': [' garden', ' \n', '.', ' to', ' Daniel', ' apple', '<|eot_id|>', ' before', 'athroom', 'b', ' garden', 'Bridge', '<|start_header_id|>', ' Bridge', 'assistant', ':', ' Bridge', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.9802322387695312e-06, 0.0007779896259307861, 0.00028889626264572144, 0.00011648237705230713]}}, 29: {'grad': {'score': [0.241656984601702, 0.25629482953009114, 0.23616669394753195, 0.2563568716841031], 'topk_tokens': [' not', ' facilities', ' bogus', 'ib', ' a', ' extra', ' wholly', 'eff', ' requis', 'goods', 'adv', 'cret', 'pend', ' fer', ' epith', 'assistant', ' faithfully', ' bogus', 'tal', ' ga'], 'evidence_proportions': [0.26312001546223956, 0.27004470825195315, 0.1628861427307129, 0.24905141194661456]}, 'weight': {'score': [0.007720642146610078, 0.0025086650840529796, 0.0013451860709623857, 0.002501730335456742], 'topk_tokens': ['Does', ' Does', '?', ' apple', ' the', '<|eot_id|>', '.\n\n', ' \n', '<|eot_id|>', ' the', ' before', 'Answer', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [5.764265855153401e-05, 0.014447784423828125, 0.018632888793945312, 0.0025028586387634277]}, 'saliency': {'score': [9.578892162867955e-05, 2.0511591439390556e-05, 5.025484345176003e-05, 2.032674398005817e-05], 'topk_tokens': ['THE', ' garden', ' the', '<|eot_id|>', 'Question', ' before', '<|eot_id|>', 'DW', 'Does', ' the', 'Answer', ' Does', 'athroom', 'assistant', 'b', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>', ':'], 'evidence_proportions': [1.5348196029663086e-06, 0.00018345713615417478, 0.00016580522060394287, 7.030864556630453e-05]}}, 30: {'grad': {'score': [0.3173408508300781, 0.3531341188082638, 0.3329370672052557, 0.35323302666162204], 'topk_tokens': [' S', 'AM', 'membership', '186', ' Burb', ' the', 'g', ' two', ' of', ' account', ' LINE', 'ar', 'b', ' its', ' forb', '2', 'b', ' account', 'b', 'deal'], 'evidence_proportions': [0.30094273885091144, 0.2596580505371094, 0.5473690032958984, 0.22845586140950522]}, 'weight': {'score': [0.013407114006224134, 0.00250928955015254, 0.0035900581966746936, 0.0024883971234405827], 'topk_tokens': [' bathroom', ' before', ' the', '.\n\n', ' the', '?', ' garden', ' Anthony', 'b', '<|eot_id|>', 'Answer', '<|eot_id|>', ' \n', 'assistant', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00020941098531087238, 0.0309961199760437, 0.024810791015625, 0.004344860712687174]}, 'saliency': {'score': [0.0011428012734367734, 4.71564418846165e-05, 0.0001356615261598067, 4.5092626691377206e-05], 'topk_tokens': [' Ramsey', ' Broadway', ' midnight', ' apple', ' \n', ' bathroom', '.', ' Bridge', ' Bench', '.\n\n', ' Anthony', ' Bridge', 'assistant', ' the', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'b', ':'], 'evidence_proportions': [1.619259516398112e-05, 0.0034605324268341064, 0.0015152916312217712, 8.964041868845622e-05]}}, 31: {'grad': {'score': [0.25550263268607004, 0.2498914466358086, 0.252577006816864, 0.24987681570370152], 'topk_tokens': [' could', ' population', ' location', ' increasing', ' the', ' the', ' location', ' the', ' location', 'had', ' the', ' him', ' the', 'did', ' August', ' location', 'membership', ' the', ' department', ' apple'], 'evidence_proportions': [0.2853660583496094, 0.25104475021362305, 0.3070693016052246, 0.1949763298034668]}, 'weight': {'score': [0.002110968033472697, 0.002312246054644245, 0.0012923370708118785, 0.0023144512180654254], 'topk_tokens': [':', ' apple', ' Where', '.\n\n', ' the', '?', ' before', ' the', '<|eot_id|>', 'Answer', ' \n', 'b', '<|start_header_id|>', 'assistant', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00022066136201222736, 0.0022144198417663576, 0.00429224967956543, 0.002460877100626628]}, 'saliency': {'score': [0.00011483544395083473, 1.9009718936217583e-05, 2.2744590585882012e-05, 1.8836504629105477e-05], 'topk_tokens': [' was', ' Market', '<|eot_id|>', '?', ' garden', ' Daniel', ' the', ' the', 'Answer', '<|begin_of_text|>', ' the', ' apple', 'athroom', ' the', ' \n', '<|end_header_id|>', '<|start_header_id|>', ':', 'b', 'assistant'], 'evidence_proportions': [2.3946166038513184e-05, 0.0003392279148101807, 8.51750373840332e-05, 3.8504600524902344e-05]}}, 'pred_res': "The apple was in Daniel's hand.<|eot_id|>", 'score': 0}
2025-01-22 00:53:06.792 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:53:06.792 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-1_2-7-8-9.pkl | len: 10 |  size: 8.67 KB
Processing depth (2, 7, 8, 9):   2%|‚ñè         | 2/100 [00:37<30:51, 18.89s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.41it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.40it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.41it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.88it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.67it/s]
Processing depth (0, 4, 5, 7):   2%|‚ñè         | 2/100 [00:44<30:51, 18.89s/it]2025-01-22 00:53:13.642 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:53:13.642 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:53:13.643 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:53:13.657 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (4852, 4857) --> . Daniel went to the
2025-01-22 00:53:13.657 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:53:13.674 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5932, 5936) -->  Daniel left the apple
2025-01-22 00:53:13.674 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 00:53:13.699 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8347, 8353) --> . Mary journeyed to the
2025-01-22 00:53:13.699 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary got the football there.
2025-01-22 00:53:13.712 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4233, 4238) --> . Mary got the football
2025-01-22 00:53:13.712 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 00:53:13.731 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (6647, 6652) --> . Mary moved to the
2025-01-22 00:53:13.732 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:53:13.743 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3853, 3859) --> . John went back to the
2025-01-22 00:53:13.743 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:53:13.753 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3056, 3062) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:53:15.984 | INFO     | test_jbb_retain:begin_test:544 - The apple was left in the garden.<|eot_id|>
2025-01-22 00:53:15.984 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12132])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.59s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.42s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:07,  1.41s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.07it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.44it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.80it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:07<00:00,  2.15it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.46it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.09it/s]
2025-01-22 00:53:26.140 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.24062638055710567, 0.22380319791022868, 0.25542103160511365, 0.22371645633093065], 'topk_tokens': ['use', ' announced', 'remark', ' S', ' or', ' S', ' first', ' compl', ' S', ' comparison', ' hundred', 'vent', ' and', ' or', 'itter', 'est', 'consider', 'deal', 'ols', ' Do'], 'evidence_proportions': [0.3041890462239583, 0.16633758544921876, 0.2108612060546875, 0.25881449381510413]}, 'weight': {'score': [0.021077417192004975, 0.0025938407354115164, 0.011730089783668518, 0.002545118225930031], 'topk_tokens': [' the', 'Daniel', ' Daniel', ' apple', ' barric', ' bathroom', '<|start_header_id|>', ':', ' Bridge', ' composing', 'b', 'Bridge', '<|eot_id|>', 'assistant', ' bedroom', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.012249191602071125, 0.0032850265502929687, 0.08253288269042969, 0.003762324651082357]}, 'saliency': {'score': [0.0011901784510839554, 3.38881753529351e-05, 0.00028202615000984884, 3.1428604460377745e-05], 'topk_tokens': ['<|start_header_id|>', '<|eot_id|>', ' top', ' Bench', ':', ' building', ' bathroom', '<|eot_id|>', ' garden', 'b', ' bedroom', ' composing', ' Bridge', ' Dan', 'Bridge', ' bedroom', 'Daniel', ' Daniel', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0004998445510864258, 0.00012673139572143554, 0.005261361598968506, 5.259613196055094e-05]}}, 25: {'grad': {'score': [0.5010525839669364, 0.4257414752974351, 0.44185829162597656, 0.4255813609043415], 'topk_tokens': [' York', ' a', ' with', ' bogus', ' incorporated', ' a', ' of', ' the', ' for', ' set', ' bogus', ' free', ' at', ' a', ' of', ' black', ' for', ' no', ' bogus', ' inverted'], 'evidence_proportions': [0.502655029296875, 0.533062744140625, 0.5503654479980469, 0.43989976247151696]}, 'weight': {'score': [0.01741002571015131, 0.0025175306008193145, 0.0046847787770358, 0.002487723963606882], 'topk_tokens': [' barric', 'Daniel', ' bathroom', 'b', ' the', ' \n', ' Daniel', 'Answer', ' Bench', ' apple', ' Dan', '<|start_header_id|>', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.003337860107421875, 0.006760787963867188, 0.06769776344299316, 0.006831397612889608]}, 'saliency': {'score': [0.0013550676050640288, 3.064893919791908e-05, 0.00011503425511446866, 2.819531126760545e-05], 'topk_tokens': [' barric', ':', 'Answer', ' THE', ' Met', ' composing', 'Den', ' Press', ' Daniel', '<|eot_id|>', '<|eot_id|>', 'athroom', ' Bench', 'Daniel', ' Daniel', '<|end_header_id|>', ' apple', ' Dan', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.666647434234619e-05, 0.000864100456237793, 0.005803100764751434, 7.725258668263753e-05]}}, 26: {'grad': {'score': [0.3030885968889509, 0.3594720124510713, 0.3287714177911932, 0.35962578889908003], 'topk_tokens': [' Milwaukee', ' favor', 'Johnson', ' when', ' Eagle', 'field', ' and', ' compl', 'b', 'b', 'bec', 'ian', 'b', 'b', 'issippi', 'RI', 'hue', ' Press', ' bitter', 'itter'], 'evidence_proportions': [0.2853075663248698, 0.37548675537109377, 0.2503681182861328, 0.295684814453125]}, 'weight': {'score': [0.021230422315143403, 0.002505090632554578, 0.004077166318893433, 0.0024697104117115563], 'topk_tokens': ['?', ' Daniel', ' apple', ' garden', 'b', '<|eot_id|>', ' apple', ' \n', 'Answer', '<|eot_id|>', ' barric', 'assistant', ' bathroom', ' the', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01223742961883545, 0.0042901277542114254, 0.0796365737915039, 0.005402892827987671]}, 'saliency': {'score': [0.0010227390698024205, 2.7161694182060755e-05, 0.00011224096471613103, 2.5277897552902873e-05], 'topk_tokens': [' garden', ' old', ' barric', ' bathroom', 'Bridge', ' Bridge', 'Answer', ' the', ' apple', ' Daniel', '<|start_header_id|>', ' Dan', 'assistant', 'athroom', ' apple', '<|end_header_id|>', '\n\n', ':', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00019094347953796387, 0.0003473401069641113, 0.0043968260288238525, 0.00016797582308451334]}}, 27: {'grad': {'score': [0.16290651048932756, 0.24086887405200608, 0.16132536801424893, 0.24114899039071322], 'topk_tokens': [' be', ' ideas', ' was', ' was', '-n', ' during', ' was', ' was', ' being', ' be', ' was', ' was', ' was', ' STR', 'str', ' were', ' step', 'ly', ' would', ' was'], 'evidence_proportions': [0.08265829086303711, 0.2967376708984375, 0.13232421875, 0.15201695760091144]}, 'weight': {'score': [0.025559575784774052, 0.0025466008887744807, 0.004852296276526017, 0.0025024396440385785], 'topk_tokens': [' the', ' THE', 'Answer', ' \n', 'Daniel', ' the', ' apple', ' garden', ' apple', '<|start_header_id|>', '.\n\n', 'assistant', 'b', ' barric', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01595138510068258, 0.007099199295043945, 0.09587812423706055, 0.003672381242116292]}, 'saliency': {'score': [0.002034381741569156, 3.852911529886806e-05, 0.0006129741668701172, 3.40178106109551e-05], 'topk_tokens': [' Bridge', ' apple', ' double', ' \n', '<|start_header_id|>', ' THE', ' the', 'NEW', '<|end_header_id|>', '<|begin_of_text|>', 'assistant', 'Daniel', ' Dan', ' bathroom', ' the', ' apple', ':', 'athroom', 'b', '.\n\n'], 'evidence_proportions': [0.0004400114218393962, 0.0004844009876251221, 0.009194672107696533, 0.00014687577883402508]}}, 28: {'grad': {'score': [0.21839359828404017, 0.23617609791859806, 0.23426943475549872, 0.23621044948011924], 'topk_tokens': [' ', ' returns', 'arp', ' prepared', ' sample', ' lie', 'na', 'S', ' lie', 'nes', ' as', ' inside', 'dent', 'ot', 'about', '.', ' half', 'half', ' half', 'ball'], 'evidence_proportions': [0.2510306040445964, 0.16412200927734374, 0.2543659210205078, 0.2070013682047526]}, 'weight': {'score': [0.014808907395317442, 0.0024287240712496296, 0.005198149518533187, 0.00240218493714066], 'topk_tokens': [' the', ' the', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', ' garden', ' bathroom', '?', ' before', 'assistant', 'b', ' apple', '<|start_header_id|>', ' the', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004805634419123331, 0.000898289680480957, 0.052071571350097656, 0.011562585830688477]}, 'saliency': {'score': [0.0002632652010236468, 2.9899497360273503e-05, 0.0001239776611328125, 2.932305017370993e-05], 'topk_tokens': [' the', '<|eot_id|>', ' bathroom', ' the', 'athroom', ' before', 'assistant', ' Bridge', '?', 'Bridge', ' garden', 'b', '<|start_header_id|>', ' apple', '<|end_header_id|>', ' Bridge', '<|begin_of_text|>', '\n\n', ' the', ':'], 'evidence_proportions': [9.832779566446939e-05, 4.0638446807861325e-05, 0.0010269954800605774, 0.00010457138220469157]}}, 29: {'grad': {'score': [0.23646703220549084, 0.23340714890425424, 0.17942671342329544, 0.23350004602890317], 'topk_tokens': [' o', ' a', ' extra', 'Emp', 'goods', 're', ' Dr', 'ION', 'tal', 'st', 'g', ' extra', ' M', ' com', ' not', 'ys', 's', ' l', 'cret', ' ga'], 'evidence_proportions': [0.29376475016276044, 0.2317169189453125, 0.14198064804077148, 0.2461186647415161]}, 'weight': {'score': [0.015832935060773577, 0.0024837527689641034, 0.0031681453639810734, 0.002459324265389974], 'topk_tokens': ['.', '<|eot_id|>', ' \n', ' the', '.\n\n', ' apple', 'Answer', ' apple', ' the', '?', ' before', 'b', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.009677122036616007, 0.004504978656768799, 0.05333590507507324, 0.0064267317454020185]}, 'saliency': {'score': [0.00032641348384675526, 2.07057080742357e-05, 6.408176638863303e-05, 2.0095872102176516e-05], 'topk_tokens': ['      ', ' Where', ' the', '<|eot_id|>', ' garden', '***', 'Answer', ' Does', ' a', ' before', ' the', '<|eot_id|>', 'b', 'athroom', '<|end_header_id|>', 'assistant', '<|start_header_id|>', ':', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.3179918924967446e-05, 0.00014937520027160644, 0.0013083815574645996, 0.0001125335693359375]}}, 30: {'grad': {'score': [0.2013341812860398, 0.3289381582844046, 0.2585671164772727, 0.3292877982477459], 'topk_tokens': [' Loan', ' Republicans', 'ar', ' its', ' Times', ' Europe', 'ire', ' Times', ' forb', ' United', ' of', 'AM', '2', ' account', 'b', 'SSION', 'deal', ' LINE', 'b', 'b'], 'evidence_proportions': [0.21754201253255206, 0.23619575500488282, 0.2734956741333008, 0.10796737670898438]}, 'weight': {'score': [0.023740622259321668, 0.0024758334605842155, 0.006876260042190552, 0.0024308971432199394], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' the', ' apple', '<|eot_id|>', ' barric', ' garden', ' bathroom', '?', 'Answer', 'b', ' the', ' \n', 'assistant', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009177784125010172, 0.004188132286071777, 0.08223152160644531, 0.015603269139925638]}, 'saliency': {'score': [0.00201416015625, 5.065242498560506e-05, 0.0002117996866052801, 4.694924088767377e-05], 'topk_tokens': [' the', '.', ' the', ' barric', '?', ' Daniel', ' \n', ' the', ' the', '<|start_header_id|>', ' apple', ' Bridge', ' bathroom', 'assistant', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', 'b', ':', 'athroom'], 'evidence_proportions': [0.00210513174533844, 0.00048026442527770996, 0.005452141165733337, 0.0009094476699829102]}}, 31: {'grad': {'score': [0.2063832055954706, 0.2195868637908571, 0.2098064747723666, 0.21962758868173618], 'topk_tokens': [' apple', ' January', ' him', ' the', ' the', ' the', ' August', 'did', 'had', ' location', ' the', ' the', ' the', ' location', 'nes', 'If', 'nes', ' location', 'membership', ' department'], 'evidence_proportions': [0.16852029164632162, 0.2830493927001953, 0.28009748458862305, 0.13121477762858072]}, 'weight': {'score': [0.0036256185599735807, 0.002281591496351672, 0.001702338457107544, 0.002280311228284132], 'topk_tokens': [' was', ' before', '.\n\n', ':', ' apple', ' Where', '<|eot_id|>', '?', ' the', 'Answer', ' \n', '<|start_header_id|>', '<|eot_id|>', 'b', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001404831806818644, 0.0005992412567138671, 0.010226964950561523, 0.003967488805452983]}, 'saliency': {'score': [0.0001649345670427595, 1.4322880712319757e-05, 2.3473392833362926e-05, 1.4044667291909388e-05], 'topk_tokens': [' garden', 'Daniel', ' Where', 'Question', ' Key', ' the', '<|eot_id|>', ' \n', ' the', ' Daniel', 'Answer', ' apple', ' the', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>', '<|start_header_id|>', 'b', 'assistant'], 'evidence_proportions': [4.0461619695027665e-05, 6.259083747863769e-05, 0.0006681010127067566, 3.9249658584594727e-05]}}, 'pred_res': 'The apple was left in the garden.<|eot_id|>', 'score': 0}
2025-01-22 00:53:26.142 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:53:26.142 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-2_0-4-5-7.pkl | len: 10 |  size: 8.66 KB
Processing depth (0, 4, 5, 7):   3%|‚ñé         | 3/100 [00:57<30:52, 19.10s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.27it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.33it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.78it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.56it/s]
Processing depth (0, 2, 3, 7):   3%|‚ñé         | 3/100 [01:03<30:52, 19.10s/it]2025-01-22 00:53:33.225 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:53:33.226 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:53:33.226 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:53:33.233 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2461, 2466) --> . Daniel went to the
2025-01-22 00:53:33.234 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:53:33.244 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3775, 3779) -->  Daniel left the apple
2025-01-22 00:53:33.244 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 00:53:33.270 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8347, 8353) --> . Mary journeyed to the
2025-01-22 00:53:33.270 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary got the football there.
2025-01-22 00:53:33.283 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4244, 4249) --> . Mary got the football
2025-01-22 00:53:33.283 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 00:53:33.302 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (6647, 6652) --> . Mary moved to the
2025-01-22 00:53:33.302 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:53:33.314 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3864, 3870) --> . John went back to the
2025-01-22 00:53:33.314 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:53:33.323 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3062, 3068) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:53:35.483 | INFO     | test_jbb_retain:begin_test:544 - The apple was in the office.<|eot_id|>
2025-01-22 00:53:35.483 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12132])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.68s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.47s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.92it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.89it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:53:44.920 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.27614357357933406, 0.2503357310819685, 0.2909379953687841, 0.2502170398392663], 'topk_tokens': [' hundred', ' state', ' first', ' Arr', '.', ' type', ' S', ' S', '.', ' compl', ' W', ' honor', 'deal', ' sop', 'ols', ' and', 'itter', ' S', 'est', ' Do'], 'evidence_proportions': [0.2998412450154623, 0.222003173828125, 0.2429366111755371, 0.3197008768717448]}, 'weight': {'score': [0.031132891064598447, 0.0025906648859842156, 0.014972762628035112, 0.0025185682187599335], 'topk_tokens': [' Newspaper', "'s", ' the', '<|start_header_id|>', ' bathroom', ' office', ' boat', ':', ' bedroom', ' apple', '<|eot_id|>', 'b', 'assistant', ' the', 'Bridge', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.015475988388061523, 0.02113562822341919, 0.11083030700683594, 0.001989235480626424]}, 'saliency': {'score': [0.0019956656864711215, 3.900324114566484e-05, 0.00036673654209483755, 3.5008861061913896e-05], 'topk_tokens': [' Bench', ':', "'s", ' garden', ' bedroom', ' bathroom', ' bathroom', ' boat', 'Daniel', '<|eot_id|>', ' back', ' apple', ' Daniel', ' Daniel', ' bedroom', ' office', ' Dan', 'Bridge', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.001261875033378601, 0.0024101316928863523, 0.005516022443771362, 3.7163496017456055e-05]}}, 25: {'grad': {'score': [0.5785031999860492, 0.5125258726694478, 0.5085155259479176, 0.5124185871711204], 'topk_tokens': [' a', ' $', ' a', ' free', ' to', ' a', ' bogus', ' to', ' morning', ' for', ' incorporated', ' ', ' set', ' black', ' bogus', ' of', ' a', ' inverted', ' bogus', ' no'], 'evidence_proportions': [0.5852508544921875, 0.573046875, 0.689208984375, 0.5024986267089844]}, 'weight': {'score': [0.021149352902457827, 0.002521176410890382, 0.005874609405344183, 0.002482723943788091], 'topk_tokens': [' the', ' Daniel', ' boat', '.\n\n', 'b', '<|start_header_id|>', ' Bench', ' \n', 'Answer', "'s", ' Dan', ' apple', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.004031062126159668, 0.01806681752204895, 0.07602262496948242, 0.004254241784413655]}, 'saliency': {'score': [0.0015182012603396462, 4.056799937335856e-05, 8.751316504044966e-05, 3.791640392802545e-05], 'topk_tokens': ['door', ' Red', 'Answer', 'Den', ':', ' Press', "'s", 'assistant', ' Daniel', 'Daniel', ' boat', 'athroom', ' apple', ' Bench', ' Daniel', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00015006462732950848, 0.002591925859451294, 0.004461467266082764, 2.9390056927998863e-05]}}, 26: {'grad': {'score': [0.44493538992745535, 0.4932680159082715, 0.48222836581143463, 0.4933720398453975], 'topk_tokens': [' Paul', 'bec', 'graph', '."', 'rich', 'b', ' barric', ' compl', 'b', ' Press', 'issippi', ' Paul', 'graph', 'field', 'hue', 'RI', ' Anthony', ' Press', ' bitter', 'itter'], 'evidence_proportions': [0.4331207275390625, 0.538592529296875, 0.3470726013183594, 0.44394429524739587]}, 'weight': {'score': [0.01815086461248852, 0.002485552058084344, 0.0049999112432653255, 0.0024537717516241665], 'topk_tokens': [' garden', ' bathroom', ' Dan', ' apple', ' apple', 'Bridge', 'b', '<|eot_id|>', 'Answer', ' \n', '<|eot_id|>', 'assistant', ' bathroom', ' the', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.016750474770863853, 0.008009153604507446, 0.054543256759643555, 0.0037410855293273926]}, 'saliency': {'score': [0.0008086675689333962, 4.0997783981530716e-05, 0.00024108859625729647, 3.9300540898992175e-05], 'topk_tokens': ['.\n\n', ' the', "'s", 'Daniel', ' bathroom', ' apple', ' \n', 'Bridge', ' the', 'assistant', ' bathroom', ' apple', '<|start_header_id|>', ':', '<|end_header_id|>', ' Dan', 'athroom', '\n\n', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.000446513295173645, 0.0004790902137756348, 0.0027594342827796936, 0.00014495849609375]}}, 27: {'grad': {'score': [0.2645685105096726, 0.4110131061366914, 0.27352647347883746, 0.4115175754078327], 'topk_tokens': [' was', ' was', ' themselves', ' be', ' be', ' was', ' would', ' be', ',', ' were', ' were', ' was', ',', ' was', ' was', ' Bottle', ' were', ' was', ' would', ' was'], 'evidence_proportions': [0.25147247314453125, 0.30547637939453126, 0.2391815185546875, 0.2604993184407552]}, 'weight': {'score': [0.029539765346617924, 0.0025405844494694294, 0.006120531396432357, 0.002487182065110076], 'topk_tokens': [' bedroom', 'Daniel', ' the', ' the', ' \n', ' bathroom', 'Answer', ' Dan', ' boat', '<|start_header_id|>', ' apple', 'assistant', 'b', '.\n\n', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.023381213347117104, 0.020600301027297974, 0.0902855396270752, 0.0026506880919138593]}, 'saliency': {'score': [0.0033760695230393182, 4.967258798766637e-05, 0.0007095540111715144, 4.2695104780080415e-05], 'topk_tokens': ['<|start_header_id|>', 'Dub', 'assistant', ' the', 'Daniel', ' THE', ' \n', '\n\n', ' bathroom', ' the', 'NEW', '<|end_header_id|>', ' apple', 'b', 'athroom', ' the', ':', '<|begin_of_text|>', ' Dan', '.\n\n'], 'evidence_proportions': [0.0004815608263015747, 0.003601396083831787, 0.012403622269630432, 6.443758805592856e-05]}}, 28: {'grad': {'score': [0.32965560186476933, 0.28340807436521426, 0.3348731994628906, 0.2832341220968021], 'topk_tokens': [' on', ' is', ',', ' became', ' on', ' sample', '.', 'dent', ' following', ' over', ' almost', 'about', 'Mal', ' half', 'ot', ' inside', ' as', 'half', ' half', 'ball'], 'evidence_proportions': [0.27730560302734375, 0.36830291748046873, 0.3949432373046875, 0.3062744140625]}, 'weight': {'score': [0.009407921915962583, 0.002415971084047749, 0.00579322332685644, 0.0023976837439210535], 'topk_tokens': [' the', '.\n\n', ' \n', ' garden', '<|eot_id|>', 'Answer', '?', ' bathroom', '<|eot_id|>', 'assistant', ' before', 'b', ' apple', ' the', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.004751394192377726, 0.004123377799987793, 0.023160457611083984, 0.009299879272778828]}, 'saliency': {'score': [0.00022073728697640554, 3.4617139912948324e-05, 0.0002766143191944469, 3.3853621799110616e-05], 'topk_tokens': [' bathroom', ' back', ' back', '?', ' office', ' garden', ' before', 'assistant', ':', 'Bridge', ' Bridge', 'athroom', ' Bridge', '<|start_header_id|>', 'b', ' apple', '<|end_header_id|>', ' the', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [7.176399230957031e-05, 0.00019338726997375488, 0.0005236119031906128, 0.00019058585166931152]}}, 29: {'grad': {'score': [0.3862693423316592, 0.31783021251802634, 0.2877642024647106, 0.31776605691887705], 'topk_tokens': [' composing', 's', ' m', ' com', ' S', 'st', ' Ch', 're', 'y', ' THE', ' M', 'g', 'ION', 'y', ' com', ' cont', ' o', 's', ' com', ' l'], 'evidence_proportions': [0.42371877034505206, 0.32187042236328123, 0.35530662536621094, 0.4231274922688802]}, 'weight': {'score': [0.0119724969069163, 0.0024845373989084794, 0.003678567030213096, 0.0024658873987797275], 'topk_tokens': [' the', ' the', '.', '<|eot_id|>', 'Answer', ' \n', ' apple', '.\n\n', '<|eot_id|>', '?', 'b', ' before', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.012198204795519512, 0.013198280334472656, 0.019991159439086914, 0.005379527807235718]}, 'saliency': {'score': [0.00034854383695693246, 2.3679017136506598e-05, 0.00010731409896503796, 2.2962664753405653e-05], 'topk_tokens': ['Print', ' PA', 'Does', 'NEW', '<|eot_id|>', '***', ' the', ' a', ' the', ' the', ' Does', ':', '<|end_header_id|>', 'assistant', '<|eot_id|>', ' before', 'b', '<|begin_of_text|>', '<|start_header_id|>', '\n\n'], 'evidence_proportions': [0.00014293690522511798, 0.0006538748741149903, 0.0007173046469688416, 5.386769771575928e-05]}}, 30: {'grad': {'score': [0.2109150659470331, 0.3274370354588355, 0.2561371543190696, 0.3277691193775274], 'topk_tokens': [' the', ' the', ' its', ' Times', 'IT', ' Times', ' The', 'ire', ' Europe', ' of', '2', 'SSION', 'AM', ' account', ' forb', ' LINE', 'b', 'deal', 'b', 'b'], 'evidence_proportions': [0.2082341512044271, 0.2577362060546875, 0.3143272399902344, 0.10563691457112631]}, 'weight': {'score': [0.01847073577699207, 0.0024549818255059455, 0.007365585728125138, 0.0024182332215662472], 'topk_tokens': [' before', '.', 'Question', ' garden', '.\n\n', ' the', '?', ' bathroom', '<|eot_id|>', 'Answer', '<|eot_id|>', ' \n', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01294275124867757, 0.012150067090988158, 0.04694366455078125, 0.010283991694450378]}, 'saliency': {'score': [0.0017566198394412087, 7.940824993559496e-05, 0.00034031678329814565, 7.602076803734868e-05], 'topk_tokens': [' Daily', '?', ' the', ' garden', ' Dan', ' \n', ' garden', ' Bridge', ' apple', 'Bridge', ' Bridge', '<|start_header_id|>', 'assistant', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ' bathroom', 'b', ':', 'athroom'], 'evidence_proportions': [0.004116957386334738, 0.0004800915718078613, 0.001987375319004059, 0.00030621886253356934]}}, 31: {'grad': {'score': [0.25235648382277714, 0.27475802528057564, 0.24606640772386032, 0.2748491308013217], 'topk_tokens': [' he', ' the', ' the', ' him', ' Press', 'nes', ' part', ' the', ' location', ' the', ' location', ' the', ' the', 'membership', ' location', ' the', ' the', 'If', ' location', ' the'], 'evidence_proportions': [0.20609410603841147, 0.340472412109375, 0.2982935905456543, 0.19456418355305988]}, 'weight': {'score': [0.003127962350845337, 0.002264069980390287, 0.0020948336883024735, 0.0022628775770365305], 'topk_tokens': ['Question', ' the', '.\n\n', ' before', ':', '?', ' Where', ' the', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015003283818562827, 0.001679927110671997, 0.005930840969085693, 0.0040937066078186035]}, 'saliency': {'score': [0.00010051755678086054, 1.5258816077378144e-05, 2.9772520065307617e-05, 1.5084342454941195e-05], 'topk_tokens': [' garden', ' Where', ' the', '\n\n', ' the', '<|eot_id|>', '<|eot_id|>', ' Key', ' \n', ' the', 'Answer', ' the', ' apple', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', 'b', '<|begin_of_text|>', 'assistant'], 'evidence_proportions': [4.974504311879476e-05, 7.047057151794433e-05, 0.0002855658531188965, 5.2963693936665855e-05]}}, 'pred_res': 'The apple was in the office.<|eot_id|>', 'score': 0}
2025-01-22 00:53:44.922 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:53:44.922 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-3_0-2-3-7.pkl | len: 10 |  size: 8.55 KB
Processing depth (0, 2, 3, 7):   4%|‚ñç         | 4/100 [01:15<30:21, 18.97s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.35it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.84it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 1, 5, 6):   4%|‚ñç         | 4/100 [01:22<30:21, 18.97s/it]2025-01-22 00:53:52.032 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:53:52.033 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:53:52.033 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:53:52.037 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1495, 1500) -->  tragedy. Daniel went to
2025-01-22 00:53:52.038 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:53:52.054 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5932, 5936) -->  Daniel left the apple
2025-01-22 00:53:52.054 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 00:53:52.076 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (7172, 7178) --> . Mary journeyed to the
2025-01-22 00:53:52.076 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Mary got the football there.
2025-01-22 00:53:52.089 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4239, 4244) --> . Mary got the football
2025-01-22 00:53:52.089 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 00:53:52.108 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (6647, 6652) --> . Mary moved to the
2025-01-22 00:53:52.108 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:53:52.120 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (3859, 3865) --> . John went back to the
2025-01-22 00:53:52.120 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 00:53:52.130 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (3062, 3068) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:53:54.111 | INFO     | test_jbb_retain:begin_test:544 - The bedroom.<|eot_id|>
2025-01-22 00:53:54.111 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12132])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.70s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.47s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.92it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.89it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:54:03.579 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.24716077532087052, 0.22735650526627524, 0.24818871238014914, 0.22728420968013135], 'topk_tokens': [' Merch', ' announced', ' appearance', 'ord', 'adj', ' the', ' first', 'remark', ' hundred', ' and', ' compl', ' comparison', '202', 'itter', 'est', 'deal', 'vent', 'consider', 'ols', ' Do'], 'evidence_proportions': [0.287841796875, 0.23450164794921874, 0.15152359008789062, 0.2807871500651042]}, 'weight': {'score': [0.025902914149420603, 0.002594297100203883, 0.015778789466077633, 0.00252982970109019], 'topk_tokens': [' building', '\n\n', ' the', ' bathroom', ' apple', ':', '<|eot_id|>', ' bathroom', ' the', ' garden', 'assistant', 'Bridge', 'b', ' Bridge', '\n\n', ' bedroom', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.028964916865030922, 0.005419564247131348, 0.07907295227050781, 0.004463677604993185]}, 'saliency': {'score': [0.0016016179607028053, 3.553867389205851e-05, 0.00030524757775393397, 3.2328182583094985e-05], 'topk_tokens': [' the', ' Bench', ' bathroom', ' Daniel', ' building', 'b', '<|eot_id|>', ' bathroom', ' top', ' bedroom', ' composing', ' garden', 'Daniel', ' Daniel', ' garden', '<|begin_of_text|>', ' Bridge', ' bedroom', 'Bridge', 'athroom'], 'evidence_proportions': [0.0012897700071334839, 0.001097583770751953, 0.005041658878326416, 4.0133794148763016e-05]}}, 25: {'grad': {'score': [0.501866340637207, 0.4778662733055212, 0.47058556296608667, 0.47783783915182476], 'topk_tokens': [' with', ' set', ' appropriated', ' a', ' little', ' of', 'light', ' incorporated', ' a', ' for', ' at', ' a', ' a', ' bogus', ' free', ' of', ' black', ' no', ' for', ' inverted'], 'evidence_proportions': [0.5863240559895834, 0.45113525390625, 0.5571250915527344, 0.42284536361694336]}, 'weight': {'score': [0.0172642639705113, 0.0025119959104684926, 0.0050771480256860905, 0.0024817088631813866], 'topk_tokens': [' bathroom', ' apple', 'Daniel', ' the', ' \n', '<|start_header_id|>', ' Daniel', ' Bench', 'Answer', ' Dan', 'b', ' apple', '<|eot_id|>', 'assistant', '<|eot_id|>', ':', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.007669568061828613, 0.007590371370315551, 0.06641149520874023, 0.0021557162205378213]}, 'saliency': {'score': [0.0013802718548547653, 2.6904829352983586e-05, 7.225166667591442e-05, 2.447195324012865e-05], 'topk_tokens': [' Geo', 'Answer', ' composing', ' top', ' Daniel', ' THE', '<|eot_id|>', ' Press', '<|eot_id|>', 'Den', ':', 'Daniel', ' Bench', ' Daniel', 'athroom', '<|end_header_id|>', '\n\n', ' apple', ' Dan', '<|begin_of_text|>'], 'evidence_proportions': [0.00022189815839131674, 0.0007108211517333985, 0.0059931352734565735, 2.1278858184814453e-05]}}, 26: {'grad': {'score': [0.29051317487444195, 0.34492510405400184, 0.3101830915971236, 0.34508280954414594], 'topk_tokens': [' PA', ' favor', ' Empire', 'rich', 'b', 'UX', 'ub', 'b', ' Eagle', 'pro', ' compl', ' prof', 'bec', ' Milwaukee', 'issippi', 'RI', 'hue', ' bitter', ' Press', 'itter'], 'evidence_proportions': [0.23422876993815106, 0.41265869140625, 0.23716354370117188, 0.2805760701497396]}, 'weight': {'score': [0.02479870120684306, 0.002497444577211208, 0.00550559704953974, 0.0024532413235217026], 'topk_tokens': [' Daniel', ' Bridge', ' bathroom', ' apple', '<|eot_id|>', ' apple', 'b', ' garden', 'Answer', ' \n', '<|eot_id|>', 'assistant', ' bathroom', '<|start_header_id|>', ' the', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.024361183245976765, 0.0055616796016693115, 0.0820002555847168, 0.0031327009201049805]}, 'saliency': {'score': [0.001096420344852266, 2.6188637918535948e-05, 0.00011761486530303955, 2.416364264496107e-05], 'topk_tokens': [' Daniel', ' Dan', ' \n', 'Answer', ' apple', ' old', '<|start_header_id|>', 'Daniel', ' garden', 'Bridge', ' Bridge', 'assistant', ' Daniel', '<|end_header_id|>', 'athroom', ' apple', '\n\n', ':', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0004014521837234497, 0.0005296528339385985, 0.004374653100967407, 7.820626099904379e-05]}}, 27: {'grad': {'score': [0.14602758770897276, 0.21288268313665276, 0.16503151980313388, 0.2130858490808571], 'topk_tokens': [' STR', ' Bottle', 'event', ' was', ' Franklin', ' mon', ' designated', ' sentinel', 'ly', ' dre', 'lin', 'Republicans', '-n', ' platform', ' conventions', ' convention', ' was', 'str', ' accepted', ' step'], 'evidence_proportions': [0.08626198768615723, 0.1882041931152344, 0.20621776580810547, 0.13051923116048178]}, 'weight': {'score': [0.029108003491447085, 0.002545946873310028, 0.005263921889391812, 0.002494871894866869], 'topk_tokens': [' bedroom', 'Daniel', ' Bridge', ' apple', ' lounge', ' garden', 'Answer', ' the', ' apple', '<|start_header_id|>', ' bathroom', '.\n\n', 'assistant', 'b', ' bathroom', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03436922033627828, 0.008215224742889405, 0.08723938465118408, 0.002503181497255961]}, 'saliency': {'score': [0.001324114345368885, 3.724336182083865e-05, 0.00042967362837357956, 3.429449012726693e-05], 'topk_tokens': [' apple', ' the', ' bedroom', ' \n', ' Dan', ' bathroom', 'assistant', ' Bridge', ' THE', ' apple', ':', ' the', 'Daniel', '<|begin_of_text|>', ' bathroom', '<|end_header_id|>', 'NEW', 'b', 'athroom', '.\n\n'], 'evidence_proportions': [0.0011020004749298096, 0.00019278526306152344, 0.004962809383869171, 6.320575873057048e-05]}}, 28: {'grad': {'score': [0.23164730980282738, 0.2568082390570277, 0.2066936492919922, 0.25694311339453757], 'topk_tokens': ['E', ' Cedar', ' half', 'arp', 'E', ' lie', 'ew', 'E', ' lie', '.', ' RID', '\n', 'ot', ' ', 'nes', 'half', ' half', 'dent', 'S', '.'], 'evidence_proportions': [0.27558104197184247, 0.1894317626953125, 0.1714334487915039, 0.26303577423095703]}, 'weight': {'score': [0.015710666066124326, 0.002414413848235669, 0.005746287378397855, 0.002385260481196368], 'topk_tokens': [' the', ' \n', 'Answer', '<|eot_id|>', '?', ' the', '<|eot_id|>', ' before', ' garden', ' bathroom', 'assistant', ' apple', 'b', '<|start_header_id|>', ' the', '<|end_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0071482062339782715, 0.0017849504947662354, 0.052147865295410156, 0.011586422721544903]}, 'saliency': {'score': [0.0002836925642830985, 2.827231511688547e-05, 0.00020048022270202637, 2.751541806103337e-05], 'topk_tokens': [' Daniel', ' near', ' garden', '?', ' the', ' before', 'athroom', ' Bridge', 'b', '<|start_header_id|>', 'Bridge', 'assistant', '\n\n', '<|end_header_id|>', ' apple', ' garden', ' Bridge', ':', ' the', '<|begin_of_text|>'], 'evidence_proportions': [0.00012993812561035156, 0.00010476112365722657, 0.001072995364665985, 6.035466988881429e-05]}}, 29: {'grad': {'score': [0.2013693309965588, 0.24785977093634115, 0.19953363591974432, 0.24802843403666375], 'topk_tokens': ['re', ' face', 't', ' a', 'pend', 'Dr', ' enough', ' faithfully', ' Dr', 'Emp', ' extra', ' Dr', ' M', 'goods', 'ys', ' extra', 'tal', ' not', 'cret', ' ga'], 'evidence_proportions': [0.2751426696777344, 0.1679014205932617, 0.13794255256652832, 0.1977704366048177]}, 'weight': {'score': [0.011979358536856515, 0.0024804558928827225, 0.004066163843328302, 0.0024610742744214874], 'topk_tokens': [' Does', '<|eot_id|>', '.', ' \n', 'Answer', '?', ' apple', ' the', '.\n\n', ' the', ' before', 'b', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.012405181924502056, 0.000974804162979126, 0.03998923301696777, 0.0020507474740346274]}, 'saliency': {'score': [0.00022045487449282692, 2.32718832757083e-05, 9.372559460726652e-05, 2.2801256045733624e-05], 'topk_tokens': ['Answer', '.', ' garden', '***', ' Where', ' the', ' Does', ' before', '<|eot_id|>', 'NEW', ' a', ' the', '<|eot_id|>', '<|end_header_id|>', ':', 'athroom', 'assistant', 'b', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002712408701578776, 2.6983022689819335e-05, 0.0006705373525619507, 3.0840436617533364e-05]}}, 30: {'grad': {'score': [0.19874173118954613, 0.26502522585271165, 0.19365432045676492, 0.265270190565467], 'topk_tokens': [' Times', ' Roman', ' United', ' of', 'ar', '2', ' B', ' Times', ' Times', 'ire', ' account', ' B', ' forb', 'SSION', 'AM', ' LINE', 'deal', 'b', 'b', 'b'], 'evidence_proportions': [0.20132573445638022, 0.22103424072265623, 0.24172401428222656, 0.14892578125]}, 'weight': {'score': [0.022313122238431658, 0.0024729685081981757, 0.008341350338675758, 0.0024278355584292865], 'topk_tokens': [' before', ' apple', '<|eot_id|>', '<|eot_id|>', ' bathroom', '?', '.\n\n', ' the', ' garden', 'Answer', ' the', 'b', ' \n', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.012866775194803873, 0.004522055387496948, 0.08083724975585938, 0.007569273312886557]}, 'saliency': {'score': [0.002521556048166184, 4.2764107582743e-05, 0.0003254223953593861, 3.794496161157707e-05], 'topk_tokens': [' garden', ' the', ' \n', '.\n\n', 'Bridge', ' Daniel', ' apple', '<|start_header_id|>', ' the', ' the', 'assistant', ' Bridge', '<|end_header_id|>', ' the', ' bathroom', 'b', ' bathroom', '<|begin_of_text|>', 'athroom', ':'], 'evidence_proportions': [0.0034404347340265913, 0.00041819214820861813, 0.0071229487657547, 0.0002878854672114054]}}, 31: {'grad': {'score': [0.15551991689772832, 0.16101262894588386, 0.15240899134765973, 0.1610378213855276], 'topk_tokens': ['m', 'm', ' Press', ' location', ' o', ' January', 'nes', 'If', 'membership', ' location', ' the', ' location', 'had', ' population', ' department', ' m', ' conn', 'n', 'd', 'g'], 'evidence_proportions': [0.12916882832845053, 0.21288347244262695, 0.23337554931640625, 0.08216428756713867]}, 'weight': {'score': [0.0032726526260375977, 0.002286386666692821, 0.0018676234917207198, 0.002285435724309687], 'topk_tokens': [' before', ' was', '.\n\n', ':', ' apple', '?', ' Where', '<|eot_id|>', ' the', 'Answer', ' \n', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00161324938138326, 0.0011899828910827638, 0.010785937309265137, 0.001658757527669271]}, 'saliency': {'score': [0.00013970761072067987, 1.3942516139863105e-05, 3.0051578174937855e-05, 1.369479315351107e-05], 'topk_tokens': [' apple', ' the', '<|eot_id|>', 'Question', ' Key', ' Daniel', ' garden', ' the', ' Where', 'Answer', ' \n', ' apple', ' the', ':', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', 'athroom', 'assistant', 'b'], 'evidence_proportions': [3.349781036376953e-05, 9.865164756774902e-05, 0.000546187162399292, 9.14434591929118e-06]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 00:54:03.581 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:54:03.581 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-4_0-1-5-6.pkl | len: 10 |  size: 8.65 KB
Processing depth (0, 1, 5, 6):   5%|‚ñå         | 5/100 [01:34<29:51, 18.86s/it]Processing depth (0, 1, 5, 6):   5%|‚ñå         | 5/100 [01:34<29:58, 18.93s/it]
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:70 - Selected idx: 5
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:72 - Answer: bathroom
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:73 - Tag: 4-hop
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:74 - Needle: [' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:75 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 00:54:03.783 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.36it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.39it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 2, 4, 6, 8):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 00:54:10.636 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:54:10.641 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Daniel journeyed to
2025-01-22 00:54:10.641 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 00:54:10.648 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2475, 2479) -->  Daniel grabbed the milk
2025-01-22 00:54:10.649 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel went to the garden.
2025-01-22 00:54:10.663 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4863, 4868) --> . Daniel went to the
2025-01-22 00:54:10.663 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel left the milk.
2025-01-22 00:54:10.683 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (7173, 7177) -->  Daniel left the milk
2025-01-22 00:54:10.683 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 00:54:10.712 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9594, 9600) -->  Mary journeyed to the office
2025-01-22 00:54:10.712 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:54:10.719 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2362, 2368) -->  the senate. Sandra journeyed
2025-01-22 00:54:10.719 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the football there.
2025-01-22 00:54:10.729 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3484, 3489) --> . Mary got the football
2025-01-22 00:54:10.729 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:54:10.744 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4766, 4772) --> . John went back to the
2025-01-22 00:54:10.744 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 00:54:10.749 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1867, 1872) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:54:12.717 | INFO     | test_jbb_retain:begin_test:544 - the garden<|eot_id|>
2025-01-22 00:54:12.717 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.61s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.44s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.42s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.06it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.92it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.89it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.13it/s]
2025-01-22 00:54:22.520 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.25608978271484373, 0.2868533679630268, 0.2736305540258234, 0.28694095609208176], 'topk_tokens': [' out', ' conventions', ' whistle', ' appearance', ' competitor', ' expedition', ' turtle', ' Do', 'adj', ' committee', ' advantage', ' whistle', 'adv', ' compromised', ' compl', ' appearance', ' absence', ' examined', 'able', 'consider'], 'evidence_proportions': [0.2937062581380208, 0.2610130310058594, 0.23048095703125, 0.22836589813232422, 0.25501441955566406]}, 'weight': {'score': [0.017769639492034913, 0.0025834223205380524, 0.016298774968494068, 0.0025271181404403448], 'topk_tokens': [' John', 'Answer', ' barric', ' bedroom', ' bathroom', ' garden', ' the', '<|eot_id|>', ' bedroom', '.', 'assistant', '<|eot_id|>', ':', '\n\n', 'Bridge', 'b', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008752445379892985, 0.020756244659423828, 0.002483677864074707, 0.07664108276367188, 0.006163636843363443]}, 'saliency': {'score': [0.0015988802909851073, 4.683192722767368e-05, 0.0008149797266179866, 4.222934412731649e-05], 'topk_tokens': [' milk', ' bathroom', ' Dan', '<|begin_of_text|>', ' Bridge', '<|eot_id|>', ' bedroom', ':', '<|eot_id|>', ' bedroom', ' back', ' the', '<|start_header_id|>', 'b', ' Daniel', ' Bench', ' bathroom', ' garden', 'athroom', 'Bridge'], 'evidence_proportions': [2.7929743131001793e-05, 0.0049472376704216, 0.00014058947563171386, 0.004040077328681946, 0.0005253702402114868]}}, 25: {'grad': {'score': [0.5805638122558594, 0.5472865966977735, 0.553005651994185, 0.5472074569187723], 'topk_tokens': [' inverted', ' free', ' down', ' the', 'M', ' "', ' for', ' hate', 'posit', ' a', ' at', ' of', ' the', ' a', ' for', 'ivery', ' the', ' a', ' for', ' no'], 'evidence_proportions': [0.6281077067057291, 0.591766357421875, 0.56007080078125, 0.5618429183959961, 0.5551096598307291]}, 'weight': {'score': [0.014314537048339843, 0.0025139457846445373, 0.0056104958057403564, 0.002483939266543687], 'topk_tokens': [' bathroom', ' barric', ' garden', ' Dan', 'Answer', ' the', 'b', '?\n', ' Daniel', ' Bench', '.', '<|eot_id|>', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0010943015416463215, 0.03983306884765625, 0.005269527435302734, 0.034909725189208984, 0.004329800605773926]}, 'saliency': {'score': [0.0006662178039550782, 3.842795932329244e-05, 0.0002395009452646429, 3.676556039695759e-05], 'topk_tokens': ['<|eot_id|>', ' Ot', ' bathroom', '<|start_header_id|>', '<|eot_id|>', ' milk', 'assistant', ':', ' barric', '.', ' garden', 'b', 'Answer', ' Daniel', 'athroom', ' Geo', ' Bench', ' Dan', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [5.2044788996378585e-05, 0.001941688358783722, 0.00023777484893798827, 0.0015737637877464294, 0.00018208225568135578]}}, 26: {'grad': {'score': [0.292315673828125, 0.293323536627243, 0.32936321605335583, 0.29326010278346537], 'topk_tokens': [' Herald', ' when', 'ente', ' proc', ',', ' Marshall', '-in', ' and', ' Gutenberg', 'agle', ' bitter', ' prof', ' Island', 'char', 'ub', ' Marshall', ' Eagle', ' Empire', ' Marshall', ' Becker'], 'evidence_proportions': [0.3793843587239583, 0.36639404296875, 0.31368408203125, 0.20880126953125, 0.19373067220052081]}, 'weight': {'score': [0.006782420873641968, 0.0024797153213856416, 0.004194819114424966, 0.0024677090478562023], 'topk_tokens': [' Dan', ' Bridge', 'CH', ' Bench', ' bathroom', '<|eot_id|>', '<|eot_id|>', '?\n', 'Bridge', 'assistant', 'Answer', ' barric', 'b', '\n\n', ' the', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00040477017561594647, 0.010462820529937744, 0.0015687942504882812, 0.02402329444885254, 0.003557244936625163]}, 'saliency': {'score': [0.00011972665786743164, 4.7644201801955354e-05, 0.00014089183373884722, 4.732578258990375e-05], 'topk_tokens': [' bedroom', ' Bridge', ' barric', ' Geo', 'IR', ' Anthony', '<|end_header_id|>', ' bathroom', '?\n', ' Dan', 'Answer', 'CH', '<|start_header_id|>', 'Bridge', 'athroom', ' the', '<|begin_of_text|>', '\n\n', 'b', ':'], 'evidence_proportions': [4.0347377459208175e-05, 0.0001686587929725647, 8.287429809570312e-05, 0.00027730315923690796, 9.214381376902261e-05]}}, 27: {'grad': {'score': [0.2808673095703125, 0.3929079680474602, 0.40187731656161224, 0.39312311354362756], 'topk_tokens': ['ition', '\n', '\n', '\n', ' received', '!"', ' several', ' excessive', '!"', '\n', 'ors', ' short', '\n', ' intended', '\n', ' designated', ' accepted', ' step', '\n', ' Thanksgiving'], 'evidence_proportions': [0.24769210815429688, 0.3004150390625, 0.29488372802734375, 0.41655731201171875, 0.1988703409830729]}, 'weight': {'score': [0.01000262975692749, 0.0025396441243707283, 0.008864971724423494, 0.0025127285857808193], 'topk_tokens': ['<|eot_id|>', ' bathroom', '?\n', 'IR', '.', 'Answer', ' garden', '.', 'assistant', '.\n\n', ' barric', 'CH', 'b', ' bathroom', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007038166125615437, 0.024294614791870117, 0.0028655827045440674, 0.029392004013061523, 0.0027947425842285156]}, 'saliency': {'score': [0.000969846248626709, 4.8305033773971256e-05, 0.0008229152722792192, 4.499318816015256e-05], 'topk_tokens': [' to', '<|end_header_id|>', '<|begin_of_text|>', ' prior', 'IR', ' the', ' Daniel', ' Bridge', ' the', ' Daniel', '.', ' bathroom', ' Bridge', 'CH', 'Bridge', ':', '<|start_header_id|>', '.\n\n', 'athroom', 'b'], 'evidence_proportions': [2.7333696683247886e-05, 0.0023046135902404785, 0.00015628337860107422, 0.003346651792526245, 0.00011594593524932861]}}, 28: {'grad': {'score': [0.3949292755126953, 0.49760977707141535, 0.33403743397105823, 0.49811924691831466], 'topk_tokens': [' a', '.', ' the', 'IO', ' the', ',', 'nes', 'nes', 'in', '.', 'antic', ' the', 'yl', '.', "'", 'nes', ' the', ',', 'dent', 'nes'], 'evidence_proportions': [0.5329653422037761, 0.4549560546875, 0.3154148101806641, 0.23824501037597656, 0.38759358723958337]}, 'weight': {'score': [0.007475254535675049, 0.0024084827813388625, 0.0022603449496355925, 0.0023982852717073194], 'topk_tokens': ['.\n\n', ' Daniel', ' to', ' the', ' discarded', ' bathroom', ' Bridge', '<|eot_id|>', 'Answer', '<|eot_id|>', '?\n', 'assistant', 'b', ' the', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00018900632858276367, 0.002882912755012512, 0.00041968822479248046, 0.0349421501159668, 0.005391438802083333]}, 'saliency': {'score': [0.00041035652160644533, 3.5039136729443334e-05, 0.00017333437095988881, 3.40124114133803e-05], 'topk_tokens': [' and', ' Dr', 'Question', 'b', ' Far', '<|eot_id|>', ' milk', ' the', 'Bridge', ' Daniel', ' Bridge', '<|start_header_id|>', ' Bridge', 'athroom', '\n\n', '<|end_header_id|>', 'assistant', ' the', ':', '<|begin_of_text|>'], 'evidence_proportions': [2.7224421501159668e-05, 0.00030587613582611084, 7.039904594421388e-05, 0.001919776201248169, 0.00014016032218933105]}}, 29: {'grad': {'score': [0.3970485687255859, 0.36854517427488886, 0.5254859057339755, 0.36820099306902476], 'topk_tokens': ['assistant', ' But', ' Pioneer', 'ball', ' a', ' In', ' Bench', ' The', ' book', 'Spring', ' M', '\n', ' bathroom', ' B', ' The', ' B', ' B', 'b', ' ga', 'b'], 'evidence_proportions': [0.44338480631510413, 0.45000457763671875, 0.4006328582763672, 0.2991523742675781, 0.377685546875]}, 'weight': {'score': [0.0023296165466308596, 0.002482597381845503, 0.0021602728150107646, 0.0024834993535320614], 'topk_tokens': ['Does', ' and', ' where', 'Question', ' was', '.\n\n', '<|eot_id|>', ' Does', '<|eot_id|>', '?\n', 'Answer', 'b', 'assistant', ' the', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [8.195141951243083e-05, 0.0016407966613769531, 0.0003079712390899658, 0.0085945725440979, 0.0025445620218912763]}, 'saliency': {'score': [0.00014918684959411621, 4.991928984946366e-05, 0.00013379075310446999, 4.9561757112294064e-05], 'topk_tokens': [' the', 'Question', ':', 'Does', ' and', '?\n', '<|eot_id|>', ' part', '<|eot_id|>', 'Answer', ' Does', ':', '<|end_header_id|>', ' the', 'assistant', '<|start_header_id|>', 'athroom', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [6.8942705790201826e-06, 0.0001090019941329956, 7.611513137817383e-06, 0.0006022751331329346, 0.00013418992360432944]}}, 30: {'grad': {'score': [0.3584033203125, 0.3979259179800035, 0.34775785966352984, 0.3980987623218189], 'topk_tokens': [' Buchanan', ' S', ' itself', ' the', ' Times', ' the', ' Europe', 'ire', 'B', ' B', ' of', ' account', ' Burb', 'deal', 'b', ' forb', ' B', 'b', ' B', 'b'], 'evidence_proportions': [0.3200531005859375, 0.36420440673828125, 0.439715576171875, 0.529144287109375, 0.21129862467447919]}, 'weight': {'score': [0.007318476438522339, 0.0024968312202495687, 0.005343292247165333, 0.002481696261313115], 'topk_tokens': [':', ' Dan', 'Question', ' AND', ' barric', 'IR', '.\n\n', ' the', '<|eot_id|>', '<|eot_id|>', 'b', 'assistant', 'Answer', '?\n', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008536179860432943, 0.004549503326416016, 0.0016279637813568113, 0.027168750762939453, 0.007137894630432129]}, 'saliency': {'score': [0.0005166792869567871, 7.227625667494466e-05, 0.00025141916491768575, 7.103255978696032e-05], 'topk_tokens': [' fifteen', '<|eot_id|>', ' bathroom', '.\n\n', ' barric', 'assistant', '.', ' the', ' bathroom', ' Bench', 'Bridge', ' AND', ' Bridge', ' Bridge', '<|end_header_id|>', ':', '<|begin_of_text|>', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [6.62455956141154e-05, 0.0004057362675666809, 9.86933708190918e-05, 0.0018066838383674622, 0.0005293935537338257]}}, 31: {'grad': {'score': [0.3533107852935791, 0.453566842986209, 0.29095573858781293, 0.45406955706149243], 'topk_tokens': [' the', ' the', ' had', ' office', ' the', ' August', ' an', ' location', ' the', ' is', ' the', ' part', ' the', ' the', ' location', ' location', ' was', ' he', ' the', ' the'], 'evidence_proportions': [0.3092182675997416, 0.3790397644042969, 0.40433828830718993, 0.2627830505371094, 0.3980795542399089]}, 'weight': {'score': [0.0020147979259490965, 0.0023000975265082177, 0.0017004107887094672, 0.0023017770504088586], 'topk_tokens': [' the', ' was', 'Question', ' the', ',', ':', ' Where', '.\n\n', '<|eot_id|>', 'Answer', 'b', '?\n', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003030101458231608, 0.0011911243200302124, 0.00037402510643005373, 0.006127119064331055, 0.0029014647006988525]}, 'saliency': {'score': [2.1202564239501955e-05, 2.9112056053482523e-05, 6.178563291376287e-05, 2.9068998592271427e-05], 'topk_tokens': [' dropped', '\n\n', ' Geo', 'ible', ' S', ' the', ' discarded', ' Emily', ' Bridge', 'CH', ' Market', 'Answer', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', ':', 'b', 'assistant', 'athroom'], 'evidence_proportions': [9.834766387939453e-07, 1.2211501598358154e-05, 9.375810623168946e-06, 5.1997601985931396e-05, 3.6741296450297036e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 00:54:22.522 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:54:22.522 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-0_1-2-4-6-8.pkl | len: 10 |  size: 9.07 KB
Processing depth (1, 2, 4, 6, 8):   1%|          | 1/100 [00:18<30:46, 18.65s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.33it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.61it/s]
Processing depth (1, 2, 5, 7, 8):   1%|          | 1/100 [00:25<30:46, 18.65s/it]2025-01-22 00:54:29.629 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:54:29.634 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1488, 1494) -->  tragedy. Daniel journeyed to
2025-01-22 00:54:29.634 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 00:54:29.641 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (2475, 2479) -->  Daniel grabbed the milk
2025-01-22 00:54:29.641 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel went to the garden.
2025-01-22 00:54:29.659 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (5936, 5941) --> . Daniel went to the
2025-01-22 00:54:29.659 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel left the milk.
2025-01-22 00:54:29.684 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8348, 8352) -->  Daniel left the milk
2025-01-22 00:54:29.684 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 00:54:29.714 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9594, 9600) -->  Mary journeyed to the office
2025-01-22 00:54:29.714 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:54:29.721 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2362, 2368) -->  the senate. Sandra journeyed
2025-01-22 00:54:29.722 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the football there.
2025-01-22 00:54:29.732 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3484, 3489) --> . Mary got the football
2025-01-22 00:54:29.732 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:54:29.748 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4766, 4772) --> . John went back to the
2025-01-22 00:54:29.748 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 00:54:29.754 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1867, 1872) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:54:31.708 | INFO     | test_jbb_retain:begin_test:544 - the garden<|eot_id|>
2025-01-22 00:54:31.709 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:40,  5.74s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.49s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.45s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.03it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.45it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.91it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.39it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.87it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.11it/s]
2025-01-22 00:54:41.517 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.3230000686645508, 0.35306126500895135, 0.36498295177112927, 0.3531016924424204], 'topk_tokens': ['were', ' of', ' the', ' Malta', 'ols', 'low', ' first', ' were', ' Do', 'Mal', ' comparison', ' absence', ' out', '.', ' St', ' S', ' out', ' examined', ' turtle', '.'], 'evidence_proportions': [0.41555913289388025, 0.251678466796875, 0.3094154357910156, 0.30838775634765625, 0.29905080795288086]}, 'weight': {'score': [0.020510473251342774, 0.0025861574324161116, 0.013615585186264732, 0.0025290796513834034], 'topk_tokens': [' Bridge', ' Daniel', ' the', 'Answer', ' bedroom', ' bathroom', '<|eot_id|>', '.', ':', 'Bridge', 'assistant', 'b', ' bedroom', '<|eot_id|>', '\n\n', '<|start_header_id|>', ' garden', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005971590677897135, 0.009273052215576172, 0.05021758079528808, 0.04013252258300781, 0.010078112284342447]}, 'saliency': {'score': [0.0016662287712097168, 5.069333581298808e-05, 0.0007698644291270863, 4.6048636594856504e-05], 'topk_tokens': [' top', ' Bench', ' Miles', ' milk', ' Bridge', ':', '<|eot_id|>', 'b', 'Answer', ' back', '<|eot_id|>', '<|start_header_id|>', ' Daniel', '.', ' bedroom', ' bathroom', 'Bridge', ' bedroom', 'athroom', ' garden'], 'evidence_proportions': [3.453592459360758e-05, 0.0011456087231636047, 0.004305708408355713, 0.002527594566345215, 0.000871191422144572]}}, 25: {'grad': {'score': [0.7317373657226562, 0.7357877360340872, 0.6467665758999911, 0.7359579330908328], 'topk_tokens': [' the', ' bogus', ' free', ' not', ' first', ' with', ' with', ' a', ' money', ' at', ' containing', ' with', ' no', 'ivery', ' l', ' for', ' a', ' the', ' of', ' no'], 'evidence_proportions': [0.8356374104817708, 0.76947021484375, 0.78099365234375, 0.7677154541015625, 0.5376497904459636]}, 'weight': {'score': [0.020003281831741333, 0.00251325170378968, 0.004430395635691556, 0.0024736361096977418], 'topk_tokens': ['189', ' East', ' bathroom', '.\n\n', 'b', ' Bench', '?\n', ' garden', '.', 'Answer', ' Daniel', '<|eot_id|>', ':', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0006193766991297404, 0.014202356338500977, 0.04516825675964355, 0.043631553649902344, 0.006531476974487305]}, 'saliency': {'score': [0.0009658539295196533, 4.132693859272724e-05, 0.0001838396896015514, 3.915800326837033e-05], 'topk_tokens': ['assistant', 'Den', ' Daniel', '189', ':', 'Answer', ' milk', ' Dan', ' THE', ' George', '<|start_header_id|>', 'athroom', '<|eot_id|>', '<|eot_id|>', ' East', ' Geo', ' garden', ' Bench', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.658335526784262e-05, 0.0012982487678527832, 0.0016337573528289794, 0.002186134457588196, 0.00027342140674591064]}}, 26: {'grad': {'score': [0.5605061340332032, 0.5441488802447733, 0.5841144214976918, 0.5440424372392969], 'topk_tokens': ['issippi', ',', ' issued', ' office', ' bitter', ' Press', 'agle', ' Marshall', 'graph', 'graph', 'graph', 'UX', 'itter', ' Marshall', 'UX', ' Gutenberg', 'graph', ' Eagle', 'graph', ' Marshall'], 'evidence_proportions': [0.6664021809895833, 0.6166887283325195, 0.6861328124999999, 0.4036102294921875, 0.41706339518229163]}, 'weight': {'score': [0.008580673933029175, 0.002495755402774357, 0.003249810500578447, 0.0024818145520548018], 'topk_tokens': [' East', '.\n\n', 'Bridge', ' barric', ' bedroom', '?\n', '<|eot_id|>', ' garden', ' bathroom', '<|eot_id|>', 'b', 'Answer', 'assistant', ' the', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00019344190756479898, 0.002695798873901367, 0.01615837812423706, 0.02163100242614746, 0.00587618350982666]}, 'saliency': {'score': [0.0004600989818572998, 4.7105147481622086e-05, 0.00015970387242057106, 4.604730433106442e-05], 'topk_tokens': [' turtle', ' bathroom', ' Daniel', ' the', ' East', 'assistant', ' Anthony', 'Bridge', '188', '?\n', 'Answer', '<|end_header_id|>', '<|start_header_id|>', ':', ' bathroom', 'athroom', ' garden', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [2.5625030199686687e-05, 0.00020266324281692505, 0.0012327194213867189, 0.0008393973112106323, 0.00016948084036509195]}}, 27: {'grad': {'score': [0.3111227416992188, 0.3467388587956056, 0.3006140102039684, 0.3468962831549202], 'topk_tokens': ['\n', ' excessive', ' avenue', ' conventions', '-n', ' convention', ' Cyrus', ' mon', '\n', 'ackers', 'ition', ' Thanksgiving', '10', '\n', ' business', ' step', ' designated', '\n', ' accepted', ' Bottle'], 'evidence_proportions': [0.2859980265299479, 0.2721281051635742, 0.375836181640625, 0.3684539794921875, 0.27009518941243493]}, 'weight': {'score': [0.012875205278396607, 0.0025405631809923834, 0.00576047734780745, 0.0025133607215555103], 'topk_tokens': [' bedroom', '?\n', ' Daniel', ' barric', 'CH', ' THE', '<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', 'b', 'assistant', ' garden', ' bathroom', '\n\n', ':', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004618217547734578, 0.007144540548324585, 0.02862212657928467, 0.030324339866638184, 0.004353841145833334]}, 'saliency': {'score': [0.0014670133590698243, 4.873788251789698e-05, 0.0006197176196358421, 4.477007214519883e-05], 'topk_tokens': [' the', ' dropped', ' the', 'THE', 'b', ' East', ' the', ' the', ' Bridge', 'Bridge', '<|begin_of_text|>', ' Daniel', ' THE', ' garden', ' bathroom', ':', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [3.0179818471272785e-05, 0.0008243098855018616, 0.004001426696777344, 0.0028650611639022827, 0.0002882728974024455]}}, 28: {'grad': {'score': [0.4435553550720215, 0.42221861351114287, 0.5114446119828657, 0.4220123343419643], 'topk_tokens': [' received', ' population', 'material', ' preceding', ' speakers', ' summer', ' before', ' scale', ' summer', ' summer', ' half', ' spring', ' returns', ' probably', ' platform', 'half', ' spring', '600', ' half', 'ball'], 'evidence_proportions': [0.5302836100260417, 0.4884833097457886, 0.39496002197265623, 0.3616032600402832, 0.42200597127278644]}, 'weight': {'score': [0.010629582405090331, 0.002412416324839983, 0.0021174387498335404, 0.0023959777489553267], 'topk_tokens': ['Question', ' the', ' discarded', ' Daniel', ' garden', '.\n\n', ' bathroom', '<|eot_id|>', '?\n', 'Answer', '<|eot_id|>', ' the', 'b', 'assistant', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00010654826958974202, 0.002105921506881714, 0.02062414288520813, 0.02806997299194336, 0.006879329681396484]}, 'saliency': {'score': [0.00020771265029907227, 4.4733126454514645e-05, 0.00013446266000921076, 4.423332990565353e-05], 'topk_tokens': ['During', ' the', ' the', ' garden', '?\n', ' Bridge', 'Answer', '.\n\n', ' bathroom', ' nearly', 'b', 'Bridge', ' Bridge', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', ':'], 'evidence_proportions': [3.1888484954833984e-06, 6.095319986343384e-05, 0.00042479038238525386, 0.00031507760286331177, 0.00025760134061177575]}}, 29: {'grad': {'score': [0.8276385498046875, 0.6562476488226912, 0.8252709128639915, 0.6555863296743307], 'topk_tokens': [' THE', ' In', ' An', ' STR', ' In', 's', ' I', 'ioneer', ' Paul', 'ION', ' The', ' Paul', 'ION', 'Spring', ' M', ' Pioneer', 'UL', 'ER', ' THE', ' The'], 'evidence_proportions': [0.9092178344726562, 0.7814178466796875, 0.80673828125, 0.7628631591796875, 0.8374735514322916]}, 'weight': {'score': [0.006351751089096069, 0.002480577388332944, 0.0027188130400397563, 0.0024721473332299337], 'topk_tokens': [' was', ' garden', 'Question', ' and', ' the', ' Does', '<|eot_id|>', '.\n\n', '?\n', '<|eot_id|>', 'b', 'Answer', ' the', 'assistant', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [5.957484245300293e-05, 0.0011776089668273926, 0.01044420599937439, 0.01866912841796875, 0.004471391439437866]}, 'saliency': {'score': [0.0003100132942199707, 4.804214093604493e-05, 0.000149867751381614, 4.731586079541523e-05], 'topk_tokens': ['THE', 'Does', '.\n\n', ' ', ':', 'THE', ' the', '<|eot_id|>', ' Does', '<|eot_id|>', 'assistant', ' and', 'Answer', ':', 'athroom', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.493561426798503e-06, 6.0074031352996826e-05, 0.00047519207000732425, 0.0009658709168434143, 0.00020627180735270181]}}, 30: {'grad': {'score': [0.3997607421875, 0.45878253547102643, 0.5105119185014204, 0.45881042329166927], 'topk_tokens': [' Europe', ' of', ' abroad', ' the', ' bathroom', 'itter', ' S', ' Buchanan', ' B', ' account', 'B', 'deal', '3', ' Burb', ' forb', 'b', ' B', ' B', 'b', 'b'], 'evidence_proportions': [0.36611175537109375, 0.4304046630859375, 0.3779296875, 0.5819587707519531, 0.3097076416015625]}, 'weight': {'score': [0.013433808088302612, 0.002484988279230399, 0.005280547521331094, 0.0024572884941904865], 'topk_tokens': ['.', 'IR', ':', ' Miles', ' barric', 'Question', ' the', '<|eot_id|>', '.\n\n', '<|eot_id|>', 'b', 'Answer', 'assistant', '?\n', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006404668092727661, 0.0032115578651428223, 0.025499629974365237, 0.03208732604980469, 0.01055145263671875]}, 'saliency': {'score': [0.000812835693359375, 6.45352033972593e-05, 0.00020747022195295855, 6.272954460885422e-05], 'topk_tokens': [' Miles', ' AND', '<|eot_id|>', ' milk', ' bathroom', ' barric', 'CH', 'Answer', 'Bridge', ' the', 'assistant', ' Bench', ' Bridge', '<|end_header_id|>', ' bathroom', ':', 'athroom', '<|begin_of_text|>', 'b', '<|start_header_id|>'], 'evidence_proportions': [4.9789746602376304e-05, 0.00021375715732574463, 0.000694674253463745, 0.002344079315662384, 0.0010529061158498128]}}, 31: {'grad': {'score': [0.46956729888916016, 0.6195732931196273, 0.38188806447115814, 0.6203152551825943], 'topk_tokens': [' the', ' the', ' the', ' the', 'membership', ' the', ' the', ' the', ' the', ' office', ' their', ' the', ' had', ' the', ' was', ' he', ' an', ' the', ' the', ' the'], 'evidence_proportions': [0.45296764373779297, 0.4755864143371582, 0.4364595413208008, 0.4395027160644531, 0.5297870635986328]}, 'weight': {'score': [0.002743873596191406, 0.002279186868914872, 0.0017006031491539695, 0.0022792787275872257], 'topk_tokens': [' the', ' was', ',', 'Question', ' the', ' Where', ':', '.\n\n', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|eot_id|>', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00025058289368947345, 0.0012399926781654358, 0.0038438916206359865, 0.005627036094665527, 0.0034009615580240884]}, 'saliency': {'score': [4.823684692382812e-05, 2.7565207831878624e-05, 5.83515925840898e-05, 2.7466538897665486e-05], 'topk_tokens': ['\n\n', ' Geo', ' the', ' left', ' the', ' Key', 'Question', ' the', ':', '?\n', ' Market', 'CH', '<|start_header_id|>', '<|eot_id|>', 'Answer', '<|begin_of_text|>', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [1.862645149230957e-06, 1.0512769222259521e-05, 8.193850517272948e-05, 8.560717105865479e-05, 6.676216920216878e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 00:54:41.518 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:54:41.518 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-1_1-2-5-7-8.pkl | len: 10 |  size: 9.15 KB
Processing depth (1, 2, 5, 7, 8):   2%|‚ñè         | 2/100 [00:37<30:47, 18.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.28it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.34it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.37it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.62it/s]
Processing depth (0, 5, 6, 8, 9):   2%|‚ñè         | 2/100 [00:44<30:47, 18.86s/it]2025-01-22 00:54:48.885 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:54:48.886 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:54:48.886 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 00:54:48.903 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5932, 5936) -->  Daniel grabbed the milk
2025-01-22 00:54:48.903 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel went to the garden.
2025-01-22 00:54:48.924 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7166, 7171) --> . Daniel went to the
2025-01-22 00:54:48.924 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel left the milk.
2025-01-22 00:54:48.951 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9590, 9594) -->  left the milk.
2025-01-22 00:54:48.951 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 00:54:48.984 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10696, 10702) --> . Mary journeyed to the
2025-01-22 00:54:48.984 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:54:48.991 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2362, 2368) -->  the senate. Sandra journeyed
2025-01-22 00:54:48.992 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the football there.
2025-01-22 00:54:49.002 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3479, 3484) --> . Mary got the football
2025-01-22 00:54:49.002 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:54:49.016 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4761, 4767) --> . John went back to the
2025-01-22 00:54:49.017 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 00:54:49.022 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1867, 1872) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:54:51.004 | INFO     | test_jbb_retain:begin_test:544 - the garden<|eot_id|>
2025-01-22 00:54:51.004 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.67s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.46s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.04it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.92it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.39it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.86it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:55:00.418 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.26632816314697266, 0.32199044180108705, 0.3083501295609908, 0.3221302239722682], 'topk_tokens': ['sur', ' St', '.', ' the', '.', ' absence', ' sop', '.', ' S', 'user', '.', ' S', 'ew', 'low', ' turtle', '.', 'ols', ' S', '.', ' Do'], 'evidence_proportions': [0.32204421361287433, 0.23973846435546875, 0.2990009307861328, 0.14651870727539062, 0.2809842427571615]}, 'weight': {'score': [0.022054048776626586, 0.002583336443535653, 0.004886228929866444, 0.0025389279620427964], 'topk_tokens': ['\n\n', ' bathroom', ' the', '.', 'Bridge', ' milk', ' Daniel', 'Answer', '<|eot_id|>', ' Bridge', 'assistant', ':', '<|eot_id|>', 'b', '\n\n', '<|start_header_id|>', ' garden', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0034408966700236005, 0.05263996124267578, 0.0337161123752594, 0.032996177673339844, 0.0032634536425272627]}, 'saliency': {'score': [0.0018267703056335449, 4.132338164421607e-05, 0.00015432455322959206, 3.742962863854664e-05], 'topk_tokens': ['During', ' Miles', '\n\n', ':', ' bathroom', ' milk', '<|eot_id|>', ' bedroom', ' Daniel', 'Answer', ' bedroom', '<|eot_id|>', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', 'b', ' Daniel', 'athroom', ' garden'], 'evidence_proportions': [0.0002280523379643758, 0.006884962320327759, 0.0018721222877502441, 0.0017040744423866272, 9.736418724060059e-05]}}, 25: {'grad': {'score': [0.6216044616699219, 0.6837538511078587, 0.5304417176680132, 0.6841609409832202], 'topk_tokens': [' o', ' favored', ' containing', ' with', ' set', 'M', ' incorporated', ' of', ' St', 'ivery', ' a', ' bogus', ' free', ' money', ' im', ' M', ' inverted', ' M', ' l', ' no'], 'evidence_proportions': [0.7145589192708333, 0.58343505859375, 0.5790451049804688, 0.7695350646972656, 0.49094200134277344]}, 'weight': {'score': [0.019519120454788208, 0.002513566167474882, 0.0015941600907932627, 0.0024801079024363896], 'topk_tokens': [' Daniel', ' milk', ' Bench', ' discarded', '.\n\n', ' THE', 'b', '?\n', 'Answer', ' garden', '<|eot_id|>', ' Daniel', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0011673768361409504, 0.06252288818359375, 0.018723589181900025, 0.027266263961791992, 0.004699865976969401]}, 'saliency': {'score': [0.0010128843784332276, 3.84522692265653e-05, 3.470480442047119e-05, 3.644612491120978e-05], 'topk_tokens': [' milk', 'b', ' Seventh', ' East', 'Den', 'Answer', ' George', ' Geo', ':', '<|start_header_id|>', ' garden', ' Bench', '<|eot_id|>', 'athroom', '<|eot_id|>', ' THE', ' Daniel', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.681316057840983e-05, 0.0044244080781936646, 0.0006582736968994141, 0.0008989125490188599, 6.609658400217693e-05]}}, 26: {'grad': {'score': [0.5573773193359375, 0.6000514767161906, 0.5751921913840554, 0.6001848231227196], 'topk_tokens': [' to', 'rich', 'issippi', ' into', ' medicine', 'graph', ' in', ' the', ' the', 'graph', ' steam', ' coincidence', ' Press', 'graph', 'UX', 'graph', 'graph', 'UX', ' bitter', 'itter'], 'evidence_proportions': [0.5273488362630209, 0.6362380981445312, 0.5634185791015626, 0.61212158203125, 0.4933013916015625]}, 'weight': {'score': [0.013834794759750366, 0.002491676637651263, 0.0022295401854948564, 0.002468720849260498], 'topk_tokens': [' Bridge', ' the', ' discarded', '.\n\n', ' Daniel', ' bathroom', ' the', '?\n', '<|eot_id|>', '<|eot_id|>', 'b', 'Answer', 'assistant', ' garden', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004188507795333862, 0.03495311737060547, 0.01604815125465393, 0.018012046813964844, 0.00477290153503418]}, 'saliency': {'score': [0.0011779439449310302, 4.91847014837338e-05, 0.00010543113405054265, 4.675069036138607e-05], 'topk_tokens': [' barric', ' East', ' Gray', ' Daniel', ' Anthony', ' turtle', 'assistant', '?\n', 'Answer', ' Bridge', ' bathroom', '<|start_header_id|>', ' Daniel', '<|end_header_id|>', ' garden', 'athroom', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.00026960670948028564, 0.005640067160129547, 0.000626218318939209, 0.00033698976039886475, 0.00013193984826405844]}}, 27: {'grad': {'score': [0.3025608825683594, 0.3614936117105832, 0.3222082528201016, 0.3616867695459944], 'topk_tokens': [' be', '\n', ' sentinel', ' excessive', ' designated', '\n', ' convention', ' one', '\n', '\n', '10', '\n', '-n', 'ides', '\n', ' step', ' business', ' accepted', '\n', ' Bottle'], 'evidence_proportions': [0.22866566975911456, 0.3187065124511719, 0.33707275390625, 0.4272804260253906, 0.2537860870361328]}, 'weight': {'score': [0.019518164396286012, 0.0025425551506568105, 0.0029687691818584094, 0.002506712567626967], 'topk_tokens': [' Daniel', ':', ' Bridge', '<|eot_id|>', '?\n', ' THE', '<|eot_id|>', ' bathroom', 'Answer', ' Daniel', '.\n\n', 'assistant', 'b', '\n\n', ':', ' garden', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005021671454111735, 0.06069755554199219, 0.02371131777763367, 0.018413543701171875, 0.003803849220275879]}, 'saliency': {'score': [0.0011444091796875, 4.762619873635297e-05, 0.00034663487564433706, 4.4816930399238913e-05], 'topk_tokens': [' the', ' dropped', 'NEW', ' the', 'THE', '\n\n', ' Daniel', 'assistant', ' the', ' bathroom', '<|begin_of_text|>', ' Bridge', ' THE', ':', ' garden', '<|end_header_id|>', '.\n\n', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.0002262592315673828, 0.0021399930119514465, 0.0023681759834289547, 0.0014819130301475525, 0.00015402833620707193]}}, 28: {'grad': {'score': [0.32529449462890625, 0.36976969555107414, 0.4334328391335227, 0.36974583923511317], 'topk_tokens': ['600', '\n', ' almost', ' summer', ' lie', ' the', ' half', ' spring', ' returns', '.', ' lie', ' summer', ' summer', ' summer', 'dent', '600', 'half', ' summer', 'ball', ' half'], 'evidence_proportions': [0.44291178385416663, 0.28614139556884766, 0.3084228515625, 0.284271240234375, 0.27518781026204425]}, 'weight': {'score': [0.015092523097991943, 0.0024237286182026458, 0.0018162226135080512, 0.002398662122591057], 'topk_tokens': [' was', '.\n\n', ' the', ' to', ' bathroom', ' garden', ' the', '?\n', '<|eot_id|>', 'Answer', ' the', '<|eot_id|>', 'b', 'assistant', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0020026365915934243, 0.018913984298706055, 0.03235982656478882, 0.013578414916992188, 0.012254754702250164]}, 'saliency': {'score': [0.0003649139404296875, 4.894389361645991e-05, 6.481056863611394e-05, 4.826232709681332e-05], 'topk_tokens': [' bathroom', ' Floral', 'Answer', 'athroom', '.\n\n', '?\n', ' the', ' Bridge', ' garden', 'Bridge', 'During', ' the', 'b', ' Bridge', 'assistant', '\n\n', '<|begin_of_text|>', '<|end_header_id|>', ':', '<|start_header_id|>'], 'evidence_proportions': [8.463859558105469e-05, 0.00019887089729309082, 0.0006331205368041992, 0.0003900378942489624, 0.0005156298478444418]}}, 29: {'grad': {'score': [0.49940673828125, 0.47146067633845895, 0.48576424338600854, 0.47137694389558865], 'topk_tokens': [' STR', 'ian', ' l', 's', ' I', ' Pioneer', 'y', ' Ch', 'ION', 'y', ' Paul', ' M', ' THE', ' THE', ' In', 'ER', 'UL', ' The', ' In', 'ION'], 'evidence_proportions': [0.6402072906494141, 0.24904680252075195, 0.3957218170166016, 0.4725837707519531, 0.6297988891601562]}, 'weight': {'score': [0.006641186475753784, 0.0024802122933254857, 0.0014914992180737581, 0.0024734140230474187], 'topk_tokens': [' where', ' to', ' Does', ' the', ' was', '<|eot_id|>', ' the', '.\n\n', '?\n', '<|eot_id|>', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0004510382811228434, 0.005433797836303711, 0.010562807321548462, 0.014815211296081543, 0.004918893178304036]}, 'saliency': {'score': [0.00022417068481445313, 3.865664903494897e-05, 5.3025104782798074e-05, 3.824729876879955e-05], 'topk_tokens': [' to', ' ', 'THE', ' where', ':', ' the', ' a', ' place', 'Does', '<|eot_id|>', 'assistant', 'b', 'athroom', ' Does', '<|eot_id|>', 'Answer', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.398093541463216e-05, 0.00010514259338378906, 0.0004587709903717041, 0.0006004124879837036, 5.7384371757507324e-05]}}, 30: {'grad': {'score': [0.3209967041015625, 0.4096176294409519, 0.36422868208451703, 0.40988321198725225], 'topk_tokens': ['0', ' the', 'B', ' its', 'itter', '2', ' S', ' Buchanan', ' of', ' the', ' account', '3', 'b', ' Burb', ' forb', ' B', ' B', 'deal', 'b', 'b'], 'evidence_proportions': [0.3332265218098958, 0.34970855712890625, 0.28016204833984376, 0.4440193176269531, 0.2416394551595052]}, 'weight': {'score': [0.013721864223480224, 0.0024743160599788546, 0.003401234745979309, 0.002449396136397666], 'topk_tokens': [' to', ' the', ' the', ' Where', ' Daniel', ':', 'Question', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'assistant', '?\n', 'Answer', 'b', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002308269341786702, 0.027337074279785156, 0.018841278553009034, 0.02151012420654297, 0.006600300470987955]}, 'saliency': {'score': [0.0005833613872528076, 7.314678915970334e-05, 8.604607798836447e-05, 7.206935160338553e-05], 'topk_tokens': [' the', '<|eot_id|>', ' Shak', ' bathroom', 'Gov', '<|eot_id|>', ' Bench', '?\n', ' the', ' Daniel', 'Answer', ' Bridge', 'assistant', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ':', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.000526095430056254, 0.0013529956340789795, 0.000555872917175293, 0.0006616786122322083, 9.823342164357503e-05]}}, 31: {'grad': {'score': [0.4340816879272461, 0.5177808148211948, 0.3729864033785733, 0.5182169373814399], 'topk_tokens': [' the', '7', ' the', ' the', ' the', ' the', ' the', ' paper', ' was', ' had', ' the', ' the', 'membership', ' the', ' he', ' the', ' the', ' an', ' the', ' the'], 'evidence_proportions': [0.32950083414713544, 0.4347879886627197, 0.4843227386474609, 0.5130348205566406, 0.44368871053059894]}, 'weight': {'score': [0.004149048328399658, 0.002287751647687977, 0.0014125284823504362, 0.002285497680791566], 'topk_tokens': [' to', 'Question', ' was', ',', ' the', ':', '.\n\n', ' Where', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007374286651611328, 0.009666204452514648, 0.00518714189529419, 0.004670083522796631, 0.002670129140218099]}, 'saliency': {'score': [6.151676177978515e-05, 2.6227113238793164e-05, 3.1677159396084875e-05, 2.6144305254246375e-05], 'topk_tokens': [' prior', 'light', ' discarded', '<|eot_id|>', 'Question', 'CH', ' the', ' Where', '\n\n', '.\n\n', '<|eot_id|>', '?\n', '<|begin_of_text|>', ':', 'Answer', '<|start_header_id|>', 'b', '<|end_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [3.013014793395996e-05, 0.00015322118997573853, 6.318092346191406e-05, 3.053992986679077e-05, 5.103151003519694e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 00:55:00.419 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:55:00.420 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-2_0-5-6-8-9.pkl | len: 10 |  size: 9.03 KB
Processing depth (0, 5, 6, 8, 9):   3%|‚ñé         | 3/100 [00:56<30:31, 18.88s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.31it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.30it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.75it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.55it/s]
Processing depth (0, 4, 6, 8, 9):   3%|‚ñé         | 3/100 [01:03<30:31, 18.88s/it]2025-01-22 00:55:07.575 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:55:07.575 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:55:07.576 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 00:55:07.589 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (4859, 4863) -->  Daniel grabbed the milk
2025-01-22 00:55:07.590 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel went to the garden.
2025-01-22 00:55:07.611 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7166, 7171) --> . Daniel went to the
2025-01-22 00:55:07.611 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel left the milk.
2025-01-22 00:55:07.638 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (9590, 9594) -->  left the milk.
2025-01-22 00:55:07.638 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 00:55:07.671 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (10696, 10702) --> . Mary journeyed to the
2025-01-22 00:55:07.671 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:55:07.678 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2362, 2368) -->  the senate. Sandra journeyed
2025-01-22 00:55:07.678 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the football there.
2025-01-22 00:55:07.688 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3479, 3484) --> . Mary got the football
2025-01-22 00:55:07.689 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:55:07.703 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4761, 4767) --> . John went back to the
2025-01-22 00:55:07.703 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 00:55:07.709 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1867, 1872) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:55:09.654 | INFO     | test_jbb_retain:begin_test:544 - the garden<|eot_id|>
2025-01-22 00:55:09.655 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:38,  5.54s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.41s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:07,  1.41s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.06it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.48it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.95it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.43it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.90it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.14it/s]
2025-01-22 00:55:19.517 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.22365470886230468, 0.27287310189656816, 0.26012529026378284, 0.2729979499945501], 'topk_tokens': ['.', ' first', ' It', 'low', ' absence', ' compl', ' hundred', ' war', ' the', ' S', '.', ' the', ' Hor', ' the', ' comparison', ' S', 'ols', 'consider', ' Do', ' sop'], 'evidence_proportions': [0.27010567982991535, 0.13876914978027344, 0.18538665771484375, 0.1577606201171875, 0.3096135457356771]}, 'weight': {'score': [0.016268882751464844, 0.002587749139891111, 0.007480064576322382, 0.002550593357384845], 'topk_tokens': [' the', ' milk', 'Bridge', ' bedroom', ' Father', '<|eot_id|>', ':', 'b', 'assistant', ' battling', '.', '<|eot_id|>', ' garden', ' Bridge', ' barric', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006319999694824219, 0.027420997619628906, 0.03425698280334472, 0.01891183853149414, 0.00203096866607666]}, 'saliency': {'score': [0.0007111334800720215, 3.7816944254976366e-05, 0.0002711008895527233, 3.6001941760184054e-05], 'topk_tokens': ['c', ':', ' milk', '<|eot_id|>', 'Answer', ' Bench', ' the', '.', '<|eot_id|>', 'Bridge', ' bathroom', '.', ' Bridge', ' barric', ' Bull', '<|start_header_id|>', ' bedroom', '<|begin_of_text|>', 'athroom', ' garden'], 'evidence_proportions': [0.0006022254625956218, 0.001613706350326538, 0.000601273775100708, 0.0011012479662895203, 4.979968070983887e-05]}}, 25: {'grad': {'score': [0.45625810623168944, 0.4291529588340604, 0.33372272144664417, 0.42927044656646685], 'topk_tokens': [' a', ' new', ' the', ' a', ' self', ' black', ' a', ' incorporated', ' free', ' l', 'M', ' set', ' for', ' a', ' im', ' at', ' bogus', ' of', ' no', ' inverted'], 'evidence_proportions': [0.5074412027994791, 0.4766387939453125, 0.47349939346313474, 0.5729522705078125, 0.29932403564453125]}, 'weight': {'score': [0.011697372198104858, 0.00250862910331684, 0.0022882358594374223, 0.002490047883187576], 'topk_tokens': [' Daniel', ' Dan', 'b', '.', ' Capt', ' Bench', ' garden', 'Answer', '?\n', ' discarded', '<|eot_id|>', ' barric', 'assistant', '<|eot_id|>', ':', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0022640426953633628, 0.03289031982421875, 0.013563960790634155, 0.014958858489990234, 0.0032722552617390948]}, 'saliency': {'score': [0.0006449258327484131, 3.364956695873145e-05, 4.097548398104581e-05, 3.2373490540021075e-05], 'topk_tokens': [' Daniel', '.', 'Answer', ' Capt', 'athroom', '.', ' milk', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'Gov', ' Dan', ' Geo', ' garden', ' Daniel', ' barric', ' Bench', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [7.594128449757895e-05, 0.0026694759726524353, 0.0005834341049194336, 0.00044305622577667236, 5.003313223520915e-05]}}, 26: {'grad': {'score': [0.3973599624633789, 0.42241813200610645, 0.3937909386374734, 0.4225219373682514], 'topk_tokens': ['bread', 'ian', 'graph', 'ian', ' barric', 'bec', ' medicine', 'graph', 'est', 'rich', 'b', 'hue', 'b', 'UX', 'UX', ' Milwaukee', ' Press', 'b', 'itter', ' bitter'], 'evidence_proportions': [0.33243433634440106, 0.55596923828125, 0.41068668365478517, 0.4181404113769531, 0.3315868377685547]}, 'weight': {'score': [0.009758431911468506, 0.002499492700840379, 0.004727497696876526, 0.002480447122408839], 'topk_tokens': [' CHIP', ' Father', ' Bridge', ' bathroom', ' discarded', '?\n', '<|eot_id|>', '<|eot_id|>', ' garden', 'b', 'Answer', 'assistant', ' the', '\n\n', ' barric', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004962056875228882, 0.016521453857421875, 0.015109562873840333, 0.0121995210647583, 0.003959457079569499]}, 'saliency': {'score': [0.0005324888229370117, 4.04939576719747e-05, 0.00023809075355529785, 3.91183998186398e-05], 'topk_tokens': ['.', 'CH', '?\n', ' bathroom', 'IR', ' barric', 'Answer', ' Dan', ' Bridge', ' Daniel', ' Father', ' the', '<|start_header_id|>', ' garden', 'athroom', '<|end_header_id|>', '\n\n', ':', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.00020305315653483075, 0.001930922269821167, 0.0005127370357513427, 0.00027992576360702515, 0.0001144707202911377]}}, 27: {'grad': {'score': [0.2584970474243164, 0.30327100610132524, 0.23661752180619675, 0.3034846671178033], 'topk_tokens': [' like', 'ree', ' was', 'ly', ' was', ' be', 'ides', ' been', ' step', ' was', ' also', ' Bre', ' be', ' was', ' was', ' were', ' was', ' would', ' business', ' Bottle'], 'evidence_proportions': [0.21761131286621094, 0.17803573608398438, 0.2573400497436523, 0.42417144775390625, 0.24353822072347003]}, 'weight': {'score': [0.014121373891830444, 0.0025472838021627776, 0.0037928589365699076, 0.0025211100370662112], 'topk_tokens': [' battling', 'Gov', ' THE', 'CH', '?\n', ' the', ' Bridge', 'Answer', 'assistant', '.\n\n', ' bathroom', 'b', ' garden', '\n\n', ':', ' barric', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010146955649058024, 0.030539751052856445, 0.021220296621322632, 0.01165097951889038, 0.002881368001302083]}, 'saliency': {'score': [0.0009224987030029296, 3.666968608218482e-05, 0.0007251948118209839, 3.358810632765894e-05], 'topk_tokens': [' the', ' prior', 'CH', ' the', ' Dan', '<|end_header_id|>', '<|start_header_id|>', ' garden', '\n\n', 'Gov', ' the', ' the', 'b', ' THE', ' bathroom', ' Bridge', '<|begin_of_text|>', ' barric', 'athroom', '.\n\n'], 'evidence_proportions': [0.0005581279595692953, 0.0006020590662956238, 0.0024092018604278563, 0.0011887028813362122, 8.410712083180745e-05]}}, 28: {'grad': {'score': [0.27728546142578125, 0.30111823198540005, 0.27412254160100763, 0.30121654007104265], 'topk_tokens': ['in', ' summer', ' lively', ' an', ' summer', '\n', ' summer', 'nes', '.', 'half', ' summer', '\n', ' the', 'ot', ' a', ' lie', ' lie', 'dent', ' half', ','], 'evidence_proportions': [0.3026936848958333, 0.37580299377441406, 0.18609008789062498, 0.2188262939453125, 0.3011678059895833]}, 'weight': {'score': [0.011688103675842285, 0.0024124759049540794, 0.0024248497052626176, 0.002393292057831373], 'topk_tokens': [' the', ' garden', ' was', ' Bridge', ' the', ' to', ' the', 'Answer', '<|eot_id|>', '?\n', '<|eot_id|>', 'assistant', 'b', '<|end_header_id|>', ' the', '\n\n', ':', 'athroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002401818831761678, 0.005015850067138672, 0.03233054876327515, 0.009369134902954102, 0.009766499201456705]}, 'saliency': {'score': [0.00034163713455200197, 3.818568206832324e-05, 0.00014280053702267733, 3.7368642478082975e-05], 'topk_tokens': [' back', ' barbecue', '?\n', 'Answer', ' bathroom', ' milk', 'Bridge', ' garden', ' the', 'athroom', 'b', ' Bridge', 'assistant', '<|end_header_id|>', ' Bridge', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', ':', ' the'], 'evidence_proportions': [0.00011801222960154216, 0.00040824711322784424, 0.0008797705173492432, 0.0001346692442893982, 0.00021038949489593506]}}, 29: {'grad': {'score': [0.2705256652832031, 0.29122649747085666, 0.26852833140980115, 0.29131052328956714], 'topk_tokens': [' S', ' l', ' THE', 'g', ' ga', ' not', 'y', ' S', ' Paul', 'UL', 'y', 're', ' The', ' M', 's', ' In', ' Ch', 'ION', ' THE', 'ION'], 'evidence_proportions': [0.33728535970052087, 0.18851470947265625, 0.12085647583007814, 0.24661636352539062, 0.3991038004557292]}, 'weight': {'score': [0.005612196922302246, 0.0024866773244412075, 0.0018845525654879484, 0.002481315297896045], 'topk_tokens': [' place', ' the', ' in', ' to', '?\n', ' Does', ' where', ' the', ' was', '<|eot_id|>', 'Answer', 'b', 'assistant', '<|end_header_id|>', ' the', ':', 'athroom', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0013112773497899375, 0.0013746023178100586, 0.013254934549331666, 0.010009765625, 0.003437519073486328]}, 'saliency': {'score': [0.00018997788429260255, 2.543715177048223e-05, 5.8107755400917746e-05, 2.5037856481032338e-05], 'topk_tokens': ['athroom', ':', ' was', 'Does', '<|eot_id|>', ' where', ' to', ' place', 'assistant', '<|eot_id|>', ':', '<|end_header_id|>', 'Answer', ' in', '<|start_header_id|>', ' Does', 'b', ' the', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [2.345442771911621e-05, 3.0390918254852295e-05, 0.0005941212177276612, 0.000296570360660553, 5.504488945007324e-05]}}, 30: {'grad': {'score': [0.27491279602050783, 0.3700004575752763, 0.22851042314009232, 0.37045409931188533], 'topk_tokens': ['0', ' LINE', ' the', 'ar', ' account', 'b', ' Times', ' the', 'moment', 'ire', ' Europe', ' of', ' its', ' forb', ' the', '2', 'b', 'b', ' account', 'deal'], 'evidence_proportions': [0.27490743001302087, 0.3445281982421875, 0.2632026672363281, 0.343355655670166, 0.19263776143391928]}, 'weight': {'score': [0.011374826431274415, 0.0024812198958874573, 0.004129081964492798, 0.002459852094788962], 'topk_tokens': ['IR', ':', 'Question', 'ORT', '<|eot_id|>', 'Gov', '<|eot_id|>', '.\n\n', ' barric', 'Answer', 'assistant', ' the', '?\n', 'b', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004452904065450032, 0.009663224220275879, 0.02057046890258789, 0.018756866455078125, 0.006853421529134114]}, 'saliency': {'score': [0.0008136487007141113, 6.29965876716578e-05, 0.00011087141253731467, 6.13588791133116e-05], 'topk_tokens': [' Bull', 'Bridge', '\n\n', ' the', ' Bridge', '.', ' bathroom', ' the', 'assistant', 'Gov', ' barric', '<|end_header_id|>', ' Bridge', ' bathroom', ' the', '<|begin_of_text|>', ':', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.0008879701296488445, 0.0006973966956138611, 0.0016827225685119628, 0.0006943568587303162, 0.00017212828000386557]}}, 31: {'grad': {'score': [0.28776060104370116, 0.34306081743120254, 0.280180194161155, 0.3432893647061677], 'topk_tokens': [' the', ' the', '7', ' the', ' location', ' location', ' the', ' office', ' location', 'If', ' the', ' is', ' the', ' he', ' August', ' the', 'membership', ' the', ' the', ' the'], 'evidence_proportions': [0.2575174967447917, 0.24222373962402344, 0.3217436790466308, 0.34041595458984375, 0.2849388122558594]}, 'weight': {'score': [0.0029544878005981446, 0.002290154319798979, 0.001428404992276972, 0.002290348514823397], 'topk_tokens': [' where', 'Question', ' to', ' was', ':', '.\n\n', '<|eot_id|>', ' the', ' Where', 'Answer', '?\n', '<|eot_id|>', 'b', 'assistant', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009958346684773762, 0.0025501549243927, 0.004506683349609375, 0.005395352840423584, 0.002261956532796224]}, 'saliency': {'score': [7.64942169189453e-05, 1.966649833083869e-05, 1.948394558646462e-05, 1.9549436952196612e-05], 'topk_tokens': [' bathroom', ' Do', ' CHIP', ':', ' Daniel', 'CH', ' the', ' S', '<|eot_id|>', 'Answer', '\n\n', '?\n', '<|start_header_id|>', ' the', ':', '<|begin_of_text|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [5.3435564041137695e-05, 6.029754877090454e-05, 0.0001729846000671387, 5.873292684555054e-05, 4.178285598754883e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 00:55:19.519 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:55:19.519 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-3_0-4-6-8-9.pkl | len: 10 |  size: 8.93 KB
Processing depth (0, 4, 6, 8, 9):   4%|‚ñç         | 4/100 [01:15<30:20, 18.96s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.36it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.38it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.40it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.86it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.65it/s]
Processing depth (2, 5, 6, 7, 8):   4%|‚ñç         | 4/100 [01:22<30:20, 18.96s/it]2025-01-22 00:55:26.545 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:55:26.552 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2467, 2473) --> . Daniel journeyed to the
2025-01-22 00:55:26.553 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 00:55:26.569 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (5932, 5936) -->  Daniel grabbed the milk
2025-01-22 00:55:26.569 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel went to the garden.
2025-01-22 00:55:26.590 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (7166, 7171) --> . Daniel went to the
2025-01-22 00:55:26.591 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Daniel left the milk.
2025-01-22 00:55:26.614 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (8348, 8352) -->  Daniel left the milk
2025-01-22 00:55:26.614 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 00:55:26.643 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (9594, 9600) -->  Mary journeyed to the office
2025-01-22 00:55:26.643 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 00:55:26.651 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (2355, 2361) -->  the senate. Sandra journeyed
2025-01-22 00:55:26.651 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary got the football there.
2025-01-22 00:55:26.661 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3479, 3484) --> . Mary got the football
2025-01-22 00:55:26.661 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  John went back to the bedroom.
2025-01-22 00:55:26.676 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4761, 4767) --> . John went back to the
2025-01-22 00:55:26.676 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 00:55:26.681 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (1860, 1865) --> . Mary moved to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:55:28.662 | INFO     | test_jbb_retain:begin_test:544 - The garden.<|eot_id|>
2025-01-22 00:55:28.662 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12146])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:39,  5.67s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:14,  2.46s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:06<00:07,  1.44s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:06<00:03,  1.05it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:02,  1.46it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:01,  1.93it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  2.89it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.12it/s]
2025-01-22 00:55:38.232 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.28458778381347655, 0.37347696963548954, 0.40261424671519885, 0.373607626514501], 'topk_tokens': [' Arr', 'low', ' exclaimed', 'were', 'able', 'consider', ' Do', ' were', '.', ' disposed', ' entrance', ' departing', ' appearance', ' compl', ' appearance', ' out', ' appearance', ' examined', ' comparison', ' absence'], 'evidence_proportions': [0.36692746480305993, 0.1637859344482422, 0.303509521484375, 0.19567251205444336, 0.32629140218098956]}, 'weight': {'score': [0.020153799057006837, 0.0025840302888959885, 0.0039837983521548185, 0.0025451905007126746], 'topk_tokens': [' East', '.', ' Daniel', ' Bridge', 'Bridge', '.', 'Answer', ' bathroom', '<|eot_id|>', 'assistant', ':', ' garden', '<|eot_id|>', 'b', ' Broadway', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0020264337460199995, 0.032351016998291016, 0.029558950662612916, 0.04128694534301758, 0.008223295211791992]}, 'saliency': {'score': [0.0019285297393798828, 4.755197135359478e-05, 0.00016851045868613502, 4.3446407734195254e-05], 'topk_tokens': ['\n\n', ':', '250', ' Fort', '188', '<|eot_id|>', ' bedroom', ' Daniel', 'Answer', '<|eot_id|>', ' Eighth', '<|end_header_id|>', ' Daniel', ' Miles', ' Daniel', 'b', '<|start_header_id|>', ' bathroom', 'athroom', ' garden'], 'evidence_proportions': [4.7365824381510414e-05, 0.004906028509140015, 0.0027350544929504394, 0.0030909404158592224, 0.0003776500622431437]}}, 25: {'grad': {'score': [0.4246612548828125, 0.469019018894302, 0.41775443337180396, 0.4692038449546046], 'topk_tokens': [' so', ' be', 'E', ' money', ' with', ' no', ' favored', ' a', 'ivery', ' lie', ' incorporated', ' containing', ' im', ' free', ' for', ' bogus', ' of', ' Aw', ' l', ' no'], 'evidence_proportions': [0.3888651529947917, 0.478057861328125, 0.41971130371093746, 0.4857940673828125, 0.3882293701171875]}, 'weight': {'score': [0.022236435413360595, 0.0025145860942501776, 0.0018517131155187433, 0.0024750502301412974], 'topk_tokens': ['.', ' Daniel', ' Daniel', 'b', '189', ' bathroom', ' garden', 'Answer', '?\n', ' East', ' Daniel', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0010034193595250447, 0.056325435638427734, 0.019464224576950073, 0.04227018356323242, 0.009697794914245605]}, 'saliency': {'score': [0.000532294511795044, 4.089946697372322e-05, 5.248866297981956e-05, 3.9863287959290304e-05], 'topk_tokens': [' Paul', 'Answer', ' Seventh', ' bathroom', ' Ramsey', ' George', 'athroom', '<|eot_id|>', ' Miss', ' May', ' THE', ' Geo', '189', '<|eot_id|>', ' Bench', ' garden', ' East', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [3.993511199951172e-05, 0.0010824277997016907, 0.00025541782379150395, 0.001485981047153473, 0.0002528379360834757]}}, 26: {'grad': {'score': [0.44102508544921876, 0.49055479826657544, 0.5012332742864435, 0.4906377032697152], 'topk_tokens': [' Press', ' ice', 'UX', ' im', 'graph', 'a', 'graph', 'graph', ' Marshall', ' medicine', 'graph', ',', ' steam', 'graph', 'AM', 'UX', 'graph', ' Marshall', ' bitter', 'itter'], 'evidence_proportions': [0.45599365234375, 0.5455856323242188, 0.4728096008300781, 0.41938018798828125, 0.3442923227945964]}, 'weight': {'score': [0.015067383050918579, 0.002500223440342513, 0.0012450502677397294, 0.0024765442814871884], 'topk_tokens': [' Daniel', ' East', ' Broadway', ' bathroom', ' Daniel', ' garden', '?\n', ' bathroom', '<|eot_id|>', 'b', '<|eot_id|>', 'assistant', 'Answer', ' the', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00039180616537729895, 0.036898016929626465, 0.014716720581054688, 0.03006458282470703, 0.00548328955968221]}, 'saliency': {'score': [0.0016012585163116455, 4.77547959169053e-05, 6.307390603152189e-05, 4.4517759606180617e-05], 'topk_tokens': ['?\n', ' Jackson', '188', ' Daniel', ' bathroom', 'Answer', ' Broadway', ' East', ' Daniel', ':', ' the', '<|end_header_id|>', ' bathroom', ' Daniel', '<|start_header_id|>', ' garden', '\n\n', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [1.9118189811706543e-05, 0.005162321031093597, 0.0014340877532958984, 0.002863489091396332, 0.0001071790854136149]}}, 27: {'grad': {'score': [0.354154052734375, 0.3690783891356799, 0.41322190111333673, 0.36902897177875743], 'topk_tokens': [' conventions', ' sentinel', 'ors', ' received', '\n', 'roduced', ' business', '\n', ' intended', ' convention', '\n', 'ers', ' excessive', '10', ' Cyrus', ' step', 'ition', ' designated', ' accepted', ' Bottle'], 'evidence_proportions': [0.31975301106770837, 0.3058319091796875, 0.451300048828125, 0.37371826171875, 0.3267720540364583]}, 'weight': {'score': [0.016121689081192016, 0.0025388049788902457, 0.00249525633725253, 0.002510824972904333], 'topk_tokens': [' Daniel', 'IR', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', ' Broadway', 'CH', ' bathroom', ' bathroom', 'assistant', '.\n\n', 'b', ' garden', '\n\n', ':', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006055633227030437, 0.03293919563293457, 0.019573479890823364, 0.03672647476196289, 0.0038131276766459146]}, 'saliency': {'score': [0.001360630989074707, 4.0984134613628674e-05, 0.00023641234094446355, 3.7902776829724744e-05], 'topk_tokens': [' the', '\n\n', 'Bridge', ' Daniel', 'THE', '<|begin_of_text|>', ' bathroom', ' Daniel', ' bathroom', ' THE', ' Daniel', ' East', ' Bridge', ':', ' garden', ' Broadway', '.\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [4.3079257011413574e-05, 0.0023559480905532837, 0.002216845750808716, 0.0029089152812957764, 0.0002689361572265625]}}, 28: {'grad': {'score': [0.38034210205078123, 0.381031744430483, 0.43922597711736505, 0.3809273788661449], 'topk_tokens': ['.', ' population', ' received', ' summer', ' before', ' returns', ' scale', ' half', ' summer', ' summer', ' spring', 'dent', ' summer', ' platform', ' summer', '600', ' probably', 'half', ' half', 'ball'], 'evidence_proportions': [0.4537480672200521, 0.4334831237792969, 0.312469482421875, 0.371551513671875, 0.33392969767252606]}, 'weight': {'score': [0.010899907350540161, 0.002430263905184133, 0.0010331354357979515, 0.0024153073476062613], 'topk_tokens': ['Question', '.\n\n', ' the', ' bathroom', ' Daniel', ' garden', ' bathroom', ' the', '<|eot_id|>', 'Answer', '?\n', 'b', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004341254631678263, 0.010165929794311523, 0.016399192810058593, 0.02532672882080078, 0.007654388745625814]}, 'saliency': {'score': [0.00032145261764526367, 5.014650036115314e-05, 6.307390603152189e-05, 4.9562542680038376e-05], 'topk_tokens': [' Prairie', ' Far', ' milk', '.\n\n', '<|eot_id|>', ' Broadway', 'Bridge', ' Nearly', 'b', ' Bridge', ' the', ' nearly', 'assistant', ' Bridge', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', ':', '<|start_header_id|>'], 'evidence_proportions': [8.91586144765218e-06, 0.0001439228653907776, 0.0002446115016937256, 0.0008065104484558105, 0.0004930049180984497]}}, 29: {'grad': {'score': [0.7631088256835937, 0.6525509248928667, 0.8509188565340909, 0.6519619278662698], 'topk_tokens': ['ION', 's', ' Paul', ' The', ' I', ' spring', ' M', 'APER', 'stage', ' An', 'ioneer', 'UL', ' STR', ' THE', 'ER', ' MILL', 'ION', 'ION', 'Spring', ' Pioneer'], 'evidence_proportions': [0.8315531412760417, 0.7509422302246094, 0.6671096801757812, 0.7123565673828125, 0.8166097005208334]}, 'weight': {'score': [0.00577547550201416, 0.0024855538585743537, 0.0015451041134920988, 0.0024804672491962193], 'topk_tokens': [' the', ' Where', 'Question', '.', ' Does', ' and', '.\n\n', '<|eot_id|>', ' the', '?\n', '<|eot_id|>', 'Answer', 'b', 'assistant', '<|end_header_id|>', ':', 'athroom', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006487121184666952, 0.0038938522338867188, 0.005389481782913208, 0.014067530632019043, 0.006950279076894125]}, 'saliency': {'score': [0.0002687251567840576, 5.623571410945673e-05, 0.00012098930098793723, 5.567904455250816e-05], 'topk_tokens': [':', ' in', '.', 'THE', 'Does', ' Does', 'THE', '<|eot_id|>', '<|eot_id|>', '.', 'Answer', 'assistant', ':', ' and', 'athroom', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.7344951629638672e-05, 0.0002509653568267822, 0.0002761900424957276, 0.000672757625579834, 0.00025636951128641766]}}, 30: {'grad': {'score': [0.34292510986328123, 0.4431914718016272, 0.4707919034090909, 0.44334842516083184], 'topk_tokens': [' need', 'moment', ' W', '2', ' its', ' Buchanan', ' of', ' Europe', ' the', ' S', ' account', ' Burb', 'b', '3', ' B', ' B', 'deal', ' forb', 'b', 'b'], 'evidence_proportions': [0.31382624308268225, 0.400177001953125, 0.349908447265625, 0.5113334655761719, 0.21576436360677081]}, 'weight': {'score': [0.010884538888931275, 0.002478504016062038, 0.002977165308865634, 0.0024602325385985266], 'topk_tokens': [' Min', ' the', ' AND', 'Question', 'IR', ' Broadway', ' Miles', '.\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', 'Answer', 'b', '?\n', '\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0014537125825881958, 0.013384819030761719, 0.01062239408493042, 0.022667407989501953, 0.011011719703674316]}, 'saliency': {'score': [0.0007842624187469482, 4.995752581942473e-05, 8.568709546869451e-05, 4.8375665560337634e-05], 'topk_tokens': ['Bridge', 'AB', ' the', ' bathroom', 'In', ' Bench', ' Shak', ' the', 'CH', 'assistant', 'Answer', ' bathroom', ' AND', 'athroom', ' Bridge', '<|end_header_id|>', ':', 'b', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [7.298092047373454e-05, 0.0008642524480819702, 0.0004211127758026123, 0.001375444233417511, 0.0013507207234700522]}}, 31: {'grad': {'score': [0.43276969909667967, 0.5313044099103611, 0.34038821133700287, 0.531855023374165], 'topk_tokens': [' the', ' location', ' office', ' the', ' had', ' the', ' the', ' their', ' location', ' the', ' an', ' was', 'membership', ' the', ' he', ' the', ' the', ' the', ' the', ' the'], 'evidence_proportions': [0.41086673736572266, 0.37604522705078125, 0.454251480102539, 0.4420757293701172, 0.4683834711710612]}, 'weight': {'score': [0.0031670737266540528, 0.0022911641203098233, 0.0013636255806142633, 0.0022910408438030225], 'topk_tokens': [' was', ' the', 'CH', ',', ' Where', ':', 'Question', '.\n\n', '<|eot_id|>', 'Answer', 'b', 'assistant', '?\n', '<|eot_id|>', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00029403964678446454, 0.00500112771987915, 0.002918910980224609, 0.005420088768005371, 0.003522197405497233]}, 'saliency': {'score': [4.3681859970092774e-05, 2.7781730326559035e-05, 5.541335452686656e-05, 2.769865323405407e-05], 'topk_tokens': [' the', ' Mary', ' the', ' the', '\n\n', '<|eot_id|>', ' the', '?\n', ' Emily', ' Market', 'CH', '<|eot_id|>', ':', 'Answer', 'b', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'athroom'], 'evidence_proportions': [4.236896832784017e-06, 6.360560655593872e-05, 3.529787063598633e-05, 4.8533082008361816e-05, 7.359683513641357e-05]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 00:55:38.233 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:55:38.233 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-4_2-5-6-7-8.pkl | len: 10 |  size: 9.2 KB
Processing depth (2, 5, 6, 7, 8):   5%|‚ñå         | 5/100 [01:34<29:53, 18.87s/it]Processing depth (2, 5, 6, 7, 8):   5%|‚ñå         | 5/100 [01:34<29:57, 18.92s/it]
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:70 - Selected idx: 6
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:71 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:72 - Answer: bathroom
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:73 - Tag: 3-hop
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:74 - Needle: [' John went back to the bedroom.', ' Mary journeyed to the office.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' Mary got the football there.', ' John journeyed to the office.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 00:55:38.490 | INFO     | __main__:<module>:75 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 00:55:38.491 | INFO     | __main__:<module>:76 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.39it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.38it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.40it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.87it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.66it/s]
Processing depth (0, 1, 4, 5):   0%|          | 0/100 [00:06<?, ?it/s]2025-01-22 00:55:45.382 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:55:45.383 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:55:45.383 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:55:45.387 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1501, 1506) -->  tragedy. Daniel went to
2025-01-22 00:55:45.388 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:55:45.401 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4872, 4876) -->  Daniel left the apple
2025-01-22 00:55:45.401 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John moved to the bedroom.
2025-01-22 00:55:45.419 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (5949, 5954) --> . John moved to the
2025-01-22 00:55:45.419 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:55:45.433 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4582, 4588) -->  Press. John went back to
2025-01-22 00:55:45.433 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 00:55:45.438 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1567, 1573) -->  ranks. Mary journeyed to
2025-01-22 00:55:45.438 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 00:55:45.441 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1069, 1074) --> . Mary moved to the
2025-01-22 00:55:45.441 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary got the football there.
2025-01-22 00:55:45.454 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (4290, 4295) --> . Mary got the football
2025-01-22 00:55:45.454 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John journeyed to the office.
2025-01-22 00:55:45.459 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1569, 1575) -->  Mary journeyed to the office
2025-01-22 00:55:45.459 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 00:55:45.480 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (6928, 6934) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:55:47.566 | INFO     | test_jbb_retain:begin_test:544 - The apple was left in the office.<|eot_id|>
2025-01-22 00:55:47.566 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12145])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:05<00:36,  5.23s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:13,  2.26s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.31s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.16it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:01,  1.65it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:06<00:00,  2.21it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.81it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  3.42it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.25it/s]
2025-01-22 00:55:56.620 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.20575637817382814, 0.20461756781916263, 0.20807916977826288, 0.2046059529131181], 'topk_tokens': ['ounding', ' hundred', 'ir', ' S', ' sop', ' S', ' the', ' malignant', '202', ' senate', 'deal', ' turtle', ' and', ' comparison', ' compl', 'ols', 'consider', 'est', 'itter', ' Do'], 'evidence_proportions': [0.21697998046875, 0.2529266357421875, 0.12034225463867188, 0.2134490966796875]}, 'weight': {'score': [0.020890344679355622, 0.0025919056032926266, 0.005086159004884607, 0.002554633121303592], 'topk_tokens': [' battling', ' Father', ' Capt', '.', ' the', ':', 'Bridge', '<|eot_id|>', 'assistant', ' the', ' Bridge', ' garden', 'b', '<|eot_id|>', '<|start_header_id|>', '\n\n', ' barric', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009994029998779297, 0.0075693786144256595, 0.059975624084472656, 0.016018664836883544]}, 'saliency': {'score': [0.0009467288851737976, 3.075414448534928e-05, 0.00022686229032628676, 2.8688064464482665e-05], 'topk_tokens': [' Bench', ' garden', ' the', '***', ' Press', ' Father', ' bedroom', ' Daniel', ' bedroom', ' Dan', '<|eot_id|>', ' the', ' Bridge', 'Bridge', 'Daniel', ' barric', 'b', '<|begin_of_text|>', ' garden', 'athroom'], 'evidence_proportions': [0.00037411848704020184, 0.0011804699897766113, 0.002355910837650299, 0.0002727746963500977]}}, 25: {'grad': {'score': [0.5281448364257812, 0.5591459202067727, 0.4315243889303768, 0.5595559709541695], 'topk_tokens': [' York', ' free', ' vigilant', ' the', ' a', ' black', ' An', ' with', 'M', ' the', 'antic', ' for', ' a', ' a', ' of', ' a', ' bogus', ' for', ' no', ' inverted'], 'evidence_proportions': [0.6059773763020834, 0.529248046875, 0.5195274353027344, 0.4405364990234375]}, 'weight': {'score': [0.0139762282371521, 0.002520517962207361, 0.0026162731296875898, 0.002501304312365024], 'topk_tokens': [' Ot', ' the', 'Daniel', 'b', ' Bench', ' apple', ' Dan', ' Capt', 'door', ' barric', '.', '<|eot_id|>', 'assistant', ':', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0022354920705159507, 0.011589312553405761, 0.04509425163269043, 0.005557608604431152]}, 'saliency': {'score': [0.0010530322790145874, 2.7583916308183515e-05, 7.117583471186021e-05, 2.5765568988863764e-05], 'topk_tokens': [' St', ' self', 'assistant', ' Ot', '<|eot_id|>', ' Capt', '.', ' Daniel', 'door', ' barric', ' Daniel', 'Daniel', 'athroom', '.', ' apple', ' Bench', '<|end_header_id|>', ' Dan', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.791452566782633e-05, 0.0010595619678497314, 0.00366993248462677, 0.00013512372970581055]}}, 26: {'grad': {'score': [0.29076614379882815, 0.30819606686800294, 0.27123742945053997, 0.3083287931896151], 'topk_tokens': [' bonds', ' broad', ' Bench', ' Press', 'bec', ' prof', 'CE', ' bonds', 'ig', ' bouncing', ' badly', 'bread', 'RI', 'b', 'b', 'b', 'b', ' Press', ' bitter', 'itter'], 'evidence_proportions': [0.19732666015625, 0.37784118652343746, 0.28078460693359375, 0.32380371093750004]}, 'weight': {'score': [0.00944887548685074, 0.0025079555718726847, 0.0028318491052178774, 0.0024955667196787626], 'topk_tokens': [' Bridge', '?', ' the', ' the', ' apple', '<|eot_id|>', 'Answer', '<|eot_id|>', ' \n', ' garden', 'b', 'assistant', ' the', '\n\n', ' barric', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004270076751708984, 0.006017839908599854, 0.024242162704467773, 0.0072598397731781]}, 'saliency': {'score': [0.00040479898452758787, 3.166017875413892e-05, 7.343029274659999e-05, 3.0925685617772726e-05], 'topk_tokens': [' apple', 'CH', ':', 'Daniel', 'Answer', ' the', ' Bridge', ' the', ' apple', 'assistant', 'Bridge', ' barric', ' Bridge', ' garden', '<|end_header_id|>', '\n\n', 'athroom', '<|start_header_id|>', '<|begin_of_text|>', 'b'], 'evidence_proportions': [6.870428721110026e-05, 5.795955657958984e-05, 0.0016939043998718262, 0.00012366771697998048]}}, 27: {'grad': {'score': [0.15525436401367188, 0.2346195375444389, 0.22136789209702434, 0.23478803923253444], 'topk_tokens': [' being', 'CE', ' been', ' were', ' was', ' was', ' business', 'Frank', ' was', ' was', ' was', ' be', 'ly', ' were', ' step', 'lin', ' was', ' Franklin', ' would', ' was'], 'evidence_proportions': [0.1566454569498698, 0.18918457031250002, 0.11527729034423828, 0.15163650512695312]}, 'weight': {'score': [0.017939944565296174, 0.0025488323077269984, 0.0023719931350034825, 0.002523876981674511], 'topk_tokens': [' the', 'CH', ' bathroom', 'Daniel', ' garden', ' \n', ' apple', ' bedroom', 'Answer', '.\n\n', ' Bridge', 'assistant', 'b', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ' barric', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009962995847066244, 0.010149532556533813, 0.04669761657714844, 0.012296557426452637]}, 'saliency': {'score': [0.0014966964721679688, 3.895729505430918e-05, 0.0001922381274840411, 3.611569328112545e-05], 'topk_tokens': ['.', ' the', 'assistant', ' the', ' Capt', ' apple', ' bathroom', ' the', '<|begin_of_text|>', '\n\n', 'Daniel', '<|start_header_id|>', ' the', ' Bridge', ' barric', ':', '<|end_header_id|>', '.\n\n', 'b', 'athroom'], 'evidence_proportions': [0.00044891734917958576, 0.00033954977989196776, 0.0045797452330589294, 0.0014447391033172608]}}, 28: {'grad': {'score': [0.20557022094726562, 0.23420717341749928, 0.2126740848316866, 0.23431506692347928], 'topk_tokens': [' lively', ' a', 'nes', ' returns', '\n', ' as', ' inside', 'E', 'nes', '.', 'S', ' lie', ' lie', 'arp', ' half', 'ot', 'dent', ' RID', 'half', ' half'], 'evidence_proportions': [0.25494639078776044, 0.17720794677734375, 0.2368011474609375, 0.14969635009765625]}, 'weight': {'score': [0.006277813017368317, 0.0024286594522756927, 0.005389323129373438, 0.0024139707110549906], 'topk_tokens': [' Bridge', '<|eot_id|>', 'Answer', ' the', ' Bridge', '?', ' the', '<|eot_id|>', ' garden', 'assistant', 'b', ' before', ' apple', ' the', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0025977790355682373, 0.0019713878631591798, 0.0022992491722106934, 0.018183130025863647]}, 'saliency': {'score': [0.00011294931173324584, 2.9354097715992657e-05, 0.00015675933922038358, 2.8857679451440452e-05], 'topk_tokens': [' back', ' the', ' before', ' near', ' garden', ' apple', 'b', ' the', 'Bridge', 'assistant', '\n\n', '<|end_header_id|>', ' garden', 'athroom', '<|start_header_id|>', ' Bridge', ' Bridge', ':', ' the', '<|begin_of_text|>'], 'evidence_proportions': [0.00017589827378590903, 3.719329833984375e-05, 5.667656660079956e-05, 0.00015818476676940916]}}, 29: {'grad': {'score': [0.19977855682373047, 0.2833734536822548, 0.1866159439086914, 0.2837837111049], 'topk_tokens': [' fire', ' numbers', ' locom', ' Gen', ' Dr', ' awake', ' from', ' fireworks', ' face', ' extra', ' facilities', ' enough', 'Emp', ' not', 'tal', 'goods', ' faithfully', ' extra', 'cret', ' ga'], 'evidence_proportions': [0.18902079264322919, 0.18572845458984374, 0.21638107299804688, 0.21345596313476561]}, 'weight': {'score': [0.0049800872802734375, 0.0024989707162987058, 0.0019188383046318503, 0.0024964985954716162], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', ' the', ' \n', ' the', '?', ' apple', 'ot', ' before', 'b', 'assistant', ' the', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0075508952140808105, 0.0011904418468475341, 0.005913436412811279, 0.00493808388710022]}, 'saliency': {'score': [0.00010630339384078979, 2.4590242603793127e-05, 3.897267229416791e-05, 2.4414679048789597e-05], 'topk_tokens': [' Where', ' garden', ' the', ' before', ' a', 'b', '<|eot_id|>', '      ', 'Answer', '***', ' Does', '<|eot_id|>', '<|end_header_id|>', ' the', 'athroom', 'assistant', '\n\n', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0001729130744934082, 3.253817558288574e-05, 0.00010021030902862549, 0.00010501146316528321]}}, 30: {'grad': {'score': [0.22523009777069092, 0.2650711315809444, 0.18495669785667868, 0.26536224382030527], 'topk_tokens': [' OF', 'Emp', ' EVENTS', ' Times', 'b', ' Europe', ' C', ' United', 'BREAK', 'b', ' Million', '2', 'ire', 'AM', 'SSION', ' Times', 'b', ' account', ' LINE', 'deal'], 'evidence_proportions': [0.18158721923828125, 0.2111602783203125, 0.38486671447753906, 0.1639620780944824]}, 'weight': {'score': [0.008741486072540283, 0.002477892064561489, 0.004817322773091933, 0.002460957012118161], 'topk_tokens': [' the', '.\n\n', ' apple', '<|eot_id|>', ' before', ' barric', '<|eot_id|>', '?', ' garden', 'Answer', ' the', 'assistant', 'b', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0054832299550374355, 0.00625523328781128, 0.009783029556274414, 0.014304411411285401]}, 'saliency': {'score': [0.0005202114582061768, 4.557709298245327e-05, 0.00021471170818104465, 4.431669408868557e-05], 'topk_tokens': [' \n', ' garden', 'Bridge', 'CH', ' the', ' before', ' the', 'b', ' bathroom', ' Bridge', ' barric', ' bathroom', ' Bridge', '<|end_header_id|>', ' the', 'assistant', '<|begin_of_text|>', '<|start_header_id|>', 'athroom', ':'], 'evidence_proportions': [0.0008846422036488851, 0.000472712516784668, 0.0005163028836250305, 0.00013352036476135253]}}, 31: {'grad': {'score': [0.15951313972473144, 0.1546024615175048, 0.12643773415509393, 0.15467352048609895], 'topk_tokens': ['re', 's', 't', 'b', ' o', '>s', 'm', 'had', 'm', ' January', 'g', ' conn', ' department', ' g', ' m', ' g', 'd', 'n', 'g', 'd'], 'evidence_proportions': [0.1447734832763672, 0.16027908325195314, 0.23669958114624023, 0.11468563079833984]}, 'weight': {'score': [0.0022920176386833193, 0.002298136074605018, 0.0018480595420388615, 0.0022994114980237116], 'topk_tokens': [':', ' was', ' the', ' Where', ' apple', ' before', '?', '<|eot_id|>', ' the', 'Answer', ' \n', 'b', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00100785493850708, 0.001342916488647461, 0.0013355016708374023, 0.005547326803207398]}, 'saliency': {'score': [7.950663566589356e-05, 1.3321449205519771e-05, 1.5265801373650046e-05, 1.3206531750341674e-05], 'topk_tokens': ['.\n\n', ' Bridge', 'ot', '<|eot_id|>', ' Key', ' garden', ' the', ' the', 'Answer', '<|begin_of_text|>', ' the', ' apple', ' \n', '\n\n', '<|end_header_id|>', ':', 'b', 'assistant', 'athroom', '<|start_header_id|>'], 'evidence_proportions': [3.7988026936848954e-05, 9.241700172424316e-05, 5.544722080230713e-05, 0.0001356661319732666]}}, 'pred_res': 'The apple was left in the office.<|eot_id|>', 'score': 0}
2025-01-22 00:55:56.621 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:55:56.622 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-0_0-1-4-5.pkl | len: 10 |  size: 8.56 KB
Processing depth (0, 1, 4, 5):   1%|          | 1/100 [00:18<29:46, 18.04s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.38it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.39it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.40it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.87it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.66it/s]
Processing depth (0, 1, 2, 4):   1%|          | 1/100 [00:24<29:46, 18.04s/it]2025-01-22 00:56:03.528 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:56:03.528 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 00:56:03.528 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:56:03.533 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1501, 1506) -->  tragedy. Daniel went to
2025-01-22 00:56:03.533 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:56:03.540 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (2488, 2492) -->  Daniel left the apple
2025-01-22 00:56:03.540 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John moved to the bedroom.
2025-01-22 00:56:03.555 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (4876, 4881) --> . John moved to the
2025-01-22 00:56:03.555 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:56:03.569 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4587, 4593) -->  Press. John went back to
2025-01-22 00:56:03.569 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 00:56:03.574 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1567, 1573) -->  ranks. Mary journeyed to
2025-01-22 00:56:03.574 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 00:56:03.577 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1069, 1074) --> . Mary moved to the
2025-01-22 00:56:03.577 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary got the football there.
2025-01-22 00:56:03.590 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (4295, 4300) --> . Mary got the football
2025-01-22 00:56:03.590 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John journeyed to the office.
2025-01-22 00:56:03.594 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1569, 1575) -->  Mary journeyed to the office
2025-01-22 00:56:03.595 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 00:56:03.615 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (6928, 6934) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:56:05.694 | INFO     | test_jbb_retain:begin_test:544 - The bedroom.<|eot_id|>
2025-01-22 00:56:05.694 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12145])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A
 12%|‚ñà‚ñé        | 1/8 [00:04<00:33,  4.82s/it][A
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:04<00:12,  2.08s/it][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:05<00:06,  1.20s/it][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:03,  1.27it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:05<00:01,  1.69it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:05<00:00,  2.12it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:06<00:00,  2.53it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  2.90it/s][A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.28it/s]
2025-01-22 00:56:14.812 | INFO     | test_jbb_retain:begin_test:589 - {24: {'grad': {'score': [0.20075244903564454, 0.19611400846670363, 0.20176076889038086, 0.19609046301724242], 'topk_tokens': ['ad', 'oggle', 'osed', 'remark', 'ottle', ' STE', 'adj', ' hundred', 'adv', ' and', 'vent', ' malignant', ' Do', '202', ' turtle', 'ols', 'consider', 'itter', 'est', ' compl'], 'evidence_proportions': [0.234039306640625, 0.1918346405029297, 0.11740255355834961, 0.23640594482421876]}, 'weight': {'score': [0.03179162442684173, 0.0025895842212102977, 0.010024365256814396, 0.002520390955183907], 'topk_tokens': [' apple', ' garden', '\n\n', 'Answer', '<|start_header_id|>', ':', ' garden', ' the', 'Bridge', 'assistant', '<|eot_id|>', ' bathroom', ' bathroom', '\n\n', 'b', '<|eot_id|>', ' bedroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04018863042195638, 0.010002452135086059, 0.0732870101928711, 0.010308080911636352]}, 'saliency': {'score': [0.0014618635177612305, 3.43427459464326e-05, 0.0004222813774557675, 3.08914205861206e-05], 'topk_tokens': ['.', ' back', '<|eot_id|>', ' Bridge', ' bathroom', ' composing', ' Daniel', ' top', 'Daniel', '<|eot_id|>', ' bedroom', ' bathroom', ' Daniel', ' garden', '<|begin_of_text|>', 'Bridge', 'b', ' garden', ' bedroom', 'athroom'], 'evidence_proportions': [0.00153428316116333, 0.0018329918384552002, 0.002512693405151367, 0.00016316771507263184]}}, 25: {'grad': {'score': [0.5083768844604493, 0.5664889800693788, 0.44745243296903725, 0.5669197295743886], 'topk_tokens': [' of', ' old', ' a', 'posit', ' for', ' the', ' a', ' a', ' for', ' a', ' bogus', ' for', ' black', ' little', ' free', ' self', ' of', ' no', ' inverted', ' for'], 'evidence_proportions': [0.62689208984375, 0.497735595703125, 0.48377132415771484, 0.396484375]}, 'weight': {'score': [0.020904286205768584, 0.0025177974625563283, 0.004083428312750424, 0.00248299001888419], 'topk_tokens': [' bathroom', ' Daniel', ' Dan', 'Daniel', ' Daniel', 'door', '<|start_header_id|>', ' \n', 'b', 'Answer', ' the', ' apple', ':', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.010419527689615885, 0.016205799579620362, 0.056899309158325195, 0.00938846468925476]}, 'saliency': {'score': [0.0016992852091789246, 2.5156729602876525e-05, 0.00011825210907879998, 2.2126482166651757e-05], 'topk_tokens': [':', ' Daniel', ' Press', ' there', ' top', ' to', ' Bench', '<|eot_id|>', 'assistant', 'door', 'Daniel', ' THE', ' Daniel', '<|eot_id|>', 'athroom', ' Dan', '\n\n', '<|end_header_id|>', ' apple', '<|begin_of_text|>'], 'evidence_proportions': [0.00034421185652414954, 0.0007384240627288819, 0.006359033286571503, 0.0005584359169006348]}}, 26: {'grad': {'score': [0.23920121192932128, 0.27743853838144655, 0.22838861802045038, 0.2776396665293973], 'topk_tokens': [' and', ' and', 'ers', 'ente', ' or', ' Milwaukee', 'gear', 'pro', 'ente', 'bec', 'rich', 'b', 'b', 'hue', ' Empire', 'ub', ' Eagle', ' bitter', 'itter', ' Press'], 'evidence_proportions': [0.184814453125, 0.31599731445312496, 0.21315693855285645, 0.248504638671875]}, 'weight': {'score': [0.01977132260799408, 0.0024966380295488133, 0.0032570362091064453, 0.002465932950197576], 'topk_tokens': [' apple', ' bedroom', ' the', ' garden', ' apple', ' bathroom', ' bathroom', '<|eot_id|>', 'b', 'Answer', '<|eot_id|>', ' \n', 'assistant', '<|start_header_id|>', '\n\n', ' the', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03143127759297689, 0.011116081476211548, 0.03036785125732422, 0.00595739483833313]}, 'saliency': {'score': [0.0005316823720932007, 2.459687379225328e-05, 8.204667007221895e-05, 2.359679085529795e-05], 'topk_tokens': ['Answer', ' Bridge', ' apple', ' bathroom', ' garden', '<|start_header_id|>', 'Daniel', ' garden', ' Daniel', ' \n', ' apple', 'assistant', 'Bridge', 'athroom', ' the', '<|end_header_id|>', ':', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0004183252652486165, 0.0006527960300445557, 0.000796198844909668, 0.0003349840641021729]}}, 27: {'grad': {'score': [0.16869640350341797, 0.18115258491624137, 0.14111030803007238, 0.1812857551694567], 'topk_tokens': ['ers', 'arr', '-n', 'sur', ' lin', ' Thanksgiving', ' dre', ' platform', ' execution', ' short', ' conventions', 'Republicans', ' mon', ' block', ' STR', ' accepted', 'CE', 'event', 'str', ' step'], 'evidence_proportions': [0.11427116394042969, 0.1541290283203125, 0.13486671447753906, 0.2756378173828125]}, 'weight': {'score': [0.03416947573423386, 0.0025460655688454457, 0.003383922226288739, 0.0024914140615144684], 'topk_tokens': [' Daniel', ' apple', ' lounge', ' \n', ' the', 'Daniel', 'Answer', '<|start_header_id|>', ' apple', ' bathroom', 'assistant', '.\n\n', ' bathroom', ' bedroom', 'b', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04367605845133463, 0.019875502586364748, 0.07570052146911621, 0.0038307130336761473]}, 'saliency': {'score': [0.0014231398701667786, 3.534929313508939e-05, 0.0001377016305923462, 3.27665421003465e-05], 'topk_tokens': [' the', ' top', ' \n', '<|begin_of_text|>', ' bathroom', ':', ' the', ' THE', 'assistant', ' bedroom', ' bathroom', ' Bridge', 'Bridge', '<|end_header_id|>', 'Daniel', ' apple', 'NEW', 'athroom', '.\n\n', 'b'], 'evidence_proportions': [0.0010228157043457031, 0.00035808086395263674, 0.00488869845867157, 0.00019614100456237795]}}, 28: {'grad': {'score': [0.19959651231765746, 0.25746585869812677, 0.21268137763528264, 0.25768746108639745], 'topk_tokens': [' half', ' ', 'nes', 'half', 'E', 'arp', 'nes', ' lie', ' lie', 'ot', 'E', ' RID', ' ', 'nes', ' Cedar', '.', ' ', 'dent', 'S', '.'], 'evidence_proportions': [0.2623605728149414, 0.17021069526672364, 0.14238548278808594, 0.1994342803955078]}, 'weight': {'score': [0.006390224397182465, 0.002410286541317967, 0.007014571743852952, 0.0023907607885477107], 'topk_tokens': [' garden', ' the', ' bathroom', ' \n', '?', '<|eot_id|>', ' before', 'Answer', ' garden', '<|eot_id|>', ' apple', 'assistant', 'b', '<|start_header_id|>', ' the', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0067702531814575195, 0.0036369383335113524, 0.0116194486618042, 0.004504096508026123]}, 'saliency': {'score': [0.00022283494472503663, 2.3847161879616425e-05, 0.00017648584702435662, 2.308897840418815e-05], 'topk_tokens': [' bathroom', ' the', ' before', ' garden', ' the', '?', ' the', ' Bridge', '<|start_header_id|>', 'athroom', 'b', 'Bridge', 'assistant', '<|end_header_id|>', ':', ' Bridge', ' garden', ' apple', ' the', '<|begin_of_text|>'], 'evidence_proportions': [0.0001744876305262248, 0.00027805566787719727, 0.00048147886991500854, 1.8715858459472656e-05]}}, 29: {'grad': {'score': [0.19362411499023438, 0.26911408264542, 0.19647760952220245, 0.26944312509947105], 'topk_tokens': [' M', ' fer', ' requis', ' face', ' anything', ' extra', 'pend', 'ys', ' were', ' arms', 'adv', ' extra', 'cret', ' facilities', ' not', ' enough', 'tal', ' faithfully', 'goods', ' ga'], 'evidence_proportions': [0.20127264658610028, 0.18437652587890624, 0.15282392501831055, 0.2263336181640625]}, 'weight': {'score': [0.016148385405540467, 0.0024761492616454353, 0.002297449637861813, 0.002454041692961025], 'topk_tokens': [' Does', '<|eot_id|>', ' apple', ' \n', ' before', '<|eot_id|>', '.\n\n', ' the', '?', 'Answer', 'b', '.', ' the', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.027034262816111248, 0.0016717910766601562, 0.03443348407745361, 0.0029338479042053224]}, 'saliency': {'score': [0.0003354668617248535, 2.614110659171297e-05, 7.115742739509133e-05, 2.5503015802067055e-05], 'topk_tokens': ['      ', ' Where', ' Does', '.', 'Answer', '***', '<|eot_id|>', 'NEW', '      ', ' the', '<|eot_id|>', ' the', '<|end_header_id|>', 'athroom', 'assistant', 'b', '\n\n', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.00018607079982757568, 4.286766052246094e-05, 0.0013218000531196594, 1.8274784088134767e-05]}}, 30: {'grad': {'score': [0.19044933319091797, 0.2791712700947751, 0.20720425774069393, 0.2795203123602056], 'topk_tokens': ['ente', ' soon', ' C', ' B', 'g', '\n', ' of', ' itself', 'AM', 'SSION', 'ire', '2', ' Times', 'ar', ' LINE', 'b', ' account', 'b', 'deal', 'b'], 'evidence_proportions': [0.206939697265625, 0.2145111083984375, 0.22187042236328125, 0.12146224975585938]}, 'weight': {'score': [0.014093731343746186, 0.0024770909247525553, 0.006499757661538965, 0.002446571371467405], 'topk_tokens': [' the', 'Question', ' the', ' apple', '<|eot_id|>', ' garden', '.\n\n', '<|eot_id|>', '?', 'b', ' the', 'Answer', 'assistant', ' \n', '<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010407586892445881, 0.01081596612930298, 0.03611421585083008, 0.004178482294082642]}, 'saliency': {'score': [0.0014420196413993836, 4.8554827792494554e-05, 0.00038817524909973145, 4.5295658733739484e-05], 'topk_tokens': [' garden', ' bedroom', ' the', ' garden', ' Daniel', ' the', ' apple', 'Bridge', '<|start_header_id|>', 'athroom', ' Bridge', ' the', ' bathroom', 'assistant', ' the', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom', ':', 'b'], 'evidence_proportions': [0.0024349043766657514, 0.0010176360607147216, 0.0022281482815742493, 4.603862762451172e-05]}}, 31: {'grad': {'score': [0.2171126753091812, 0.19354409829228578, 0.1752014580894919, 0.19355668951326782], 'topk_tokens': [' the', ' population', ' the', ' location', ' the', ' August', ' the', 'd', ' the', ' the', ' the', 'had', ' location', ' January', ' the', 'g', 'membership', ' apple', ' the', ' department'], 'evidence_proportions': [0.16679716110229492, 0.25610677003860477, 0.2744542956352234, 0.19262390136718752]}, 'weight': {'score': [0.0015890315175056458, 0.0022904408931889186, 0.0025814973256167244, 0.0022907825724357454], 'topk_tokens': [' was', 'Question', '.\n\n', ':', ' apple', '?', ' Where', '<|eot_id|>', ' the', 'Answer', ' \n', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011879801750183105, 0.0013962984085083008, 0.002879202365875244, 0.0012308895587921143]}, 'saliency': {'score': [8.753687143325806e-05, 1.2772615936425001e-05, 3.936799133525175e-05, 1.2574209464331668e-05], 'topk_tokens': [':', ' was', '<|eot_id|>', '.\n\n', ' the', ' Key', ' Where', ' garden', ' the', 'athroom', ' the', 'Answer', ' apple', ' \n', ':', ' the', '<|end_header_id|>', '<|start_header_id|>', 'b', 'assistant'], 'evidence_proportions': [2.9593706130981445e-05, 0.00011662840843200684, 0.000224284827709198, 1.8578767776489257e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 00:56:14.813 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 00:56:14.813 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /data/zecheng/acl2025/Long-form-reasoning/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample200_gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-1_0-1-2-4.pkl | len: 10 |  size: 8.62 KB
Processing depth (0, 1, 2, 4):   2%|‚ñè         | 2/100 [00:36<29:36, 18.13s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.37it/s][A
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.38it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.85it/s][ALoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.63it/s]
Processing depth (1, 3, 4, 5):   2%|‚ñè         | 2/100 [00:43<29:36, 18.13s/it]2025-01-22 00:56:21.893 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 00:56:21.897 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (1494, 1500) -->  tragedy. Daniel journeyed to
2025-01-22 00:56:21.898 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Daniel went to the garden.
2025-01-22 00:56:21.909 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (3781, 3786) --> . Daniel went to the
2025-01-22 00:56:21.909 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Daniel left the apple.
2025-01-22 00:56:21.922 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (4872, 4876) -->  Daniel left the apple
2025-01-22 00:56:21.922 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  John moved to the bedroom.
2025-01-22 00:56:21.939 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (5949, 5954) --> . John moved to the
2025-01-22 00:56:21.940 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 0 -->  John went back to the bedroom.
2025-01-22 00:56:21.953 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 0 at --> (4582, 4588) -->  Press. John went back to
2025-01-22 00:56:21.953 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 00:56:21.958 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 1 at --> (1561, 1567) -->  ranks. Mary journeyed to
2025-01-22 00:56:21.958 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 00:56:21.961 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 2 at --> (1062, 1067) --> . Mary moved to the
2025-01-22 00:56:21.961 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 3 -->  Mary got the football there.
2025-01-22 00:56:21.974 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 3 at --> (4290, 4295) --> . Mary got the football
2025-01-22 00:56:21.974 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 4 -->  John journeyed to the office.
2025-01-22 00:56:21.979 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 4 at --> (1563, 1569) -->  Mary journeyed to the office
2025-01-22 00:56:21.979 | INFO     | test_jbb_retain:find_multi_needle_idx:449 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 00:56:21.999 | INFO     | test_jbb_retain:find_multi_needle_idx:457 - find evidence 5 at --> (6928, 6934) --> . Sandra journeyed to the
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 00:56:24.086 | INFO     | test_jbb_retain:begin_test:544 - the apple was left in the office.<|eot_id|>
2025-01-22 00:56:24.086 | INFO     | test_jbb_retain:begin_test:546 - torch.Size([1, 12145])
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !

  0%|          | 0/8 [00:00<?, ?it/s][A  0%|          | 0/8 [00:05<?, ?it/s]
Processing depth (1, 3, 4, 5):   2%|‚ñè         | 2/100 [00:53<43:30, 26.64s/it]
is_0k: False
Traceback (most recent call last):
  File "/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis/test2_pjlab_llama_jbb_random5x100_retain_grad.py", line 204, in <module>
    begin_test(args, question, answer, s_id, model, tokenizer, depth_percent, background_text, disturb_pos,disturb_tok_needles, evidence, evidence_list, save_file_name, model_name, is_0k = (context_length == 0), with_adapter= True if args.adapter_path else False,                                   start_layer = 24)
  File "/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis/test_jbb_retain.py", line 570, in begin_test
    flow_res = test_model_with_attention_adapter(model, inp, label, search_pos, attack_pos,
  File "/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis/test_jbb_retain.py", line 489, in test_model_with_attention_adapter
    proportion1, proportion2, proportion3, proportion4, evidence_proportions, topk_indices = calculate_portions(saliency, search_pos, attack_pos, target_poss, is_0k)
  File "/data/zecheng/acl2025/MyRLHF/reetrievalheaddetect/analysis/test_jbb_retain.py", line 319, in calculate_portions
    saliency = saliency.float().detach().clone().cpu()
KeyboardInterrupt
