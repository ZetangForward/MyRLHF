nohup: 忽略输入
2025-01-23 22:14:25.380 | INFO     | test_jbb_retain_zero_embed:<module>:7 - ['/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/analysis', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python310.zip', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages', '/mnt/petrelfs/tangzecheng/DeepSpeed', '/mnt/petrelfs/tangzecheng/modelzipper/src', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-23 22:14:27.440 | INFO     | __main__:<module>:11 - ['/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/faiss_attn', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/analysis', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python310.zip', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages', '/mnt/petrelfs/tangzecheng/DeepSpeed', '/mnt/petrelfs/tangzecheng/modelzipper/src', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:99 - Selected idx: 0
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:101 - Answer: office
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:103 - Needle: [' John went back to the bedroom.', ' Mary took the football.', ' John took the milk.', ' Sandra journeyed to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Daniel went back to the kitchen.']
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:104 - Real Needle: [' Mary took the football.', ' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Daniel went back to the kitchen.']
2025-01-23 22:14:28.268 | INFO     | __main__:<module>:105 - =============================================
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-01-24 02:14:20
Pid: 132973
begin to read data from ./haystack_for_detect/reasoning_needle_new.jsonl | file size: 246.97 KB | file type: jsonl
begin to testing with [3900, 7900]
  0%|          | 0/100 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.08it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.10it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.49it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Processing depth (1, 4, 6, 7):   0%|          | 0/100 [00:23<?, ?it/s]2025-01-23 22:14:52.077 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:14:52.082 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (430, 434) -->  Mary took the football
2025-01-23 22:14:52.082 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:14:52.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1480, 1486) -->  John journeyed to the office
2025-01-23 22:14:52.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:14:52.098 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1817, 1823) --> . Mary journeyed to the
2025-01-23 22:14:52.099 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:14:52.113 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2843, 2849) --> . Daniel went back to the
2025-01-23 22:14:52.113 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:14:52.129 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3325, 3331) --> . John went back to the
2025-01-23 22:14:52.129 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John took the milk.
2025-01-23 22:14:52.131 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (478, 482) -->  John took the milk
2025-01-23 22:14:52.132 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-23 22:14:52.137 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1099, 1105) --> . Sandra journeyed to the
2025-01-23 22:14:52.137 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John journeyed to the office.
2025-01-23 22:14:52.145 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1479, 1485) --> . John journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-23 22:14:59.106 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football.<|eot_id|>
2025-01-23 22:14:59.106 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4216])
your chose emoji: ['🕴️', '🙋🏽\u200d♀️', '🚶🏼\u200d♂\u200d➡', '🧑🏾\u200d🦯', '🏾', '🦒', '🧑\u200d⚖️', '👨🏼\u200d❤\u200d👨🏿', '🏃🏾\u200d♂\u200d➡️', '👬🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4218, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 190650.18it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A
 38%|███▊      | 3/8 [00:00<00:00, 28.68it/s][A
 88%|████████▊ | 7/8 [00:00<00:00, 30.64it/s][A100%|██████████| 8/8 [00:00<00:00, 30.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 50%|█████     | 4/8 [00:00<00:00, 31.54it/s][A
100%|██████████| 8/8 [00:00<00:00, 31.80it/s][A100%|██████████| 8/8 [00:00<00:00, 31.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A
 50%|█████     | 4/8 [00:00<00:00, 30.66it/s][A
100%|██████████| 8/8 [00:00<00:00, 30.79it/s][A100%|██████████| 8/8 [00:00<00:00, 30.74it/s]
2025-01-23 22:15:02.660 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.3007664680480957, 1.1450399052424134, 1.116943359375, 1.1443672027094374, 0.9312788262424698], 'topk_tokens': [' bathroom', 'posit', ' of', ' Mary', ' speech', ' of', ' of', ' newspaper', ' location', 'of', ' wonderful', ' office', ' of', ' of', ' duty', ' enter', ' tele', ' offices', ' office', ' offices'], 'evidence_proportions': [1.17095947265625, 1.6028849283854167, 1.1549293200174968, 1.2310231526692708]}, 'weight': {'score': [0.2675569382580844, 0.00742876343164132, 0.0016601492058147085, 0.006088104511369743, 0.0013053582375308118], 'topk_tokens': [' web', ' ', '\n', 'ANK', '<|end_header_id|>', 'NEW', '\n\n', '\n\n', 'user', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.4613699913024902, 0.0011983712514241536, 0.0013304948806762695, 0.004266579945882161]}, 'saliency': {'score': [0.0032802603461525655, 0.00021883641842588422, 0.00013791431080211294, 0.00020312701736161396, 9.842234921742634e-05], 'topk_tokens': [' newspaper', ',\n', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'APER', '\n\n', ' offices', ' Project', 'assistant', 'NEW', '<|start_header_id|>', '***', ' web', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>', '<|eot_id|>', ' the'], 'evidence_proportions': [0.01662987470626831, 8.160869280497234e-05, 0.00010138750076293945, 0.0007580419381459554]}}, 25: {'grad': {'score': [0.9414284446022727, 0.9200194039125474, 0.8283913352272727, 0.920389508995418, 1.287348460002118], 'topk_tokens': ['.', ' starting', 'Good', ' *', '.', ' Gray', ' revisit', '.', '\n', 'ENCES', ' $', '<|start_header_id|>', ' S', ' o', '.', '\n', ' press', '.', '\n', '.'], 'evidence_proportions': [0.4808349609375, 1.1697998046875, 1.1513264973958333, 0.8102213541666666]}, 'weight': {'score': [0.3652285770936446, 0.007460071894725729, 0.0011681453748182817, 0.00560753602243578, 0.0009067022656819906], 'topk_tokens': [' Think', 'UL', 'edit', 'RE', '\n\n', '<|end_header_id|>', 'MIN', '\n\n', 'user', '<|start_header_id|>', 'assistant', '\n\n', 'office', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [2.0035476684570312, 0.00042680899302164715, 0.001011808713277181, 0.0020343859990437827]}, 'saliency': {'score': [0.00397774035280401, 0.00023188306687284047, 5.6822191585193984e-05, 0.00021306238622258712, 3.768880683255483e-05], 'topk_tokens': ['\n\n', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'Dub', 'ORE', 'ENCES', ' Think', '<|start_header_id|>', 'Cut', '<|eot_id|>', 'UL', ' Mary', '<|eot_id|>', 'RE', 'MIN', ' the', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.021641209721565247, 1.3162692387898764e-05, 6.622076034545898e-05, 7.819135983784993e-05]}}, 26: {'grad': {'score': [0.35786264592950995, 0.4128979411154576, 0.41639570756392047, 0.4131695811806767, 0.29456800437835323], 'topk_tokens': [' the', 'hand', ' first', '600', ' hand', '\n', ' one', 'hand', ' Project', ' the', ' of', ' in', 'system', ' the', ' hand', ' the', ',\n', ' newspaper', ' and', ' the'], 'evidence_proportions': [0.6557693481445312, 0.317352294921875, 0.1217791239420573, 0.43585205078125]}, 'weight': {'score': [0.31079822236841376, 0.007263070545585419, 0.002774235877123746, 0.005686880086218848, 0.002275693847472409], 'topk_tokens': ['\n\n\n\n', '\n\n\n', '.\n', 'edit', 'E', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'assistant', 'user', '\n\n', 'office', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.692655324935913, 0.001686374346415202, 0.0030687650044759116, 0.006401459376017253]}, 'saliency': {'score': [0.008453862233595415, 0.00017435535710815458, 0.0002709600058468905, 0.00013020718903077752, 9.222777493028755e-05], 'topk_tokens': ['.\n', '\n\n\n\n\n\n\n', '\n', 'RE', '<|start_header_id|>', '<|start_header_id|>', ' INCIDENT', 'UL', 'early', 'user', '<|eot_id|>', 'remark', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'E', '<|eot_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [0.046189263463020325, 8.203585942586263e-05, 5.202492078145345e-05, 7.059176762898763e-05]}}, 27: {'grad': {'score': [1.3467018821022727, 1.3241830955132765, 1.2909740101207385, 1.3242394411226865, 1.8647416815700302], 'topk_tokens': ['ails', '\u200d', 'aths', '\u200d', ' Democrats', '\u200d', 'x', 'ed', 'es', '\n\n', 'latest', 'es', 'es', ' Reporter', 'icians', ' anxious', ' Pioneer', '<|start_header_id|>', '\u200d', ':'], 'evidence_proportions': [0.818603515625, 1.2784016927083333, 1.6038411458333333, 1.5099283854166667]}, 'weight': {'score': [0.45288525928150525, 0.007378556028594899, 0.002141594886779785, 0.00505801594080283, 0.0019488966608621988], 'topk_tokens': ['<|start_header_id|>', '\n\n\n', 'PA', 'ANK', 'UL', 'user', '\n\n', '\n\n', '<|eot_id|>', '\n\n', 'RE', 'NEW', 'assistant', '<|eot_id|>', '<|eot_id|>', 'office', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [2.4739174842834473, 0.0019593238830566406, 0.002701282501220703, 0.0066403547922770185]}, 'saliency': {'score': [0.014155260541222313, 0.0003552691752224412, 0.00023769519545815208, 0.0002831529360041415, 0.0003356718155274908], 'topk_tokens': ['<|eot_id|>', 'edit', 'the', '\n\n\n', '<|eot_id|>', 'early', 'remark', 'en', 'UL', '\n\n', 'office', 'manager', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'PA', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', ' the'], 'evidence_proportions': [0.0761101245880127, 0.00020147363344828287, 0.00021565953890482584, 0.0007454057534535726]}}, 28: {'grad': {'score': [0.4728941483931108, 0.5441926944419748, 0.5883067737926136, 0.5443359762501591, 0.34258665521460846], 'topk_tokens': [' One', ',', ' two', ' tour', ',', ' the', ' Out', ' first', ' not', 's', ' the', ' four', ' on', ' nineteenth', ' one', ' em', ' four', ' four', ' of', '.'], 'evidence_proportions': [0.4066429138183594, 0.3419596354166667, 0.4798990885416667, 0.6409912109375]}, 'weight': {'score': [0.4802711985327981, 0.007156565608770263, 0.0017353702675212514, 0.004691482804069464, 0.0012031822319490363], 'topk_tokens': ['\n', '\n\n', '\n\n\n', '\n\n\n\n', '<|start_header_id|>', '.\n\n', '<|eot_id|>', '\n\n', '\n\n', 'E', '.\n', '\n\n', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [2.630034923553467, 0.0010224580764770508, 0.0014429887135823567, 0.005172332127888997]}, 'saliency': {'score': [0.007924104278737848, 0.0002135182653574539, 0.00011208653450012207, 0.0001734125168630822, 4.76715076400573e-05], 'topk_tokens': [',', ',', '\n', 'UG', ' scale', ',', ' By', '<|start_header_id|>', 'ANK', '\n\n', '<|eot_id|>', 'E', '.\n', 'assistant', 'office', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ' the', '<|end_header_id|>'], 'evidence_proportions': [0.0430990606546402, 3.471970558166504e-05, 6.586313247680664e-05, 0.00022175908088684082]}}, 29: {'grad': {'score': [0.8897829922762784, 0.9747149493243243, 0.9656205610795454, 0.9752105361945788, 0.41683316518025226], 'topk_tokens': [' press', ' in', ' of', ',\n', ' Papers', '600', ' the', ' one', ' enabling', ' the', ' to', ' printed', ' six', ' of', '\n', ' the', ' press', '\n', ' the', ' no'], 'evidence_proportions': [1.24249267578125, 0.7094319661458334, 0.6766815185546875, 1.048095703125]}, 'weight': {'score': [0.5015790895982222, 0.00738843824694992, 0.0012887011874805796, 0.004815845981875732, 0.001201812761375703], 'topk_tokens': ['\n\n', ' OF', 'E', ',', 'G', '<|eot_id|>', 'A', '\n\n', '\n\n', ' of', '\n\n\n', ',', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'office', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [2.7451274394989014, 0.0008650620778401693, 0.001867532730102539, 0.006305774052937825]}, 'saliency': {'score': [0.014328590848229149, 0.0003927301150919307, 0.00010741027918728915, 0.00032078188803416434, 0.00010843305702669075], 'topk_tokens': ['E', '\n\n', ',', 'office', ' an', 'assistant', '<|eot_id|>', ',\n', 'manager', ' the', '.', 'APER', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' in', ',', '<|end_header_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [0.07746000587940216, 4.880626996358236e-05, 0.00022316972414652506, 0.0006261865297953287]}}, 30: {'grad': {'score': [1.4571810635653408, 1.317747923704955, 1.281492753462358, 1.3172041011530644, 1.6474499070500752], 'topk_tokens': [' ne', '\u200d', '.', ' and', ' o', 'aining', ' considerable', '�', ' and', ' was', '️', ' During', ' *\n\n', '<|end_header_id|>', ' office', ' Pennsylvania', '<|start_header_id|>', ' web', ' O', ' offices'], 'evidence_proportions': [1.24114990234375, 1.36962890625, 1.62420654296875, 1.521728515625]}, 'weight': {'score': [0.16278086467222733, 0.007051407301691597, 0.0036544745618646794, 0.006248504680255182, 0.002159051148288221], 'topk_tokens': ['edit', '.', 'UL', '\n\n\n', 'RE', '<|eot_id|>', '\n\n\n\n', 'MIN', '.\n', 'NEW', '\n\n', 'ANK', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'office', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [0.8723955154418945, 0.0036974549293518066, 0.005651950836181641, 0.005916754404703776]}, 'saliency': {'score': [0.0031947005878795276, 0.0003738546326240695, 0.00023290514945983887, 0.0003597296392397842, 0.0003234133662947689], 'topk_tokens': ['APER', ' ST', 'ORE', '***', '.', ' force', ' office', '<|eot_id|>', 'assistant', '<|start_header_id|>', '\n\n', '.\n', ' web', 'MIN', '<|eot_id|>', '<|start_header_id|>', ' the', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.016010642051696777, 0.0001404881477355957, 0.0004228651523590088, 0.0004767874876658122]}}, 31: {'grad': {'score': [1.4538199684836648, 1.4165013595009484, 1.5224165482954546, 1.4157464137532005, 1.522484285285674], 'topk_tokens': [' I', ' ST', ' been', ' few', '�', ' tele', ' o', ' DAYS', 'S', 'ioneer', 'G', 'RI', ' B', ' em', ' D', ' EVENTS', 'D', ' to', ' DAYS', 'ISC'], 'evidence_proportions': [0.7917861938476562, 1.7931315104166667, 1.4398600260416667, 1.56982421875]}, 'weight': {'score': [0.033519950780001556, 0.0065251481990895605, 0.0017014687711542304, 0.006408290338676171, 0.0012159656329327319], 'topk_tokens': ['Answer', ' Where', '.\n\n', 'Question', ' \n', ',', 'If', 'If', '.\n\n', 'Just', '.\n\n', '.\n', '<|eot_id|>', '<|start_header_id|>', ' the', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.1722888946533203, 0.001390993595123291, 0.0008160869280497233, 0.005840142567952474]}, 'saliency': {'score': [0.0005290291526100852, 0.00029127238315674524, 7.879192178899592e-05, 0.0002911391587250531, 7.37546438194183e-05], 'topk_tokens': [' office', ' Knowledge', 'Cut', ' the', ' the', 'Answer', ' the', 'PA', '\n', '.\n\n', '.\n\n', 'If', 'If', 'UL', ',', '.\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0025577545166015625, 4.795193672180176e-05, 3.085533777872721e-05, 0.00015579660733540854]}}, 'pred_res': 'Mary took the football.<|eot_id|>', 'score': 0}
2025-01-23 22:15:02.666 | INFO     | modelzipper.tutils:auto_save_data:296 - /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label not exist! --> Create data dir /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label
2025-01-23 22:15:02.671 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:15:02.671 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-0_pid-0_1-4-6-7.pkl | len: 10 |  size: 9.22 KB
Processing depth (1, 4, 6, 7):   1%|          | 1/100 [00:34<56:27, 34.22s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.26s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.25s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Processing depth (1, 2, 3, 5):   1%|          | 1/100 [00:46<56:27, 34.22s/it]2025-01-23 22:15:15.237 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:15:15.239 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (418, 422) -->  took the football.
2025-01-23 22:15:15.240 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:15:15.244 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (932, 938) --> . Mary journeyed to the
2025-01-23 22:15:15.245 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:15:15.249 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (932, 938) --> . Mary journeyed to the
2025-01-23 22:15:15.250 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:15:15.261 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2137, 2143) --> . Daniel went back to the
2025-01-23 22:15:15.261 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:15:15.278 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3328, 3334) --> . John went back to the
2025-01-23 22:15:15.278 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John took the milk.
2025-01-23 22:15:15.280 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (451, 455) -->  John took the milk
2025-01-23 22:15:15.280 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-23 22:15:15.286 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1142, 1148) --> . Sandra journeyed to the
2025-01-23 22:15:15.286 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John journeyed to the office.
2025-01-23 22:15:15.291 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (933, 939) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:15:15.881 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football.<|eot_id|>
2025-01-23 22:15:15.881 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4215])
your chose emoji: ['\U0001faf8🏾', '🐓', '🇮🇨', '👳\u200d♂️', '👨🏻\u200d🦼\u200d➡️', '🇹🇹', '🧑🏽\u200d❤️\u200d🧑🏿', '🏬', '👩🏼\u200d❤️\u200d💋\u200d👨🏽', '❤️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4217, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 188508.04it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 140.16it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.38it/s]
2025-01-23 22:15:18.903 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7342321222478693, 0.6853195816412734, 0.8230452104048296, 0.684335627716964, 0.4963723159417873], 'topk_tokens': [' wonderful', ' presses', ' offices', ' of', ' print', ' press', 'SP', ' newspaper', ' offices', ' office', ' newspaper', ' offices', ' newspaper', 'office', ' EVENTS', ' newspaper', ' printed', ' office', ' office', ' offices'], 'evidence_proportions': [0.6151123046875, 0.7720769246419271, 0.7720769246419271, 0.7379557291666666]}, 'weight': {'score': [0.0023026683113791728, 0.007482759811766826, 0.0013856508515097878, 0.0075422130612598, 0.001474951825490812], 'topk_tokens': ['Just', '.\n\n', ' ', 'Answer', ' \n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', '\n\n', '\n\n', '<|end_header_id|>', 'office', ' under', '<|begin_of_text|>'], 'evidence_proportions': [0.007953763008117676, 0.001154025395711263, 0.001154025395711263, 0.0008325576782226562]}, 'saliency': {'score': [0.00018750537525523794, 0.00019099675905147605, 6.068836558948863e-05, 0.00019170214968164162, 6.367811342565024e-05], 'topk_tokens': ['If', 'APER', '<|end_header_id|>', 'que', ' bathroom', 'Just', '<|end_header_id|>', '\n\n', ' adher', 'Question', '<|eot_id|>', 'Answer', ' Project', 'assistant', '\n\n', '***', '\n\n', '<|begin_of_text|>', ' under', 'office'], 'evidence_proportions': [0.0005301237106323242, 8.291006088256836e-05, 8.291006088256836e-05, 0.00016828378041585287]}}, 25: {'grad': {'score': [0.4739601828835227, 0.5864856440449372, 0.6368449818004261, 0.5868133830132886, 0.918952383646151], 'topk_tokens': [' gold', '-per', '      ', '      ', '�', ' press', ' the', '      ', '5', 'cap', '      ', '      ', ' Wood', '      ', '️', '      ', '<|start_header_id|>', '      ', 'ENCES', 'RE'], 'evidence_proportions': [0.3064117431640625, 0.5190836588541666, 0.5190836588541666, 0.4954121907552083]}, 'weight': {'score': [0.0009813444180922074, 0.0074908066790958195, 0.0005946430292996494, 0.007561480958999389, 0.0010128101197684684], 'topk_tokens': ['RE', 'Question', '<|start_header_id|>', 'UL', 'Just', 'MIN', '\n\n', '\n\n', '<|eot_id|>', 'user', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'Answer', '\n\n', 'assistant', 'office', '<|end_header_id|>', ' under', '<|begin_of_text|>'], 'evidence_proportions': [0.001017928123474121, 0.001051555077234904, 0.001051555077234904, 0.0008165339628855387]}, 'saliency': {'score': [0.00012443282387473366, 0.00015830285520329702, 7.283687591552734e-05, 0.00015893199304982451, 2.8644393130046565e-05], 'topk_tokens': ['PA', 'ENCES', 'Minnesota', ' Stephen', '\n\n', 'Just', 'Question', 'Min', ' Min', 'RE', 'assistant', 'If', ' Mary', 'Min', 'MIN', 'UL', 'office', '<|begin_of_text|>', '<|end_header_id|>', ' under'], 'evidence_proportions': [9.208917617797852e-05, 0.00018394986788431802, 0.00018394986788431802, 2.696116765340169e-05]}}, 26: {'grad': {'score': [0.44901622425426135, 0.5220781398209627, 0.5589127974076704, 0.5222691294388779, 0.6355367520960366], 'topk_tokens': [' the', ' Project', '-per', 'EF', ' per', 'ioneer', ' as', ' CONNECT', 'str', ' the', '❤', ' Daily', ' Press', ' and', ' Date', ' hand', ' per', ' the', ' old', 'ED'], 'evidence_proportions': [0.59417724609375, 0.438995361328125, 0.438995361328125, 0.372283935546875]}, 'weight': {'score': [0.0012330033562400124, 0.007417644910516122, 0.0009254867380315608, 0.007484476828557994, 0.0048525943988707], 'topk_tokens': ['Answer', '.\n\n', '\n\n', 'Question', '<|start_header_id|>', '<|start_header_id|>', ' *\n\n', '.\n\n', 'assistant', '<|eot_id|>', '.\n\n', 'user', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '\n\n', 'office', ' under', '<|begin_of_text|>'], 'evidence_proportions': [0.002148866653442383, 0.001071612040201823, 0.001071612040201823, 0.0009452104568481445]}, 'saliency': {'score': [7.434595714915882e-05, 0.00022771597643556657, 4.663250663063743e-05, 0.00022947921074182331, 0.0002219233571029291], 'topk_tokens': ['<|end_header_id|>', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', '\n', '<|eot_id|>', '\n\n\n\n', 'then', 'assistant', '\n\n', 'Question', 'office', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', 'Answer', '<|start_header_id|>', '\n\n', 'user', ' under'], 'evidence_proportions': [0.00015838444232940674, 6.906191507975261e-05, 6.906191507975261e-05, 2.8888384501139324e-05]}}, 27: {'grad': {'score': [0.8663607510653409, 1.1122670561492174, 1.1322021484375, 1.113458373350632, 1.7169526262981136], 'topk_tokens': ['ian', ' exclaimed', ' Pioneer', 'iously', ' compos', 'ian', 'ian', 'ian', '\u200d', '\u200d', 'ian', ' rival', 'ian', 'ian', 'ian', 'ian', ' anxious', ' attempted', '\u200d', '\u200d'], 'evidence_proportions': [0.628021240234375, 0.8943684895833334, 0.8943684895833334, 0.96923828125]}, 'weight': {'score': [0.001287687908519398, 0.007494530916383602, 0.0011248642748052423, 0.007560834106483706, 0.002822655003245284], 'topk_tokens': ['NEW', '.\n\n', 'RE', '.\n\n', 'If', 'user', '\n\n', '\n\n', '<|eot_id|>', ' *\n\n', '<|start_header_id|>', 'Answer', '<|eot_id|>', 'Question', 'assistant', 'Just', 'office', '<|end_header_id|>', '<|begin_of_text|>', ' under'], 'evidence_proportions': [0.0018270015716552734, 0.001162389914194743, 0.001162389914194743, 0.001178741455078125]}, 'saliency': {'score': [0.00012075088240883567, 0.0003248682399765134, 5.285848270763051e-05, 0.00032737837573649527, 0.00050279207345916], 'topk_tokens': ['\n\n\n', 'user', 'Penn', 'at', '�', '\n\n', 'action', '<|start_header_id|>', 'then', 'just', '�', 'assistant', 'office', '<|eot_id|>', 'Answer', 'RE', 'Question', '<|begin_of_text|>', '<|end_header_id|>', ' under'], 'evidence_proportions': [6.633996963500977e-05, 8.458892504374187e-05, 8.458892504374187e-05, 0.00022934873898824057]}}, 28: {'grad': {'score': [0.8592862215909091, 0.9636449888472255, 0.9893160733309659, 0.9640598292548451, 0.6986234711437691], 'topk_tokens': [' a', ' ', ' true', ' out', ' of', ' PA', ' absent', ' During', ' Brown', ' ran', ' into', ' ', ' out', ' One', 'PA', ' at', ' brought', ' Douglas', 'out', ' state'], 'evidence_proportions': [0.569091796875, 0.7901611328125, 0.7901611328125, 1.1909993489583333]}, 'weight': {'score': [0.0005431825464422053, 0.00723163115347848, 0.00044960596344687724, 0.007302647310568222, 0.0010420672777222424], 'topk_tokens': ['\n', '.', '\n\n', 'Answer', 'Question', ',', '<|eot_id|>', '.\n\n', '\n\n', '<|start_header_id|>', ' \n', '.\n\n', '.\n\n', 'assistant', '<|eot_id|>', 'Just', 'office', '<|end_header_id|>', '<|begin_of_text|>', ' under'], 'evidence_proportions': [0.0003427863121032715, 0.0005460381507873535, 0.0005460381507873535, 0.0006710688273111979]}, 'saliency': {'score': [5.651604045521129e-05, 0.0002719269337064993, 3.3524903384121984e-05, 0.0002743194293495908, 9.348334335699315e-05], 'topk_tokens': ['<|start_header_id|>', '\n', ':', ' \n', ',', '\n', '\n', ',', 'assistant', '.\n\n', '.\n\n', '<|eot_id|>', 'Question', '\n\n', '.\n\n', 'office', '<|begin_of_text|>', 'Just', '<|end_header_id|>', ' under'], 'evidence_proportions': [2.199411392211914e-05, 6.079673767089844e-05, 6.079673767089844e-05, 7.096926371256511e-05]}}, 29: {'grad': {'score': [0.9728594693270597, 0.7181252037882381, 0.8703294233842329, 0.7159798295555599, 0.6535429605623571], 'topk_tokens': [' enabling', ' no', 'ERS', ' the', ' newspaper', ' the', ' of', ' in', ' the', '\n', ' Press', ' the', ' Papers', 'RI', ' Press', ' boys', ' of', ' to', ' press', ' press'], 'evidence_proportions': [1.830810546875, 0.8553365071614584, 0.8553365071614584, 0.6359380086263021]}, 'weight': {'score': [0.0009906779636036265, 0.0074575884183323265, 0.0007520101287148216, 0.0075270336022229605, 0.0013247481206568276], 'topk_tokens': ['If', '"The', '.\n\n', '.\n\n', ' of', '\n\n', ' \n', 'A', '.\n\n', ',', 'Answer', '\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>', ' under'], 'evidence_proportions': [0.002517223358154297, 0.0004907945791880289, 0.0004907945791880289, 0.000972747802734375]}, 'saliency': {'score': [0.00021393732591108844, 0.0003603153215190769, 0.00022807988253506747, 0.00036178416780736444, 8.056076561532369e-05], 'topk_tokens': ['<|start_header_id|>', ',', 'A', 'NEW', 'E', ' an', 'APER', ',', '\n\n', ' the', '<|start_header_id|>', '"The', '<|start_header_id|>', 'office', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' under'], 'evidence_proportions': [0.0010677576065063477, 2.645452817281087e-05, 2.645452817281087e-05, 1.9689400990804035e-05]}}, 30: {'grad': {'score': [1.2920587713068181, 1.7094503090170738, 1.5819577303799717, 1.712322930766329, 1.9565280821265243], 'topk_tokens': [' and', ' and', ' and', ' and', ' and', ' and', ' and', ' double', ' and', ' and', ' and', ' and', ' and', ' and', ' and', ' and', ' and', ' great', ' office', ' offices'], 'evidence_proportions': [1.0867919921875, 1.2880859375, 1.2880859375, 1.4368489583333333]}, 'weight': {'score': [0.0023644187233664775, 0.007239211247321806, 0.0013702701438557017, 0.00729585206682894, 0.0038584354447155464], 'topk_tokens': ['.', '\n\n', '\n\n\n\n', 'Answer', ' *\n\n', 'Just', '.\n\n', '.\n\n', ' \n', 'ANK', 'Question', '.\n\n', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'office', ' under', '<|begin_of_text|>'], 'evidence_proportions': [0.00434112548828125, 0.002005388339360555, 0.002005388339360555, 0.00176467498143514]}, 'saliency': {'score': [0.00022686340592124245, 0.0005230933590355942, 7.625330578197132e-05, 0.0005270108141374623, 0.0007897135688037407], 'topk_tokens': ['.\n\n', ' under', 'In', 'Just', ' person', ' FR', '\n\n\n\n', ' *\n\n', ' a', 'NEW', '.', 'ANK', ' \n', '.\n\n', 'assistant', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0005086362361907959, 0.00015664100646972656, 0.00015664100646972656, 0.00017945965131123862]}}, 31: {'grad': {'score': [1.1902535178444602, 1.4227615946244367, 1.1102350408380681, 1.4256350098826347, 1.674605206745427], 'topk_tokens': [' senator', ' he', ' affairs', ' got', '"', ' fall', 'ISC', ' bill', ' assistance', 'ot', 'ot', '<|end_header_id|>', 'ot', 'ot', '202', 'G', 'user', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.23120880126953125, 1.2579752604166667, 1.2579752604166667, 1.6941731770833333]}, 'weight': {'score': [0.00107906081459739, 0.006677090683508066, 0.001434702764857899, 0.006734241220609992, 0.0012572335033881955], 'topk_tokens': [':', ' was', ' Do', ',', ' Where', '.\n\n', '<|eot_id|>', ' \n', 'If', '.\n\n', 'Answer', '<|start_header_id|>', 'Question', 'assistant', 'Just', '<|eot_id|>', '<|end_header_id|>', ' under', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019080638885498047, 0.0008869965871175131, 0.0008869965871175131, 0.0009105205535888672]}, 'saliency': {'score': [0.00013184547424316406, 0.0003192766825961139, 9.507753632285379e-05, 0.00032144679230178773, 0.00010924441058461259], 'topk_tokens': [' write', '<|end_header_id|>', '.\n', ':', 'If', '.\n\n', ' the', ' \n', '<|eot_id|>', ',', 'Answer', 'Question', 'Just', '<|start_header_id|>', 'If', 'assistant', ' under', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00042244791984558105, 4.252791404724121e-05, 4.252791404724121e-05, 0.00011674563090006511]}}, 'pred_res': 'Mary took the football.<|eot_id|>', 'score': 0}
2025-01-23 22:15:18.911 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:15:18.911 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-0_pid-1_1-2-3-5.pkl | len: 10 |  size: 9.26 KB
Processing depth (1, 2, 3, 5):   2%|▏         | 2/100 [00:50<38:36, 23.64s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.22s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Processing depth (1, 3, 6, 7):   2%|▏         | 2/100 [01:02<38:36, 23.64s/it]2025-01-23 22:15:30.690 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:15:30.692 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (426, 430) -->  Mary took the football
2025-01-23 22:15:30.693 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:15:30.700 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1511, 1517) -->  tragedy. Mary journeyed to
2025-01-23 22:15:30.701 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:15:30.708 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1511, 1517) -->  tragedy. Mary journeyed to
2025-01-23 22:15:30.708 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:15:30.723 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2909, 2915) --> . Daniel went back to the
2025-01-23 22:15:30.723 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:15:30.740 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3388, 3394) --> . John went back to the
2025-01-23 22:15:30.740 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John took the milk.
2025-01-23 22:15:30.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (474, 478) -->  John took the milk
2025-01-23 22:15:30.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-23 22:15:30.749 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1214, 1220) --> . Sandra journeyed to the
2025-01-23 22:15:30.749 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John journeyed to the office.
2025-01-23 22:15:30.757 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1513, 1519) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:15:31.644 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football. Mary journeyed to the office.<|eot_id|>
2025-01-23 22:15:31.644 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4227])
your chose emoji: ['👨🏿\u200d🌾', '🏋🏾\u200d♀️', '🧞\u200d♀️', '🧑🏻\u200d🦲', '🏋🏿\u200d♂', '🇸🇪', '👩🏽\u200d❤️\u200d💋\u200d👨🏽', '👨🏽\u200d🦯\u200d➡', '🤛🏼', '🇸🇬']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4229, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 228261.44it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.93it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.12it/s]
2025-01-23 22:15:34.770 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.9677789861505682, 0.9441086540664164, 1.063294844193892, 0.9433576759329264, 0.7891566499750665], 'topk_tokens': [' announcement', ' news', ' newspaper', ' politically', 'of', 'posit', ' streets', ' duty', ' tele', ' news', ' office', ' newspaper', ' office', ' office', ' newspaper', ' newspaper', ' offices', ' offices', ' office', ' offices'], 'evidence_proportions': [1.16400146484375, 0.8978983561197916, 0.8978983561197916, 0.9767252604166666]}, 'weight': {'score': [0.1747533841566606, 0.007446470371119151, 0.0010683970017866654, 0.006600488414331434, 0.0006939990723386724], 'topk_tokens': ['manager', ' ', '\n', 'NEW', 'ANK', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'user', '<|start_header_id|>', '\n\n', '<|eot_id|>', '\n\n', 'assistant', 'office', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [0.954575777053833, 0.0019180774688720703, 0.0019180774688720703, 0.0005424022674560547]}, 'saliency': {'score': [0.00039130991155450994, 0.00017733534731518774, 5.110827359286222e-05, 0.00017687407018463245, 6.119875197714947e-05], 'topk_tokens': ['NEW', ' the', 'APER', ':', ' office', 'manager', '<|end_header_id|>', '\n\n', ' offices', '<|eot_id|>', 'user', ' web', ' Project', '***', '<|start_header_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.001908913254737854, 5.3981939951578774e-05, 5.3981939951578774e-05, 5.4230292638142906e-05]}}, 25: {'grad': {'score': [0.6352206143465909, 0.7636265568655415, 0.5418909246271307, 0.7654672055261537, 1.0523772544049201], 'topk_tokens': [' latter', ' *', ' starting', ' *', ' O', ' com', ' ', '\n', ' Grow', ',', ' AND', ' summer', '202', ' EAR', '\n', ' those', '�', ' Jul', '0', 'com'], 'evidence_proportions': [0.6602783203125, 0.5109659830729166, 0.5109659830729166, 0.8670247395833334]}, 'weight': {'score': [0.21496318687092175, 0.007442536605506621, 0.0005704435435208408, 0.006387753270148092, 0.000536211627594968], 'topk_tokens': [' ', '.\n\n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'MIN', 'RE', 'user', '\n\n', 'UL', '\n\n', '<|eot_id|>', '<|eot_id|>', 'office', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.178094506263733, 0.0011882781982421875, 0.0011882781982421875, 0.0004254579544067383]}, 'saliency': {'score': [0.001228630542755127, 0.00014249130917880855, 3.3020973205566406e-05, 0.0001373570879727709, 1.6426152371345682e-05], 'topk_tokens': ['announcement', 'ORE', '<|start_header_id|>', 'PA', 'ENCES', '<|eot_id|>', 'assistant', '<|eot_id|>', ' Think', '\n\n', ' Mary', '<|start_header_id|>', 'office', '<|eot_id|>', 'MIN', 'RE', ' the', 'UL', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.006479635834693909, 8.47776730855306e-05, 8.47776730855306e-05, 1.5666087468465168e-05]}}, 26: {'grad': {'score': [0.5221280184659091, 0.5161757452264129, 0.4543235085227273, 0.5164696040546595, 0.6951840177495429], 'topk_tokens': [' be', 'EF', ' and', ' representatives', '\n', ' the', 'hand', ' Dub', ' the', ' some', ' the', ',\n', ' the', 'hand', '600', ' and', ' hand', ' hand', ' newspaper', ' the'], 'evidence_proportions': [0.6583251953125, 0.395751953125, 0.395751953125, 0.68408203125]}, 'weight': {'score': [0.23060913519425827, 0.007271364130402152, 0.0015630830417979848, 0.006127314242838104, 0.0015828546057356165], 'topk_tokens': ['\n', '\n\n\n', ' *\n\n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '.\n\n', 'user', 'office', '<|eot_id|>', '\n\n', 'assistant', '<|end_header_id|>', ' *\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.257104754447937, 0.003262122472127279, 0.003262122472127279, 0.000972747802734375]}, 'saliency': {'score': [0.007784626700661399, 0.00014037346270496498, 8.712031624533914e-05, 0.00010046856377714423, 4.759494294511511e-05], 'topk_tokens': ['extent', '<|start_header_id|>', ' ', '\n\n\n', '\n\n\n\n', ' INCIDENT', '<|start_header_id|>', '<|eot_id|>', '\n\n', '\n', 'user', '.\n\n', '\n\n', 'manager', '\n\n', '<|start_header_id|>', 'E', '<|eot_id|>', ' *\n\n', ' the'], 'evidence_proportions': [0.04210129380226135, 0.0002075632413228353, 0.0002075632413228353, 6.097555160522461e-05]}}, 27: {'grad': {'score': [0.7768443714488636, 0.8804654824759104, 0.7624733664772727, 0.8816304743503585, 1.228658473238032], 'topk_tokens': ['<|eot_id|>', '\n', ' and', ':\n\n', ' in', ' the', "'", '.\n\n', '\n', '1', 'SP', '.\n\n', 'reading', '\n', '\u200d', '<|start_header_id|>', '\n\n', ' by', '\n\n', ':'], 'evidence_proportions': [0.7186279296875, 0.70086669921875, 0.70086669921875, 0.9676106770833334]}, 'weight': {'score': [0.32322647354819556, 0.007376566878107034, 0.001954241232438521, 0.005744689510714623, 0.0011490469283245979], 'topk_tokens': ['Question', '<|eot_id|>', 'UL', '\n\n', 'user', '\n\n', '\n\n', 'RE', '<|eot_id|>', 'ANK', '.\n\n', ' *\n\n', 'NEW', '<|eot_id|>', 'assistant', 'office', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.7667148113250732, 0.0030630826950073242, 0.0030630826950073242, 0.0012276967366536458]}, 'saliency': {'score': [0.005806334994056008, 0.00021307899416916532, 0.00014052336866205388, 0.00018405739124530534, 7.529905501832353e-05], 'topk_tokens': ['manager', 'en', 'as', 'If', 'UL', '<|start_header_id|>', 'From', '\n\n', '<|eot_id|>', ' Joseph', '\n\n\n', '\n\n', 'assistant', 'ANK', 'PA', '<|eot_id|>', ' *\n\n', '<|end_header_id|>', '<|begin_of_text|>', ' the'], 'evidence_proportions': [0.031560927629470825, 9.470184644063313e-05, 9.470184644063313e-05, 5.987286567687988e-05]}}, 28: {'grad': {'score': [0.5015896883877841, 0.5142168472156539, 0.5304385098544034, 0.5141979513772215, 0.26416242883560503], 'topk_tokens': [' one', 'During', ' ', ' getting', ' next', ',', ' of', ' out', ' of', ' not', ' one', ' after', ' next', ' not', 'from', ' not', ' consisted', ' em', '.', ' of'], 'evidence_proportions': [0.6226806640625, 0.4411875406901042, 0.4411875406901042, 0.5416666666666666]}, 'weight': {'score': [0.35793627392161975, 0.007110468516481715, 0.001938630234111439, 0.005293410624796914, 0.0007547482531121436], 'topk_tokens': ['\n\n\n', 'Just', ',\n', '\n\n', '\n\n', '\n\n', ' *\n\n', '.\n\n', 'E', ' *\n\n', '\n\n', '<|eot_id|>', 'assistant', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [1.9603290557861328, 0.002294778823852539, 0.002294778823852539, 0.0009574095408121744]}, 'saliency': {'score': [0.0008592226288535378, 0.00014243021054199104, 0.00011740218509327282, 0.00013879369521511357, 1.876912218459109e-05], 'topk_tokens': ['.\n\n', ',', ',', 'assistant', ':', '\n\n', ' *\n\n', ' By', 'In', '<|start_header_id|>', 'E', '\n\n', ' *\n\n', ' the', 'Another', '<|begin_of_text|>', '<|eot_id|>', '.\n\n', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.004305437207221985, 0.00011261304219563802, 0.00011261304219563802, 5.496541659037272e-05]}}, 29: {'grad': {'score': [0.9836647727272727, 0.8785281755253902, 0.9095500599254261, 0.8778124081191196, 0.47316279309861203], 'topk_tokens': [' the', 'RI', ' Papers', ' one', ' in', ' the', ' mailing', ' enabling', ' of', '600', ' printed', ' the', ' six', ' to', ' no', ' the', ' the', ' press', '\n', '\n'], 'evidence_proportions': [1.239501953125, 0.9639485677083334, 0.9639485677083334, 0.8525390625]}, 'weight': {'score': [0.4261490214954723, 0.007436311676249095, 0.0015969601544466886, 0.005265890198938878, 0.0008591971498854617], 'topk_tokens': ['\n\n', ',', ' of', 'G', '\n\n', 'E', ' OF', ',', 'A', '\n\n\n', '.\n\n', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' the', '<|begin_of_text|>'], 'evidence_proportions': [2.338268756866455, 0.0014665921529134114, 0.0014665921529134114, 0.0007673899332682291]}, 'saliency': {'score': [0.008933284065940163, 0.000266659933501988, 0.0003123256293210116, 0.00022086056045021778, 3.9149472054014815e-05], 'topk_tokens': ['office', ' that', '\n\n\n\n', ' the', '<|eot_id|>', ' the', ' the', ' OF', 'A', ',', '\n\n', '<|start_header_id|>', '.', ' an', 'APER', '<|end_header_id|>', ' in', ',', '<|begin_of_text|>', ' the'], 'evidence_proportions': [0.04886734485626221, 8.011857668558757e-05, 8.011857668558757e-05, 1.690785090128581e-05]}}, 30: {'grad': {'score': [1.3009255149147727, 1.1960831969363324, 0.9683851762251421, 1.1967290333664689, 1.5866556370511968], 'topk_tokens': ['.', '.', '️', ' Published', '.', '.', '�', '.', ' and', ' Pioneer', '<|start_header_id|>', '.', '<|end_header_id|>', '.', ' and', ' web', 'of', ' of', ' offices', ' office'], 'evidence_proportions': [1.30096435546875, 1.3145345052083333, 1.3145345052083333, 1.273681640625]}, 'weight': {'score': [0.06762738661332564, 0.007141465976126061, 0.00357863036069003, 0.00684222873108931, 0.0018152594566345215], 'topk_tokens': ['.', ',\n', ' *\n\n', '<|eot_id|>', '.\n\n', 'NEW', 'Question', ' *\n\n', '\n\n\n', '<|eot_id|>', 'ANK', '.\n\n', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'assistant', '<|start_header_id|>', ' the', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.35211610794067383, 0.006107409795125325, 0.006107409795125325, 0.0010081926981608074]}, 'saliency': {'score': [0.0007338903167031028, 0.0004084552819916587, 0.00022679567337036133, 0.00040769947330014536, 0.00018789159490707072], 'topk_tokens': [' *\n\n', ' *\n\n', '***', 'ANK', ',\n', 'If', ' FR', '\n\n\n', 'In', '.', 'NEW', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '.\n\n', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', 'office'], 'evidence_proportions': [0.003362908959388733, 0.0002056757609049479, 0.0002056757609049479, 3.764033317565918e-05]}}, 31: {'grad': {'score': [1.0275490500710227, 1.2323442857353983, 1.143310546875, 1.233888906390009, 1.3295132251496011], 'topk_tokens': [' he', ' I', ' o', ' fifty', 'ioneer', 'UG', 'S', ' DAYS', ' ST', 'ol', ' D', ' B', ' EVENTS', 'RI', 'RE', ' em', ' DAYS', ' to', 'G', 'ISC'], 'evidence_proportions': [0.739166259765625, 1.0673014322916667, 1.0673014322916667, 1.1402994791666667]}, 'weight': {'score': [0.010969140312888405, 0.00656577367595421, 0.0029909502376209607, 0.00656141813225786, 0.0013147915931458168], 'topk_tokens': [' Do', ':', 'Answer', ' Where', ' return', '.\n\n', ' \n', 'If', '.\n\n', ' the', 'Just', '<|eot_id|>', 'Question', '<|start_header_id|>', '.\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.047606468200683594, 0.0034089485804239907, 0.0034089485804239907, 0.0016646385192871094]}, 'saliency': {'score': [0.0002858449112285267, 0.00019797893471400996, 0.00013585253195329145, 0.00019784362512677373, 5.0322806581537777e-05], 'topk_tokens': ['Question', '<|end_header_id|>', ' item', ' Do', ' the', ',', '.\n\n', ' the', 'PA', ' the', '<|eot_id|>', ' traveled', ' the', '<|start_header_id|>', 'Just', 'UL', '.\n\n', '<|begin_of_text|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0009831786155700684, 0.00018321474393208823, 0.00018321474393208823, 2.621610959370931e-05]}}, 'pred_res': 'Mary took the football. Mary journeyed to the office.<|eot_id|>', 'score': 100}
2025-01-23 22:15:34.777 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:15:34.777 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-0_pid-2_1-3-6-7.pkl | len: 10 |  size: 9.09 KB
Processing depth (1, 3, 6, 7):   3%|▎         | 3/100 [01:06<32:28, 20.09s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.26s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Processing depth (2, 5, 8, 9):   3%|▎         | 3/100 [01:20<32:28, 20.09s/it]2025-01-23 22:15:48.494 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:15:48.499 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1020, 1024) -->  Mary took the football
2025-01-23 22:15:48.500 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:15:48.508 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1567, 1573) -->  John journeyed to the office
2025-01-23 22:15:48.508 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:15:48.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2131, 2137) -->  journeyed to the office.
2025-01-23 22:15:48.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:15:48.536 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3638, 3644) --> . Daniel went back to the
2025-01-23 22:15:48.537 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:15:48.553 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3348, 3354) --> . John went back to the
2025-01-23 22:15:48.553 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John took the milk.
2025-01-23 22:15:48.556 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (495, 499) -->  John took the milk
2025-01-23 22:15:48.556 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-23 22:15:48.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1251, 1257) --> . Sandra journeyed to the
2025-01-23 22:15:48.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John journeyed to the office.
2025-01-23 22:15:48.570 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1565, 1571) -->  ranks. John journeyed to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:15:49.169 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football.<|eot_id|>
2025-01-23 22:15:49.169 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4200])
your chose emoji: ['👱🏾\u200d♀', '🧑🏽\u200d🤝\u200d🧑🏾', '🦻🏾', '4⃣', '🛠', '⚖', '🏿', '🌞', '🇧🇯', '🧑🏻\u200d🦯\u200d➡️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4202, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 203360.19it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 143.61it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.09it/s]
2025-01-23 22:15:52.050 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.9455455433238636, 0.6259668015230843, 0.7963520396839489, 0.6233743995127051, 0.8125836955967234], 'topk_tokens': [' Gree', ' the', ' it', ' offices', ' office', 'over', ' the', ',', ' scale', ' rival', ' offices', ' offices', ' office', ' office', ' office', ' office', ' office', ' offices', ' offices', 'office'], 'evidence_proportions': [0.9049072265625, 1.1902058919270833, 0.9737904866536459, 0.6997324625651041]}, 'weight': {'score': [0.0022981004281477494, 0.007365381076527686, 0.0016475455327467485, 0.007422445183364516, 0.003480566971337617], 'topk_tokens': [' that', ' person', '.\n\n', ' happened', ' Where', ' \n', ' information', ' was', '<|start_header_id|>', 'Answer', ' bathroom', '\n\n', '<|eot_id|>', '<|eot_id|>', '.\n\n', 'assistant', ' football', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002452254295349121, 0.0018390615781148274, 0.0015701452891031902, 0.0033823251724243164]}, 'saliency': {'score': [0.0011379447850314054, 0.000532197759356855, 0.00013503432273864746, 0.0005310941424835411, 0.00040162321346909254], 'topk_tokens': ['�', ' location', ' front', '\n\n', '<|eot_id|>', ' Pioneer', ' office', ' office', '<|eot_id|>', 'Answer', '<|end_header_id|>', ' information', '<|begin_of_text|>', ' bathroom', 'assistant', ' offices', ' football', ' offices', ' offices', 'office'], 'evidence_proportions': [0.0004884153604507446, 0.0027673145135243735, 0.0009459257125854492, 0.00013361374537150064]}}, 25: {'grad': {'score': [1.2616327459161931, 1.2200426322287006, 0.9833928888494318, 1.2210746937614614, 0.7735560402941348], 'topk_tokens': [' office', ' took', ' and', ' and', ' Eagle', ' only', '000', 'manager', ' which', ' issued', ',', ' proof', ' company', 'of', ' office', 'cont', ' awarded', ' take', ' and', '800'], 'evidence_proportions': [1.93798828125, 1.1426595052083333, 1.5629069010416667, 0.6284281412760416]}, 'weight': {'score': [0.0022975233468142424, 0.007375396245550622, 0.0006507445465434681, 0.0074378434656445025, 0.0020974856704028683], 'topk_tokens': [' Press', ' Stewart', ' proprietor', ' Senator', '\n\n', ' directly', ' *\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ' Southern', ' person', 'Answer', ' football', ' Do', 'office', 'assistant', '.\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0029039382934570312, 0.00017844637235005698, 0.0006094872951507568, 0.005700359741846721]}, 'saliency': {'score': [0.00043085217475891113, 0.0002799681797871869, 0.00011516430161216043, 0.0002800418299628657, 0.00012077947161090907], 'topk_tokens': [' person', ' person', ' actions', ' dropped', 'Min', ' Southern', ' met', 'men', ' obtained', ' ty', 'stage', ' Ear', ' spring', 'Answer', ' Minnesota', ' morning', ' Square', ' Min', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005616694688796997, 8.07642936706543e-06, 0.00010360280672709148, 0.0010936657587687175]}}, 26: {'grad': {'score': [0.6390658291903409, 0.5430493942616611, 0.3949862393465909, 0.5433247737324886, 0.5623619876690765], 'topk_tokens': ['\n', ' office', '800', ' one', ' of', ' to', 'Dub', ' Dub', ' cap', ' a', ' of', ' o', 'ian', ' Dub', '\n', '\n', '10', '\n', ' two', ',\n'], 'evidence_proportions': [0.71429443359375, 0.5839640299479166, 0.698974609375, 0.5841064453125]}, 'weight': {'score': [0.0015504847873340952, 0.007347089230929142, 0.0010068985548886385, 0.007411305078122982, 0.00585998172190652], 'topk_tokens': [' Do', '<|start_header_id|>', ' *\n\n', ' \n', ' Herald', ' bathroom', ' Square', ' met', '.\n\n', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|start_header_id|>', '.\n\n', 'office', '<|end_header_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [0.0021701455116271973, 0.0005689462025960287, 0.0003121296564737956, 0.003357271353403727]}, 'saliency': {'score': [0.00019554116509177467, 0.0002222481737359259, 0.00012277202172712847, 0.00022291580950657643, 0.00042213343862277356], 'topk_tokens': ['♀', '.\n\n', ' proprietor', ' football', ' Square', 'enan', ' intercept', ' Pioneer', ' *\n\n', ' Southern', ' Hale', ' carrier', 'assistant', '<|start_header_id|>', ' Stewart', ' Pioneer', ' corner', ' kitchen', ' met', '<|begin_of_text|>'], 'evidence_proportions': [0.0004497021436691284, 4.416704177856445e-05, 2.1884838740030926e-05, 0.00035113096237182617]}}, 27: {'grad': {'score': [0.5042793967507102, 0.586962170171942, 0.45318187366832385, 0.5881074774202215, 0.5310305979714465], 'topk_tokens': [' went', ' an', ' an', 'ORE', ' news', 'tickets', 'news', ' man', ' D', 'office', 'paper', 'columns', ' far', ' ever', ' portion', 'man', 'an', ' veto', 'pend', ' ever'], 'evidence_proportions': [0.27582550048828125, 0.4668172200520833, 0.6417643229166666, 0.5565592447916666]}, 'weight': {'score': [0.0030478645454753528, 0.007448080209026673, 0.0013730742714621804, 0.007503504661942629, 0.005771608495000583], 'topk_tokens': ['illustr', ' Hale', ' Minnesota', ' fall', '      ', ' Do', 'Just', ' *\n\n', '\n\n', '<|eot_id|>', ' *\n\n', '<|eot_id|>', ' Pioneer', '<|start_header_id|>', ' football', 'assistant', '.\n\n', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.005256175994873047, 0.0012054940064748128, 0.00040338436762491864, 0.006062507629394531]}, 'saliency': {'score': [0.00036717003042047674, 0.0003073187682357418, 9.248744357715954e-05, 0.00030813876856629124, 0.00058389955492162], 'topk_tokens': [' fall', '.\n\n', 'Minnesota', ' nineteenth', ' *', ' fully', ' pur', '<|start_header_id|>', ' met', 'illustr', ' ty', ' Min', ' Pioneer', ' *\n\n', ' *\n\n', '<|end_header_id|>', ' football', ' *\n\n', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00032921135425567627, 0.0001911322275797526, 1.7742315928141277e-05, 0.0009179413318634033]}}, 28: {'grad': {'score': [0.5890738747336648, 0.5427255389918492, 0.6501908735795454, 0.5419117100483067, 0.3520920027547808], 'topk_tokens': ['RI', ' take', 'E', ' Date', '600', ' Min', ' Min', ' Min', 'LES', ' Min', ' came', ' left', ' Min', 'Min', '\n\n', ' edition', 'Min', ' past', ' be', '\n\n'], 'evidence_proportions': [0.4564056396484375, 0.7321980794270834, 0.3113759358723958, 0.8120930989583334]}, 'weight': {'score': [0.0011761215600100431, 0.007201746269273281, 0.0006612078710035844, 0.007268233904762874, 0.004109838115635203], 'topk_tokens': [' a', ' Square', '.\n', ' football', ' the', ' \n', 'Just', ' directly', ' that', '<|start_header_id|>', ' Do', ' after', '.\n\n', ' the', '.\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0007161796092987061, 0.00030995408693949383, 0.00045482317606608075, 0.003070215384165446]}, 'saliency': {'score': [7.516958496787332e-05, 0.00017182694225865056, 8.754296736283736e-05, 0.00017278430380461134, 0.0001811821069290389], 'topk_tokens': ['.\n', '<|start_header_id|>', ' second', ' expedition', '.\n\n', ' after', '.\n\n', ' Do', ' the', ' the', '\n\n', ' bathroom', 'even', ' much', ' Square', 'assistant', 'Bridge', 'office', 'Just', '<|begin_of_text|>'], 'evidence_proportions': [0.00015024840831756592, 1.3043483098347982e-05, 2.4318695068359375e-05, 0.00013809402783711752]}}, 29: {'grad': {'score': [1.2751492587002842, 1.2509655232999166, 1.4175248579545454, 1.2499563007070331, 0.7226912085689715], 'topk_tokens': [' of', ' the', ' business', ' the', ' the', ' newspaper', "'s", ' a', ' senator', 'message', ' the', ' newspaper', ' press', 'APER', ' the', ' printer', ' business', ' state', ' two', ' press'], 'evidence_proportions': [0.6471099853515625, 1.477294921875, 1.48828125, 1.278564453125]}, 'weight': {'score': [0.001293645663694902, 0.007371480325356601, 0.0007880330085754395, 0.007438471235295452, 0.0041305084726703705], 'topk_tokens': [' *\n\n', '<|start_header_id|>', ' after', ' bathroom', ' that', ' the', ' \n', ' was', '.\n\n', '\n\n', '<|start_header_id|>', ' football', 'Answer', '<|eot_id|>', ' Do', '.\n\n', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0007226467132568359, 0.0002955496311187744, 0.0004556576410929362, 0.0035103956858317056]}, 'saliency': {'score': [9.810653599825773e-05, 0.00038428864894396686, 7.078864357688211e-05, 0.0003874615702048811, 0.0003871161546280135], 'topk_tokens': ['Answer', 'papers', 'cap', 'press', 'their', '<|eot_id|>', ' two', ' location', 'paper', ' Do', 'time', 'Question', ' football', 'question', 'doctor', 'From', 'columns', 'As', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00010144710540771484, 5.1130851109822593e-05, 5.513429641723633e-05, 0.00018582741419474283]}}, 30: {'grad': {'score': [1.4847911487926135, 0.9070917099558246, 1.2730047052556819, 0.9020990515741493, 0.7519305784310868], 'topk_tokens': [' made', ' Gen', ' office', ' Good', ' combined', ' o', ' and', ' office', ' bend', ' offices', ' office', ' trembling', ' offices', ' fifteen', ' O', ' office', ' office', ' office', ' offices', ' o'], 'evidence_proportions': [1.06048583984375, 1.8968098958333333, 1.658203125, 1.1822306315104167]}, 'weight': {'score': [0.0036482702602039685, 0.007280821348587029, 0.001691260121085427, 0.00732961559364462, 0.011800961707954976], 'topk_tokens': [' was', ' Do', '.\n\n', '�', ' \n', 'his', ' Where', '<|start_header_id|>', ' football', 'Answer', '<|eot_id|>', '.', ' the', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '.\n\n', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00386810302734375, 0.001675566037495931, 0.0013651053110758464, 0.00775758425394694]}, 'saliency': {'score': [0.0006344914436340332, 0.0005574456173598568, 0.00017875974828546697, 0.0005590415964222918, 0.0008770334186838634], 'topk_tokens': ['♀', ' idol', ' Where', ' morning', 'columns', '<|eot_id|>', ' Ear', '<|end_header_id|>', ' entire', ' Min', '.', ' Southern', 'Question', 'Answer', ' offices', ' offices', ' football', ' offices', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0005709528923034668, 0.0007801751295725504, 0.00020520885785420737, 0.0009604493776957194]}}, 31: {'grad': {'score': [0.5494745427911932, 0.43209671826659923, 0.4294530695134943, 0.43148965913553317, 0.3853213751493995], 'topk_tokens': [' the', '\n', '<|start_header_id|>', 'of', ' ', ' Hon', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '\n', 'reading', ' ', '\n', ' the', ' INCIDENT', ' a', '\n', ' a', ' composing', '\n'], 'evidence_proportions': [0.7850341796875, 0.5330403645833334, 0.412841796875, 0.545501708984375]}, 'weight': {'score': [0.0019143792715939608, 0.0067710613194447026, 0.0008859363469210538, 0.006827896277224461, 0.005466318842190415], 'topk_tokens': ['?', ' football', ' they', ' was', 'Just', ':', ':', '.\n\n', '.', ' the', '<|start_header_id|>', ' \n', ' Do', 'Answer', '.\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.002045869827270508, 0.0005826056003570557, 0.0008313258488972982, 0.004241545995076497]}, 'saliency': {'score': [0.00019972161813215777, 0.00019899999340734613, 7.210536436601119e-05, 0.0001996675754407669, 0.00040253151708574437], 'topk_tokens': [' \n', ' be', ' the', ' directly', ' a', ' the', ' they', '<|start_header_id|>', ' football', ':', ' not', ' Do', '.', ' return', ' offices', 'Just', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00016418099403381348, 0.00014569362004597983, 0.00011639793713887532, 0.00036076704661051434]}}, 'pred_res': 'Mary took the football.<|eot_id|>', 'score': 0}
2025-01-23 22:15:52.058 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:15:52.059 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-0_pid-3_2-5-8-9.pkl | len: 10 |  size: 9.38 KB
Processing depth (2, 5, 8, 9):   4%|▍         | 4/100 [01:23<30:22, 18.98s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.27s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.22s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Processing depth (2, 7, 8, 9):   4%|▍         | 4/100 [01:36<30:22, 18.98s/it]2025-01-23 22:16:05.366 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:16:05.371 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (911, 915) -->  Mary took the football
2025-01-23 22:16:05.371 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:16:05.379 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1516, 1522) -->  John journeyed to the office
2025-01-23 22:16:05.379 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:16:05.393 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2813, 2819) --> . Mary journeyed to the
2025-01-23 22:16:05.394 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:16:05.414 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3632, 3638) --> . Daniel went back to the
2025-01-23 22:16:05.414 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:16:05.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3339, 3345) --> . John went back to the
2025-01-23 22:16:05.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John took the milk.
2025-01-23 22:16:05.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (429, 433) -->  John took the milk
2025-01-23 22:16:05.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-23 22:16:05.439 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1118, 1124) --> . Sandra journeyed to the
2025-01-23 22:16:05.439 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John journeyed to the office.
2025-01-23 22:16:05.446 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1514, 1520) -->  tragedy. John journeyed to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:16:06.041 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football.<|eot_id|>
2025-01-23 22:16:06.041 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4185])
your chose emoji: ['🇬🇵', '🙋🏿\u200d♂️', '🕕', '🀄', '🇷🇪', '🤸🏿\u200d♂️', '🤳🏻', '💁', '🍱', '🧼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4187, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 203360.19it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.84it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 141.39it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 133.43it/s]
2025-01-23 22:16:09.100 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7933516068892046, 0.6663162672408646, 0.8702222650701349, 0.6645589152195015, 0.5949508960430439], 'topk_tokens': ['state', ' or', 'ot', 'Min', ' Min', ' gold', 'ol', ' Min', ' office', ' Min', ' Where', ' office', 'ot', ' office', ' Good', 'Min', ' office', ' offices', ' office', ' office'], 'evidence_proportions': [0.705474853515625, 1.3540852864583333, 0.6800537109375, 0.4045003255208333]}, 'weight': {'score': [0.0003390745683149858, 0.007477238037605003, 0.0013019225814125755, 0.007547934763735975, 0.00044986261771275447], 'topk_tokens': ['\n', ' versatile', ' speech', ' \n', ' made', 'office', 'Answer', 'user', '\n\n', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [7.565319538116455e-05, 0.0002898871898651123, 0.00023065010706583658, 0.000672300656636556]}, 'saliency': {'score': [1.146034760908647e-05, 0.00015830817130348237, 3.134662454778498e-05, 0.00015976214334062984, 3.677835831275353e-05], 'topk_tokens': [' made', ' Good', 'From', 'system', ' Project', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', ':', '<|start_header_id|>', 'assistant', '\n\n', '***', '\n\n', 'Answer', '<|end_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [4.664063453674316e-06, 7.4108441670735674e-06, 6.3478946685791016e-06, 2.5153160095214844e-05]}}, 25: {'grad': {'score': [0.7377638383345171, 0.6690171494357535, 0.7899027737704191, 0.6680101711854192, 0.47759158794696516], 'topk_tokens': [' very', ' suddenly', ' Min', ' time', ' six', ' went', ' several', ' summer', ' and', 'ORE', ' the', ' war', ' for', ' very', 'action', ' and', '600', 'Civil', '.', ':'], 'evidence_proportions': [0.8527603149414062, 0.4090983072916667, 0.847686767578125, 0.8798421223958334]}, 'weight': {'score': [0.0002083453265103427, 0.007488225677687028, 0.0005407116629860618, 0.007563775442603588, 0.00035125819536355825], 'topk_tokens': [' time', ' approaching', '\n\n', '\n\n', 'user', ' \n', 'office', '<|start_header_id|>', ' made', ' possibly', '<|eot_id|>', ' versatile', '\n\n', ' speech', 'Answer', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.583333015441895e-05, 7.32739766438802e-05, 0.0002061923344930013, 0.0004405776659647624]}, 'saliency': {'score': [1.67028470472856e-05, 9.339170920584063e-05, 3.294511274857955e-05, 9.41199206684397e-05, 2.7331022115854117e-05], 'topk_tokens': ['PA', ' \n', 'RE', '<|start_header_id|>', ' versatile', 'E', 'system', 'assistant', '<|eot_id|>', ' speech', 'user', '\n\n', '.\n\n', '<|eot_id|>', '<|begin_of_text|>', 'office', 'UL', '<|eot_id|>', 'Answer', '<|end_header_id|>'], 'evidence_proportions': [7.644295692443848e-06, 8.642673492431641e-07, 2.555052439371745e-05, 2.9732783635457356e-05]}}, 26: {'grad': {'score': [0.6138212030584161, 0.5807696721250298, 0.6992714621803977, 0.5799648994815945, 0.512435142810528], 'topk_tokens': [' a', 'hand', ' five', ' the', ' some', ' summer', ' one', '600', ' and', ' two', ' a', ' no', ' hand', '000', ' in', ' and', '600', ' the', ' six', '600'], 'evidence_proportions': [0.6724853515625, 0.6086832682291666, 0.38886133829752606, 0.8048095703125]}, 'weight': {'score': [0.00101122801954096, 0.007068452272490326, 0.001648602160540494, 0.007129397393423896, 0.0012144836095663218], 'topk_tokens': ['<|start_header_id|>', ' possibly', ' speech', ' versatile', 'Answer', ' made', 'user', '\n\n', ' \n', '<|eot_id|>', '.\n\n', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'assistant', 'office', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010441243648529053, 0.0002592007319132487, 0.0008204778035481771, 0.002558549245198568]}, 'saliency': {'score': [3.175572915510698e-05, 0.00019408926520792885, 0.00010672482577237216, 0.00019541520183856954, 0.00010214516749748817], 'topk_tokens': ['assistant', ' often', ' time', '\n', '<|end_header_id|>', ' \n', '\n\n', '\n\n', '<|eot_id|>', ' day', ' speakers', '\n\n', ' speech', ' possibly', ' approaching', '<|eot_id|>', '<|eot_id|>', ' made', ' versatile', '<|begin_of_text|>'], 'evidence_proportions': [1.475214958190918e-06, 1.0251998901367188e-05, 1.7891327540079754e-05, 8.73108704884847e-05]}}, 27: {'grad': {'score': [0.9750872525301847, 1.0369155552155482, 0.9693118008700284, 1.037602860369948, 0.873354985163762], 'topk_tokens': ['600', ' had', ' book', ' as', ' method', ' would', ' trials', ' were', ' conditions', ' galaxy', ' force', ' newspaper', ' offices', 'APER', ' Articles', 'SP', ' football', ' office', ',', ' Papers'], 'evidence_proportions': [2.02734375, 1.1496988932291667, 0.763458251953125, 0.31060028076171875]}, 'weight': {'score': [0.0005849057977849787, 0.00735479099300103, 0.0015319775451313365, 0.007421660259268924, 0.0010824398352549626], 'topk_tokens': [' versatile', 'Johnson', 'Question', ' \n', 'From', 'NEW', 'user', ' made', '<|eot_id|>', '<|eot_id|>', '\n\n', '\n\n', 'Answer', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0001846402883529663, 0.00014824668566385904, 0.0007185737291971842, 0.0011547406514485676]}, 'saliency': {'score': [3.304806622591886e-05, 0.0002302716334679829, 5.8436935598200016e-05, 0.00023223139253929852, 9.131431579589844e-05], 'topk_tokens': ['Answer', ' \n', '<|start_header_id|>', 'Cut', 'E', '\n', '<|eot_id|>', '\n\n\n', 'system', 'NEW', 'PA', 'From', '<|eot_id|>', '\n\n', 'office', '\n\n', '<|eot_id|>', '<|begin_of_text|>', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [6.183981895446777e-06, 1.0599692662556967e-05, 5.103151003519694e-05, 5.542238553365072e-05]}}, 28: {'grad': {'score': [0.8223003040660511, 0.6892979059066754, 0.6071555397727273, 0.6890278309116092, 0.8266648512620193], 'topk_tokens': ['\n\n', ' and', ' and', ' not', ' and', ' and', ' and', ' balance', ' M', 'E', ' and', '10', ' and', ' Date', ' the', 'Min', ' almost', ' be', '\n\n', 'still'], 'evidence_proportions': [0.6387405395507812, 0.8046875, 1.061279296875, 0.7233072916666666]}, 'weight': {'score': [0.00047752532092007726, 0.006625988388334848, 0.0015376616608012807, 0.006685657559343504, 0.0005579900283079881], 'topk_tokens': ['\n', ' approaching', ' speech', ' made', '\n', 'Answer', '\n\n', ' possibly', '\n\n', 'E', ' \n', '<|eot_id|>', '\n\n', '.\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.9561262130737305e-05, 0.00017549594243367514, 0.00044582287470499676, 0.0010965665181477864]}, 'saliency': {'score': [2.048503268848766e-05, 0.000165515906035772, 7.478486407886852e-05, 0.00016676783993311501, 2.6359007908747746e-05], 'topk_tokens': ['\n', 'assistant', '<|eot_id|>', '\n\n\n', '\n\n', 'ern', ' a', '\n\n\n\n', 'ANK', ' Project', '\n', '<|start_header_id|>', '<|eot_id|>', 'From', ' \n', 'office', '.\n\n', '\n\n', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [4.082918167114258e-06, 7.4108441670735674e-06, 1.395742098490397e-05, 5.1021575927734375e-05]}}, 29: {'grad': {'score': [0.37706063010475854, 0.4566238436119238, 0.604052110151811, 0.4562634692052813, 0.7730079064002404], 'topk_tokens': ['hand', 'district', ' no', ' the', ' six', '�', ' and', ' two', ' business', ' state', ' the', '�', ' half', '600', ' ', '�', '\n', '�', ' press', ' printer'], 'evidence_proportions': [0.431610107421875, 0.34356689453125, 0.3417714436848958, 0.40947723388671875]}, 'weight': {'score': [0.0006101375276392156, 0.007328214752437791, 0.0007518584078008479, 0.0073988103446481655, 0.0006663226164304293], 'topk_tokens': [' ', ' ', '\n\n\n', 'Answer', 'ern', '\n\n', 'Question', '\n\n', 'NEW', 'nes', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'E', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00014841556549072266, 0.00011324882507324219, 0.00042630235354105633, 0.001598676045735677]}, 'saliency': {'score': [5.770542404868386e-05, 0.0001806613081874442, 6.105141206221147e-05, 0.00018194937170803503, 5.399951568016639e-05], 'topk_tokens': ['G', ' OF', '\n\n', 'E', 'political', ' ', '\n\n', 'Question', '<|end_header_id|>', '\n\n', 'APER', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'NEW', 'office', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.1309981346130371e-05, 7.10288683573405e-06, 3.7282705307006836e-05, 0.00015966097513834635]}}, 30: {'grad': {'score': [1.1055325594815342, 1.1025671675349296, 0.8904196999289773, 1.1036779581853051, 1.4709097055288463], 'topk_tokens': ['      ', ' Gutenberg', ' Date', 'de', 'Just', ' Hor', ' At', ' Good', ' That', ' proof', ' web', ' Pioneer', ' Published', ' of', ' P', ' office', ' O', ' offices', ' office', ' office'], 'evidence_proportions': [0.9689178466796875, 0.8624267578125, 1.2611490885416667, 1.2840983072916667]}, 'weight': {'score': [0.0012343851002779875, 0.0068797504870567565, 0.00233437256379561, 0.0069338649820654165, 0.0029033697568453276], 'topk_tokens': ['\n', '\n\n', '\n\n\n\n', '\n\n\n', ' *\n\n', 'NEW', '\n\n', '.\n\n', '\n\n', '<|eot_id|>', 'Answer', ' \n', '<|eot_id|>', 'Question', '<|start_header_id|>', 'ANK', '<|end_header_id|>', 'assistant', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0003130584955215454, 0.0005794763565063477, 0.0005088349183400472, 0.003229061762491862]}, 'saliency': {'score': [6.982413205233487e-05, 0.0003591240349207683, 0.00020482323386452416, 0.000361479626397076, 0.00015105650975153997], 'topk_tokens': ['\n', 'user', '\n\n', 'Johnson', '<|eot_id|>', ' \n', '<|eot_id|>', '\n\n\n\n\n\n\n', '.\n\n', ' ', 'NEW', '\n', ' FR', ' ', 'Answer', '<|start_header_id|>', 'ANK', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [2.35140323638916e-05, 2.242128054300944e-05, 5.821386973063151e-05, 0.00015971064567565918]}}, 31: {'grad': {'score': [1.2147660688920454, 1.3108232606132075, 1.0370289195667615, 1.3127872320640614, 1.3667461688701923], 'topk_tokens': [' agreement', ' Do', ' they', ' to', 'RI', ' were', ' EVENTS', ' to', 'S', ' em', ' was', 'ol', ' Min', ' D', ' DAYS', ' B', 'G', 'ISC', ' been', 'ot'], 'evidence_proportions': [0.851806640625, 1.3971354166666667, 1.3043619791666667, 1.1847737630208333]}, 'weight': {'score': [0.001848426732149991, 0.006633454699319275, 0.002519217404452237, 0.006680711237037064, 0.0012676887787305391], 'topk_tokens': ['ern', 'Johnson', 'Just', '.', ':', ' Do', '?', '.\n\n', ' Where', ':', 'Question', '<|eot_id|>', ' \n', 'Answer', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0010306239128112793, 0.0015523334344228108, 0.000762393077214559, 0.0037757555643717446]}, 'saliency': {'score': [0.00025671449574557215, 0.0002648703716579644, 0.0001663375984538685, 0.0002654369056383078, 5.3970859600947457e-05], 'topk_tokens': ['<|eot_id|>', 'PA', ' return', '?', ':', ' dropped', 'UL', ' ', ' ', '.\n\n', ' ', 'Just', 'ern', '<|eot_id|>', ' Where', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', 'Answer', '<|end_header_id|>'], 'evidence_proportions': [0.0001280754804611206, 0.00016489624977111816, 2.3106733957926433e-05, 0.0006678998470306396]}}, 'pred_res': 'Mary took the football.<|eot_id|>', 'score': 0}
2025-01-23 22:16:09.109 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:16:09.110 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-0_pid-4_2-7-8-9.pkl | len: 10 |  size: 9.21 KB
Processing depth (2, 7, 8, 9):   5%|▌         | 5/100 [01:40<28:57, 18.29s/it]Processing depth (2, 7, 8, 9):   5%|▌         | 5/100 [01:40<31:56, 20.17s/it]
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:99 - Selected idx: 1
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:101 - Answer: office
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:103 - Needle: [' Mary journeyed to the office.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary got the apple.', ' John took the milk.', ' Mary journeyed to the bathroom.', ' Sandra journeyed to the bedroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:104 - Real Needle: [' Mary journeyed to the office.', ' Mary got the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-23 22:16:09.324 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.19s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Processing depth (1, 2, 6, 8, 9):   0%|          | 0/100 [00:13<?, ?it/s]2025-01-23 22:16:22.875 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:16:22.878 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (470, 476) --> . Mary journeyed to the
2025-01-23 22:16:22.878 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the apple.
2025-01-23 22:16:22.883 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (957, 961) -->  Mary got the apple
2025-01-23 22:16:22.883 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:16:22.885 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (470, 476) --> . Mary journeyed to the
2025-01-23 22:16:22.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:16:22.905 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3297, 3301) -->  dropped the apple.
2025-01-23 22:16:22.905 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-23 22:16:22.923 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3612, 3618) -->  Daniel went back to the kitchen
2025-01-23 22:16:22.923 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:16:22.932 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1835, 1841) -->  Hon. John went back to
2025-01-23 22:16:22.932 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the office.
2025-01-23 22:16:22.935 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (471, 477) -->  Mary journeyed to the office
2025-01-23 22:16:22.935 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John took the milk.
2025-01-23 22:16:22.945 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2065, 2069) -->  John took the milk
2025-01-23 22:16:22.945 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:16:22.945 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (32, 38) -->  journeyed to the bedroom.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:16:23.426 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:16:23.426 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4219])
your chose emoji: ['👋🏾', '🙍🏼\u200d♀️', '👩🏼\u200d🤝\u200d👨🏾', '🇮🇱', '👨🏻\u200d🦱', '👴🏻', '🤲🏽', '💂🏿', '🔈', '🌝']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4221, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 179435.47it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.23it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 133.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 138.25it/s]
2025-01-23 22:16:26.476 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.2590050330528846, 1.2931353278251598, 1.5508089932528408, 1.2919895255305685, 0.7556668133802817], 'topk_tokens': [' out', ' there', ':', ' he', ' few', ' get', ' manager', ' only', ' to', ' and', '\n', ' an', ' the', ' *', ' the', ' and', ' be', ' apple', '.', ' office'], 'evidence_proportions': [1.3170166015625, 1.29345703125, 1.3170166015625, 1.47454833984375, 0.976318359375]}, 'weight': {'score': [0.00790573083437406, 0.007474953968415919, 0.5061415867371992, 0.004843306204354537, 0.0007414482009242958], 'topk_tokens': ['\n\n', '\n', 'system', 'ol', 'office', 'user', '\n\n', '***', '\n\n', 'UL', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', ' bedroom', '<|begin_of_text|>'], 'evidence_proportions': [0.013469696044921875, 0.00014513731002807617, 0.013469696044921875, 0.010140478610992432, 0.0004616975784301758]}, 'saliency': {'score': [0.0007819762596717248, 0.00020789652462678904, 0.0037208497524261475, 0.00018579946157316953, 4.938706545762613e-05], 'topk_tokens': ['\n', '<|eot_id|>', '<|eot_id|>', ' apple', ':', 'user', '\n\n', '<|start_header_id|>', 'ol', '\n\n', 'UL', '<|end_header_id|>', '<|eot_id|>', '\n\n', 'assistant', '***', 'system', '<|begin_of_text|>', ' bedroom', '<|start_header_id|>'], 'evidence_proportions': [0.0003234744071960449, 2.0459294319152832e-05, 0.0003234744071960449, 0.004005566239356995, 5.7597955067952476e-05]}}, 25: {'grad': {'score': [0.9683931790865384, 0.9399456285351516, 1.157957597212358, 0.9386190290563031, 1.3180292693661972], 'topk_tokens': [',', ' "', ' O', ',', ',', ',', 'ORE', ' journey', ',', ' the', ',', ',', ',', ',', '\n', ' o', ' o', 'ed', 'Cut', 'ting'], 'evidence_proportions': [1.1530965169270833, 0.7406005859375, 1.1530965169270833, 0.9932861328125, 0.7342529296875]}, 'weight': {'score': [0.0064048239817986125, 0.007457604370984781, 0.5601130778139288, 0.004550571510782066, 0.0005835160403184487], 'topk_tokens': ['-text', 'system', '\n\n', 'ol', 'office', 'user', '\n\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'UL', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '***', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.01055765151977539, 0.00012069940567016602, 0.01055765151977539, 0.009481072425842285, 0.00023775299390157065]}, 'saliency': {'score': [0.0008388597231644851, 0.0002393448022382854, 0.009058356285095215, 0.0001891157965908055, 4.7024706719626846e-05], 'topk_tokens': ['user', '<|end_header_id|>', ' prepared', ' apple', ' Mary', '<|start_header_id|>', '\n\n', 'office', 'system', '<|eot_id|>', '\n\n', 'UL', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '***', ' bedroom'], 'evidence_proportions': [0.001235872507095337, 1.4036893844604492e-05, 0.001235872507095337, 0.0017004460096359253, 2.0325183868408203e-05]}}, 26: {'grad': {'score': [1.4749415471003606, 1.9624751058694623, 1.3543222600763494, 1.9687188717777884, 1.6020439040492958], 'topk_tokens': [' to', ' the', ' of', 'that', ' or', ' John', ' or', ' new', ' get', ' the', ' an', ' when', ' of', ' with', ',', ' and', ' that', ' had', ' and', ' state'], 'evidence_proportions': [0.8642450968424479, 2.28955078125, 0.8642450968424479, 1.5791015625, 2.0838216145833335]}, 'weight': {'score': [0.014299913094593929, 0.007452457557213699, 0.47558886354619806, 0.004941785432667919, 0.0009946134728445134], 'topk_tokens': ['\n\n', '-text', '\n\n', '<|start_header_id|>', 'ol', 'office', '\n\n', '***', 'user', 'UL', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.01999664306640625, 0.00015382468700408936, 0.01999664306640625, 0.0317760705947876, 0.0006864070892333984]}, 'saliency': {'score': [0.0008522272109985352, 0.00023223878760610082, 0.005833379246971824, 0.00019884679404888798, 2.566441683702066e-05], 'topk_tokens': ['<|start_header_id|>', ' Mary', '<|eot_id|>', '<|start_header_id|>', 'pleasant', 'gro', '-text', '<|eot_id|>', 'assistant', '\n\n', 'UL', 'ol', 'user', '<|end_header_id|>', '***', '<|eot_id|>', '<|end_header_id|>', ' bedroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015696982542673747, 6.884336471557617e-06, 0.0015696982542673747, 0.000790715217590332, 2.185503641764323e-05]}}, 27: {'grad': {'score': [0.6737506573016827, 0.5888176768686331, 0.7200303511186079, 0.5875967479626281, 0.40944080621423856], 'topk_tokens': [',\n', ' election', ' of', 'RE', ' in', 'urous', ' of', '\n', 'In', ' in', '<|eot_id|>', '\n\n', ' of', ' of', 'S', ' of', ' the', ' wield', '.', '.'], 'evidence_proportions': [0.8523763020833334, 0.9930419921875, 0.8523763020833334, 0.487274169921875, 0.22795613606770834]}, 'weight': {'score': [0.009729626087042002, 0.007482630357559301, 0.653985473242673, 0.004060276072287542, 0.0005239516916409345], 'topk_tokens': ['-text', ' apple', '***', '\n\n', 'ol', '<|eot_id|>', '\n\n', 'UL', 'office', 'user', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.012481689453125, 0.00013403594493865967, 0.012481689453125, 0.025050431489944458, 0.0004086891810099284]}, 'saliency': {'score': [0.001594699346102201, 0.00017203890127865689, 0.0010174377398057418, 0.0001587180419417271, 3.4917408311870735e-05], 'topk_tokens': ['<|start_header_id|>', '\n\n', '<|eot_id|>', ' by', 'ol', 'ine', ' prepared', ' PA', '***', ' apple', '-text', '\n', '\n\n', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'E', '<|eot_id|>', 'UL', '<|end_header_id|>'], 'evidence_proportions': [0.001627047856648763, 1.4275312423706055e-05, 0.001627047856648763, 0.005423799157142639, 3.0885140101114907e-05]}}, 28: {'grad': {'score': [1.2016930213341346, 0.8934419004308813, 1.0408103249289773, 0.8907444083430686, 0.8883452079665493], 'topk_tokens': ['ed', ' him', ' election', ' different', ' get', ' different', ' be', ' to', ' *', 'Today', '.', '.', ':', ' of', ' Mary', ' of', ' be', ' be', '\n', '.'], 'evidence_proportions': [1.6388346354166667, 1.2193603515625, 1.6388346354166667, 0.6197509765625, 0.7035929361979166]}, 'weight': {'score': [0.009168205352929922, 0.0073421783736463235, 0.5978051396933469, 0.004217888450211197, 0.0010383985411952918], 'topk_tokens': ['UL', '***', '\n', ' ', '\n\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'user', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'assistant', 'office', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.015050252278645834, 0.00022761523723602295, 0.015050252278645834, 0.012991875410079956, 0.0008153915405273438]}, 'saliency': {'score': [0.0004908327872936542, 0.00012660385223113934, 0.006082713603973389, 9.293398238930975e-05, 4.411331364806269e-05], 'topk_tokens': ['26', 'andra', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '.', 'system', '3', '\n', 'user', ':', '<|end_header_id|>', '\n\n', '\n\n', 'office', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.0009088913599650065, 4.604458808898926e-05, 0.0009088913599650065, 0.00031745433807373047, 6.682674090067546e-05]}}, 29: {'grad': {'score': [0.8239980844350961, 0.5998710410003554, 1.196628917347301, 0.5953285113073441, 0.24014088805292694], 'topk_tokens': [',', ',', '\n\n\n', ' Mary', ' ', '\n\n', '\n\n', '.', '<|start_header_id|>', 'ed', '.', 'user', 'system', ' to', ' A', ' journey', 'ed', 'Today', ' Date', 'S'], 'evidence_proportions': [1.1878255208333333, 0.59051513671875, 1.1878255208333333, 0.4260711669921875, 0.5172831217447916]}, 'weight': {'score': [0.01428182996236361, 0.007422404050883511, 0.6222897930578752, 0.004138093571168126, 0.0008056785019350724], 'topk_tokens': ['UL', '\n\n\n', '<|start_header_id|>', 'system', '\n\n', '<|end_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '***', 'user', '<|start_header_id|>', 'office', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.024872779846191406, 0.0002469569444656372, 0.024872779846191406, 0.017315596342086792, 0.00043400128682454425]}, 'saliency': {'score': [0.0027168461909660925, 0.0005970058269451026, 0.024886778809807518, 0.0004557427416138017, 0.0001150545939593248], 'topk_tokens': ['-text', ' back', 'user', ' apple', '\n', '<|start_header_id|>', '\n\n', '<|start_header_id|>', ' PA', 'office', '<|end_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'system', ' bedroom'], 'evidence_proportions': [0.003953893979390462, 1.3530254364013672e-05, 0.003953893979390462, 0.005742281675338745, 2.8004248936971028e-05]}}, 30: {'grad': {'score': [1.6181922325721154, 2.012772123386046, 1.309327559037642, 2.018939115376659, 1.8853699590118838], 'topk_tokens': ['\n', '\n', '.\n', '.\n\n', '\n', '?\n', ' value', '.', '\n', '\n', ' The', ' the', 'peak', '.\n', ' The', '.\n\n', ' get', ',', ' supper', '.\n'], 'evidence_proportions': [0.9000447591145834, 2.00341796875, 0.9000447591145834, 2.195068359375, 2.4130859375]}, 'weight': {'score': [0.03130728694108816, 0.007283508763497431, 0.4725560031153939, 0.004680917556126455, 0.00143165991339885], 'topk_tokens': [' PA', '.', '\n\n', '<|eot_id|>', 'UL', '<|start_header_id|>', 'system', '\n\n', '***', '<|end_header_id|>', 'assistant', '<|start_header_id|>', 'user', '<|start_header_id|>', 'office', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.050359090169270836, 0.0009096860885620117, 0.050359090169270836, 0.050386980175971985, 0.0007489522298177084]}, 'saliency': {'score': [0.004884859690299401, 0.00038228236055859905, 0.017774855548685246, 0.0002625355068054126, 0.00014303603642423389], 'topk_tokens': ['assistant', ' PA', ' Jul', '\n\n', 'system', '<|end_header_id|>', 'UL', 'user', '.', 'ol', '<|start_header_id|>', '<|start_header_id|>', ' apple', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'office', '***', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.006406893332799275, 9.518861770629883e-05, 0.006406893332799275, 0.01232956349849701, 7.07705815633138e-05]}}, 31: {'grad': {'score': [2.8685822120079627, 4.100455220178867, 2.4093907096169214, 4.117045710819847, 4.115351287411972], 'topk_tokens': ['cap', ' injunction', '�', 'fect', ' going', '.', ' "', '.', 'engers', 'iously', '.', ' goods', 'user', ' speakers', 'extent', '<|end_header_id|>', 'AYS', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [2.0199896494547525, 2.95751953125, 2.0199896494547525, 2.8988037109375, 4.486328125]}, 'weight': {'score': [0.01390773974932157, 0.006888524922847861, 0.45448336005210876, 0.004485077532880917, 0.0007949018142592739], 'topk_tokens': ['***', 'system', '<|eot_id|>', ' Mary', '\n', '<|start_header_id|>', ' apple', '\n\n', ' PA', '\n\n', '\n\n', 'user', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.02297210693359375, 0.000525355339050293, 0.02297210693359375, 0.019644826650619507, 0.0008758703867594401]}, 'saliency': {'score': [0.0013495431496546818, 0.0002770276183738609, 0.003215155818245628, 0.0002548555061738896, 9.46538549073985e-05], 'topk_tokens': ['<|end_header_id|>', '?\n', ' journey', 'UL', '***', '\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'system', 'user', ' apple', '<|start_header_id|>', '<|eot_id|>', ' bedroom', 'assistant', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0008240143458048502, 4.4539570808410645e-05, 0.0008240143458048502, 0.006013423204421997, 0.00016134977340698242]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:16:26.487 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:16:26.488 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-1_pid-0_1-2-6-8-9.pkl | len: 10 |  size: 9.83 KB
Processing depth (1, 2, 6, 8, 9):   1%|          | 1/100 [00:17<28:09, 17.07s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
Processing depth (3, 4, 6, 8, 9):   1%|          | 1/100 [00:29<28:09, 17.07s/it]2025-01-23 22:16:39.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:16:39.130 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1453, 1459) --> . Mary journeyed to the
2025-01-23 22:16:39.130 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the apple.
2025-01-23 22:16:39.139 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1822, 1826) -->  Mary got the apple
2025-01-23 22:16:39.139 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:16:39.147 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1453, 1459) --> . Mary journeyed to the
2025-01-23 22:16:39.147 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:16:39.164 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3352, 3356) -->  Mary dropped the apple
2025-01-23 22:16:39.165 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-23 22:16:39.183 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3606, 3612) -->  Daniel went back to the kitchen
2025-01-23 22:16:39.183 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:16:39.192 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1825, 1831) -->  apple. John went back to
2025-01-23 22:16:39.192 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the office.
2025-01-23 22:16:39.199 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1454, 1460) -->  Mary journeyed to the office
2025-01-23 22:16:39.200 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John took the milk.
2025-01-23 22:16:39.209 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2069, 2073) -->  John took the milk
2025-01-23 22:16:39.210 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:16:39.210 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (32, 38) -->  journeyed to the bedroom.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:16:39.789 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary's hand.<|eot_id|>
2025-01-23 22:16:39.790 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4213])
your chose emoji: ['\U0001faf2🏽', '🥎', '👚', '🧖🏿', '🍬', '👩🏼\u200d❤️\u200d👩🏽', '🪓', '👢', '👩🏻\u200d❤️\u200d💋\u200d👩🏽', '🦴']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4215, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 198546.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 133.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 134.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.16it/s]
2025-01-23 22:16:42.815 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.3269160344050481, 1.259161384193357, 2.3038884943181817, 1.2532229111124236, 1.350948509803185], 'topk_tokens': [' order', 'Cut', 'announcement', ' tele', ' no', 'called', '.', ' proc', 'tele', ' office', ' delegates', ' tele', ' newspaper', ' offices', ' legislative', ' bedroom', ' tele', ' Knowledge', ' enabling', ' work'], 'evidence_proportions': [1.7216796875, 1.8363037109375, 1.7216796875, 0.95745849609375, 0.4441019694010417]}, 'weight': {'score': [0.0033092338305253247, 0.007517740650109125, 0.0013010908256877553, 0.007576820917314896, 0.007051443136655368], 'topk_tokens': [' moved', ' apple', ' new', '<|eot_id|>', ':', ' persons', '<|start_header_id|>', ' else', ' anything', 'estead', ' item', '<|eot_id|>', "'clock", ' discarded', ' was', 'Answer', '<|end_header_id|>', 'assistant', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0036953787008921304, 0.008175075054168701, 0.0036953787008921304, 0.0003321021795272827, 0.0012778043746948242]}, 'saliency': {'score': [0.0001062154769897461, 0.000282610579883218, 5.274198272011497e-05, 0.00028492480638017464, 0.00022704784686748797], 'topk_tokens': [' item', ' new', 'estead', ' moved', ' steps', '?\n', ' of', "'clock", ' item', '.\n', '<|start_header_id|>', 'user', ' discarded', ':', ' was', '<|begin_of_text|>', 'Answer', '<|end_header_id|>', 'assistant', 'office'], 'evidence_proportions': [0.00012448430061340332, 0.00020503997802734375, 0.00012448430061340332, 7.013976573944092e-05, 2.7845303217569988e-05]}}, 25: {'grad': {'score': [0.8901648888221154, 0.865871747108541, 1.29656982421875, 0.8634462660955874, 1.2075880784254807], 'topk_tokens': [' on', '�', "'s", ' greet', 'ant', ' he', ' Knowledge', 'itol', 'ien', ' reached', 'e', 'es', ' passengers', ' noon', ' hardly', 'Cut', 'ed', ' the', 'ting', ' journey'], 'evidence_proportions': [0.8227132161458334, 1.070556640625, 0.8227132161458334, 0.52630615234375, 1.1473795572916667]}, 'weight': {'score': [0.003542629572061392, 0.007544925458914869, 0.0013860464096069336, 0.007602414067540491, 0.007579880494337815], 'topk_tokens': [' offices', ' waving', ' versatile', ' either', ' item', ' else', ' writer', ' anything', ' item', '-known', ' persons', 'estead', ' discarded', "'clock", 'Answer', ' was', 'assistant', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004061828056971232, 0.009091556072235107, 0.004061828056971232, 0.000667273998260498, 0.0007218519846598307]}, 'saliency': {'score': [0.0001725187668433556, 0.00039765079674771673, 9.330836209383878e-05, 0.00040066230774841235, 0.00029054513344397914], 'topk_tokens': [' item', ' persons', ' write', '\n\n\n\n', ' apple', ' new', ' moved', ' else', 'estead', ' anything', ' item', ' discarded', 'Answer', '<|eot_id|>', '<|eot_id|>', ' was', '<|begin_of_text|>', 'assistant', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00017528732617696127, 0.0004505664110183716, 0.00017528732617696127, 0.00011421740055084229, 2.0484129587809246e-05]}}, 26: {'grad': {'score': [0.5086262042705829, 0.6092991223680309, 0.8004802357066761, 0.6089179155950727, 0.6649686153118427], 'topk_tokens': [' St', ' it', ' It', '.', ' the', ' Ch', ' the', ' the', '.', ' second', '.', ' it', ' it', ' St', ' apple', ' St', ' set', ' It', ' St', '�'], 'evidence_proportions': [0.7049763997395834, 0.90283203125, 0.7049763997395834, 0.1787548065185547, 0.07303619384765625]}, 'weight': {'score': [0.005371937384972205, 0.007498707890086327, 0.0038324919613924894, 0.0075313339480571045, 0.008905751888568585], 'topk_tokens': [' anything', '-known', ' moved', ' persons', ' new', ' either', ' item', 'estead', '<|eot_id|>', ' item', '<|eot_id|>', "'clock", ' discarded', '<|start_header_id|>', 'Answer', ' was', 'assistant', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.006443182627360026, 0.008725404739379883, 0.006443182627360026, 0.003221273422241211, 0.0024275779724121094]}, 'saliency': {'score': [0.00025893403933598445, 0.0003834279547948147, 0.00020294026895002887, 0.0003851576334342471, 0.00046845307716956503], 'topk_tokens': ['ye', '<|end_header_id|>', 'city', 'S', ' was', ' until', 'his', 'tickets', 'en', '<|start_header_id|>', '<|eot_id|>', 'had', '<|begin_of_text|>', 'office', 'stage', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'cuts'], 'evidence_proportions': [0.00033164024353027344, 0.00031594932079315186, 0.00033164024353027344, 0.00031444430351257324, 3.8504600524902344e-05]}}, 27: {'grad': {'score': [0.7598699422983023, 0.9751055336224792, 1.1203113902698865, 0.975681870682279, 1.0006976787860578], 'topk_tokens': ['ian', ' dropped', 'rich', 'ed', ' const', 'ian', ' Gal', 'ed', ' connected', ' engaged', 'ayers', ' charges', ' ever', ' seen', ' strange', ' imported', ' fr', ' heard', 'en', ' extended'], 'evidence_proportions': [0.8974202473958334, 0.92431640625, 0.8974202473958334, 0.8994359970092773, 0.2820943196614583]}, 'weight': {'score': [0.003837099442115197, 0.007538712434779989, 0.002283410592512651, 0.007589554426222074, 0.009340179883516752], 'topk_tokens': [' anything', ' is', ' offices', ' either', ' day', ' versatile', ' item', '-known', ' item', ' writer', 'estead', ' persons', ' discarded', "'clock", 'Answer', 'assistant', ' was', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004205067952473958, 0.008610129356384277, 0.004205067952473958, 0.0012283921241760254, 0.0016582806905110676]}, 'saliency': {'score': [0.000636146618769719, 0.000846769065902185, 0.0005586445331573486, 0.0008496044206768024, 0.0009869254552401028], 'topk_tokens': [' person', '<|end_header_id|>', ' new', ' item', ' writer', "'clock", ' *\n\n', ' persons', ' item', 'cuts', '<|eot_id|>', 'old', ' discarded', ' was', '<|start_header_id|>', 'Answer', ' *\n\n', 'assistant', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.0006156563758850098, 0.0015506744384765625, 0.0006156563758850098, 0.000445440411567688, 0.0001945793628692627]}}, 28: {'grad': {'score': [1.109130859375, 1.3793235190539739, 1.497802734375, 1.3803838661657069, 1.3608210637019231], 'topk_tokens': [' Paul', ' Pennsylvania', 'ARR', ' conclusion', '\n\n\n', ' *\n\n', ' Grow', 'SP', ' Clean', '�', ' came', 'ENCES', ' Paul', ' Gutenberg', 'A', 'UL', ' PA', ' Pa', ' PA', ' PA'], 'evidence_proportions': [1.081298828125, 1.1092529296875, 1.081298828125, 1.193603515625, 1.1083984375]}, 'weight': {'score': [0.00206981713955219, 0.0072174610757884325, 0.0009973076256838713, 0.007282419587486163, 0.006637942790985107], 'topk_tokens': [' waving', ' versatile', ' item', '<|eot_id|>', ' writer', '-known', ' proprietor', ' persons', ' offices', ' discarded', 'estead', '<|start_header_id|>', '<|eot_id|>', 'Answer', "'clock", ' was', 'assistant', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.002585887908935547, 0.0037512779235839844, 0.002585887908935547, 0.0007731020450592041, 0.0007811784744262695]}, 'saliency': {'score': [6.0388675102820765e-05, 0.0002787982300365806, 4.180994900790128e-05, 0.0002814121946660283, 9.885017688457783e-05], 'topk_tokens': [' *\n\n', ' affairs', ' INCIDENT', ' of', ' persons', ' need', ':', ' waving', ' was', ' writer', 'S', ' versatile', ' dropped', 'ian', 'Answer', '<|start_header_id|>', 'office', '<|begin_of_text|>', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [8.279085159301758e-05, 7.443130016326904e-05, 8.279085159301758e-05, 9.03010368347168e-06, 4.046161969502767e-05]}}, 29: {'grad': {'score': [0.6669065035306491, 1.1561398326290035, 1.2437300248579546, 1.1587299651770047, 1.2350193903996394], 'topk_tokens': [' and', '�', ' time', '�', 'ioneer', ' and', ' another', ' *', ' before', '\n', '<|start_header_id|>', '�', ' ever', '�', '<|end_header_id|>', '�', '<|eot_id|>', '\n', '<|eot_id|>', ' until'], 'evidence_proportions': [0.6366373697916666, 0.96173095703125, 0.6366373697916666, 0.9842529296875, 0.3193308512369792]}, 'weight': {'score': [0.0019710614131047176, 0.0074022539711224794, 0.0017135685140436346, 0.007466175758154924, 0.008004008806668794], 'topk_tokens': [' versatile', '<|start_header_id|>', ' item', ' discarded', '<|eot_id|>', ' day', '-known', ' persons', ' offices', ' writer', ' waving', '<|eot_id|>', 'Answer', 'estead', "'clock", 'assistant', ' was', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.002229452133178711, 0.0033960342407226562, 0.002229452133178711, 0.0005906820297241211, 0.001424551010131836]}, 'saliency': {'score': [0.00012304232670710637, 0.00026715641768377445, 0.00026703693649985575, 0.0002680562484856558, 0.0005061791493342473], 'topk_tokens': [' persons', 'assistant', ' affairs', '-known', ' writer', ' luxury', 'Answer', 'tele', ' item', ' day', '❤', ' absent', 'estead', '<|start_header_id|>', ' was', '<|eot_id|>', '<|eot_id|>', 'office', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00010886788368225098, 5.327165126800537e-05, 0.00010886788368225098, 8.128583431243896e-05, 0.00022574265797932944]}}, 30: {'grad': {'score': [0.7079725999098557, 0.9345928441021649, 0.6495638760653409, 0.9375116738707778, 1.0473407451923078], 'topk_tokens': ['      ', ' *', '\n', 'of', '      ', '.\n\n', '      ', '      ', ' ', '      ', '      ', '      ', '<|end_header_id|>', '      ', '<|eot_id|>', '      ', '      ', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.8822021484375, 0.57232666015625, 0.8822021484375, 0.8548583984375, 0.352020263671875]}, 'weight': {'score': [0.004078975090613732, 0.0070715758299912425, 0.004157564856789329, 0.007105632911976905, 0.006301590112539438], 'topk_tokens': [' item', '-known', ' anything', '<|start_header_id|>', ',', ' either', ' *\n\n', '<|eot_id|>', ' persons', ' discarded', 'Answer', "'clock", ' was', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'office', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.005027294158935547, 0.006174802780151367, 0.005027294158935547, 0.0030472278594970703, 0.0014729499816894531]}, 'saliency': {'score': [0.0004081863623399001, 0.00039592484986654804, 0.00047884475101124156, 0.0003954105620936922, 0.00023684960145216723], 'topk_tokens': ['<|eot_id|>', ' gathering', ' Gen', ' location', 'stage', ' Wood', ' Red', ' *\n\n', '<|begin_of_text|>', ' order', ' local', 'advance', ' block', '<|start_header_id|>', ' legislative', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.0005037983258565267, 0.0009049177169799805, 0.0005037983258565267, 0.0002266913652420044, 6.804863611857097e-06]}}, 31: {'grad': {'score': [0.6128023587740384, 0.9791904146278173, 0.7976712313565341, 0.9824348378301611, 1.080205829326923], 'topk_tokens': ['ate', ' will', '.', ' participate', ' dispatch', ' pleasure', '<|end_header_id|>', ' paper', ' question', ' he', ' morning', ' hear', ' ever', '?', ' they', ' "', ' been', '!', '.', '."'], 'evidence_proportions': [0.5771687825520834, 0.64654541015625, 0.5771687825520834, 0.620849609375, 0.6562093098958334]}, 'weight': {'score': [0.0023897427778977612, 0.006497158444224728, 0.0027360049161044035, 0.006542643969769039, 0.0023699118540837214], 'topk_tokens': [',', ' location', ':', ' that', ' dropped', ' party', "'clock", ' world', 'Answer', ',', ' was', '.\n', '\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0024360020955403647, 0.0019447803497314453, 0.0024360020955403647, 0.00397336483001709, 0.0015381177266438801]}, 'saliency': {'score': [8.008342522841234e-05, 0.000347012953119063, 0.00016390735452825373, 0.00034964518035548125, 0.0001971914218022273], 'topk_tokens': [' Gen', '0', 'rich', '\n', 'un', ' Gal', 'that', ' that', ' report', ' both', ' John', ' gathering', '<|eot_id|>', 'of', ' party', ' world', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [6.069739659627279e-05, 3.726780414581299e-05, 6.069739659627279e-05, 0.0002910494804382324, 6.7551930745442706e-06]}}, 'pred_res': "Mary's hand.<|eot_id|>", 'score': 0}
2025-01-23 22:16:42.826 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:16:42.826 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-1_pid-1_3-4-6-8-9.pkl | len: 10 |  size: 10.01 KB
Processing depth (3, 4, 6, 8, 9):   2%|▏         | 2/100 [00:33<27:10, 16.64s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Processing depth (1, 3, 5, 6, 8):   2%|▏         | 2/100 [00:45<27:10, 16.64s/it]2025-01-23 22:16:55.254 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:16:55.257 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (470, 476) --> . Mary journeyed to the
2025-01-23 22:16:55.257 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the apple.
2025-01-23 22:16:55.264 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1515, 1519) -->  Mary got the apple
2025-01-23 22:16:55.265 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:16:55.267 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (470, 476) --> . Mary journeyed to the
2025-01-23 22:16:55.267 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:16:55.279 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2462, 2466) -->  Mary dropped the apple
2025-01-23 22:16:55.279 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-23 22:16:55.296 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3340, 3346) -->  Daniel went back to the kitchen
2025-01-23 22:16:55.296 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:16:55.305 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1841, 1847) --> . John went back to the
2025-01-23 22:16:55.305 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the office.
2025-01-23 22:16:55.308 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (471, 477) -->  Mary journeyed to the office
2025-01-23 22:16:55.308 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John took the milk.
2025-01-23 22:16:55.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2061, 2065) -->  John took the milk
2025-01-23 22:16:55.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:16:55.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (32, 38) -->  journeyed to the bedroom.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:16:55.919 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary got the apple.<|eot_id|>
2025-01-23 22:16:55.919 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4201])
your chose emoji: ['🤹🏼\u200d♂️', '🏋\u200d♂️', '🚄', '🙀', '🐆', '🧸', '👎🏾', '🙆🏻\u200d♀', '🧜🏻', '🐿️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4203, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 152520.15it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 103.01it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 100.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 104.34it/s]
2025-01-23 22:16:59.028 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.38266695462740385, 0.2477892581377885, 0.272658261385831, 0.2468135810787784, 0.1716400722287736], 'topk_tokens': [' ', ' held', 'Cut', ' work', ' of', ' state', ' Mary', ' of', ' journey', ' boat', ' of', ' getting', 'During', ' ', ' D', ' offices', ' of', 'ting', ' get', ' open'], 'evidence_proportions': [0.4027506510416667, 0.41302490234375, 0.4027506510416667, 0.312835693359375, 0.3688151041666667]}, 'weight': {'score': [0.005218725938063402, 0.007580043984230944, 0.026932889764959164, 0.007492350040073142, 0.013290063390191996], 'topk_tokens': ['�', ' dollars', 'ed', '️', ' $', 'ree', 'half', 'office', '186', ':', '      ', "'clock", 'cery', 'men', ' enough', '�', 'assistant', ' seldom', ' bedroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006656328837076823, 0.002645254135131836, 0.006656328837076823, 0.0021817684173583984, 0.0060838063557942705]}, 'saliency': {'score': [6.204843521118164e-05, 3.871903826785944e-05, 0.00010092962871898304, 3.824365985379179e-05, 5.562350435077019e-05], 'topk_tokens': ['.\n\n', ' back', ' course', '\n\n', '<|eot_id|>', ' hear', ' by', 'wing', '<|eot_id|>', '\n\n', ' journey', ' ', ' seldom', '\n\n', '<|start_header_id|>', '\n', ' back', ' prepared', ':', ' '], 'evidence_proportions': [6.663799285888672e-05, 5.647540092468262e-05, 6.663799285888672e-05, 2.644956111907959e-05, 8.031725883483887e-05]}}, 25: {'grad': {'score': [0.207796390240009, 0.2380525024907804, 0.17421757091175427, 0.23857982557459714, 0.26529089009986734], 'topk_tokens': [' completed', ' they', ' *', ' o', ' and', ' could', ' I', ' *', ' more', ' of', '\n', ' *', ' you', '      ', '.', '.', ' and', ' "', ' so', ' Wood'], 'evidence_proportions': [0.1699854532877604, 0.12384033203125, 0.1699854532877604, 0.273529052734375, 0.29556719462076825]}, 'weight': {'score': [0.0055994437291071964, 0.007525147922033927, 0.03677238117564808, 0.007382339059804464, 0.014187848792885835], 'topk_tokens': [' the', ' Married', '      ', 'cery', ' $', '186', ' dollars', 'men', ':', '<|eot_id|>', 'half', "'clock", '<|end_header_id|>', ' enough', ' seldom', '�', 'assistant', 'office', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.008016586303710938, 0.0028541088104248047, 0.008016586303710938, 0.002152562141418457, 0.004893302917480469]}, 'saliency': {'score': [0.00014485991918123685, 6.894443592740444e-05, 0.00020681998946449974, 6.773936619397104e-05, 8.724208148020618e-05], 'topk_tokens': ['�', '!', ' the', '\n\n\n\n\n\n\n', ' OF', ' rates', ' Mary', '\n', 'antics', 'out', ' or', 'Answer', "'clock", '\n\n', ' bedroom', ':', 'assistant', '<|begin_of_text|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.00024753808975219727, 6.186962127685547e-05, 0.00024753808975219727, 1.5407800674438477e-05, 8.113185564676921e-05]}}, 26: {'grad': {'score': [0.11436799856332633, 0.10437368955210564, 0.14680654352361505, 0.1040864753952669, 0.1050514005265146], 'topk_tokens': ['\n', ' office', ' It', '�', '\n', 'Cut', ' until', '<|eot_id|>', 'ting', ' ', '<|start_header_id|>', 'in', ' ', '.\n', ' in', ' Date', ',', ' John', '.\n\n', '\n\n'], 'evidence_proportions': [0.12497711181640625, 0.0730428695678711, 0.12497711181640625, 0.0944671630859375, 0.13396708170572916]}, 'weight': {'score': [0.007409315842848558, 0.007586385475973728, 0.025265780362215908, 0.007493884181861556, 0.012286798009332621], 'topk_tokens': ['ed', '<|eot_id|>', 'ree', '<|end_header_id|>', '️', '️', '<|eot_id|>', 'half', 'cery', 'office', '      ', 'men', "'clock", ':', ' enough', '�', 'assistant', ' seldom', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.009456634521484375, 0.004355192184448242, 0.009456634521484375, 0.0036575794219970703, 0.007851918538411459]}, 'saliency': {'score': [9.299012330862191e-05, 5.3202981129731844e-05, 0.00013769756663929331, 5.25066281627375e-05, 8.770204939932194e-05], 'topk_tokens': [' and', '️', 'office', ' to', 'UL', ' of', '<|end_header_id|>', ' prepared', '4', 'had', 'assistant', ' Knowledge', 'user', 'ol', ' journey', ' have', '-text', '\n\n', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.00012570619583129883, 0.00011157989501953125, 0.00012570619583129883, 2.9325485229492188e-05, 5.760788917541504e-05]}}, 27: {'grad': {'score': [0.2541093092698317, 0.17831138449284736, 0.25567158785733307, 0.1774274686039857, 0.18167387764408904], 'topk_tokens': [' the', ' to', ' ', ' the', ' to', 'fect', ' apple', 'posit', 'itor', ' no', 'nes', ' congressional', ' the', 'S', '<|end_header_id|>', 'Today', '<|eot_id|>', '<|start_header_id|>', ' the', ' back'], 'evidence_proportions': [0.2323481241861979, 0.482421875, 0.2323481241861979, 0.31304931640625, 0.10612996419270833]}, 'weight': {'score': [0.00748293216411884, 0.007551987852450073, 0.06233631480823864, 0.007262346517881905, 0.014667834875718603], 'topk_tokens': ['gro', ' Press', '�', ' dollars', ' *', "'clock", '186', 'ed', ' $', 'half', '      ', 'cery', 'office', 'men', 'assistant', ' enough', '�', ' seldom', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.00974114735921224, 0.006023406982421875, 0.00974114735921224, 0.002463102340698242, 0.00728607177734375]}, 'saliency': {'score': [0.0003239145645728478, 0.00011094551610572266, 0.00036872787909074265, 0.00010824794516810155, 0.00013543637293689657], 'topk_tokens': [':', '.', ' daily', 'ed', ' the', ' prepared', '<|end_header_id|>', 'system', ' bedroom', ' PA', 'user', '.\n\n', 'assistant', '.', ' back', 'office', 'UL', '\n', '\n\n', '3'], 'evidence_proportions': [0.000583420197168986, 0.00011654198169708252, 0.000583420197168986, 3.832578659057617e-05, 0.0001335442066192627]}}, 28: {'grad': {'score': [0.48095703125, 0.3410573374394034, 0.6079656427556818, 0.3387686792550391, 0.2943760134139151], 'topk_tokens': [' to', ' back', ' the', 'andra', ' the', ',', ' the', ' John', '.', ' to', ' the', ' of', ',', ' PA', ',', ',', '.', ',', ',', 'ed'], 'evidence_proportions': [0.6560872395833334, 0.415435791015625, 0.6560872395833334, 0.30438232421875, 0.2920939127604167]}, 'weight': {'score': [0.005292122180645282, 0.00742164374021811, 0.07238214666193182, 0.007091014738117314, 0.013549813684427514], 'topk_tokens': [' dollars', 'gro', ':', ' $', '      ', "'clock", 'fortunate', ' Married', 'half', 'cery', '186', 'men', ' enough', '<|end_header_id|>', '�', ' seldom', 'assistant', 'office', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.006079037984212239, 0.003310680389404297, 0.006079037984212239, 0.0017473697662353516, 0.0074024200439453125]}, 'saliency': {'score': [0.00020282314373896673, 0.00011752559660503816, 0.0004950247027657249, 0.00011499305350972714, 0.00014178932837720186], 'topk_tokens': [',', '.', '.', '<|eot_id|>', '<|begin_of_text|>', 'ley', '186', ' back', '�', ' Married', ',', '?\n', 'es', '<|start_header_id|>', '\n', 'As', ',', ' Newspaper', ' bedroom', '<|end_header_id|>'], 'evidence_proportions': [0.00026792287826538086, 0.00021101534366607666, 0.00026792287826538086, 6.286799907684326e-05, 0.00016046563784281412]}}, 29: {'grad': {'score': [0.2930133526141827, 0.21903182471504729, 0.39974862878972833, 0.21761201980168496, 0.34991671004385316], 'topk_tokens': ['️', '186', 'ree', ' Press', '️', ',', '.', ' to', ' I', "'clock", ' wrote', ',', ' back', ',', ',', 'ed', 'Cut', 'system', ' to', ' the'], 'evidence_proportions': [0.3969930013020833, 0.2586212158203125, 0.3969930013020833, 0.140167236328125, 0.20987955729166666]}, 'weight': {'score': [0.009844816648043118, 0.007370631353871584, 0.15493926134976474, 0.006573798940500197, 0.010803137185438624], 'topk_tokens': ['<|start_header_id|>', '      ', '\n\n', ' dollars', ' Newspaper', '186', 'cery', '<|eot_id|>', 'half', ' $', 'men', '<|end_header_id|>', '�', ' enough', '<|eot_id|>', 'assistant', ' seldom', 'office', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.014744758605957031, 0.007503032684326172, 0.014744758605957031, 0.0034224987030029297, 0.005887667338053386]}, 'saliency': {'score': [0.0005603432655334473, 0.0001579769560191962, 0.003837561065500433, 0.00013597638454965162, 0.00021349821450575342], 'topk_tokens': ['ine', 'posit', ':\n\n', 'cuts', '.\n\n', '.', 'men', 'ed', 'ol', '�', ' apple', ' bedroom', ' seldom', '<|start_header_id|>', ' back', 'ting', 'office', '<|begin_of_text|>', 'system', '.'], 'evidence_proportions': [0.0006184677282969157, 0.0012167543172836304, 0.0006184677282969157, 0.00033801794052124023, 0.00015470385551452637]}}, 30: {'grad': {'score': [0.4284714918870192, 0.5110793257792053, 0.46061567826704547, 0.5118634422476684, 0.5295531074955778], 'topk_tokens': [' Republicans', 'await', ' lesser', ' bogus', '\n', ' ', 'iscal', '.', ' readiness', '\n', ' inaugur', '\n', 'estead', ' A', '\n', ' m', '\n', 'aths', ' *\n\n', ' back'], 'evidence_proportions': [0.4036661783854167, 0.2554931640625, 0.4036661783854167, 0.415191650390625, 0.6022542317708334]}, 'weight': {'score': [0.018121609321007363, 0.007260524287780863, 0.28159247745167126, 0.0057400209952490995, 0.006064945796750627], 'topk_tokens': [',', "'clock", '�', ' Jul', ':', '.\n\n', '<|start_header_id|>', ' seldom', ' PA', '<|end_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'UL', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'office', '<|begin_of_text|>', ' bedroom'], 'evidence_proportions': [0.028809229532877605, 0.018786191940307617, 0.028809229532877605, 0.000946044921875, 0.007753690083821614]}, 'saliency': {'score': [0.0008586324178255522, 0.00017944411553004626, 0.004552860151637684, 0.0001520376056253408, 0.00017381614109255233], 'topk_tokens': ['.\n\n', ' to', '�', ' to', '<|end_header_id|>', '<|eot_id|>', '***', '<|start_header_id|>', '.', '<|eot_id|>', '<|end_header_id|>', ' Jul', ' PA', 'UL', ' back', '<|eot_id|>', 'office', '<|begin_of_text|>', '<|start_header_id|>', ' bedroom'], 'evidence_proportions': [0.0015018284320831299, 0.0005308836698532104, 0.0015018284320831299, 1.2382864952087402e-05, 0.0003549059232076009]}}, 31: {'grad': {'score': [0.2102078657883864, 0.14966295175600017, 0.2461221001364968, 0.14877335632177324, 0.1295635295364092], 'topk_tokens': ['ant', 'old', '\n', ' boats', ' and', '\n', ' the', ' the', ' When', ' business', ' the', ' and', '\n', '26', '.', '<|start_header_id|>', 'user', ' ', ' ', 'ting'], 'evidence_proportions': [0.23521137237548828, 0.2712554931640625, 0.23521137237548828, 0.08706283569335938, 0.20159912109375]}, 'weight': {'score': [0.005854336115030142, 0.006247876004153934, 0.15823479132218796, 0.005445594391667886, 0.004744074254665735], 'topk_tokens': [' where', '186', ' the', ' else', ' it', ' to', '?\n', ' seldom', ' or', '<|start_header_id|>', ' dropped', 'Answer', ':', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ' bedroom', 'office'], 'evidence_proportions': [0.009824117024739584, 0.0032581090927124023, 0.009824117024739584, 0.00027373433113098145, 0.0033659934997558594]}, 'saliency': {'score': [9.93792827312763e-05, 0.00011277385986676648, 0.001732758500359275, 0.00010428014071266, 9.048322461686044e-05], 'topk_tokens': [' dollars', ' it', 'Bridge', ' or', 'user', '<|eot_id|>', '️', ' seldom', ' of', '<|start_header_id|>', 'Answer', 'assistant', ' dropped', 'system', '️', ' journey', '<|begin_of_text|>', ' bedroom', '<|eot_id|>', 'office'], 'evidence_proportions': [0.00016240278879801431, 0.0001262873411178589, 0.00016240278879801431, 3.427267074584961e-06, 1.9361575444539387e-05]}}, 'pred_res': 'Mary got the apple.<|eot_id|>', 'score': 0}
2025-01-23 22:16:59.037 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:16:59.037 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-1_pid-2_1-3-5-6-8.pkl | len: 10 |  size: 9.22 KB
Processing depth (1, 3, 5, 6, 8):   3%|▎         | 3/100 [00:49<26:35, 16.44s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.30s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
Processing depth (4, 5, 6, 7, 9):   3%|▎         | 3/100 [01:03<26:35, 16.44s/it]2025-01-23 22:17:12.475 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:17:12.484 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1830, 1836) --> . Mary journeyed to the
2025-01-23 22:17:12.484 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the apple.
2025-01-23 22:17:12.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2123, 2127) -->  Mary got the apple
2025-01-23 22:17:12.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:17:12.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1830, 1836) --> . Mary journeyed to the
2025-01-23 22:17:12.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:17:12.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2855, 2859) -->  Mary dropped the apple
2025-01-23 22:17:12.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-23 22:17:12.536 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3617, 3623) -->  Daniel went back to the kitchen
2025-01-23 22:17:12.536 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:17:12.546 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1835, 1841) -->  the office. John went back
2025-01-23 22:17:12.546 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the office.
2025-01-23 22:17:12.555 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1831, 1837) -->  Mary journeyed to the office
2025-01-23 22:17:12.555 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John took the milk.
2025-01-23 22:17:12.565 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2064, 2068) -->  John took the milk
2025-01-23 22:17:12.565 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:17:12.565 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (32, 38) -->  journeyed to the bedroom.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:17:13.187 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary got the apple.<|eot_id|>
2025-01-23 22:17:13.187 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4228])
your chose emoji: ['👨🏼\u200d🎤', '🧝🏻\u200d♂', '🕣', '🧑🏾\u200d✈️', '🙆🏽\u200d♂️', '🏃🏼\u200d♂️', '🧑🏽\u200d❤\u200d🧑🏼', '💛', '🌧️', '🏃\u200d♂️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4230, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 217885.92it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.39it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.61it/s]
2025-01-23 22:17:16.187 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7595097468449519, 0.5057382858673168, 1.0809437144886365, 0.5011346088192328, 0.5119829177856445], 'topk_tokens': [' agreement', 'ot', ' office', 'Min', ' evening', 'Min', ' office', 'ot', ' heading', 'Min', 'ot', 'ot', 'ot', 'ot', ' Min', ' offices', ' offices', ' office', ' offices', ' office'], 'evidence_proportions': [0.779541015625, 1.3194580078125, 0.779541015625, 0.56494140625, 0.475860595703125]}, 'weight': {'score': [0.1771246836735652, 0.007446531651995143, 0.36258443919095124, 0.004523369069877051, 0.0018978923559188842], 'topk_tokens': ['\n\n', '<|start_header_id|>', ' ', 'Bridge', '?\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'user', 'If', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'ENCES', 'assistant', 'office', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.38076988855997723, 0.0037724971771240234, 0.38076988855997723, 0.001800537109375, 0.002285162607828776]}, 'saliency': {'score': [0.0018631311563345103, 0.00017345193993678894, 0.0023435164581645618, 0.00015153106977242937, 7.146000862121583e-05], 'topk_tokens': ['Bridge', 'In', '<|begin_of_text|>', '<|end_header_id|>', '\n\n', ' Good', '<|eot_id|>', ' Grow', 'ENCES', ' news', '<|start_header_id|>', ' offices', 'assistant', ' ', '.', '<|eot_id|>', 'If', 'If', '.', 'If'], 'evidence_proportions': [0.0037781596183776855, 0.00024156272411346436, 0.0037781596183776855, 0.00012977421283721924, 0.00026969114939371747]}}, 25: {'grad': {'score': [0.43306908240685094, 0.4873648280511229, 0.47998046875, 0.48774123773342226, 0.3967131614685059], 'topk_tokens': [' legislative', ' tele', 'SP', '.', 'es', ' res', '185', ' the', 'icians', ' tele', ' tele', 'RE', ' the', '185', ' tele', '185', ' occurred', '185', ' the', '185'], 'evidence_proportions': [0.5483296712239584, 0.404083251953125, 0.5483296712239584, 0.19229888916015625, 0.38238525390625]}, 'weight': {'score': [0.2925039713199322, 0.007451252678043735, 0.4781181812286377, 0.0032030357691957756, 0.0016938813030719757], 'topk_tokens': ['Answer', '\n\n', '\n\n\n\n\n\n\n', '.\n\n', ' obtained', 'Just', 'If', '\n\n', 'user', '<|eot_id|>', '<|eot_id|>', '\n\n', '.\n\n', 'ENCES', 'office', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.6300180753072103, 0.005826115608215332, 0.6300180753072103, 0.0019731521606445312, 0.0022815465927124023]}, 'saliency': {'score': [0.0008510511655073899, 0.00014178723308211523, 0.00039926442232998937, 0.0001360231583794585, 7.396340370178223e-05], 'topk_tokens': ['ISC', '<|eot_id|>', 'If', ' presidents', ' Square', 'Times', '\n\n', ' apple', ' Paul', 'tele', ' Douglas', 'If', '.', 'Answer', ' obtained', '.', 'S', 'assistant', 'If', '<|end_header_id|>'], 'evidence_proportions': [0.0016644199689229329, 0.00026476383209228516, 0.0016644199689229329, 0.00015912950038909912, 7.645289103190105e-05]}}, 26: {'grad': {'score': [0.5558448204627404, 0.4000492032819888, 0.5065813931551847, 0.398520175490523, 0.3706220626831055], 'topk_tokens': ['600', ' manager', ' as', ' were', 'ly', 'u', ' a', ' and', ' and', 'ian', ' newspaper', ' and', 'ian', ' hand', ' generally', ' season', ' summer', ' and', 'ian', 'Dub'], 'evidence_proportions': [0.6903889973958334, 0.3606414794921875, 0.6903889973958334, 0.178985595703125, 0.6681315104166666]}, 'weight': {'score': [0.22745348856999323, 0.007367545324014434, 0.3414198485287753, 0.00424191519610908, 0.004868179559707642], 'topk_tokens': ['<|end_header_id|>', 'nes', '<|start_header_id|>', '<|eot_id|>', ' *\n\n', '.\n', '\n\n\n', '\n\n', '<|end_header_id|>', 'assistant', 'user', 'ENCES', 'office', '<|eot_id|>', '\n\n', '\n\n', '.\n\n', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.48657433191935223, 0.003889322280883789, 0.48657433191935223, 0.002412080764770508, 0.008282184600830078]}, 'saliency': {'score': [0.005811771521201501, 0.00017010255344652398, 0.00470827113498341, 0.00011115393987516077, 0.000174809992313385], 'topk_tokens': ['Just', ' company', '.\n\n', '<|end_header_id|>', ',', '<|start_header_id|>', '\n\n', ' Marshall', 'If', '      ', 'Bridge', '\n\n', '<|begin_of_text|>', ' instance', 'ENCES', 'user', '\n\n', '<|eot_id|>', '.', '.'], 'evidence_proportions': [0.012317975362141928, 5.854666233062744e-05, 0.012317975362141928, 0.00011403858661651611, 0.0004333357016245524]}}, 27: {'grad': {'score': [0.5081575833834134, 0.4349649776521868, 0.4460053877397017, 0.43445185073421994, 0.3906217575073242], 'topk_tokens': [' of', 'irie', ' paper', ' Wood', ' news', 'of', ' of', '600', ' offices', 'ANK', 'Bridge', ' offices', ' departing', ' prize', ' four', ' expedition', ' offices', ' decided', 'que', 'que'], 'evidence_proportions': [0.44775390625, 0.674041748046875, 0.44775390625, 0.5016021728515625, 0.5227457682291666]}, 'weight': {'score': [0.42684224935678333, 0.007454892873200401, 0.3938716541637074, 0.0028147111379148173, 0.0029660895466804504], 'topk_tokens': [' ', 'Just', '\n\n', '\n\n', 'user', 'If', ' *\n\n', '\n\n', '<|eot_id|>', 'If', '<|eot_id|>', 'ENCES', 'If', '.\n\n', '<|end_header_id|>', 'assistant', 'office', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.9186523755391439, 0.004950761795043945, 0.9186523755391439, 0.0037543773651123047, 0.0065415700276692705]}, 'saliency': {'score': [0.007616182932486901, 0.0002160114317639218, 0.003137837756763805, 0.00015463299126444908, 0.00015807747840881347], 'topk_tokens': ['paper', 'tele', 'ioneer', ' Daniel', ' Joseph', ' *\n\n', 'If', '\n', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'RE', ' *\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '.\n\n', '.', '.'], 'evidence_proportions': [0.015829841295878094, 0.00032594799995422363, 0.015829841295878094, 0.00019153952598571777, 0.000998785098393758]}}, 28: {'grad': {'score': [0.36141146146334135, 0.45383693253176716, 0.35674285888671875, 0.45492232991769976, 0.4973456382751465], 'topk_tokens': [' would', ' attention', ' scale', ' be', ' offices', ' *\n\n', 'S', ' he', 'ye', ' laid', '\n\n', '\n', ' as', ' OF', ' and', ' and', ' He', ' He', 'LES', 'up'], 'evidence_proportions': [0.21118927001953125, 0.5504150390625, 0.21118927001953125, 0.537109375, 0.4187215169270833]}, 'weight': {'score': [0.4864071515890268, 0.0071909771461577, 0.4205371249805797, 0.002037166579952085, 0.001990342140197754], 'topk_tokens': [',', ' *\n\n', '.', '\n\n', 'Bridge', 'ern', '.\n', '?\n', 'If', ' return', 'ENCES', 'Just', '.\n\n', '<|eot_id|>', 'office', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [1.0517799059549968, 0.0016914606094360352, 1.0517799059549968, 0.0015041828155517578, 0.002074082692464193]}, 'saliency': {'score': [0.004673375533177302, 0.00013821203939739976, 0.0015101866288618608, 0.00010279891366653041, 0.00012274906039237975], 'topk_tokens': [' As', ',', 'If', ' return', ' *\n\n', ':', '�', ' *\n\n', 'Question', ' Square', 'Just', '.', '.\n\n', '<|eot_id|>', 'ern', 'office', 'Bridge', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.009991119305292765, 8.815526962280273e-05, 0.009991119305292765, 7.84844160079956e-05, 0.0001579622427622477]}}, 29: {'grad': {'score': [1.1792837289663463, 0.8716936548832742, 1.4897128018465908, 0.8665301534104496, 0.5478044509887695], 'topk_tokens': [' the', ' no', '600', ' leading', ' a', ' mailing', '\n', ' six', ';', ' five', ' company', ' senate', ' printer', ' business', ' press', '\n', '\n', ' press', ' two', ' States'], 'evidence_proportions': [1.400390625, 0.964599609375, 1.400390625, 0.9781494140625, 1.0142822265625]}, 'weight': {'score': [0.5289793564723089, 0.007445438195627632, 0.5326314080845226, 0.0014401839601543176, 0.001304510235786438], 'topk_tokens': ['Question', '\n\n', ',', ' ', '<|start_header_id|>', ',', '\n\n', '<|eot_id|>', '<|eot_id|>', '.', '.\n\n', ',', ' ', 'If', 'office', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [1.1439395745595295, 0.001271963119506836, 1.1439395745595295, 0.0012663602828979492, 0.0026725133260091147]}, 'saliency': {'score': [0.017260721096625693, 0.0002545927715076059, 0.00902552767233415, 0.00010272287569908022, 7.175803184509278e-05], 'topk_tokens': ['the', '.\n\n', 'E', ',', '<|start_header_id|>', ' ', ' another', '<|start_header_id|>', '<|end_header_id|>', ' The', 'If', ' ', 'office', 'The', 'The', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.037174353996912636, 4.1931867599487305e-05, 0.037174353996912636, 8.07046890258789e-05, 0.00036599238713582355]}}, 30: {'grad': {'score': [0.6508953387920673, 0.8217768820183216, 0.8201169100674716, 0.8228480057645676, 0.9797927856445312], 'topk_tokens': [' trembling', ' and', ' Published', ' It', ' Collection', ' compelled', ' fifteen', ' offices', ' offices', ' O', ' o', ' office', ' o', ' office', ' o', ' offices', ' offices', ' office', ' office', ' office'], 'evidence_proportions': [0.41217041015625, 0.99609375, 0.41217041015625, 1.0693359375, 0.6192525227864584]}, 'weight': {'score': [0.1578813332777757, 0.007160214273078503, 0.328448317267678, 0.00453298152319727, 0.006074100732803345], 'topk_tokens': [',', 'Answer', '\n\n', '<|start_header_id|>', '.\n', '\n', 'If', 'Just', ' *\n\n', '.\n', '<|eot_id|>', '<|eot_id|>', 'ANK', '.\n\n', '<|end_header_id|>', 'assistant', 'office', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.3364906311035156, 0.004150271415710449, 0.3364906311035156, 0.0026482343673706055, 0.006638844807942708]}, 'saliency': {'score': [0.0010746419429779053, 0.0002684355909379098, 0.001021707599813288, 0.00025946061500574857, 0.0002722904086112976], 'topk_tokens': ['<|eot_id|>', ' ', ' *\n\n', '.', '.', ' James', '.\n\n', ' offices', '.\n', 'paper', 'If', ' *\n\n', '.', 'ford', '.\n', '\n', '<|start_header_id|>', 'ANK', '<|begin_of_text|>', 'assistant'], 'evidence_proportions': [0.0016325116157531738, 0.00028187036514282227, 0.0016325116157531738, 0.0006006807088851929, 0.0008033911387125651]}}, 31: {'grad': {'score': [0.39308929443359375, 0.5236345439937943, 0.4407196044921875, 0.524882345322728, 0.35367813110351565], 'topk_tokens': [' fifty', 'S', ' almost', ' an', ' N', ' o', ' he', 'ISC', ' DAYS', 'f', 'ot', ' were', ' L', ' EVENTS', ' du', 'ot', ' Ch', ' DAYS', ' em', 'G'], 'evidence_proportions': [0.4629720052083333, 0.337371826171875, 0.4629720052083333, 0.243194580078125, 0.39039866129557294]}, 'weight': {'score': [0.06854721216055062, 0.007148386907915697, 0.20602848313071512, 0.005720426225593819, 0.003280015289783478], 'topk_tokens': ['.', '\n', 'If', ' ', 'If', 'Just', '\n', '<|start_header_id|>', '?\n', '.\n\n', 'Answer', '.\n', 'If', '<|eot_id|>', '<|end_header_id|>', '.', 'assistant', 'office', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.14484850565592447, 0.0030460357666015625, 0.14484850565592447, 0.0022420883178710938, 0.0038154919942220054]}, 'saliency': {'score': [0.0017471978297600378, 0.0003552022273377042, 0.0033341266892173075, 0.0003308769705647889, 0.0002465873956680298], 'topk_tokens': ['Bridge', ' location', ' write', '?\n', ' not', ' item', ' return', 'If', '<|eot_id|>', 'If', '<|begin_of_text|>', '.\n', '.', '<|start_header_id|>', 'If', 'assistant', 'Answer', '.', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.003634283939997355, 0.00010451674461364746, 0.003634283939997355, 8.997321128845215e-05, 0.00017296274503072104]}}, 'pred_res': 'Mary got the apple.<|eot_id|>', 'score': 0}
2025-01-23 22:17:16.195 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:17:16.195 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-1_pid-3_4-5-6-7-9.pkl | len: 10 |  size: 9.21 KB
Processing depth (4, 5, 6, 7, 9):   4%|▍         | 4/100 [01:06<26:45, 16.73s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Processing depth (0, 3, 4, 6, 9):   4%|▍         | 4/100 [01:18<26:45, 16.73s/it]2025-01-23 22:17:27.959 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:17:27.960 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:17:27.960 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the apple.
2025-01-23 22:17:27.967 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1434, 1438) -->  Mary got the apple
2025-01-23 22:17:27.967 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:17:27.976 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1785, 1791) --> . Mary journeyed to the
2025-01-23 22:17:27.976 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:17:27.988 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2351, 2355) -->  Mary dropped the apple
2025-01-23 22:17:27.988 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-23 22:17:28.006 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3595, 3601) -->  Daniel went back to the kitchen
2025-01-23 22:17:28.006 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:17:28.015 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1790, 1796) -->  the bathroom. John went back
2025-01-23 22:17:28.015 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the office.
2025-01-23 22:17:28.016 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:17:28.016 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John took the milk.
2025-01-23 22:17:28.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2020, 2024) -->  John took the milk
2025-01-23 22:17:28.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:17:28.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (32, 38) --> ed to the office. Sandra
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:17:28.509 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:17:28.509 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4216])
your chose emoji: ['🇬🇮', '🥥', '✋🏼', '🇨🇫', '🧑\u200d🎤', '🐥', '🔍', '🏋🏽\u200d♀️', '🧎🏽\u200d♂️', '👩🏽\u200d🤝\u200d👩🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4218, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 199728.76it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.84it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.65it/s]
2025-01-23 22:17:31.415 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.9894373967097356, 0.4396433787302928, 1.2447509765625, 0.4319678363754309, 0.29359889375990716], 'topk_tokens': [' party', ' both', ' OF', ' dropped', ' the', '202', ' small', ' the', ' now', ' back', ' to', ' profit', ' much', ' soon', ' Sandra', ' Knowledge', 'Mary', ' city', ' Mary', ' office'], 'evidence_proportions': [1.2569580078125, 1.49151611328125, 0.870849609375, 0.7836990356445312, 0.6429443359375]}, 'weight': {'score': [0.00895020136466393, 0.007572671484189782, 0.01835616339336742, 0.0075071912446468, 0.006585898606673531], 'topk_tokens': ['ye', ' a', ' anything', ' laying', ' to', ' several', 'S', ' between', 'Answer', ' others', '0', 'enan', ' apple', '<|end_header_id|>', 'ayers', ' discarded', '.', 'assistant', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.03206029534339905, 0.0013015270233154297, 0.002240806818008423, 0.0028706789016723633, 0.001701633135477702]}, 'saliency': {'score': [0.0005395435369931734, 0.00031279777112646994, 0.0007537955587560481, 0.00030905740152445914, 0.00020877347476240519], 'topk_tokens': [' return', ' anything', 'Just', ' the', ' location', ' the', '<|eot_id|>', '<|eot_id|>', '.', ' dropped', ' Where', ' prior', ' was', ' apple', ' discarded', '<|end_header_id|>', ':', 'Answer', 'assistant', 'office'], 'evidence_proportions': [0.0009762843449910482, 0.00025109946727752686, 0.00043050448099772137, 0.0002709627151489258, 0.000583191712697347]}}, 25: {'grad': {'score': [0.6114912766676682, 0.47612318578858465, 0.6604905561967329, 0.47430648254833635, 0.43522942584493884], 'topk_tokens': [' stake', '.', ' journey', 'ed', '<|start_header_id|>', ':', ' "', '\n\n', '.', ',', '<|eot_id|>', ',', '.', '.', "'clock", ',', '�', '.', '.', '<|start_header_id|>'], 'evidence_proportions': [0.8080240885416666, 0.594207763671875, 0.879150390625, 0.35935211181640625, 0.3269144694010417]}, 'weight': {'score': [0.011139596884067241, 0.007578623243958648, 0.024232303554361515, 0.007468559387490618, 0.007754424343938413], 'topk_tokens': ['ern', ' between', 'ian', 'enan', '�', ' others', ' discarded', ' laying', '�', ' large', '0', ' a', ' injunction', ' the', 'ayers', '.', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', 'office'], 'evidence_proportions': [0.043376813332239784, 0.002436041831970215, 0.0010986725489298503, 0.0020003318786621094, 0.0008385181427001953]}, 'saliency': {'score': [0.00039939696972186747, 0.00018963364697904483, 0.0006665966727516868, 0.0001858094184518718, 0.00020944813023442808], 'topk_tokens': [' a', ' apple', ' anything', 'Min', '.', 'ched', '.', ' ', ' discarded', '�', ' had', '�', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', ' injunction', '.', 'assistant', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.0010719796021779378, 0.0006105154752731323, 8.821487426757812e-05, 0.0001758486032485962, 4.628300666809082e-05]}}, 26: {'grad': {'score': [1.5184420072115385, 1.373318682580607, 1.3760653409090908, 1.3723993461480815, 1.3238330785778984], 'topk_tokens': [',\n', '\n', '\n', ' When', '\n', '\n', '\n', '\n', ',', '\n', ' it', '\n', '\n', ' Mary', '\n', ' could', '\n', '\n', '\n', '\n'], 'evidence_proportions': [1.0208333333333333, 2.016357421875, 1.509765625, 1.81298828125, 1.4964192708333333]}, 'weight': {'score': [0.010204957081721379, 0.0075720519802478774, 0.01930048248984597, 0.007493759173569347, 0.007339688314907793], 'topk_tokens': [' Do', ' apple', ' between', '<|start_header_id|>', ' a', 'enan', ' several', ' directly', 'ye', '0', ' others', ' discarded', 'ayers', 'office', 'assistant', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', '.', '<|end_header_id|>'], 'evidence_proportions': [0.031555612881978355, 0.003539562225341797, 0.0036644538243611655, 0.003700733184814453, 0.004174550374348958]}, 'saliency': {'score': [0.0003035893807044396, 0.00022733145101537294, 0.0004186792807145552, 0.00022584647297573318, 0.00027414249337237817], 'topk_tokens': ["'clock", '***', ' efforts', 'ed', ' bathroom', ' of', '<|eot_id|>', '.', ' on', '<|end_header_id|>', '.', ' on', ' stake', ' rival', ',', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>'], 'evidence_proportions': [0.0002435147762298584, 0.0004356503486633301, 0.0004888474941253662, 0.00012755393981933594, 0.00020772218704223633]}}, 27: {'grad': {'score': [0.7725483820988581, 0.575293188696509, 0.8821196122602983, 0.5724445516828724, 0.8272487253382586], 'topk_tokens': ['.', 'ed', '\n', '\n', 'ENCES', 'out', ' stake', ' had', "'s", ' PA', 'APER', '<|end_header_id|>', 'Mary', 'D', '.', ' Mary', "'clock", '.', ',', ' In'], 'evidence_proportions': [1.0089925130208333, 1.3321533203125, 0.35920461018880206, 0.6358642578125, 0.6675008138020834]}, 'weight': {'score': [0.017452239990234375, 0.00757919752863472, 0.038929874246770683, 0.007352239736836019, 0.0076408558997555056], 'topk_tokens': [' of', ' a', ' of', ' discarded', 'S', ' hopes', ' way', ' there', 'were', ' several', 'ye', ' between', '0', 'enan', ' others', 'ayers', 'assistant', '<|begin_of_text|>', 'office', '.'], 'evidence_proportions': [0.07034651438395183, 0.002199888229370117, 0.0006308158238728842, 0.0016506314277648926, 0.0020820299784342446]}, 'saliency': {'score': [0.0008215514513162466, 0.0004660423585395216, 0.0011078390208157625, 0.00046043977748861704, 0.00035385204398113746], 'topk_tokens': [' kids', ' his', 'UL', ' of', '�', ' and', ' Republicans', '<|end_header_id|>', ' and', ' extraordinary', ' and', ' Among', 'D', ' PA', '<|end_header_id|>', '.', 'ed', 'assistant', '<|start_header_id|>', 'office'], 'evidence_proportions': [0.0015804171562194824, 0.0013151168823242188, 0.00011754035949707031, 5.379319190979004e-05, 0.0009494920571645101]}}, 28: {'grad': {'score': [1.6919673039362981, 1.5102702285961949, 1.7323386452414773, 1.5079657611801185, 1.5223547894021738], 'topk_tokens': [' ', 'APER', 'ting', ',', ' Gutenberg', '\n', ' Sandra', 'ed', ' A', '\n\n\n', '\n', 'A', ' bedroom', '\n\n', ' A', ' journey', '\n\n\n\n', ' the', 'Cut', ' PA'], 'evidence_proportions': [1.7560526529947917, 1.73583984375, 1.3119710286458333, 1.74169921875, 1.9454752604166667]}, 'weight': {'score': [0.02316885728102464, 0.007538964139052961, 0.05368825251405889, 0.007198038104055025, 0.008991887603980907], 'topk_tokens': [' the', ' way', 'ian', 'enan', 'were', ' a', ' laying', 'ot', '0', ' others', ' between', ' injunction', '�', 'ian', 'ayers', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', 'office', '.'], 'evidence_proportions': [0.09797112147013347, 0.0009274482727050781, 0.000578761100769043, 0.0007745623588562012, 0.0007138252258300781]}, 'saliency': {'score': [0.00039237049909738393, 0.00017303039022620225, 0.0007960417053916237, 0.00016837593176953798, 0.0002819541571796804], 'topk_tokens': ['.', ' very', '.', 'an', ' location', 'ayers', ' a', ' large', ' the', ' the', 'ian', '<|begin_of_text|>', ' injunction', '�', '<|eot_id|>', '.', '�', 'office', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [0.0013971328735351562, 6.064772605895996e-05, 8.407235145568848e-05, 0.00013928115367889404, 8.578101793924968e-05]}}, 29: {'grad': {'score': [1.5299588716947115, 1.332969842786866, 1.5287420099431819, 1.3307087630676708, 1.4552771526834238], 'topk_tokens': [' o', ' and', 'SP', '30', '�', 'ot', ' engaged', 'out', ' all', 'outs', ' very', 'antics', ' and', 'ed', ' of', ' and', ' announcement', ' and', ' printed', '�'], 'evidence_proportions': [1.2724202473958333, 1.7637939453125, 1.0154012044270833, 1.00433349609375, 2.49658203125]}, 'weight': {'score': [0.04870168520854069, 0.007557895184693714, 0.11350637132471258, 0.006742402375173226, 0.007943721785061602], 'topk_tokens': ['ye', 'ot', ' the', '<|end_header_id|>', ' others', 'ian', ' laying', '0', ' way', ' to', ' between', ' which', ' hopes', 'enan', 'were', 'ayers', 'assistant', 'office', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.20764029026031494, 0.0013260841369628906, 0.0009715755780537924, 0.0006990432739257812, 0.0010786851247151692]}, 'saliency': {'score': [0.0009124943843254676, 0.00026067493650346865, 0.001903964714570479, 0.0002479412001099804, 0.00019280547681062117], 'topk_tokens': ['\n\n\n', ' PA', ' Min', ' incorporated', "'s", 'ed', '\n\n\n\n', '-text', ' compos', ' bedroom', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', 'office', '<|start_header_id|>', 'assistant', '.', '.', '<|eot_id|>', '***'], 'evidence_proportions': [0.0032982329527537027, 0.00025798380374908447, 0.00027614831924438477, 6.966292858123779e-05, 0.00016132990519205728]}}, 30: {'grad': {'score': [1.3812596247746394, 0.8577273691248815, 1.4442665793678977, 0.851368687764632, 0.9344290581302367], 'topk_tokens': ['\n', '<|end_header_id|>', '️', '\u200d', ' PA', ' PA', ' Sandra', ' to', ' to', ' OF', '<|start_header_id|>', ' to', ' the', '<|end_header_id|>', ' the', '<|start_header_id|>', '<|eot_id|>', ' apple', ' dropped', ' office'], 'evidence_proportions': [1.83544921875, 1.4184341430664062, 1.2371826171875, 1.5968017578125, 0.9026692708333334]}, 'weight': {'score': [0.02709321792309101, 0.007399351916758582, 0.05022290619936856, 0.007050632801559046, 0.006778461345727893], 'topk_tokens': ['ian', ' several', ' there', ' hopes', 'ye', 'were', ' way', 'enan', ' between', '<|start_header_id|>', ' others', '0', '<|eot_id|>', 'ayers', 'assistant', '<|begin_of_text|>', 'office', '.', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.08244931697845459, 0.018848419189453125, 0.010927200317382812, 0.0045746564865112305, 0.008412043253580729]}, 'saliency': {'score': [0.0020223512099339412, 0.00035777299423362475, 0.0035490474917671895, 0.0003305578689209277, 0.0003409826237222423], 'topk_tokens': ['�', 'ern', 'ayers', ' OF', ' both', 'NEW', ' now', ' *', 'office', '<|end_header_id|>', ' office', ' to', ' back', '<|eot_id|>', '<|eot_id|>', '.', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.004452764987945557, 0.0020729899406433105, 0.0017408231894175212, 0.0011839866638183594, 0.0003986159960428874]}}, 31: {'grad': {'score': [1.074572049654447, 1.2612334785443338, 0.9009538130326704, 1.2642980708206872, 1.289649852807971], 'topk_tokens': [' obtained', '.', '.', ' tele', '<|eot_id|>', ' printed', ' block', ' contract', '<|eot_id|>', ' the', ',', ' temper', ',', ' announcement', 'ed', ' passed', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [0.4868621826171875, 1.4276123046875, 0.8329264322916666, 0.940093994140625, 1.7582194010416667]}, 'weight': {'score': [0.01634524418757512, 0.006940742644612514, 0.025869369506835938, 0.006782242205503175, 0.004797486291415449], 'topk_tokens': ['men', 'enan', 'were', ' several', ' between', '<|eot_id|>', ' large', ' a', '.', 'ye', ' way', ' there', ' others', '0', 'assistant', 'ayers', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.03950786590576172, 0.011174201965332031, 0.009681065877278646, 0.014552593231201172, 0.0044892628987630205]}, 'saliency': {'score': [0.0013615626555222732, 0.0003619775663568489, 0.0028059211644259367, 0.00034285143410845055, 0.0002878239189369091], 'topk_tokens': [' a', '<|start_header_id|>', '.\n', ' there', ' house', ' of', ' way', ' lesser', 'office', ' Gree', '0', ' of', 'ayers', ' back', '<|eot_id|>', ' office', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [0.0038003623485565186, 0.0009109377861022949, 0.0007162988185882568, 0.0009710937738418579, 0.0001287559668223063]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-23 22:17:31.422 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:17:31.423 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-1_pid-4_0-3-4-6-9.pkl | len: 10 |  size: 9.41 KB
Processing depth (0, 3, 4, 6, 9):   5%|▌         | 5/100 [01:22<25:37, 16.19s/it]Processing depth (0, 3, 4, 6, 9):   5%|▌         | 5/100 [01:22<26:02, 16.44s/it]
2025-01-23 22:17:31.631 | INFO     | __main__:<module>:99 - Selected idx: 2
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:101 - Answer: office
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:103 - Needle: [' Daniel went back to the kitchen.', ' John went back to the bedroom.', ' Mary took the football.', ' John journeyed to the office.', ' John moved to the bedroom.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John took the milk.', ' Mary journeyed to the bathroom.', ' Daniel went back to the hallway.']
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:104 - Real Needle: [' Mary took the football.', ' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Daniel went back to the hallway.']
2025-01-23 22:17:31.632 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Processing depth (2, 6, 8, 9):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-23 22:17:42.400 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:17:42.405 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1022, 1026) -->  Mary took the football
2025-01-23 22:17:42.405 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:17:42.415 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1900, 1906) -->  John journeyed to the office
2025-01-23 22:17:42.415 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:17:42.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2513, 2519) --> . Mary journeyed to the
2025-01-23 22:17:42.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:17:42.432 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (865, 871) --> . Daniel went back to the
2025-01-23 22:17:42.432 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-23 22:17:42.437 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (865, 871) --> . Daniel went back to the
2025-01-23 22:17:42.437 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:17:42.447 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2054, 2060) --> . John went back to the
2025-01-23 22:17:42.447 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:17:42.457 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1899, 1905) --> . John journeyed to the
2025-01-23 22:17:42.457 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:17:42.473 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3278, 3283) --> . John moved to the
2025-01-23 22:17:42.473 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-23 22:17:42.489 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3252, 3258) --> . Sandra journeyed to the
2025-01-23 22:17:42.490 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:17:42.497 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1543, 1547) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:17:43.112 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football.<|eot_id|>
2025-01-23 22:17:43.113 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4232])
your chose emoji: ['🧄', '🕵🏿\u200d♂', '👨🏾\u200d❤️\u200d💋\u200d👨🏿', '🧑🏾\u200d✈', '🙋🏼\u200d♂', '🏌🏾\u200d♂', '🙅🏻', '🚶\u200d♀️\u200d➡', '\U0001fa8f', '👨🏿\u200d🦲']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4234, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.87it/s]
2025-01-23 22:17:46.084 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [2.053469571200284, 2.2316994863013697, 1.924701112689394, 2.2350620142892663, 2.638261307117551], 'topk_tokens': [',', ' room', ' EVENTS', 'Min', ' offices', ' rebellion', ' patent', 'ot', ' offices', ' Min', ' time', '.', 'Min', 'Min', 'ot', ',', ' offices', ' Min', ' office', ' office'], 'evidence_proportions': [1.7293853759765625, 1.7793782552083333, 1.8081868489583333, 2.7888997395833335]}, 'weight': {'score': [0.0004436671733856201, 0.007469373657053577, 0.00033759709560509884, 0.007562676880113757, 0.00032347024873245593], 'topk_tokens': ['system', 'Today', ' ', 'Good', '\n', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0020697414875030518, 4.5468409856160484e-05, 0.0001462399959564209, 5.5243571599324547e-05]}, 'saliency': {'score': [6.275556304238059e-05, 0.0003562876791902235, 4.3316320939497515e-05, 0.00036030437250860744, 6.888910781505496e-05], 'topk_tokens': ['NEW', '4', ':', '<|end_header_id|>', 'Good', 'system', '<|begin_of_text|>', ' \n', '***', 'Today', 'user', '<|start_header_id|>', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', '\n\n', 'office', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.00028505921363830566, 1.647075017293294e-05, 1.9202629725138347e-05, 4.390875498453776e-06]}}, 25: {'grad': {'score': [2.5655517578125, 3.5845660685817196, 3.299905487985322, 3.5921784433117034, 4.599177870639535], 'topk_tokens': [' This', '\n', ' newspaper', ' proceedings', ' receiving', ' money', ' newspaper', ' state', ',', ' discover', ' newspaper', ' edition', ' Republican', 'pay', ' courts', 'otyping', ' tele', ' pay', 'state', ' await'], 'evidence_proportions': [2.5555419921875, 1.5195719401041667, 3.3606770833333335, 2.8230794270833335]}, 'weight': {'score': [0.0006840608336708763, 0.007431919979046807, 0.00038565469510627514, 0.007523085247189499, 0.0002607384393381518], 'topk_tokens': [' president', 'Good', 'Cut', 'UL', ' ', '<|end_header_id|>', ' about', '<|start_header_id|>', '\n\n', '\n\n', 'user', '<|eot_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'office', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003338843584060669, 5.188584327697754e-05, 6.278355916341145e-05, 0.00016765793164571127]}, 'saliency': {'score': [0.0001450126821344549, 0.0005786795068812021, 8.690537828387636e-05, 0.0005848458663902091, 4.220771235088969e-05], 'topk_tokens': [' members', '<|start_header_id|>', 'and', 'Min', ' president', ' ', '<|eot_id|>', 'Cut', 'user', 'MIN', '\n\n', 'PA', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', 'assistant', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.0007483363151550293, 4.867712656656901e-06, 1.0778506596883139e-05, 1.7176071802775066e-05]}}, 26: {'grad': {'score': [1.362640380859375, 1.2290500357965282, 1.746219519412879, 1.2242628664855153, 1.7814650424691134], 'topk_tokens': ['185', ',', 'nes', ' per', ' the', ' ', ' the', '.', ' the', '185', '<|start_header_id|>', 'right', ' Think', '.', 'nes', ' decided', '.', ' by', 'vent', 'nes'], 'evidence_proportions': [0.506561279296875, 1.7670084635416667, 1.0524393717447917, 1.8391927083333333]}, 'weight': {'score': [0.0005195791071111506, 0.0073530127769041356, 0.000979526476426558, 0.007439315861051349, 0.0008207919985749], 'topk_tokens': ['Good', ' about', ' adher', '\n', '\n\n', '<|start_header_id|>', ' *\n\n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'user', 'assistant', '\n\n', '<|eot_id|>', 'office', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0019981563091278076, 8.077422777811687e-05, 0.0003630717595418294, 0.00012917319933573404]}, 'saliency': {'score': [0.00011584704572504216, 0.0002459537661970929, 0.00017369696588227242, 0.0002472092871975174, 9.476584057475245e-05], 'topk_tokens': ['ed', '\n\n', 'system', '3', 'assistant', '<|start_header_id|>', ' ', '\n\n', '<|start_header_id|>', '\n', '<|end_header_id|>', ' about', '<|eot_id|>', ' adher', 'office', '<|eot_id|>', '\n\n', 'user', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005171597003936768, 3.556410471598307e-06, 5.693236986796061e-05, 1.9510587056477863e-05]}}, 27: {'grad': {'score': [1.4351529208096592, 2.196996689743741, 1.8607640121922349, 2.203662456978905, 3.636662150538245], 'topk_tokens': ['�', 'the', ';', ' will', 'es', '�', ' now', 'antics', ' him', ' work', 'RE', ' Associated', ' day', ' scale', ',', ' connected', '<|start_header_id|>', ',', ' day', ' days'], 'evidence_proportions': [1.2171630859375, 1.4617919921875, 2.0704549153645835, 0.9185384114583334]}, 'weight': {'score': [0.0005103891546075994, 0.007315912728721824, 0.0007556767174691865, 0.007403543575096997, 0.0006117196970207747], 'topk_tokens': ['\n\n\n', '<|start_header_id|>', 'PA', 'RE', 'NEW', '<|start_header_id|>', '<|eot_id|>', 'Good', ' *\n\n', '\n\n', '<|eot_id|>', '\n\n', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0019885897636413574, 0.00010780493418375651, 0.0002247293790181478, 0.00021316607793172201]}, 'saliency': {'score': [0.00023911486972462046, 0.0006312703433102125, 0.00013586246606075403, 0.0006372468640970311, 0.00018786205801852915], 'topk_tokens': ['UL', '"The', '<|end_header_id|>', 'The', ' *\n\n', 'NEW', '\n\n', '\n\n', '<|start_header_id|>', ' *\n\n', 'RE', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', 'Good', '<|eot_id|>', 'assistant', 'user', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.001108616590499878, 2.574920654296875e-05, 4.47233517964681e-05, 6.720423698425293e-05]}}, 28: {'grad': {'score': [4.358309659090909, 3.8475185533036136, 4.221294981060606, 3.8418779631042117, 4.005439226017442], 'topk_tokens': [' thought', 'ors', ' court', ' convention', ' balance', ' connected', ' getting', ' contract', ' ranks', ' more', ' hour', ' hour', ' competition', ' one', ' thought', 'iture', ' hour', ' him', ' conducted', 'action'], 'evidence_proportions': [3.99658203125, 4.688151041666667, 3.8626302083333335, 4.765299479166667]}, 'weight': {'score': [0.0005120337009429932, 0.006789480451710898, 0.0013601400635459206, 0.006865401021542518, 0.0009054555449374887], 'topk_tokens': ['E', '.\n', '\n', 'Today', '<|start_header_id|>', '\n', '\n\n\n', 'user', '\n\n', '<|eot_id|>', '\n\n', '<|eot_id|>', ' *\n\n', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001479610800743103, 7.298588752746582e-05, 0.000564277172088623, 0.0002537866433461507]}, 'saliency': {'score': [5.721774968233976e-05, 0.0004710877454106291, 0.00012326601779822147, 0.000476013147879457, 6.874910620755927e-05], 'topk_tokens': [' Jul', '<|start_header_id|>', ',', ' *\n\n', ' December', 'E', '\n\n\n\n', '\n\n\n', '\n\n', '<|end_header_id|>', ':', '<|eot_id|>', ',', '\n\n', '\n', 'office', '<|eot_id|>', '\n\n', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0001325756311416626, 5.9405962626139326e-06, 6.0439109802246094e-05, 5.503495534261068e-05]}}, 29: {'grad': {'score': [3.3287908380681817, 4.127994730160605, 3.7986505681818183, 4.134802780644293, 5.213481104651163], 'topk_tokens': [' through', 'PA', ' had', 'un', 'far', '�', ' containing', ' boys', ' July', 'en', ' o', ' press', ' in', 'to', ' book', 'RI', 'in', ' in', ' papers', ' paper'], 'evidence_proportions': [2.7301025390625, 2.97607421875, 3.3111165364583335, 4.098307291666667]}, 'weight': {'score': [0.0006378482688557018, 0.0071491814470538905, 0.000838288755127878, 0.007233294581476939, 0.0007520775462305823], 'topk_tokens': ['ern', ' Knowledge', 'E', '\n\n', ' the', '<|eot_id|>', ' *\n\n', '\n\n', 'user', '<|start_header_id|>', '\n\n\n', '<|eot_id|>', '\n\n', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0019040107727050781, 0.00011785825093587239, 0.0008823474248250326, 6.923079490661621e-05]}, 'saliency': {'score': [8.840452541004528e-05, 0.0008885804695738449, 9.227702111908884e-05, 0.0008990810401817918, 0.0002100890459016312], 'topk_tokens': ['NEW', '-text', 'PA', 'ting', 'APER', ',', ' *\n\n', ' ', ' Knowledge', '\n\n', '<|end_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002513378858566284, 1.043081283569336e-05, 0.0001318057378133138, 1.4354785283406576e-05]}}, 30: {'grad': {'score': [2.657259854403409, 3.1623349484825223, 2.932232481060606, 3.1668109076820112, 4.3315003860828485], 'topk_tokens': [',', ' could', 'of', 'out', ',', ' office', 'es', ' office', ' and', ' and', ' and', ' four', ' for', 'ab', 'ot', ' offices', ' office', ' offices', ' office', ' o'], 'evidence_proportions': [1.82794189453125, 2.940673828125, 3.7158203125, 1.8681640625]}, 'weight': {'score': [0.002874574877999046, 0.006786774845690901, 0.003494846098350756, 0.006833365429072438, 0.0014420068541238474], 'topk_tokens': ['UL', '\n\n', '\n', 'user', '<|start_header_id|>', '\n\n', ' *\n\n', '\n\n', 'NEW', 'Good', '<|eot_id|>', '<|eot_id|>', 'ANK', ' *\n\n', '\n\n\n', '<|start_header_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007885456085205078, 0.00022135178248087564, 0.004050801197687785, 0.001010984182357788]}, 'saliency': {'score': [0.00040802088650790125, 0.0008645991306007612, 0.0004962971716216116, 0.00086991109183942, 0.0003491258898446726], 'topk_tokens': [' John', '.\n', 'RE', '.\n', '.', '\n\n', 'Min', '\n\n\n', 'user', '<|eot_id|>', ' *\n\n', 'ANK', ' *\n\n', 'Good', '<|eot_id|>', 'office', '<|start_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0006307363510131836, 2.4259090423583984e-05, 0.0006843904654184977, 0.00036693612734476727]}}, 31: {'grad': {'score': [2.1512284712357954, 4.343287781943788, 3.2018636067708335, 4.363841096759841, 7.215207122093023], 'topk_tokens': ['inen', '<|start_header_id|>', 'ed', '\n', ' continued', '\n', '<|eot_id|>', ' prize', '️', 'ess', ' actions', 'Answer', ' asked', 'ot', '<|eot_id|>', ' celebrity', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [2.03228759765625, 0.46588134765625, 4.836995442708333, 1.2301025390625]}, 'weight': {'score': [0.0010530704801732843, 0.006957043777054979, 0.0008728955731247411, 0.007036168999180152, 0.0007425872392432634], 'topk_tokens': ['\n\n', '.\n', ' bathroom', ' *\n\n', ' \n', 'PA', 'If', 'user', '\n\n', 'Today', 'UL', 'Good', '\n\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003494173288345337, 0.00021735827128092447, 0.0007785061995188395, 0.0005359450976053873]}, 'saliency': {'score': [7.740746844898571e-05, 0.0010299917995000595, 3.931197253140536e-05, 0.0010428296290222153, 8.555137833883596e-05], 'topk_tokens': ['Just', '<|eot_id|>', '\n\n', 'Cut', '\n\n\n', '.\n', 'ANK', 'Good', 'UL', ' Collection', ',', ' the', 'assistant', 'ern', 'PA', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00023666024208068848, 4.72863515218099e-05, 3.2573938369750977e-05, 4.6193599700927734e-05]}}, 'pred_res': 'Mary took the football.<|eot_id|>', 'score': 0}
2025-01-23 22:17:46.091 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:17:46.092 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-2_pid-0_2-6-8-9.pkl | len: 10 |  size: 9.43 KB
Processing depth (2, 6, 8, 9):   1%|          | 1/100 [00:14<23:42, 14.37s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.33s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.27s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
Processing depth (0, 4, 7, 8):   1%|          | 1/100 [00:26<23:42, 14.37s/it]2025-01-23 22:17:58.292 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:17:58.312 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:17:58.323 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1807, 1813) --> . Mary journeyed to the
2025-01-23 22:17:58.324 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:17:58.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1807, 1813) --> . Mary journeyed to the
2025-01-23 22:17:58.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:17:58.337 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (779, 785) --> . Daniel went back to the
2025-01-23 22:17:58.337 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-23 22:17:58.341 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (779, 785) --> . Daniel went back to the
2025-01-23 22:17:58.341 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:17:58.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1974, 1980) --> . John went back to the
2025-01-23 22:17:58.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:17:58.360 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1808, 1814) -->  Mary journeyed to the office
2025-01-23 22:17:58.360 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:17:58.376 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3204, 3209) --> . John moved to the
2025-01-23 22:17:58.376 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-23 22:17:58.395 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3163, 3169) --> . Sandra journeyed to the
2025-01-23 22:17:58.395 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:17:58.402 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1435, 1439) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:17:59.090 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football. PAUL<|eot_id|>
2025-01-23 22:17:59.091 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4214])
your chose emoji: ['🇪🇺', '🚳', '👩🏾\u200d🦳', '🏌🏾\u200d♀️', '🤳🏻', '🚴🏾\u200d♀️', '🕕', '🍉', '👩\u200d❤️\u200d💋\u200d👩', '🇱🇾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4216, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 134.19it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.21it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.81it/s]
2025-01-23 22:18:01.934 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.0517569647894964, 0.7555888282732448, 0.7963881059126421, 0.7539856103574242, 1.0444846994736616], 'topk_tokens': ['there', 'de', ' waiting', ' there', ' the', ' print', 'action', 'there', ' print', ' offices', ' printing', 'the', ' office', ' clerk', ' printed', ' office', ' offices', ' office', ' offices', ' office'], 'evidence_proportions': [1.03546142578125, 1.03546142578125, 1.0843480428059895]}, 'weight': {'score': [0.0009858111540476482, 0.007367100616107629, 0.0018747286363081498, 0.007438195810741594, 0.0019846383263083067], 'topk_tokens': ['<|end_header_id|>', 'system', '***', '\n', ' ', '<|start_header_id|>', '<|start_header_id|>', '\n\n', '\n\n', '\n\n', 'user', '<|eot_id|>', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0012471675872802734, 0.0012471675872802734, 0.00046309828758239746]}, 'saliency': {'score': [8.211533228556316e-05, 0.0003283205928114831, 0.0001364079388705167, 0.0003309051815916796, 0.00028280212598688463], 'topk_tokens': [' *', '\n', ':', '<|end_header_id|>', ' *', 'system', 'office', '<|eot_id|>', '\n\n', ' *', '<|eot_id|>', 'user', '\n\n', 'assistant', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', '***'], 'evidence_proportions': [9.80993111928304e-05, 9.80993111928304e-05, 5.014737447102865e-05]}}, 25: {'grad': {'score': [0.7740003797743056, 1.6501425466229838, 1.4781475645123106, 1.6552917407197254, 1.5965437047621782], 'topk_tokens': [' state', 'possible', 'ian', 'right', ' press', '<|end_header_id|>', ' milk', ' Press', 's', ' Papers', ' determined', ' press', ' prohibiting', ' James', 'AILY', ' PA', ' Daily', ' PA', ' confirmed', ' press'], 'evidence_proportions': [0.308258056640625, 0.308258056640625, 1.7054850260416667]}, 'weight': {'score': [0.0010315444734361437, 0.007379598816612866, 0.002806925412380334, 0.007443263450590503, 0.00322421859292423], 'topk_tokens': ['fax', '<|end_header_id|>', 'system', 'Mary', '\n\n\n', '<|start_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', 'user', 'office', 'assistant', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0014130274454752605, 0.0014130274454752605, 0.00026857852935791016]}, 'saliency': {'score': [0.0001174012819925944, 0.0003052543021696116, 0.00023402228499903822, 0.0003066305352860138, 0.0005695942570181454], 'topk_tokens': ['<|eot_id|>', ' ', '\n\n\n\n', 'E', '\n\n\n\n\n\n\n', 'system', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'user', '<|start_header_id|>', '\n\n', '<|eot_id|>', ' PA', '<|eot_id|>', 'office', 'assistant', '<|begin_of_text|>', 'Mary', '<|end_header_id|>'], 'evidence_proportions': [0.00016808509826660156, 0.00016808509826660156, 1.6033649444580078e-05]}}, 26: {'grad': {'score': [1.3095160590277777, 1.8364106093231143, 1.5219245679450757, 1.8411794307566778, 1.3159601548138786], 'topk_tokens': [' Gutenberg', ' but', ' their', ' about', ' in', ' and', '.', '185', 'vent', ' to', ' boat', ' be', '185', ' to', ' type', ' boats', '185', ' a', '***', '185'], 'evidence_proportions': [1.3965047200520833, 1.3965047200520833, 1.1355387369791667]}, 'weight': {'score': [0.0011538399590386285, 0.007321209337498703, 0.0045690428126942025, 0.007369668915921471, 0.003954783958547255], 'topk_tokens': ['\n', '\n\n\n', ' journey', 'ed', '\n\n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'office', 'user', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016065339247385662, 0.0016065339247385662, 0.00024845202763875324]}, 'saliency': {'score': [6.545583407084148e-05, 0.00028436242176866624, 0.00036879741784298057, 0.00028463948388345815, 0.00038434302105623135], 'topk_tokens': [' journey', ' elect', ' Among', 'made', '<|start_header_id|>', '.', '<|end_header_id|>', '<|start_header_id|>', ' office', 'ed', 'office', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'user', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [8.254249890645345e-05, 8.254249890645345e-05, 3.1282504399617515e-05]}}, 27: {'grad': {'score': [1.8961046006944444, 1.226175789588769, 1.4139210094105115, 1.2217930018687162, 1.0674027835621553], 'topk_tokens': [' in', ' Eagle', ' is', ' is', ' was', ',', ' as', ',', 'in', '.', ' days', '.', ' em', ' as', ' as', ',', 'as', ' as', ' as', ' as'], 'evidence_proportions': [2.0174153645833335, 2.0174153645833335, 1.6534830729166667]}, 'weight': {'score': [0.001485887500974867, 0.0074046467920408305, 0.006383610494209059, 0.0074383159073031675, 0.0048480095232234285], 'topk_tokens': [' the', 'Mary', '***', ' to', '�', '<|start_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'user', 'assistant', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0021033883094787598, 0.0021033883094787598, 0.0002508858839670817]}, 'saliency': {'score': [0.00019732448789808486, 0.00020958665595787525, 0.0011434067379344594, 0.0002022408351463144, 0.0005262651864220114], 'topk_tokens': ['ed', 'RE', ' to', '<|start_header_id|>', 'ed', '<|eot_id|>', '\n\n', '<|eot_id|>', '\n\n', '�', ' the', '***', ' Joseph', 'user', '<|eot_id|>', '<|end_header_id|>', 'office', '<|end_header_id|>', 'Mary', '<|begin_of_text|>'], 'evidence_proportions': [0.0002880692481994629, 0.0002880692481994629, 1.5834967295328777e-05]}}, 28: {'grad': {'score': [1.3375091552734375, 0.7760121528292516, 0.938792373194839, 0.7722957798842193, 0.8551483154296875], 'topk_tokens': [' as', ' one', ' em', 'ra', ' absent', ' em', ' the', ' PA', ' PA', ' is', ' of', ' an', ' they', 'action', ' ST', '.', '000', ' were', '600', ' PA'], 'evidence_proportions': [1.1116714477539062, 1.1116714477539062, 1.7891845703125]}, 'weight': {'score': [0.0027653475602467856, 0.007151165316181798, 0.00387344757715861, 0.0071960895430712565, 0.001952669199775247], 'topk_tokens': ['\n\n\n', 'system', 'ed', ' ', '\n', '\n\n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '<|start_header_id|>', 'user', '<|eot_id|>', '\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004059274991353353, 0.004059274991353353, 0.00017749269803365073]}, 'saliency': {'score': [0.00022010339630974663, 0.00023585077255002914, 0.00021527572111649946, 0.00023608184805294188, 0.0001579952590605792], 'topk_tokens': ['ed', '<|end_header_id|>', 'fax', '\n\n\n', '<|start_header_id|>', ':', '\n\n', '\n\n', '<|start_header_id|>', '<|start_header_id|>', ':', '<|end_header_id|>', 'user', 'assistant', '<|eot_id|>', 'office', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0003180503845214844, 0.0003180503845214844, 2.420941988627116e-05]}}, 29: {'grad': {'score': [1.3166639539930556, 1.1831970793924929, 1.1829769250118372, 1.1826220160915928, 0.9360077801872703], 'topk_tokens': [' of', ' NEW', ' the', ' its', ' the', ' the', ' the', ' the', ' Date', 'NEW', 'SP', ' his', ' the', 'the', ' the', 'an', 'SP', 'vent', ' the', "'"], 'evidence_proportions': [1.1185709635416667, 1.1185709635416667, 1.7128499348958333]}, 'weight': {'score': [0.001734650797314114, 0.007358575002970686, 0.0045500343496149235, 0.0074051326205607365, 0.00323442676488091], 'topk_tokens': ['<|end_header_id|>', 'ed', ' ', '\n\n\n', 'system', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '\n\n', '\n\n', 'user', 'assistant', '<|eot_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0025223990281422934, 0.0025223990281422934, 0.00015915433565775552]}, 'saliency': {'score': [0.0003956788116031223, 0.0006420948247529524, 0.001280110893827496, 0.0006381046585962266, 0.0011914959725211648], 'topk_tokens': ['\n', '\n', '�', '�', '�', '\n\n', 'ed', '\n\n', 'user', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'system', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005840659141540527, 0.0005840659141540527, 1.8904606501261394e-05]}}, 30: {'grad': {'score': [1.8888346354166667, 2.1204910423090606, 2.1758552320075757, 2.121053538602941, 1.6900484421673942], 'topk_tokens': [' clerk', 'LY', ' proc', 'APER', ' available', 'ab', ' await', ' attention', ' prof', ' offices', 'EF', ' office', '.', ' ST', ' after', ' offices', ' ST', ' offices', ' office', ' office'], 'evidence_proportions': [1.7874755859375, 1.7874755859375, 2.091552734375]}, 'weight': {'score': [0.007409483194351196, 0.0071446814166074455, 0.015788860393292976, 0.007075047721954383, 0.006301099763197058], 'topk_tokens': [' John', '\n\n', 'system', 'Mary', '\n\n\n', 'ed', '<|eot_id|>', '<|end_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'user', '<|eot_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.011023908853530884, 0.011023908853530884, 0.0001806318759918213]}, 'saliency': {'score': [0.001591112878587511, 0.000594923120748386, 0.0020833412806193032, 0.0005788248698679911, 0.0017124300493913539], 'topk_tokens': ['\n\n', '<|start_header_id|>', '<|eot_id|>', ' PA', '\n\n\n', 'user', 'Mary', ' John', '***', ' to', ' journey', 'assistant', '�', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0023762484391530356, 0.0023762484391530356, 2.084175745646159e-05]}}, 31: {'grad': {'score': [1.5667453342013888, 3.7790369833669355, 3.103134617660985, 3.7939531965129802, 4.281190759995404], 'topk_tokens': ['antically', ' grat', 'tickets', 'itol', 'ors', ' acquaintance', '�', ' forthcoming', 'ess', 'inen', '<|start_header_id|>', '️', 'latest', '202', 'user', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [1.5788777669270833, 1.5788777669270833, 1.54248046875]}, 'weight': {'score': [0.005577243036693997, 0.006739685159932956, 0.005065238837039832, 0.006757975841055111, 0.0016030993531731999], 'topk_tokens': ['.', 'system', '<|eot_id|>', '\n', ' ', 'Mary', '<|start_header_id|>', 'ed', '\n\n\n', '\n\n', '\n\n', '\n\n', 'user', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.008173008759816488, 0.008173008759816488, 0.0003857115904490153]}, 'saliency': {'score': [0.0005465613471137153, 0.0004390301412604113, 0.0007334044485381155, 0.00043623304309822075, 0.00016622157657847684], 'topk_tokens': ['\n\n\n', 'Cut', '\n', ' PA', '<|eot_id|>', '\n\n', '.', 'Mary', '<|start_header_id|>', '-text', 'ed', 'system', '\n\n', 'user', 'assistant', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0008019407590230306, 0.0008019407590230306, 3.580252329508463e-05]}}, 'pred_res': 'Mary took the football. PAUL<|eot_id|>', 'score': 0}
2025-01-23 22:18:01.942 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:18:01.942 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-2_pid-1_0-4-7-8.pkl | len: 10 |  size: 9.23 KB
Processing depth (0, 4, 7, 8):   2%|▏         | 2/100 [00:30<24:53, 15.24s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
Processing depth (1, 5, 6, 7):   2%|▏         | 2/100 [00:41<24:53, 15.24s/it]2025-01-23 22:18:12.819 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:18:12.821 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (423, 427) -->  Mary took the football
2025-01-23 22:18:12.821 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:18:12.831 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1839, 1845) -->  journeyed to the office.
2025-01-23 22:18:12.831 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:18:12.842 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2124, 2130) --> . Mary journeyed to the
2025-01-23 22:18:12.842 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:18:12.846 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (794, 800) --> . Daniel went back to the
2025-01-23 22:18:12.846 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-23 22:18:12.850 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (794, 800) --> . Daniel went back to the
2025-01-23 22:18:12.850 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:18:12.860 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1982, 1988) -->  John went back to the bedroom
2025-01-23 22:18:12.860 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:18:12.870 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1838, 1844) -->  John journeyed to the office
2025-01-23 22:18:12.870 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:18:12.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3269, 3274) --> . John moved to the
2025-01-23 22:18:12.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-23 22:18:12.906 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3213, 3219) --> . Sandra journeyed to the
2025-01-23 22:18:12.906 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:18:12.913 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1494, 1498) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:18:13.572 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary took the football. Paul<|eot_id|>
2025-01-23 22:18:13.572 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4239])
your chose emoji: ['🧝🏼\u200d♀', '🕵️\u200d♂', '👦🏿', '🧎🏼\u200d♀\u200d➡', '👩🏻\u200d🤝\u200d👨🏼', '👩🏻\u200d🌾', '🧎🏻\u200d♂️\u200d➡', '🦹🏽\u200d♀️', '👳', '🧚🏽\u200d♀️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4241, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 118.24it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.00it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.36it/s]
2025-01-23 22:18:16.691 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7915469082919034, 0.6070228222264796, 0.756984248305812, 0.6048708258208281, 0.6816388201969926], 'topk_tokens': ['office', ' EVENTS', 'posit', ' employed', '�', ' duty', ' office', ' business', '�', ' office', ' duty', ' execution', ' office', ' offices', ' offices', ' offices', ' office', ' offices', ' office', ' office'], 'evidence_proportions': [0.64825439453125, 1.6959635416666667, 0.4473368326822917, 0.3268686930338542]}, 'weight': {'score': [0.0016384910453449595, 0.007425403122743383, 0.0025824236147331467, 0.007493996144594101, 0.0037298305060273857], 'topk_tokens': ['.\n\n', 'Minnesota', '<|eot_id|>', ' ', 'user', 'Answer', '\n\n', 'If', '<|eot_id|>', ' bathroom', ' \n', '\n\n', 'Question', '<|eot_id|>', '<|start_header_id|>', ' Where', 'office', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0026692748069763184, 0.001241554816563924, 0.001338362693786621, 0.0016483664512634277]}, 'saliency': {'score': [0.0009640482338992032, 0.0002977750234462214, 0.0007904460935881643, 0.000290389415241575, 0.00016605469488328503], 'topk_tokens': [' old', ' office', ' ', 'super', 'If', ' office', ' office', ' offices', 'Answer', 'Minnesota', ' Where', ' offices', ' office', '***', 'Question', ' offices', ' bathroom', 'assistant', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00021348893642425537, 0.0032480855782826743, 4.141529401143392e-05, 0.00010301669438680013]}}, 25: {'grad': {'score': [0.8678866299715909, 0.870521018314519, 0.7603559783010772, 0.8714033410245018, 1.0399911531838038], 'topk_tokens': ['\n', ' and', '\n', ' office', ' P', ' O', ',', ' other', ' Date', ' upper', '600', ' up', ' out', ' forthcoming', ' office', ' ', '.', ' AND', ' Good', '600'], 'evidence_proportions': [1.07757568359375, 0.9576822916666666, 0.74755859375, 0.7586263020833334]}, 'weight': {'score': [0.0019840571013363924, 0.007422251342687087, 0.0010680447925220835, 0.007500925277103036, 0.0024728364841912383], 'topk_tokens': [' directly', '.\n\n', ' else', ' anything', '.\n\n', '<|start_header_id|>', ' \n', '\n\n', 'MIN', ' S', 'Minnesota', '<|eot_id|>', '<|eot_id|>', ' remains', 'Answer', ' after', 'office', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006642758846282959, 0.00042996803919474286, 0.0018077890078226726, 0.0006086130936940511]}, 'saliency': {'score': [0.00044659592888572, 0.0002445324050230509, 0.00010384935321229877, 0.00024457950086270117, 0.00011884973895165228], 'topk_tokens': ['<|eot_id|>', 'Question', ' Do', ' location', 'Min', ' soon', ' Mary', ' majority', 'MIN', ' Douglas', ' was', 'Minnesota', 'Min', ' remains', ' after', 'Civil', 'office', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0017157644033432007, 3.001093864440918e-05, 0.0004132290681203206, 5.043546358744303e-05]}}, 26: {'grad': {'score': [0.32366943359375, 0.3359815385706496, 0.33520276618726325, 0.33605238563184014, 0.524932122999622], 'topk_tokens': [' tie', ' EAR', '186', ' Hoe', ' breaking', ' of', ' Eagle', ' course', 'machine', ' still', ' slow', 'RE', ' pet', 'E', ' ago', ' hand', ' Douglas', ' force', ' Eagle', '�'], 'evidence_proportions': [0.385711669921875, 0.23746236165364584, 0.2688446044921875, 0.42333984375]}, 'weight': {'score': [0.0009286674586209384, 0.0073399614596305045, 0.0011578057751511083, 0.007422393281324252, 0.0051488831479062315], 'topk_tokens': [' Where', 'If', '<|start_header_id|>', '.\n\n', '\n\n', '<|eot_id|>', '.\n\n', 'user', 'Question', ' \n', '.\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'Answer', '<|eot_id|>', '<|end_header_id|>', 'office', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0026477575302124023, 0.0007320245107014974, 0.00048414866129557293, 0.00042376915613810223]}, 'saliency': {'score': [0.00016608834266662598, 0.00016561196227591994, 0.00018410068569761333, 0.00016546370421535823, 0.00021631551045243457], 'topk_tokens': ['<|start_header_id|>', '.\n\n', 'Civil', ' bathroom', '\n\n', 'Minnesota', 'assistant', ' Reporter', 'district', ' Where', 'Question', 'Answer', '<|start_header_id|>', 'office', '<|eot_id|>', '.\n\n', '<|end_header_id|>', '<|eot_id|>', 'user', '<|begin_of_text|>'], 'evidence_proportions': [6.604194641113281e-05, 0.0005256036917368571, 1.9619862238566082e-05, 1.9739071528116863e-05]}}, 27: {'grad': {'score': [0.6013710715553977, 0.4761357863987709, 0.44720528342507104, 0.47570566941947956, 0.4178516839140205], 'topk_tokens': [' Mary', ' city', ' newspaper', ' Moore', ' course', ' Note', ' paper', ' news', ' fact', ' news', ' force', 'SP', ' Gen', ' stone', ' news', ' office', ' office', ' office', ' prof', ' office'], 'evidence_proportions': [0.659210205078125, 0.8957112630208334, 0.5438028971354166, 0.3260396321614583]}, 'weight': {'score': [0.00206491080197421, 0.0073877445680581855, 0.0016710089914726489, 0.0074607867603374905, 0.0048943270919143515], 'topk_tokens': ['.\n\n', 'Just', '\n', 'As', '\n\n', 'If', 'user', '<|eot_id|>', ' \n', 'If', '<|start_header_id|>', 'Minnesota', '<|eot_id|>', '\n\n', 'Answer', 'Question', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007079452276229858, 0.0007279316584269205, 0.0012592673301696777, 0.0008645057678222656]}, 'saliency': {'score': [0.00019553845578973943, 0.00024368596566057914, 0.0002000675056919907, 0.00024428287306529054, 0.0005288220220996488], 'topk_tokens': ['�', 'action', '\n\n', 'Answer', '<|end_header_id|>', 'Min', 'being', '\n\n', 'Just', '<|eot_id|>', 'PA', 'until', ' Alexander', 'Question', 'assistant', 'If', 'As', 'If', '<|begin_of_text|>', 'office'], 'evidence_proportions': [9.974837303161621e-05, 0.000352710485458374, 0.0001555184523264567, 0.00014224648475646973]}}, 28: {'grad': {'score': [0.291782639243386, 0.3208657741209326, 0.3022145935983369, 0.32116565895718474, 0.3022240054222845], 'topk_tokens': [' took', ' discover', ' to', 'made', '�', ' newspaper', ' not', ' facts', ' not', ' thought', ' have', ' not', ' contents', ' not', '�', ' out', ' came', 'cont', 'out', ' consisted'], 'evidence_proportions': [0.425750732421875, 0.2416226069132487, 0.233184814453125, 0.3112284342447917]}, 'weight': {'score': [0.0003898035396229137, 0.007221680209198753, 0.0007407791686780525, 0.0073086775864247095, 0.002368547583139071], 'topk_tokens': ['<|eot_id|>', ' Where', ' after', '<|start_header_id|>', '.\n\n', '\n', '\n\n', 'If', '.\n\n', ' a', ' ', '\n', '.\n', 'Just', ' \n', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005338937044143677, 0.0002144773801167806, 0.0005930662155151367, 0.0002658069133758545]}, 'saliency': {'score': [1.9517811861905184e-05, 0.0001442628462228593, 4.208268541278261e-05, 0.00014572398718384197, 0.00011758137774723833], 'topk_tokens': [' \n', ' a', ',', 'If', 'of', ' after', ' During', '.', 'just', ' ', ':', ' Where', '\n', 'If', 'assistant', 'Just', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [4.170835018157959e-05, 5.4140885670979815e-06, 2.6514132817586262e-05, 1.1831521987915039e-05]}}, 29: {'grad': {'score': [0.9284945401278409, 0.8090089214660457, 1.1992557410037878, 0.8053044712378314, 0.40110975696194556], 'topk_tokens': [' States', ' the', ' court', ' the', ' approaching', ' Republican', ' return', ' county', ' six', ' the', ' company', ' the', ' office', ' no', ' return', '600', ' The', ' the', ' the', ' press'], 'evidence_proportions': [0.57757568359375, 1.3450927734375, 0.7871297200520834, 0.88720703125]}, 'weight': {'score': [0.000607211481441151, 0.007383917278064025, 0.000914933103503603, 0.007470530752809892, 0.0030384819994690596], 'topk_tokens': ['.\n\n', 'Just', ' Where', ' a', '\n\n', 'If', 'user', '?', 'If', '<|eot_id|>', '<|start_header_id|>', 'Question', ' \n', 'I', '<|eot_id|>', 'Answer', 'office', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011376738548278809, 0.0006040334701538086, 0.0005179246266682943, 0.0003460347652435303]}, 'saliency': {'score': [5.5245377800681375e-05, 0.00019044406401101947, 0.0001075159419666637, 0.0001918083734052123, 0.00010020117605886151], 'topk_tokens': ['to', 'then', ' directly', 'Question', ' a', ' the', 'ideas', '.', 'Answer', 'tele', 'being', ' \n', '<|start_header_id|>', 'user', 'I', '<|eot_id|>', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.9177794456481934e-05, 0.0001243452231089274, 1.2268622716267904e-05, 5.316734313964844e-05]}}, 30: {'grad': {'score': [0.8935694261030718, 0.6941268550312426, 0.7273471716678503, 0.6928167721328699, 0.7459547391501806], 'topk_tokens': ['.', ' Fourth', ' P', ' trembling', ' offices', ' fifteen', ' Col', 'of', ' Col', ' o', ' o', ' office', ' offices', ' office', ' office', ' o', ' o', ' o', ' offices', ' office'], 'evidence_proportions': [0.37469959259033203, 1.3783060709635417, 0.7962849934895834, 0.8520304361979166]}, 'weight': {'score': [0.002082117579200051, 0.007256370477872028, 0.0024280746777852378, 0.007321627841745332, 0.010502048718032016], 'topk_tokens': ['♀', 'Minnesota', '.\n', '�', ' a', 'I', '?', '\n', ' Where', '<|start_header_id|>', '<|eot_id|>', 'Answer', 'Question', '.\n\n', ' \n', '<|eot_id|>', '<|end_header_id|>', 'assistant', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004037141799926758, 0.003362417221069336, 0.0008147954940795898, 0.0007657905419667562]}, 'saliency': {'score': [0.00042304667559537023, 0.0004143242677465078, 0.0003482157533819025, 0.0004147995873837168, 0.00041857842476137225], 'topk_tokens': ['<|eot_id|>', '<|start_header_id|>', '.\n', ' Do', 'I', ' have', 'Min', 'assistant', '?', ' offices', ' offices', '<|start_header_id|>', 'Minnesota', ' \n', '<|begin_of_text|>', '.\n\n', 'Question', '<|end_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0006043612957000732, 0.0010303854942321777, 4.992882410685221e-05, 6.794929504394531e-05]}}, 31: {'grad': {'score': [0.3153103915127841, 0.366575680485145, 0.2878233013731061, 0.3674659492066188, 0.43692352951213875], 'topk_tokens': ['ot', ' M', 'SP', ' DAYS', ' o', ' H', 'RE', ' INCIDENT', 'ot', ' fifty', 'ioneer', 's', 'ot', 'user', 'S', 'G', ' DAYS', ' em', ' EVENTS', 'ISC'], 'evidence_proportions': [0.19354248046875, 0.3974507649739583, 0.2790730794270833, 0.3505859375]}, 'weight': {'score': [0.0008644949306141247, 0.0066530591501039425, 0.0020613670349121094, 0.006719679850684474, 0.004515591488089612], 'topk_tokens': [' a', 'If', '?', '<|eot_id|>', '.\n', ' Where', ' Do', '.\n\n', 'Just', ' was', '<|start_header_id|>', '.\n\n', 'Question', ' \n', 'Answer', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.001298367977142334, 0.0004043976465861003, 0.0006373325983683268, 0.0012625058492024739]}, 'saliency': {'score': [0.00012139840559525923, 0.00023143990257387312, 0.0001703374313585686, 0.00023249993470087613, 0.0002312852490332819], 'topk_tokens': [' the', ' Grow', ' Do', ' the', ' but', '.\n\n', ' after', ' was', 'assistant', '.\n', ' the', '<|eot_id|>', '.\n\n', 'If', ' \n', '.\n\n', 'Question', 'Answer', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0001777559518814087, 0.00016999244689941406, 1.5447537104288738e-05, 0.00014118353525797525]}}, 'pred_res': 'Mary took the football. Paul<|eot_id|>', 'score': 0}
2025-01-23 22:18:16.699 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:18:16.699 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-2_pid-2_1-5-6-7.pkl | len: 10 |  size: 9.17 KB
Processing depth (1, 5, 6, 7):   3%|▎         | 3/100 [00:44<24:16, 15.02s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.33s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Processing depth (3, 4, 6, 8):   3%|▎         | 3/100 [00:56<24:16, 15.02s/it]2025-01-23 22:18:28.653 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:18:28.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1477, 1481) -->  Mary took the football
2025-01-23 22:18:28.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:18:28.670 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1849, 1855) -->  Hon. Mary journeyed to
2025-01-23 22:18:28.671 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:18:28.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1849, 1855) -->  Hon. Mary journeyed to
2025-01-23 22:18:28.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:18:28.684 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (792, 798) --> . Daniel went back to the
2025-01-23 22:18:28.684 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-23 22:18:28.688 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (792, 798) --> . Daniel went back to the
2025-01-23 22:18:28.688 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:18:28.700 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2001, 2007) --> . John went back to the
2025-01-23 22:18:28.700 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:18:28.709 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1851, 1857) -->  Mary journeyed to the office
2025-01-23 22:18:28.709 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:18:28.726 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3265, 3270) --> . John moved to the
2025-01-23 22:18:28.726 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-23 22:18:28.742 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3209, 3215) --> . Sandra journeyed to the
2025-01-23 22:18:28.742 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:18:28.749 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1482, 1486) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:18:29.247 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the office<|eot_id|>
2025-01-23 22:18:29.247 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4230])
your chose emoji: ['💨', '🙇🏿\u200d♀', '🙆\u200d♂️', '👌🏽', '🙍🏿\u200d♂', '👩🏾\u200d❤\u200d💋\u200d👩🏿', '🧑🏾\u200d🚒', '👨🏽\u200d🚀', '🙅🏿\u200d♀', '🇬🇶']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4232, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 208412.62it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.94it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 139.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.85it/s]
2025-01-23 22:18:32.134 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.9140111749822443, 0.5807347432877481, 0.8862942782315341, 0.5765653523012928, 0.3384935742332822], 'topk_tokens': [' he', ' expedition', ' room', ' day', ' duty', ' evening', ' bedroom', ' compos', ' Mary', ' Mary', ' duty', 'room', ' work', ' senate', ' offices', ' offices', ' office', ' office', ' office', ' office'], 'evidence_proportions': [1.0439605712890625, 1.0987548828125, 1.0987548828125, 0.4578908284505208]}, 'weight': {'score': [0.044713583859530365, 0.007534874860190263, 0.05068060484799472, 0.006998188078389203, 0.012175256297701881], 'topk_tokens': ['men', ' ever', ' ground', 'ails', '<|end_header_id|>', '�', ' to', ' of', 'half', 'cery', 'gro', ' fire', ' fact', '�', 'es', ' dollars', ' day', ' time', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0018508434295654297, 0.0002412597338358561, 0.0002412597338358561, 0.16223339239756265]}, 'saliency': {'score': [0.00014709071679548785, 0.0001958295111394785, 0.0006302829944726193, 0.00019265385601033655, 0.0002409673872448149], 'topk_tokens': [' offices', ' fore', ' *\n\n', ' minds', ' *', 'street', ' Project', ' house', ' journey', ' offices', '<|eot_id|>', ':', 'ed', ' upper', ' bedroom', '<|end_header_id|>', ' Grow', ' fight', '***', ' office'], 'evidence_proportions': [0.00013127923011779785, 2.0523866017659504e-05, 2.0523866017659504e-05, 0.0004107654094696045]}}, 25: {'grad': {'score': [0.4796725186434659, 0.5310172818333531, 0.5632093024976326, 0.5310333814522796, 0.5833748408726284], 'topk_tokens': [' ranks', ',', '\n', 'ation', 'system', 'ulations', ' before', '�', ' it', 'ian', "'s", '�', ' up', '<|start_header_id|>', ' Bench', '<|start_header_id|>', 'ian', ' the', '\n', ' Wood'], 'evidence_proportions': [0.5299072265625, 0.4660441080729167, 0.4660441080729167, 0.4734395345052083]}, 'weight': {'score': [0.054794994267550384, 0.007545476599767212, 0.061046293287566215, 0.006873938093808727, 0.01607426078546615], 'topk_tokens': [' fact', ' ever', 'half', ' happened', 'gro', 'cery', ' minutes', 'time', ' dollars', 'es', '�', ' to', ' fire', '�', ' ground', ' day', ' time', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0007756352424621582, 0.00020863612492879233, 0.00020863612492879233, 0.19998061656951904]}, 'saliency': {'score': [0.00010626153512434526, 0.00018197934659081733, 0.00047640547607884264, 0.00018005206135719452, 0.00020543592316763743], 'topk_tokens': [' *\n\n', ' room', ' Note', ':\n', ' next', '<|eot_id|>', 'Civil', ' fire', ' ground', 'assistant', '<|begin_of_text|>', ' to', ' *\n\n', '<|start_header_id|>', ' *\n\n', ' *\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.00014346837997436523, 4.530946413675944e-05, 4.530946413675944e-05, 0.00020336111386617026]}}, 26: {'grad': {'score': [1.0962801846590908, 0.9730870728231332, 1.084542939157197, 0.971557675636297, 0.6212350754510789], 'topk_tokens': [' John', ',\n', ' year', '-per', ' ', ' paper', ' were', ' regular', ' Pa', ' paper', 'str', '1', ' work', ' St', ' paper', ' In', ' STR', ' Press', ' St', ' spring'], 'evidence_proportions': [0.8138427734375, 1.3103841145833333, 1.3103841145833333, 0.8563639322916666]}, 'weight': {'score': [0.028625263409181076, 0.007535361612676898, 0.034974650903181595, 0.007207500854690366, 0.015516658624013266], 'topk_tokens': [' happened', 'gro', 'time', '<|eot_id|>', 'cery', ' to', ' fact', ' dollars', ' ever', '<|start_header_id|>', '�', ' ground', ' fire', 'es', '�', ' day', ' time', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0017092227935791016, 0.000362853209177653, 0.000362853209177653, 0.1030941108862559]}, 'saliency': {'score': [0.0002346174283461137, 0.00025061048947570456, 0.0004247318614612926, 0.00024931909423254594, 0.0009195485285350255], 'topk_tokens': ['<|start_header_id|>', '<|begin_of_text|>', 'papers', '�', ' about', 'out', 'was', 'ian', '\u200d', ' made', ' distant', ' Reporter', 'assistant', ' *\n\n', '<|end_header_id|>', '�', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [7.103383541107178e-05, 1.243750254313151e-05, 1.243750254313151e-05, 0.0007880330085754395]}}, 27: {'grad': {'score': [0.666961669921875, 0.9876074403355387, 0.963175918116714, 0.989489280695204, 1.1009573255266463], 'topk_tokens': ['\n', '\n', ' the', ' had', '\u200d', 'ed', ' and', ' of', '-column', ' as', '\n', ',', 'The', '--', '\n', ' This', ' as', 'graph', '\n', ' as'], 'evidence_proportions': [0.488189697265625, 0.5849863688151041, 0.5849863688151041, 0.9500935872395834]}, 'weight': {'score': [0.04318924654613842, 0.007549812302472235, 0.05307194500258475, 0.007002458236763744, 0.017716833523341587], 'topk_tokens': ['ails', 'half', ' of', 'gro', 'cery', ' ground', ' dollars', 'es', ' minutes', ' fact', ' to', '�', 'time', ' ever', '�', ' fire', ' day', ' time', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0013039112091064453, 0.00048097968101501465, 0.00048097968101501465, 0.15652933716773987]}, 'saliency': {'score': [0.0005366910587657582, 0.0005105680914592202, 0.0013815453558257132, 0.0005035494285420936, 0.001704012354214986], 'topk_tokens': ['�', ' happened', ' up', 'gro', '�', ' ever', ' Herald', ' to', ' fact', '�', ' time', ' York', ' day', 'time', ' minutes', ' fire', '<|end_header_id|>', ' ground', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [7.170438766479492e-05, 1.2129545211791992e-05, 1.2129545211791992e-05, 0.0018958051999409993]}}, 28: {'grad': {'score': [0.8954523259943182, 0.9967592524220227, 0.9532359730113636, 0.99763668134277, 0.9407588413783482], 'topk_tokens': [' four', ' from', ' called', ' ', ' out', ' consisted', ' four', ' came', 're', ' cash', ' out', ' PA', ' most', ' Gov', 'From', '❤', ' from', ' twenty', ' Out', 'PA'], 'evidence_proportions': [0.967529296875, 0.8826497395833334, 0.8826497395833334, 0.8730061848958334]}, 'weight': {'score': [0.04682890935377641, 0.007410040875238372, 0.05156490116408377, 0.006853582772279374, 0.016743608173869905], 'topk_tokens': [' of', 'half', ' up', ' minutes', ' to', ' dollars', 'time', ' fact', ' ground', 'es', ' fire', '�', ' ever', '�', 'office', '<|begin_of_text|>', ' time', ' day', '<|end_header_id|>', '.'], 'evidence_proportions': [0.0005356669425964355, 0.00012715657552083334, 0.00012715657552083334, 0.17109457651774088]}, 'saliency': {'score': [0.00039067864418029785, 0.00017228607081285272, 0.0004979733264807498, 0.00016856274879917677, 0.00019427140553792319], 'topk_tokens': [' next', 'ye', '<|start_header_id|>', 'half', ' the', 'assistant', ' fire', ' interesting', '�', ' still', '<|begin_of_text|>', ' ground', 'men', '.', 'es', 'time', ' up', ' day', '<|end_header_id|>', 'office'], 'evidence_proportions': [1.4960765838623047e-05, 3.7054220835367837e-06, 3.7054220835367837e-06, 0.0014151036739349365]}}, 29: {'grad': {'score': [1.0074462890625, 0.7680736750871338, 0.8694971257990057, 0.7660116278328963, 0.9209155128115699], 'topk_tokens': [' the', ' paper', ' boats', ' the', ' the', ' the', ' his', ' the', ' the', ' five', ' boats', ' the', ' the', ' the', ' press', ' the', ' the', ' court', ' the', ' two'], 'evidence_proportions': [0.85345458984375, 0.786865234375, 0.786865234375, 1.55126953125]}, 'weight': {'score': [0.05039613355289806, 0.0075034905922210056, 0.05579767263296879, 0.006896034007954888, 0.016913572947184246], 'topk_tokens': [' next', 'half', 'ails', ' to', ' ever', 'gro', ' dollars', ' minutes', 'time', '�', 'es', ' fact', ' of', '�', ' ground', ' fire', '<|begin_of_text|>', ' day', ' time', '.'], 'evidence_proportions': [0.0015797615051269531, 0.00031415621439615887, 0.00031415621439615887, 0.18310433626174927]}, 'saliency': {'score': [0.00073949857191606, 0.0003728093632227532, 0.0011397582111936626, 0.0003648188258575885, 0.000637591594741458], 'topk_tokens': [' instance', ' still', ' about', 'office', ' moved', '<|begin_of_text|>', '<|eot_id|>', '\u200d', ' next', ' fact', ' fire', ' the', '.', ' of', '<|eot_id|>', ' ground', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0005831867456436157, 3.411372502644857e-05, 3.411372502644857e-05, 0.0022544761498769126]}}, 30: {'grad': {'score': [1.3127857555042615, 0.9584365585125236, 1.1455378676905776, 0.9550920467728315, 1.156382061186291], 'topk_tokens': [' during', ' to', ' the', ' Sh', ' O', '.\n', ' to', 'us', ' Good', ' S', ' office', ' to', ' to', ' o', ' office', ' office', ' office', ' to', ' the', ' o'], 'evidence_proportions': [1.79931640625, 1.1200968424479167, 1.1200968424479167, 1.373809814453125]}, 'weight': {'score': [0.030434971505945378, 0.007270251420134633, 0.02775973623449152, 0.006986369006737071, 0.013196672712053572], 'topk_tokens': ['ian', 'gro', ' fact', ' fire', ' dollars', ' of', 'half', '�', 'es', '�', '<|eot_id|>', '<|start_header_id|>', ' day', 'assistant', '<|eot_id|>', '<|begin_of_text|>', ' time', '.', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.003916501998901367, 0.0016429026921590169, 0.0016429026921590169, 0.10569808880488078]}, 'saliency': {'score': [0.0005589452656832608, 0.00043713866199572733, 0.0004325454885309393, 0.00043653340210662324, 0.0015142630963098434], 'topk_tokens': [' new', '♀', 'edit', ' New', '<|start_header_id|>', '?', '�', ' Min', '�', ' facts', '�', '<|end_header_id|>', '<|eot_id|>', ' good', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'assistant', 'office', '<|eot_id|>'], 'evidence_proportions': [0.0012122541666030884, 0.00018295645713806152, 0.00018295645713806152, 0.0008753836154937744]}}, 31: {'grad': {'score': [1.1883212002840908, 0.777110743387435, 1.0019235321969697, 0.7731688108802669, 0.7138155074346633], 'topk_tokens': ['      ', ' with', ' could', ' of', '-column', ' of', ',\n', ' of', ' dispatch', ',', ' of', ' to', ' of', ',', ',', ' St', ',', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [1.27496337890625, 1.19091796875, 1.19091796875, 1.1253662109375]}, 'weight': {'score': [0.0593163858760487, 0.006984521400725684, 0.0481042175581961, 0.006384030380459074, 0.009980304610161554], 'topk_tokens': [' dollars', ' to', ' distributed', '<|start_header_id|>', 'men', ' fact', '�', ',', 'half', ' of', '�', 'es', 'assistant', ' day', ' time', '<|eot_id|>', '<|begin_of_text|>', '.', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0030786991119384766, 0.001641670862833659, 0.001641670862833659, 0.21215760707855225]}, 'saliency': {'score': [0.0011370100758292458, 0.000404445739684799, 0.0010625864520217433, 0.0003953877892652881, 0.0006470006136667161], 'topk_tokens': [' throughout', ' office', ' second', ',', ' happened', ' obtained', ' was', 'office', 'men', ' they', ' distributed', '<|begin_of_text|>', '�', ' war', ' dropped', ',', '.', '<|end_header_id|>', '\n', '<|eot_id|>'], 'evidence_proportions': [8.302927017211914e-05, 7.741649945576985e-05, 7.741649945576985e-05, 0.003958851099014282]}}, 'pred_res': 'the office<|eot_id|>', 'score': 100}
2025-01-23 22:18:32.141 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:18:32.141 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-2_pid-3_3-4-6-8.pkl | len: 10 |  size: 8.97 KB
Processing depth (3, 4, 6, 8):   4%|▍         | 4/100 [01:00<24:17, 15.19s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.15it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Processing depth (6, 7, 8, 9):   4%|▍         | 4/100 [01:12<24:17, 15.19s/it]2025-01-23 22:18:44.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the football.
2025-01-23 22:18:44.105 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2354, 2358) -->  Mary took the football
2025-01-23 22:18:44.105 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary journeyed to the office.
2025-01-23 22:18:44.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1804, 1810) -->  John journeyed to the office
2025-01-23 22:18:44.119 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:18:44.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2841, 2847) --> . Mary journeyed to the
2025-01-23 22:18:44.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:18:44.137 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (782, 788) -->  went back to the kitchen.
2025-01-23 22:18:44.137 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-23 22:18:44.141 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (781, 787) -->  Daniel went back to the kitchen
2025-01-23 22:18:44.141 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:18:44.155 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1948, 1954) --> . John went back to the
2025-01-23 22:18:44.155 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:18:44.164 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1803, 1809) --> . John journeyed to the
2025-01-23 22:18:44.164 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:18:44.180 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3202, 3207) --> . John moved to the
2025-01-23 22:18:44.180 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-23 22:18:44.196 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3161, 3167) --> . Sandra journeyed to the
2025-01-23 22:18:44.196 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:18:44.204 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1483, 1487) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:18:44.704 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the office<|eot_id|>
2025-01-23 22:18:44.705 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4225])
your chose emoji: ['🧜🏽\u200d♀️', '🛏️', '🎠', '🚩', '👩🏽\u200d🚒', '🕷️', '🙂\u200d↕️', '🧑🏼\u200d❤️\u200d💋\u200d🧑🏽', '👩🏽\u200d❤️\u200d💋\u200d👩🏻', '♟']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4227, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 225197.53it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 133.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.79it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.72it/s]
2025-01-23 22:18:47.861 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.4064913662997159, 0.4403143459272238, 0.40403701319839014, 0.4407796521429133, 0.47650957711135283], 'topk_tokens': [' the', ' Minnesota', ' the', ' the', ' gathering', '\n', ' it', ' grat', 'from', ' the', 'Min', '\n', ' of', ' scramble', ' Minnesota', ' OF', ' the', ' a', ' Min', 'cont'], 'evidence_proportions': [0.26824951171875, 0.5876057942708334, 0.3697509765625, 0.354278564453125]}, 'weight': {'score': [0.052104662765156136, 0.007402398836141731, 0.009799043337504068, 0.007147715452876324, 0.0012432366986817951], 'topk_tokens': [' doctor', '\n\n', '<|start_header_id|>', ' bathroom', 'Bridge', '<|eot_id|>', ' the', '<|eot_id|>', ':', ' hallway', ' football', 'assistant', ' bedroom', ' bedroom', '<|end_header_id|>', ' bathroom', ' football', ' office', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.18207550048828125, 0.00882414976755778, 0.05278211832046509, 0.008060495058695475]}, 'saliency': {'score': [0.00430191917852922, 0.00016344518618632186, 0.00040979999484437883, 0.00013967334138352712, 3.310245803639859e-05], 'topk_tokens': [' It', '<|start_header_id|>', 'Bridge', ' the', '<|eot_id|>', ' bathroom', ' the', ' bedroom', ' office', '<|begin_of_text|>', '<|eot_id|>', ' football', ' doctor', ' bathroom', 'office', ' hallway', ' bedroom', ' football', ' office', ' bedroom'], 'evidence_proportions': [0.019606783986091614, 0.0006018181641896566, 0.0014141698678334553, 0.0006865262985229492]}}, 25: {'grad': {'score': [0.47646175731312146, 0.3599738000779956, 0.39140181107954547, 0.35911093827976365, 0.29453586626656447], 'topk_tokens': [' the', ' very', ' location', 'human', ' the', 'the', ' web', ' the', ' great', ' the', ' the', ' clerk', ' the', ' the', ' location', 'ible', ' do', ' location', ' location', ' actions'], 'evidence_proportions': [0.5169677734375, 0.5423990885416666, 0.4415760040283203, 0.4184061686197917]}, 'weight': {'score': [0.03277588974345814, 0.007309102549574351, 0.004669442321314957, 0.007195689191297054, 0.001506259924248804], 'topk_tokens': ['.\n\n', '<|start_header_id|>', '\n\n', ' \n', ' Senator', 'Answer', ' football', '<|eot_id|>', ' Mary', '<|eot_id|>', ' office', ' doctor', ' football', '.', ' the', ':', 'assistant', 'office', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.08993911743164062, 0.0028131604194641113, 0.05249901612599691, 0.004906674226125081]}, 'saliency': {'score': [0.0015394172885201194, 6.882109934200145e-05, 8.647369615959398e-05, 6.092664779432668e-05, 4.97231000586401e-05], 'topk_tokens': [' Paul', '<|start_header_id|>', ' way', ' football', ':', ' the', ' \n', 'cap', ' random', ' boat', '.', 'assistant', '<|eot_id|>', '<|begin_of_text|>', ' office', ' Senator', '<|eot_id|>', ' doctor', '<|end_header_id|>', ' football'], 'evidence_proportions': [0.006861984729766846, 0.0001465280850728353, 0.0008615354696909586, 6.181001663208008e-05]}}, 26: {'grad': {'score': [0.43220658735795453, 0.41318019761355573, 0.47111464991714014, 0.41262161240253104, 0.3824008989937698], 'topk_tokens': ['\n', ' generally', ' presses', ' dispatch', 'str', '.', '\n', '.', 'ol', ' press', '\n', ' printers', ' STR', ' Ch', 'Penn', 'ER', ',\n', ' Guards', 'burn', ' Gutenberg'], 'evidence_proportions': [0.1815948486328125, 0.4331258138020833, 0.4461568196614583, 0.58441162109375]}, 'weight': {'score': [0.026857208121906628, 0.007233155148796982, 0.009793584997003729, 0.007109419925786619, 0.0014550482170491278], 'topk_tokens': ['<|start_header_id|>', ' bedroom', ' bathroom', '<|eot_id|>', ' bathroom', ' hallway', ' the', '<|eot_id|>', ' football', ' bedroom', 'Bridge', 'Answer', ' office', ' the', ' \n', 'assistant', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.03253650665283203, 0.004005273183186849, 0.061187426249186196, 0.011592725912729899]}, 'saliency': {'score': [0.0012640790505842729, 0.0001795668677570748, 0.0004703258023117528, 0.00017154809669703102, 4.511618915992447e-05], 'topk_tokens': [' formally', '?', ' Senator', ' Gal', ' hallway', ' \n', ' office', ' kitchen', 'cap', ' the', ' bedroom', '<|begin_of_text|>', ' the', 'Answer', ' bedroom', '<|end_header_id|>', 'Bridge', ' office', ':', 'office'], 'evidence_proportions': [0.000840112566947937, 0.0007339219252268473, 0.002502302328745524, 0.0008386572202046713]}}, 27: {'grad': {'score': [0.4468314430930398, 0.4158749279187367, 0.45059157862807764, 0.41543708208766217, 0.2876778131798853], 'topk_tokens': ['.', '.', ' hour', ' issued', '.', ',', 'ulation', 'itated', ',', 'I', '.', ',', ' hour', ',', ' each', ' other', ' it', ' decision', '.', ' hour'], 'evidence_proportions': [0.5186767578125, 0.18448384602864584, 0.5034586588541666, 0.6046549479166666]}, 'weight': {'score': [0.02842478860508312, 0.007358825373147604, 0.00843988223509355, 0.007239188252450865, 0.0017460388473317593], 'topk_tokens': ['?', '<|eot_id|>', ' before', ' hallway', ' bathroom', ' \n', 'Bridge', 'Answer', ' bathroom', ' football', ' the', ' bedroom', ' office', ' bedroom', 'assistant', '.\n\n', '<|end_header_id|>', ':', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.05069446563720703, 0.005897204081217448, 0.04569902022679647, 0.018831690152486164]}, 'saliency': {'score': [0.0017585591836409135, 0.00018233974577265482, 0.000793744217265736, 0.0001691918130803314, 7.87814961204046e-05], 'topk_tokens': [' the', ' bathroom', ' upper', '.', '<|eot_id|>', '.', ' the', ' the', ' the', 'Bridge', ' bedroom', ' bedroom', ' the', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', ':', ' office', '.\n\n', 'office'], 'evidence_proportions': [0.001678183674812317, 0.0011471410592397053, 0.003142197926839193, 0.0010399222373962402]}}, 28: {'grad': {'score': [0.3346002752130682, 0.26677269774293233, 0.2970835367838542, 0.2661752709949234, 0.270918447760087], 'topk_tokens': [' completed', "'clock", '26', ' line', ' and', ' fore', 'antically', ' bend', "'clock", ' at', 'in', 'hom', ' be', ' reached', ' else', ' item', "'clock", ' out', ' half', 'half'], 'evidence_proportions': [0.366455078125, 0.315460205078125, 0.3934529622395833, 0.273651123046875]}, 'weight': {'score': [0.03759287704120983, 0.0070739621554307686, 0.012583337046883324, 0.00686944981149379, 0.001382053652896157], 'topk_tokens': ['.\n\n', '?', '<|eot_id|>', ' the', 'Answer', ' the', ' \n', 'Bridge', ' football', ' the', ' the', ' before', ' bathroom', '<|eot_id|>', 'assistant', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.021099090576171875, 0.0008783737818400065, 0.11959898471832275, 0.003297130266825358]}, 'saliency': {'score': [0.00206839767369357, 7.970957206670121e-05, 0.0003976731589346221, 6.670766971712487e-05, 2.3736229425744164e-05], 'topk_tokens': [' bedroom', ' the', ' the', ' bedroom', ' bathroom', ' the', ' bedroom', ' the', ' the', 'office', ' bathroom', ' was', '<|eot_id|>', ' the', '.\n\n', 'Bridge', '<|end_header_id|>', '<|begin_of_text|>', ' the', ':'], 'evidence_proportions': [0.001025080680847168, 1.8845001856486004e-05, 0.006703555583953857, 0.00017833709716796875]}}, 29: {'grad': {'score': [0.3234405517578125, 0.4602645682701975, 0.32243705518317944, 0.4620762739977009, 0.47656399690652196], 'topk_tokens': ['E', ' regular', '\n', ' in', ' message', ' an', ' were', ' mailing', ',', "'", ' message', ' in', ' were', ' mail', '\n', ' of', '\n', 'mail', '\n', ' of'], 'evidence_proportions': [0.2867584228515625, 0.370880126953125, 0.24521382649739584, 0.3786824544270833]}, 'weight': {'score': [0.013851309364492243, 0.007239290070020479, 0.010866687153324936, 0.007175730978882553, 0.0008568190321137633], 'topk_tokens': ['.', '<|eot_id|>', ' the', '<|eot_id|>', '?', '.\n\n', ' bathroom', ' football', ' \n', ' to', ' the', 'Answer', ' the', ' before', ' the', 'assistant', 'office', '<|end_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0125732421875, 0.0014601945877075195, 0.03818597396214803, 0.0027598043282826743]}, 'saliency': {'score': [0.0002293695103038441, 4.9942050782224655e-05, 0.00022593230912179658, 4.7603823880335514e-05, 3.569035590449466e-05], 'topk_tokens': [' was', ':', '***', ' the', ' office', ' the', ' to', ' Do', ' the', '?', ' the', 'Answer', '.', ' the', 'assistant', ':', '<|end_header_id|>', ' before', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0002745985984802246, 3.8335720698038735e-05, 0.0005792180697123209, 4.040201505025228e-05]}}, 30: {'grad': {'score': [0.5436061512340199, 0.40398227031952333, 0.5309458067922881, 0.4022417329073181, 0.4374233197562302], 'topk_tokens': [' In', ' Note', 'the', 'ch', ' Queen', ' Date', ' Knowledge', 'Bridge', ' Bench', 'Civil', ' Collection', 'outs', ' Associated', ' Online', 'Today', 'ed', ' Falls', ' Out', ' Bank', ' Clean'], 'evidence_proportions': [0.5056076049804688, 0.43648529052734375, 0.53924560546875, 0.680419921875]}, 'weight': {'score': [0.03756461360237815, 0.007023766894190481, 0.01737667213786732, 0.006780827176879488, 0.003999106491668315], 'topk_tokens': [' bathroom', '<|eot_id|>', ' before', ' bedroom', '<|start_header_id|>', ' the', 'Answer', ' the', ' the', ' football', '.\n\n', '?', ' bathroom', 'assistant', ' \n', ' the', '<|end_header_id|>', 'office', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.037522315979003906, 0.00448155403137207, 0.09442524115244548, 0.01381524403889974]}, 'saliency': {'score': [0.0013246021487496116, 0.00020040470515409766, 0.0008799451770204487, 0.00018910145028096947, 0.00020675493192069138], 'topk_tokens': ['4', ' the', ' bedroom', ' John', ' Joseph', 'Answer', 'assistant', ' football', 'Bridge', '<|start_header_id|>', ' double', ' bathroom', ' the', ' \n', ' bathroom', '<|end_header_id|>', 'office', ' office', '<|begin_of_text|>', ':'], 'evidence_proportions': [0.003173127770423889, 0.000798116127649943, 0.0013382633527119954, 0.0006050765514373779]}}, 31: {'grad': {'score': [0.4155675714666193, 0.3592139145485865, 0.44446482802882337, 0.3582424235138203, 0.33017993878714647], 'topk_tokens': ['\n', 'RI', ' Gal', ' to', ' an', '\n', ' the', ' by', ' an', '\n', ' on', 'inen', '\n', ' in', ' in', ' an', ' to', ' the', ' an', ' in'], 'evidence_proportions': [0.4093971252441406, 0.46807861328125, 0.35154978434244794, 0.4311879475911458]}, 'weight': {'score': [0.005523703315041282, 0.006693186420398258, 0.0032657150066260137, 0.006726464269145223, 0.0007955182956743844], 'topk_tokens': [' was', ' bathroom', '<|eot_id|>', ' the', ':', '.\n\n', '<|start_header_id|>', ' before', '?', ' Where', 'Answer', ' football', ' \n', ' the', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.010735511779785156, 0.0008943478266398112, 0.009857654571533203, 0.0023445685704549155]}, 'saliency': {'score': [0.00027227672663601964, 0.00010965239615312896, 0.00012488256801258433, 0.00010867436860231745, 2.0777122883857052e-05], 'topk_tokens': [' boat', ' Daniel', '.\n\n', ' the', ' bathroom', ' hallway', ' bedroom', 'office', ' bedroom', ' the', 'Answer', ':', ' office', ' the', ' \n', ' the', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0005837231874465942, 0.00027372439702351886, 0.00024688243865966797, 8.85923703511556e-05]}}, 'pred_res': 'the office<|eot_id|>', 'score': 100}
2025-01-23 22:18:47.868 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:18:47.868 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-2_pid-4_6-7-8-9.pkl | len: 10 |  size: 9.09 KB
Processing depth (6, 7, 8, 9):   5%|▌         | 5/100 [01:16<24:21, 15.38s/it]Processing depth (6, 7, 8, 9):   5%|▌         | 5/100 [01:16<24:10, 15.27s/it]
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:99 - Selected idx: 3
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:101 - Answer: office
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:103 - Needle: [' Sandra journeyed to the bedroom.', ' Mary journeyed to the office.', ' John went back to the bedroom.', ' Mary picked up the apple.', ' John journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:104 - Real Needle: [' Mary journeyed to the office.', ' Mary picked up the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-23 22:18:48.071 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.22s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
Processing depth (0, 1, 2, 3, 8):   0%|          | 0/100 [00:11<?, ?it/s]2025-01-23 22:18:59.298 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:18:59.299 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:18:59.299 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the apple.
2025-01-23 22:18:59.301 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (476, 481) --> . Mary picked up the
2025-01-23 22:18:59.301 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:18:59.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1040, 1046) -->  cable. Mary journeyed to
2025-01-23 22:18:59.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:18:59.314 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1510, 1514) -->  Mary dropped the apple
2025-01-23 22:18:59.314 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:18:59.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2258, 2264) --> . Daniel went back to the
2025-01-23 22:18:59.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:18:59.328 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (426, 432) --> . Sandra journeyed to the
2025-01-23 22:18:59.328 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:18:59.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (916, 922) --> . John went back to the
2025-01-23 22:18:59.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:18:59.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:18:59.333 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:18:59.345 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2258, 2264) --> . Daniel went back to the
2025-01-23 22:18:59.345 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:18:59.356 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2237, 2242) -->  John moved to the bedroom
2025-01-23 22:18:59.356 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:18:59.366 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2158, 2162) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:19:00.027 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary picked up the apple.<|eot_id|>
2025-01-23 22:19:00.027 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4245])
your chose emoji: ['👩🏽\u200d🔬', '💇🏽', '🛡', '🤽🏿', '🏃🏻\u200d➡️', '👨🏼\u200d🍳', '🙋🏻\u200d♀️', '💈', '👩🏽\u200d❤️\u200d💋\u200d👨🏼', '🏃\u200d♂\u200d➡️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4247, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 216480.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.15it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 134.45it/s]
2025-01-23 22:19:03.146 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3808675695348669, 0.19717231177430392, 0.2944326111764619, 0.19522118642445763, 0.19828940857024419], 'topk_tokens': ['\n\n', ' apple', ' as', 'user', '.', 'ed', ' the', ' wield', ' December', ' to', ' Date', ' may', ' up', ' politically', ' ', 'Mary', ' Knowledge', ' journey', ' office', '202'], 'evidence_proportions': [0.7064412434895834, 0.433123779296875, 0.4017537434895833, 0.13067054748535156, 0.15765889485677084]}, 'weight': {'score': [0.00468652778201633, 0.0075282152327251686, 0.005106741731817072, 0.007565624878461759, 0.00758301644098191], 'topk_tokens': [' Do', ' to', ' was', ' was', 'Answer', ' an', ' or', ':', ' the', ' the', ' the', ' Where', ' it', ':', ' the', ' the', 'Question', ' the', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0006081859270731608, 0.0004791259765625, 0.007745583852132161, 0.005696296691894531, 0.00853880246480306]}, 'saliency': {'score': [7.83050501788104e-05, 7.17451579716168e-05, 7.131966677579013e-05, 7.170620982732925e-05, 8.576398804074241e-05], 'topk_tokens': [' the', ' the', '<|end_header_id|>', ' in', ' information', ' One', ' office', ' gang', ' one', '<|eot_id|>', ' in', 'Question', ' and', ' for', ' bill', ' the', '.', ' as', ' the', ' and'], 'evidence_proportions': [0.00010722875595092773, 2.4044513702392577e-05, 8.701284726460774e-05, 3.065168857574463e-05, 0.0001176595687866211]}}, 25: {'grad': {'score': [0.3789378978587963, 0.4165354370088445, 0.40008406205610797, 0.41690754805028024, 0.5090495291210356], 'topk_tokens': [',', ',', 'asca', '\n', ',', '\n', '�', 'Cut', ',', 'asca', '      ', ' ', ' ', '\n', ' ', ' the', '.', '�', ' to', ' or'], 'evidence_proportions': [0.3856404622395833, 0.2463623046875, 0.3711140950520833, 0.38958740234375, 0.4834391276041667]}, 'weight': {'score': [0.0050441865567807794, 0.007522560552566057, 0.004948720787510727, 0.007558828240679988, 0.007989934512547084], 'topk_tokens': [' the', '�', 'es', 'gro', ' of', ' the', '0', ':', 'until', 'advance', ' it', 'cery', 'Question', 'was', '.', 'office', ' the', ' O', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.00046185652414957684, 0.000354766845703125, 0.009933869043986002, 0.006706714630126953, 0.00753633181254069]}, 'saliency': {'score': [7.933819735491718e-05, 0.0001161900693903762, 8.535023891564572e-05, 0.00011667077513449455, 0.00010979033651806059], 'topk_tokens': [' a', ' of', ' the', ':', ' moved', 'ed', ' Where', ' the', 'advance', 'burn', 'hours', 'nes', ' the', ' paper', ' it', ' the', ' Min', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [1.806020736694336e-05, 1.068115234375e-05, 9.990731875101726e-05, 0.00012892484664916992, 0.00014420350392659506]}}, 26: {'grad': {'score': [0.3152053267867477, 0.3763919522236284, 0.3521340110085227, 0.3769777059725881, 0.3328635806129092], 'topk_tokens': [' for', ' ', '\n', ' ', '\n', ' John', '\n', '\n', '\n', '\n', '3', '\n', '\n', ' for', '\n', '\n', '\n', ' The', '202', ' journey'], 'evidence_proportions': [0.22743733723958334, 0.1715545654296875, 0.4432779947916667, 0.335662841796875, 0.3809712727864583]}, 'weight': {'score': [0.005274357619108977, 0.007531876336105127, 0.006056720560247248, 0.007558060512355953, 0.008801358086722237], 'topk_tokens': [' Min', 'Republicans', 'a', ' it', ' moved', 'was', ' the', '.', 'fax', 'tele', ' the', 'Question', '�', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0012499888737996419, 0.0016786575317382813, 0.007568041483561198, 0.005378246307373047, 0.009932200113932291]}, 'saliency': {'score': [7.837127756189418e-05, 0.00011398528698390586, 0.00012672308719519413, 0.00011411455157607729, 0.00013754410403115408], 'topk_tokens': [' extraordinary', ' very', '.', ' passed', ' arranged', ' bill', ' the', '\n', ' rival', ' the', ' temper', '.', '<|eot_id|>', ' legislative', ' was', ' the', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [3.9964914321899414e-05, 2.6404857635498047e-05, 9.248654047648112e-05, 3.74913215637207e-05, 0.0001732210318247477]}}, 27: {'grad': {'score': [0.31049120867693863, 0.2244731776428582, 0.32602125225645123, 0.22311813267028385, 0.20330301920572916], 'topk_tokens': [' to', ' ', '?\n', ',', '.\n', ' the', ',', '.\n', ',', ' Knowledge', ' the', '.\n\n', ' the', ' the', 'system', 'ting', '\n\n', 'Cut', ' Date', '.'], 'evidence_proportions': [0.5123125712076823, 0.39190673828125, 0.18790690104166666, 0.2037353515625, 0.23457845052083334]}, 'weight': {'score': [0.005559214839228877, 0.007528762691633432, 0.0047444358016505385, 0.007563408160796164, 0.008886127244858514], 'topk_tokens': ['was', 'gro', 'eward', ' a', 'tele', 'iscal', 'advance', 'cery', ' the', ' the', 'tele', ' in', '186', ' O', '�', 'assistant', ' Senator', '0', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0010026295979817708, 0.00124053955078125, 0.012279351552327475, 0.005549907684326172, 0.00700076421101888]}, 'saliency': {'score': [0.0001431195824234574, 0.0001580771598250046, 0.00014493682167746803, 0.00015827718030475392, 0.0002737151724951608], 'topk_tokens': [' interest', '\n', '<|start_header_id|>', ' some', '\n', ' legislative', ',', '\n', ' very', ' the', ' temper', 'year', ',', ' bill', ' passed', ' One', ' the', ':\n\n', '\n', ' rival'], 'evidence_proportions': [3.989537556966146e-05, 0.00012748241424560548, 0.0001311798890431722, 1.919269561767578e-05, 0.00035393238067626953]}}, 28: {'grad': {'score': [0.47626636646412035, 0.26687206019506415, 0.4571470780806108, 0.26402211468288656, 0.3090308053152902], 'topk_tokens': [' journey', ',', ' to', ' If', ',', ',', ' a', ' the', '<|eot_id|>', 'ed', '\n', 'Cut', ':', ',', ' PA', ' journey', ' office', ' the', 'ed', ' the'], 'evidence_proportions': [1.0128580729166667, 0.70029296875, 0.247528076171875, 0.08123779296875, 0.24507649739583334]}, 'weight': {'score': [0.007036482846295392, 0.0075173437497125725, 0.005817740252523711, 0.0075338400859436445, 0.009454298587072463], 'topk_tokens': ['ed', 'fax', 'advance', 'gro', '186', ' the', 'Republicans', '0', 'was', ' Senator', 'cery', ' in', '�', ' to', ' O', '�', '.', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005269845326741537, 0.0006123065948486328, 0.01751534144083659, 0.007151126861572266, 0.008344173431396484]}, 'saliency': {'score': [5.856487486097548e-05, 8.131593486856847e-05, 4.607619661273378e-05, 8.174038912761972e-05, 0.0001046849148614066], 'topk_tokens': ['️', ' veto', 'polit', ' Paul', ' joke', 'hours', ' cap', 'until', 'graph', ' Times', '.', ' At', 'ian', 'office', '�', 'assistant', ' the', '<|begin_of_text|>', '<|end_header_id|>', 'ed'], 'evidence_proportions': [1.52587890625e-05, 3.7050247192382814e-05, 0.0001404285430908203, 3.33935022354126e-05, 5.4717063903808594e-05]}}, 29: {'grad': {'score': [0.7208670156973379, 0.5746316678501001, 0.6804060502485796, 0.5728549998274047, 0.347372191292899], 'topk_tokens': [' the', ' the', '.', ',', ' The', ',', 'ed', ',', '26', ' journey', ' Jul', ' to', 'ting', 'user', 'ed', ' journey', 'system', ' office', ' the', 'Cut'], 'evidence_proportions': [1.2389322916666667, 0.4511962890625, 0.708343505859375, 0.654052734375, 0.4845937093098958]}, 'weight': {'score': [0.011188621874208804, 0.007505227796942143, 0.0051181587305935946, 0.007500289091210904, 0.010475519157591321], 'topk_tokens': ['186', ' as', ' the', ' O', 'Republicans', ' Senator', ' the', ' a', 'cery', 'was', '186', ' president', '�', '0', '<|end_header_id|>', '0', 'assistant', 'tele', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0009144941965738932, 0.0006259441375732422, 0.038314501444498696, 0.005370140075683594, 0.007018089294433594]}, 'saliency': {'score': [0.00012735746524952077, 0.0001988640414256614, 0.00016209212216463956, 0.00019961497309329204, 0.00025830595266251334], 'topk_tokens': ['.', '<|end_header_id|>', ',', '<|eot_id|>', '.', 'tele', '<|eot_id|>', '\n', '\n', ' some', 'system', ' PA', '.', '.', 'assistant', '\n', ':\n\n', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [8.484721183776855e-05, 3.8564205169677734e-05, 0.00014688571294148764, 6.149709224700928e-05, 0.0002682407697041829]}}, 30: {'grad': {'score': [0.5942654079861112, 0.5279947150194255, 0.5619391239050663, 0.5272998322385976, 0.7118652888706752], 'topk_tokens': [' were', '�', ' bogus', '�', ' journey', '\u200d', 'extent', 'description', '\u200d', '�', 'sur', '\u200d', '\u200d', '\u200d', '�', '\u200d', '\u200d', '\u200d', ' genu', ' office'], 'evidence_proportions': [0.8058268229166666, 0.5518310546875, 0.474365234375, 0.51373291015625, 0.5916544596354166]}, 'weight': {'score': [0.013108562540124965, 0.007413710092751593, 0.005251181848121412, 0.007394030707987732, 0.009259365853809175], 'topk_tokens': [' the', 'a', ' the', ' information', ' in', '�', ' was', ' the', 'was', ' interest', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', '.', '<|end_header_id|>', '<|eot_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0018816391626993816, 0.002824878692626953, 0.04376665751139323, 0.005397319793701172, 0.007387955983479817]}, 'saliency': {'score': [0.0005383160379197863, 0.0001672037579127977, 0.0003769379673582135, 0.00016315760064700182, 0.00021711914312271845], 'topk_tokens': ['\n', '\n', ' The', '.\n', '\n', ' five', '.\n\n', '\n', ' journey', '<|start_header_id|>', ' PA', '.', '.\n\n', '<|eot_id|>', ' office', '\n', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.0011012256145477295, 0.00010679960250854492, 0.0009480416774749756, 7.59810209274292e-05, 0.00023350119590759277]}}, 31: {'grad': {'score': [0.12430932786729601, 0.06975681708227867, 0.10545759490042021, 0.06912365647583153, 0.12377845673334031], 'topk_tokens': ['�', ',', ',', ' rely', ',', ',', ',', '26', ' Jul', '�', ',', '202', '202', ':', 'Mary', ' or', '3', ' journey', 'ting', 'user'], 'evidence_proportions': [0.2586568196614583, 0.171734619140625, 0.069549560546875, 0.04904937744140625, 0.055373827616373696]}, 'weight': {'score': [0.008470756036263925, 0.006701518149271499, 0.004057118386933298, 0.006710951101076697, 0.002998210134960356], 'topk_tokens': ['.\n\n', ' apple', '?\n', ' the', ' discarded', '.', ',', ' was', ' or', ' left', ' dropped', ':', '<|start_header_id|>', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.00795125961303711, 0.001275014877319336, 0.02273281415303548, 0.002414226531982422, 0.004762331644694011]}, 'saliency': {'score': [0.0007938588107073749, 0.00013537076548573437, 0.0003516601793693774, 0.00012941979154755797, 6.845380578722273e-05], 'topk_tokens': [' the', ':', ' a', 'system', ' the', ' the', ' left', ' Senator', '<|end_header_id|>', ' was', ' it', 'ye', ' dropped', 'assistant', '<|eot_id|>', ' journey', '<|begin_of_text|>', ' office', 'office', '<|eot_id|>'], 'evidence_proportions': [0.0016422172387440999, 5.735158920288086e-05, 0.0017261505126953125, 7.332861423492432e-05, 0.00010731816291809082]}}, 'pred_res': 'Mary picked up the apple.<|eot_id|>', 'score': 0}
2025-01-23 22:19:03.153 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:19:03.153 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-3_pid-0_0-1-2-3-8.pkl | len: 10 |  size: 8.9 KB
Processing depth (0, 1, 2, 3, 8):   1%|          | 1/100 [00:14<24:44, 15.00s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Processing depth (0, 2, 3, 6, 7):   1%|          | 1/100 [00:25<24:44, 15.00s/it]2025-01-23 22:19:13.424 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:19:13.425 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:13.425 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the apple.
2025-01-23 22:19:13.430 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (956, 961) --> . Mary picked up the
2025-01-23 22:19:13.430 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:19:13.438 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1505, 1511) --> . Mary journeyed to the
2025-01-23 22:19:13.438 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:19:13.449 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2375, 2379) -->  Mary dropped the apple
2025-01-23 22:19:13.450 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:19:13.461 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2251, 2257) -->  went back to the kitchen.
2025-01-23 22:19:13.461 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:19:13.464 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (426, 432) --> . Sandra journeyed to the
2025-01-23 22:19:13.464 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:19:13.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (883, 889) -->  John went back to the bedroom
2025-01-23 22:19:13.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:19:13.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:13.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:19:13.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2250, 2256) -->  Daniel went back to the kitchen
2025-01-23 22:19:13.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:19:13.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2240, 2245) --> . John moved to the
2025-01-23 22:19:13.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:19:13.502 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2163, 2167) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:19:14.163 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary picked up the apple.<|eot_id|>
2025-01-23 22:19:14.163 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4230])
your chose emoji: ['👋', '❌', '🧎🏻\u200d♀', '👩🏻\u200d✈', '\U0001faf1🏾', '🟧', '🦸🏽\u200d♂', '🐗', '👳🏽\u200d♂️', '👨🏽\u200d❤️\u200d👨🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4232, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.83it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.60it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.23it/s]
2025-01-23 22:19:17.049 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7059124487417715, 0.472057320895853, 0.5675427985913826, 0.46978859864853945, 0.3946591805720675], 'topk_tokens': [' daily', '***', ',', ' city', ' Mary', '\n\n', ' tele', ' A', ' the', ' Paul', ' Mary', 'ISC', 'RI', ':', 'user', ' ', 'ine', 'Mary', '202', ' office'], 'evidence_proportions': [1.16973876953125, 0.6166748046875, 0.35454384485880536, 1.00537109375, 0.4681803385416667]}, 'weight': {'score': [0.013919903172387017, 0.007550816004127546, 0.002228018009301388, 0.00755169974820383, 0.009184843388156614], 'topk_tokens': ['ely', ' Do', '�', '<|end_header_id|>', 'es', '\n', 'and', 'ian', 'ot', ' not', 'us', '8', ' where', '.', '�', ' else', 'ot', 'nes', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00011479854583740234, 0.022764933109283448, 0.03494413693745931, 0.0029169321060180664, 0.006665229797363281]}, 'saliency': {'score': [0.00042616879498517073, 0.00023507384586875064, 0.00015833702954378995, 0.00023444411224281112, 0.00026629282080608863], 'topk_tokens': [' return', ' was', ' anything', '<|eot_id|>', ' during', ' carrier', ' the', 'of', ' boat', ' which', 'assistant', ' else', ' to', ' was', ' as', ' city', ' to', ' Mary', '<|end_header_id|>', 'office'], 'evidence_proportions': [8.266170819600423e-05, 0.0002658247947692871, 0.0004520416259765625, 0.0010781288146972656, 0.00044278303782145184]}}, 25: {'grad': {'score': [0.6749213889793113, 0.5550205396569293, 0.6433560920484138, 0.5535458521783523, 0.6595795120018116], 'topk_tokens': ['ucci', '.', '      ', ' will', '.', '.', '.', '<|start_header_id|>', '.', ' and', '-text', ' I', ' I', ' Jackson', '      ', 'ine', '.', '\n\n', ' *', ' to'], 'evidence_proportions': [0.9355061848958334, 0.775146484375, 0.6322428385416666, 0.4832763671875, 0.501257578531901]}, 'weight': {'score': [0.017855675132186326, 0.0075559611581899715, 0.001634081204732259, 0.007536145664701526, 0.010040504344995472], 'topk_tokens': ['ian', 'ed', 'and', '\n', 'ot', 'ely', 'es', 'nes', ' where', 'nes', '�', '\n', '8', 'us', 'ian', '�', 'ot', 'office', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [5.3177277247111e-05, 0.0323466420173645, 0.046932498613993325, 0.002266228199005127, 0.004898508389790853]}, 'saliency': {'score': [0.0007458836944014938, 0.00019539310993003483, 7.586406938957445e-05, 0.00019277595090728936, 0.00024974778078604436], 'topk_tokens': ['.', '<|eot_id|>', ' unless', 'ot', ' Mary', '\n', '\n', ' Mary', '�', 'v', '\n', '<|eot_id|>', 'e', ' to', 'ed', '<|begin_of_text|>', ' message', '<|start_header_id|>', 'office', '<|end_header_id|>'], 'evidence_proportions': [6.020069122314453e-06, 0.000781404972076416, 0.002072155475616455, 0.0007534027099609375, 0.0001248617966969808]}}, 26: {'grad': {'score': [1.1743435329861112, 1.212202162282904, 1.2493526574337122, 1.2121533168013692, 1.1427886520606885], 'topk_tokens': ['\n', ' I', '\n', ' could', '.', ' I', '\n', ' their', ' a', '\n', ' Mary', ' The', ' a', ' There', '\n', ' it', ' The', ' first', '\n', '\n'], 'evidence_proportions': [0.93804931640625, 0.984033203125, 0.6353556315104166, 2.08642578125, 1.5001627604166667]}, 'weight': {'score': [0.01230657321435434, 0.007554281411414336, 0.0028265263095046535, 0.007560921881137164, 0.009835230267566183], 'topk_tokens': ['�', ' not', '8', 'ely', ' not', ' else', '.', 'and', 'ian', 'es', 'us', 'ot', '�', 'ot', '<|start_header_id|>', 'nes', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00021548072497049967, 0.020773792266845705, 0.026601791381835938, 0.0037665367126464844, 0.008739789326985678]}, 'saliency': {'score': [0.0002099628801699038, 0.00021124566378800765, 0.0001403797756541859, 0.00021181450594191583, 0.0004255106483680614], 'topk_tokens': ['assistant', ' heard', '�', 'far', ' to', ' sounded', '�', '�', ' of', '.', ' boat', '<|end_header_id|>', 'icians', ' illustrated', ' daily', 'for', ' in', ' out', ' from', '<|start_header_id|>'], 'evidence_proportions': [2.1070241928100586e-05, 0.00018540620803833007, 0.00010641415913899739, 0.00029380619525909424, 0.00046697258949279785]}}, 27: {'grad': {'score': [0.8185000949435763, 0.4771967637291027, 0.6355442856297349, 0.47373543626852943, 0.4529652249985847], 'topk_tokens': [' the', '.', ':\n\n', ' the', ' ', ' the', 'In', 'ol', ' the', ' The', ' in', ' in', ',', 'Mary', 'P', '\n\n\n', '.', ' had', 'The', ' Mary'], 'evidence_proportions': [1.00897216796875, 0.5418212890625, 0.46728515625, 1.4740943908691406, 0.7727457682291666]}, 'weight': {'score': [0.01805623593153777, 0.0075566664985988454, 0.0018365202528057675, 0.0075339619088150055, 0.009883084158966507], 'topk_tokens': ['nes', '�', 'ian', 're', ' up', '.', 'es', '\n', 'ian', ' where', 'and', '�', 'ely', 'ot', 'nes', '8', 'ot', 'us', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.00018283724784851074, 0.03527379035949707, 0.04349835713704427, 0.00162506103515625, 0.007093667984008789]}, 'saliency': {'score': [0.0004969150931746872, 0.0004402255966893216, 0.00025455879442619556, 0.00044132731962066824, 0.0009966622228207796], 'topk_tokens': [' river', '      ', '�', ' turned', '      ', ' the', ' out', ' *', ' laying', ' *', ' to', ' arrival', '♀', '�', '<|end_header_id|>', ' or', ' and', ' daily', '<|start_header_id|>', 'office'], 'evidence_proportions': [1.491109530131022e-05, 0.0006023406982421875, 0.0008871555328369141, 0.0002383887767791748, 0.0006731748580932617]}}, 28: {'grad': {'score': [1.2492810002079717, 1.115543012132266, 1.7116625236742424, 1.1099622667921585, 1.0673270847486414], 'topk_tokens': [' It', 'Cut', ',', '.', ',', ' the', ',', ' PA', ' the', ' to', '-text', '.', ',', ',', ',', '.', ',', ',', ' the', 'ed'], 'evidence_proportions': [2.3876953125, 0.74356689453125, 0.49011675516764325, 1.0305862426757812, 1.437255859375]}, 'weight': {'score': [0.02410565040729664, 0.0075303696955084124, 0.0014243089791500206, 0.007471397361956529, 0.010663808255955793], 'topk_tokens': ['ed', '185', ' up', 'nes', 'ab', 'ely', '.', ' where', 'and', 're', ' of', '\n', 'us', 'ot', '�', '8', 'ian', '<|end_header_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [5.497535069783529e-05, 0.050931882858276364, 0.06045909722646078, 0.0008129477500915527, 0.004976153373718262]}, 'saliency': {'score': [0.00015338041164256907, 0.00012199887706561891, 3.8826104366418086e-05, 0.0001224536709451721, 0.0002071796983912371], 'topk_tokens': [' was', 'rail', 'ot', ' news', ' of', '.', ' Foster', 'ian', ' copy', ' where', 'ot', 'road', '\n', 'assistant', '�', '<|eot_id|>', '<|begin_of_text|>', '�', 'office', '<|end_header_id|>'], 'evidence_proportions': [4.500150680541992e-06, 0.00027408599853515623, 0.0002943972746531169, 9.632110595703125e-05, 9.86953576405843e-05]}}, 29: {'grad': {'score': [1.2860028302228008, 1.0477679131837783, 1.322918516216856, 1.0440497174359007, 1.3041481349779211], 'topk_tokens': [' the', ' from', ' and', '�', ' speakers', ' *', ' Adams', ' Jackson', 'made', ' *', 'aths', ' happened', '.', ' daily', '.', '�', '�', '�', '️', ' Eagle'], 'evidence_proportions': [2.2522786458333335, 0.724566650390625, 0.37481689453125, 1.44677734375, 1.5915934244791667]}, 'weight': {'score': [0.02858554654651218, 0.007543991103289484, 0.001332882678869999, 0.007456945221467535, 0.011635903862939365], 'topk_tokens': ['and', ' up', 'and', 'nes', '.', '.', 're', 'nes', 'ast', 'ot', 'nes', 'ely', 'ian', '\n', '�', 'us', '8', ' where', 'ot', '<|begin_of_text|>'], 'evidence_proportions': [0.0001196761926015218, 0.06226158142089844, 0.0690150260925293, 0.001162111759185791, 0.006840864817301433]}, 'saliency': {'score': [0.0003141959508260091, 0.00017521206451498944, 9.49520053285541e-05, 0.00017494744731047238, 0.000342756077863168], 'topk_tokens': ['<|start_header_id|>', ' else', '      ', ' arrival', 'Republicans', '      ', '-text', '\n', ' now', ' with', ' gun', ' ever', ' *\n\n', '<|eot_id|>', ' ahead', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [3.0289093653361004e-05, 0.0005671620368957519, 0.00014799833297729492, 0.00038070976734161377, 0.0005091528097788492]}}, 30: {'grad': {'score': [1.1958369502314814, 0.9183858427974362, 1.1474997780539773, 0.91477799529883, 0.9120279947916666], 'topk_tokens': [' there', ' Papers', 'man', ' man', ' Mr', ' em', ' INCIDENT', ' PA', 'RI', ' Paul', '<|start_header_id|>', '25', 'men', ' the', ' a', 'AILY', ' to', '185', ' the', ' office'], 'evidence_proportions': [2.0730794270833335, 1.193408203125, 0.7034505208333334, 1.1650390625, 0.8335367838541666]}, 'weight': {'score': [0.02213797745881257, 0.007418811884629478, 0.0034422097784100156, 0.007355008049299253, 0.009714631066806074], 'topk_tokens': ['.', 're', 'ian', 'ast', 'ian', '�', ' up', 'nes', 'ot', '<|eot_id|>', '<|start_header_id|>', 'ot', 'ely', ' where', '8', 'us', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0003998676935831706, 0.05615081787109375, 0.03654936949412028, 0.012729644775390625, 0.00739288330078125]}, 'saliency': {'score': [0.0007588642614859122, 0.0003516248696693175, 0.00021149714787801108, 0.0003500977247124779, 0.00043589135874872625], 'topk_tokens': [' could', ' Eagle', ' city', ' laying', ' of', ' and', ' *', ' both', ' Mary', ' turned', ' Fourth', '<|eot_id|>', '<|end_header_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'office', '<|end_header_id|>'], 'evidence_proportions': [0.00015528003374735513, 0.002128887176513672, 0.0004360477129618327, 0.001019597053527832, 0.00036975741386413574]}}, 31: {'grad': {'score': [1.0092140480324074, 1.4571530895557656, 1.2577396739612927, 1.4616293591712375, 1.4658393859863281], 'topk_tokens': [' laid', ' ', ' laying', '\n\n\n\n\n\n\n', ' participate', ' sounded', 'user', ' heard', ' successful', ' Eagle', ' ', 'ine', ' paper', ',', ',', ' river', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.7466227213541666, 0.74560546875, 0.5157877604166666, 1.3856201171875, 1.7339680989583333]}, 'weight': {'score': [0.021786592624805593, 0.007171582634823083, 0.002440131071842078, 0.007114423629992989, 0.010505849036617556], 'topk_tokens': ['assistant', '\n', ' up', '.', 'ast', 'ely', ' and', 're', 'nes', 'ot', '�', 'ian', 'ot', 'us', ' where', '8', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.0021575291951497397, 0.05881080627441406, 0.03589316209157308, 0.01042032241821289, 0.004033088684082031]}, 'saliency': {'score': [0.0005047806987056026, 0.00033896967120792556, 0.00034254969972552675, 0.00033786827171523187, 0.0004348573477371879], 'topk_tokens': [' in', 'ien', ' another', '�', ' during', ' the', ',\n', 'us', '<|eot_id|>', '8', 'ot', 'assistant', ' where', 'Answer', ' and', 'ian', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0005783140659332275, 0.0005863785743713378, 0.0003724992275238037, 0.0006452500820159912, 0.0004018843173980713]}}, 'pred_res': 'Mary picked up the apple.<|eot_id|>', 'score': 0}
2025-01-23 22:19:17.057 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:19:17.057 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-3_pid-1_0-2-3-6-7.pkl | len: 10 |  size: 8.91 KB
Processing depth (0, 2, 3, 6, 7):   2%|▏         | 2/100 [00:28<23:26, 14.35s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.11s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.17s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Processing depth (0, 2, 3, 4, 6):   2%|▏         | 2/100 [00:40<23:26, 14.35s/it]2025-01-23 22:19:28.454 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:19:28.454 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:28.455 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the apple.
2025-01-23 22:19:28.459 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (960, 965) --> . Mary picked up the
2025-01-23 22:19:28.459 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:19:28.467 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1529, 1535) -->  tragedy. Mary journeyed to
2025-01-23 22:19:28.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:19:28.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1909, 1913) -->  Mary dropped the apple
2025-01-23 22:19:28.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:19:28.489 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2328, 2334) -->  went back to the kitchen.
2025-01-23 22:19:28.489 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:19:28.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (404, 410) --> . Sandra journeyed to the
2025-01-23 22:19:28.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:19:28.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (886, 892) --> . John went back to the
2025-01-23 22:19:28.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:19:28.496 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:28.496 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:19:28.507 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2327, 2333) -->  Daniel went back to the kitchen
2025-01-23 22:19:28.507 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:19:28.519 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2294, 2299) --> . John moved to the
2025-01-23 22:19:28.519 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:19:28.529 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2190, 2194) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:19:29.063 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:19:29.063 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4230])
your chose emoji: ['🦲', '🧑🏻\u200d❤\u200d💋\u200d🧑🏿', '🙆\u200d♂', '⛹🏻\u200d♀', '🪱', '🚶🏻\u200d♀', '🌟', '🕵️\u200d♂️', '👰\u200d♂️', '💆\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4232, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 200924.74it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 119.65it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.22it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 134.28it/s]
2025-01-23 22:19:31.974 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.28816279658564814, 0.1722010881103054, 0.2672019149317886, 0.1706991721433967, 0.19289376079172327], 'topk_tokens': [' leve', ' wield', 'tele', ' PA', ' next', ' ', ' occur', ' journey', ' rejo', ' office', ' Knowledge', ' meet', ' office', '.', ' office', '202', 'Mary', ' lyn', ' satisfaction', ' office'], 'evidence_proportions': [0.6184488932291666, 0.1737548828125, 0.283233642578125, 0.0710601806640625, 0.202880859375]}, 'weight': {'score': [0.007077181780779803, 0.007552095532191949, 0.00429796450065844, 0.0075809088100751544, 0.006994516953178074], 'topk_tokens': ['itol', 'ails', '186', '0', ' the', 'Question', ' Adams', '4', 'polit', ' it', 'assistant', ' to', 'icians', ' Do', ' Gen', ' during', ':', ' which', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003033320109049479, 0.012713241577148437, 0.005888621012369792, 0.010438919067382812, 0.0053717295328776045]}, 'saliency': {'score': [0.00020394501862702546, 5.080179084226179e-05, 0.00012888872262203333, 4.919303343600075e-05, 5.0487725631050445e-05], 'topk_tokens': [' directly', ' Do', ' not', 'Question', ' as', 'system', ' fight', '\n', ':', ' I', ' head', ' PA', ' one', ' of', ' it', ' ', ' journey', '***', ' lyn', ' office'], 'evidence_proportions': [0.0005070269107818604, 7.48276710510254e-05, 0.00023968021074930826, 7.763504981994629e-05, 5.693236986796061e-05]}}, 25: {'grad': {'score': [0.30753354673032407, 0.356546268571302, 0.40337394945549243, 0.35649306387823587, 0.5159115998641305], 'topk_tokens': [' of', ',', ',', ' four', ' and', '♀', '.', '�', ' or', ' of', ',', ' a', ' ', '.', ' o', ' o', '.', ' of', ' or', ' or'], 'evidence_proportions': [0.248321533203125, 0.3546142578125, 0.2484130859375, 0.32110595703125, 0.3775838216145833]}, 'weight': {'score': [0.007172090035897714, 0.007540104294544357, 0.003758690573952415, 0.007572396489597807, 0.007269278816554857], 'topk_tokens': [' Buchanan', ' during', 'us', '186', 'gro', ' Bank', 'I', ' Adams', 'cery', 'Question', ' which', ':', '4', ' of', '0', ' to', ' Gen', 'assistant', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0026486714680989585, 0.018920516967773436, 0.0035764376322428384, 0.010080337524414062, 0.0035619735717773438]}, 'saliency': {'score': [0.00013793397832799842, 8.325442136573431e-05, 9.627053231904001e-05, 8.279759497564797e-05, 5.934722181679546e-05], 'topk_tokens': [' shore', ' discarded', '<|begin_of_text|>', ' Red', '\n\n', ' Where', ' item', ' get', ' within', '\n\n\n', 'ree', '0', '<|end_header_id|>', ' propriet', '186', ' propriet', ' Min', '<|eot_id|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.00026755531628926593, 0.0002331376075744629, 6.352861722310384e-05, 7.24494457244873e-05, 4.7037998835245766e-05]}}, 26: {'grad': {'score': [0.1630096435546875, 0.1553646729438652, 0.18878497499408145, 0.1550508464399943, 0.11925009022588315], 'topk_tokens': [' D', ',', ' Gutenberg', ' remin', ',', ',', ' would', ' Joseph', '\n\n', ',', ' the', ',', ',', ',', ',', ',', ',', ',', ',', ' to'], 'evidence_proportions': [0.2661692301432292, 0.10242919921875, 0.1518834431966146, 0.180084228515625, 0.110076904296875]}, 'weight': {'score': [0.007264349195692275, 0.007555021906169466, 0.0060581438469164296, 0.007568743176336828, 0.008528640304786572], 'topk_tokens': [' Times', ' presidents', 'assistant', '4', 'fax', ' Buchanan', ' Adams', ':', 'itol', '<|end_header_id|>', ' Gen', ' to', 'office', ' which', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003467718760172526, 0.012103271484375, 0.0050919850667317705, 0.009233474731445312, 0.007888158162434896]}, 'saliency': {'score': [7.117898375899704e-05, 7.02191306197125e-05, 8.592822334983133e-05, 7.008866175709155e-05, 6.467795026475105e-05], 'topk_tokens': [',', ' the', 'user', '\n\n', ' to', ' Gutenberg', ' Joseph', ' the', ' and', ' of', ' lyn', '\n', 'UL', ' of', '\n', ' the', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [5.9505303700764976e-05, 8.76307487487793e-05, 6.220738093058269e-05, 8.501112461090088e-05, 6.889303525288899e-05]}}, 27: {'grad': {'score': [0.16826940465856483, 0.15145001474535533, 0.16036155007102273, 0.1512706752934369, 0.18897943911345108], 'topk_tokens': [' looked', ' as', '-text', ' three', ' the', ' the', ' satisfaction', ' the', ' assistance', '\n', ' own', ' had', ' journey', ' the', '4', ' Date', ' o', ' Joseph', ' prepared', 'Today'], 'evidence_proportions': [0.20479329427083334, 0.12627410888671875, 0.13736852010091147, 0.225341796875, 0.1595942179361979]}, 'weight': {'score': [0.007485778243453415, 0.007547335723848109, 0.003550847371419271, 0.007579345831134975, 0.007010763969974241], 'topk_tokens': [' Buchanan', '4', 'cery', ' John', 'us', ' of', 'street', 'L', 'ails', '186', '0', 'polit', ' Wood', 'icians', ' Adams', ' to', 'office', ' which', ' Gen', '<|begin_of_text|>'], 'evidence_proportions': [0.003154118855794271, 0.019987869262695312, 0.003950754801432292, 0.008516311645507812, 0.004247029622395833]}, 'saliency': {'score': [0.0001734119874459726, 0.00015909268409858805, 0.00020817554358280066, 0.00015861177433028545, 0.00017327070236206055], 'topk_tokens': ['user', '<|start_header_id|>', ' PA', 'office', ' had', '\n', 'E', '\n', ' Gutenberg', ' by', 'assistant', ' the', ' Project', ' and', 'ine', 'ucci', 'ol', ' prepared', ' Pa', ' Joseph'], 'evidence_proportions': [0.00023678938547770181, 0.00013996362686157225, 0.00015858809153238931, 2.4005770683288574e-05, 0.0002523362636566162]}}, 28: {'grad': {'score': [0.37505510118272567, 0.307842233005228, 0.4196555397727273, 0.30652282107710727, 0.25764797044836957], 'topk_tokens': [',', ' to', ',', ',', ',', ' PA', ',', ',', ',', ',', ' the', ' a', ',', ' a', ' the', ':', ' the', ',', ' the', ' the'], 'evidence_proportions': [0.5892740885416666, 0.26868133544921874, 0.39111328125, 0.209014892578125, 0.3441162109375]}, 'weight': {'score': [0.007443074826841001, 0.007503474367138118, 0.0024322018478855944, 0.007543978389363275, 0.005963325500488281], 'topk_tokens': ['0', 'gro', '4', '186', 'L', ' of', 'street', ':', ' Buchanan', ' Wood', ' Adams', ' Bank', 'us', ' which', '<|end_header_id|>', ' to', ' Gen', 'assistant', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0015180905659993489, 0.02672920227050781, 0.003139972686767578, 0.006012916564941406, 0.0025528271993001304]}, 'saliency': {'score': [8.88175434536404e-05, 8.838915306588878e-05, 5.784359845248136e-05, 8.862799217465511e-05, 6.395232850226803e-05], 'topk_tokens': ['As', "'s", 'hom', ',', '.', '?\n', ' old', 'P', '.', '<|begin_of_text|>', ' in', ' Min', ' *', 'office', ' not', '4', ' it', ' Newspaper', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [7.29064146677653e-05, 9.682178497314454e-05, 0.0001525282859802246, 6.854534149169922e-06, 8.89897346496582e-05]}}, 29: {'grad': {'score': [0.41727447509765625, 0.2531884523330429, 0.3086755925958807, 0.25168763779725234, 0.3184494903122169], 'topk_tokens': ['user', 'district', ',', ' journey', ',', '�', '\n\n\n\n', 'ting', ',', ',', '\n\n', ',', 'ed', 'system', ' to', ' journey', '.', 'Cut', ' the', ' office'], 'evidence_proportions': [0.81640625, 0.42216796875, 0.31122716267903644, 0.35577392578125, 0.16111246744791666]}, 'weight': {'score': [0.010418538694028501, 0.007493920948194421, 0.0033625111435398912, 0.007507672588754363, 0.005820378013279127], 'topk_tokens': ['186', ' of', 'L', 'street', 'gro', ' Buchanan', ' Wood', ' *', ':', 'cery', 'ot', ' Adams', 'assistant', '0', ' to', ' which', 'us', ' Gen', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.003173828125, 0.04088668823242188, 0.004211107889811198, 0.0033178329467773438, 0.0032143592834472656]}, 'saliency': {'score': [0.0003865626123216417, 0.00017154467466422876, 0.0004121137387824781, 0.0001682502682805633, 0.0002821070560510608], 'topk_tokens': ['�', 'gro', 'user', '.', ',', ',', ' memorable', 'remember', '.', '0', 'E', '�', '<|end_header_id|>', 'estead', ' journey', 'iously', 'ed', 'system', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.00039710601170857746, 0.0004120349884033203, 0.0006232162316640218, 7.380545139312744e-05, 0.0003266433874766032]}}, 30: {'grad': {'score': [0.594546565303096, 0.5763624223734345, 0.5793752959280303, 0.5762209083067034, 0.6831709267436594], 'topk_tokens': [' concerned', '�', '\n', ' seldom', ' genu', ' legislative', '\n', ' bogus', '\n', '-column', '\u200d', ' printers', 'men', 'ivery', '\u200d', ' the', ' the', '\n', '\n', ' office'], 'evidence_proportions': [0.6266988118489584, 0.31110076904296874, 0.7395833333333334, 0.4244384765625, 0.7669677734375]}, 'weight': {'score': [0.012508074442545572, 0.00738409831979099, 0.004738916050304066, 0.007371860462547149, 0.00518694735955501], 'topk_tokens': [' president', ' Buchanan', '4', '<|end_header_id|>', ' Gen', ' Adams', ' journey', '<|eot_id|>', 'assistant', ' to', ':', 'UL', 'us', ' which', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.0035034815470377603, 0.026365089416503906, 0.023981094360351562, 0.003974437713623047, 0.004181226094563802]}, 'saliency': {'score': [0.0020714799563090005, 0.00020459611215302984, 0.0004308133414297393, 0.00019072481964144244, 0.0002060791720514712], 'topk_tokens': [' of', ' their', ' which', ' an', ' fully', '<|end_header_id|>', '<|eot_id|>', '***', 'UL', ' office', '<|start_header_id|>', '<|end_header_id|>', ' PA', ' office', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', ' journey', 'office'], 'evidence_proportions': [0.0013862848281860352, 0.00027920007705688475, 0.007411340872446696, 8.976459503173828e-05, 0.00023152430852254233]}}, 31: {'grad': {'score': [0.13210635715060765, 0.09814908869550448, 0.14728707978219696, 0.09754065150611124, 0.18093197587607562], 'topk_tokens': ['�', ' strife', ',', ' inaugur', ',', 'estead', '�', ' or', 'antic', ',', ' republic', ' especial', ' enraged', '\u200d', '�', '\u200d', ' journey', ' o', 'user', 'ting'], 'evidence_proportions': [0.197601318359375, 0.05198974609375, 0.1196301778157552, 0.0922698974609375, 0.1724090576171875]}, 'weight': {'score': [0.014242030956127026, 0.006756502649058466, 0.003498323035962654, 0.006733830228862232, 0.0031843919684921484], 'topk_tokens': [' during', ' discarded', ' it', ' left', 'Answer', ' or', '<|start_header_id|>', ' was', ' dropped', '?\n', ' Gen', ',', ':', ' journey', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.007965405782063803, 0.019056320190429688, 0.03577375411987305, 0.001947641372680664, 0.0031712849934895835]}, 'saliency': {'score': [0.00197565997088397, 0.00013631325926357046, 0.0004016016468857274, 0.0001223111312661395, 0.0001066223434779955], 'topk_tokens': [' dropped', ' the', ' was', 'Bridge', ' I', 'I', ' to', '<|start_header_id|>', 'L', ' Square', ' during', 'assistant', ' it', ' office', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' journey', 'office', '<|eot_id|>'], 'evidence_proportions': [0.0019365251064300537, 0.00022379159927368164, 0.006701469421386719, 1.634657382965088e-05, 5.50846258799235e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 0}
2025-01-23 22:19:31.981 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:19:31.981 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-3_pid-2_0-2-3-4-6.pkl | len: 10 |  size: 9.03 KB
Processing depth (0, 2, 3, 4, 6):   3%|▎         | 3/100 [00:43<23:37, 14.61s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Processing depth (0, 4, 7, 8, 9):   3%|▎         | 3/100 [00:53<23:37, 14.61s/it]2025-01-23 22:19:42.034 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:19:42.034 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:42.034 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the apple.
2025-01-23 22:19:42.043 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1840, 1845) -->  Mary picked up the apple
2025-01-23 22:19:42.044 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:19:42.058 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2866, 2872) --> . Mary journeyed to the
2025-01-23 22:19:42.058 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:19:42.075 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3329, 3333) -->  Mary dropped the apple
2025-01-23 22:19:42.075 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:19:42.087 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2260, 2266) -->  went back to the kitchen.
2025-01-23 22:19:42.087 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:19:42.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (401, 407) -->  Sandra journeyed to the bedroom
2025-01-23 22:19:42.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:19:42.093 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (882, 888) --> . John went back to the
2025-01-23 22:19:42.093 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:19:42.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:42.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:19:42.105 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2259, 2265) -->  Daniel went back to the kitchen
2025-01-23 22:19:42.105 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:19:42.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2236, 2241) --> . John moved to the
2025-01-23 22:19:42.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:19:42.127 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2159, 2163) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:19:42.701 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary's hand.<|eot_id|>
2025-01-23 22:19:42.702 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4233])
your chose emoji: ['🏋🏿\u200d♀', '💆🏽', '👳🏼\u200d♂️', '🤵🏼\u200d♀', '🗜', '👩🏾\u200d❤️\u200d👩🏽', '🏃🏾', '👩🏾\u200d🏫', '🎰', '🥝']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4235, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 243148.06it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.04it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.20it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 134.66it/s]
2025-01-23 22:19:45.815 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.8269291630497686, 1.3089299549881936, 1.3576919093276516, 1.305194596159244, 1.0729255676269531], 'topk_tokens': ['.', ' panic', ' copies', ' announced', ' Mary', ' clerk', ' a', ' tele', ' office', ' tele', ' whistle', ' printers', ' offices', ' announcement', ' office', ' customary', 'Mary', ' tele', ' Mary', ' office'], 'evidence_proportions': [2.67822265625, 0.69302978515625, 2.513671875, 1.544342041015625, 1.4222005208333333]}, 'weight': {'score': [0.0005227790938483344, 0.007513826318445847, 0.2909819556005073, 0.0053184514702437164, 0.0010646457473436992], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '.', 'in', 'assistant', ' John', 'district', ',', 'nes', 'ot', 'office', ' to', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00028782089551289874, 0.0006267309188842773, 0.0011322697003682454, 0.0003705322742462158, 0.000163118044535319]}, 'saliency': {'score': [5.7251365096480756e-05, 0.00019978999810066808, 0.0036990172935254645, 0.00017305324177542132, 8.072621292538113e-05], 'topk_tokens': ['in', '.', '<|eot_id|>', ',', ' John', '\n\n', '<|eot_id|>', 'ot', 'ian', 'nes', '\n\n', '.', ' to', '.', ' back', ' went', '<|begin_of_text|>', '<|start_header_id|>', 'office', 'assistant'], 'evidence_proportions': [0.00011198719342549641, 2.9242038726806642e-05, 7.637341817220052e-05, 4.495680332183838e-05, 1.4930963516235352e-05]}}, 25: {'grad': {'score': [1.5136797869646992, 1.5368409644332939, 1.4696451822916667, 1.5375218776314559, 2.2601521809895835], 'topk_tokens': ['      ', '�', '      ', '      ', '      ', '      ', '      ', '�', ' and', '�', '      ', '�', '❤', '�', '      ', ' $', '      ', '      ', '�', '      '], 'evidence_proportions': [1.9532877604166667, 1.363702392578125, 1.261932373046875, 1.12518310546875, 1.7097981770833333]}, 'weight': {'score': [0.0006347254470542625, 0.007496847988160188, 0.34559158484141034, 0.004868860201921291, 0.00047958559460110136], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ',', 'assistant', '.', 'in', ' John', 'district', 'office', 'nes', ',', 'ot', ' to', '<|end_header_id|>', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00015694896380106607, 0.0003171086311340332, 0.0020195444424947104, 0.0005575716495513916, 4.3799479802449547e-05]}, 'saliency': {'score': [0.00044303911703604237, 0.00037590373646129263, 0.01026150674530954, 0.00029733186710380506, 2.572933832804362e-05], 'topk_tokens': ['<|eot_id|>', '.', 'in', ' John', 'district', '<|eot_id|>', 'E', ' to', 'nes', 'ot', ',', 'ian', ' back', 'assistant', ' went', '.', '.', '<|begin_of_text|>', 'office', '<|end_header_id|>'], 'evidence_proportions': [2.3633241653442383e-05, 2.849102020263672e-05, 0.0017446577548980713, 0.0002987682819366455, 2.463658650716146e-06]}}, 26: {'grad': {'score': [0.5100035490813078, 0.4026848550951889, 0.384074355616714, 0.40213791898624623, 0.418870660993788], 'topk_tokens': [' Min', ' one', 'of', ' N', ',', ' so', ' OF', ' hopes', ' strife', ' of', ':', ' doubtful', ' nearly', '\n\n', ' People', ' some', ' Date', ' Date', ' competition', 'ed'], 'evidence_proportions': [0.4708658854166667, 0.279443359375, 0.8431599934895834, 0.586456298828125, 0.35714975992838544]}, 'weight': {'score': [0.002706148006297924, 0.0074644320689519995, 0.27770713242617523, 0.005359154118749196, 0.0027348374327023826], 'topk_tokens': ['assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'office', '.', 'in', ' John', '<|start_header_id|>', 'district', 'nes', 'ot', ',', ' to', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0010902086893717449, 0.0030801773071289064, 0.00616908073425293, 0.0015628337860107422, 0.0013096729914347331]}, 'saliency': {'score': [0.0003153770058243363, 0.00033596606293985666, 0.0035977128780249395, 0.0003103177418965779, 0.00018806672758526273], 'topk_tokens': ['nes', ' back', 'in', 'district', '<|end_header_id|>', ' court', ',', 'office', '.', 'user', ' Proof', ' went', ' *\n\n', '.', '<|eot_id|>', 'istributed', '.', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [2.4706125259399414e-05, 0.0001999974250793457, 0.001047601302464803, 7.730722427368164e-05, 0.00012868642807006836]}}, 27: {'grad': {'score': [1.6115711353443287, 1.5596091674660566, 1.2233905214251894, 1.56193066260058, 1.7754380967881944], 'topk_tokens': ['ian', ' printed', 'outs', 'c', ' J', ' Bor', ' mob', 'ian', 'st', '<|start_header_id|>', ' Clean', 'ian', 'ian', 'ian', 'ian', ' possessed', 'iously', '\u200d', 'ed', 'ian'], 'evidence_proportions': [0.9801432291666666, 2.116552734375, 1.5406850179036458, 1.775634765625, 1.78369140625]}, 'weight': {'score': [0.00124731770268193, 0.007503219928763974, 0.3577540733597495, 0.004775227401070966, 0.000896515945593516], 'topk_tokens': ['<|eot_id|>', ' *\n\n', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '.', 'in', 'district', ' John', ',', 'nes', 'ot', 'office', ' to', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0007302761077880859, 0.0006116867065429687, 0.003795007864634196, 0.000590980052947998, 0.00018392006556193033]}, 'saliency': {'score': [0.000302186718693486, 0.0003599095935663901, 0.002984792897195527, 0.000339535310596763, 0.00014831456873151992], 'topk_tokens': [',', 'nes', '<|end_header_id|>', ' *\n\n', 'in', '.', 'As', ' *\n\n', '<|eot_id|>', 'ot', 'ian', '.', ' *\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' Proof', '<|end_header_id|>', ' *\n\n', 'office'], 'evidence_proportions': [8.492668469746907e-05, 0.00018782615661621093, 0.0010017951329549153, 0.00010722875595092773, 4.511078198750814e-05]}}, 28: {'grad': {'score': [1.6983326099537037, 1.6791318855150532, 1.6722966974431819, 1.6790617398016467, 1.541961669921875], 'topk_tokens': [' to', '240', ' completed', 'void', ' Project', ' PA', ' Gutenberg', 'A', 'ION', ' Pa', ' Proof', 'ISC', 'NEW', ' Press', ' ', ' one', ' Wright', ' office', ' Moore', ' one'], 'evidence_proportions': [2.2268880208333335, 1.58818359375, 1.5469563802083333, 1.0555419921875, 1.8414713541666667]}, 'weight': {'score': [0.0005334792313752351, 0.0071380876453314085, 0.35052081129767676, 0.004466637237343246, 0.0006035673949453565], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'office', '.', 'in', 'district', ' John', ',', 'nes', 'ot', ' to', '<|end_header_id|>', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0001695255438486735, 0.0006152272224426269, 0.001209716002146403, 0.00043785572052001953, 0.00021682182947794595]}, 'saliency': {'score': [6.529799214115849e-05, 0.0001953855878206183, 0.005232661059408477, 0.0001564112823166533, 3.7112997637854685e-05], 'topk_tokens': [' Min', ' nearly', 'In', 'As', ',', ',', '\n', 'ot', ' to', '<|eot_id|>', 'nes', '<|eot_id|>', '.', ' back', 'office', '<|end_header_id|>', ' went', '<|begin_of_text|>', '.', 'assistant'], 'evidence_proportions': [1.424551010131836e-05, 2.9146671295166016e-05, 0.00021219253540039062, 2.8148293495178223e-05, 2.434849739074707e-05]}}, 29: {'grad': {'score': [1.4208656593605324, 0.6554827008928571, 1.301684755267519, 0.6454252140250748, 1.1751352945963542], 'topk_tokens': [' up', 'un', 'u', '�', 'Mary', 'Cut', '<|start_header_id|>', ' journey', ' People', '�', ' to', '.', 'ting', ' Cl', 'UL', ' journey', ' PA', 'ed', ' the', ' office'], 'evidence_proportions': [3.6979166666666665, 0.86556396484375, 1.2234598795572917, 0.404449462890625, 0.4815826416015625]}, 'weight': {'score': [0.0005995564990573459, 0.007467985997656019, 0.42581006613644684, 0.004205748620861305, 0.000526534186469184], 'topk_tokens': [',', '<|eot_id|>', 'office', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '.', ' John', 'district', 'in', ',', 'nes', ' to', 'ot', 'ian', ' back', ' went', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005472501118977865, 0.0005106091499328613, 0.0013431509335835774, 0.00018650293350219727, 0.00025776028633117676]}, 'saliency': {'score': [0.0001018400545473452, 0.0003120635729612961, 0.014883185877944485, 0.0001982501595320102, 3.883573744032118e-05], 'topk_tokens': ['<|eot_id|>', 'The', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', ' John', ',', 'district', 'in', '.', '<|start_header_id|>', ' to', '.', 'nes', 'ian', 'ot', ' back', ' went', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00018795331319173178, 2.3925304412841796e-05, 0.00022267301877339682, 3.6209821701049805e-06, 2.530217170715332e-05]}}, 30: {'grad': {'score': [1.6442780671296295, 2.121848942960449, 1.5052566528320312, 2.1298110888818065, 2.1464606391059027], 'topk_tokens': ['sur', ' and', ' two', ' small', ' and', ' then', ' second', ' the', 'description', ' nineteenth', ' during', ' Buchanan', ' N', 'ian', ' John', ' and', ' in', ' than', ' Gen', ' office'], 'evidence_proportions': [1.1270751953125, 1.76396484375, 1.5836995442708333, 1.843017578125, 1.9898274739583333]}, 'weight': {'score': [0.004962227962635181, 0.007089603608727033, 0.0987161686926177, 0.006379127559547653, 0.0028152863184611], 'topk_tokens': ['Just', 'assistant', ' *\n\n', 'istributed', 'nes', ',', '<|eot_id|>', 'ot', ' to', '<|eot_id|>', 'ian', 'office', ' back', '.', ' went', '<|eot_id|>', '.', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.002116680145263672, 0.006441688537597657, 0.0100553830464681, 0.006210446357727051, 0.0006495912869771322]}, 'saliency': {'score': [0.0005682177013821072, 0.0005636040688686978, 0.001219036001147646, 0.0005583935726188614, 0.00019068850411309136], 'topk_tokens': ['Times', '\n', 'assistant', '\n', 'message', 'istributed', 'Min', 'Good', 'In', ' governor', '\n\n\n', '<|begin_of_text|>', '\n', 'Just', '<|eot_id|>', 'office', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.00028653939565022785, 0.0005344271659851075, 0.0013120671113332112, 0.0007127970457077026, 3.781914710998535e-05]}}, 31: {'grad': {'score': [1.8540988498263888, 2.5506447387839435, 1.670004179983428, 2.5621100986503555, 2.7912665473090277], 'topk_tokens': ['graph', 'ot', ' it', ' illustrated', 'announcement', 'itated', 'ot', 'ot', '�', ' court', '<|end_header_id|>', 'latest', 'ot', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', ' arrest', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.23826090494791666, 2.3763671875, 2.3032633463541665, 1.730712890625, 2.6678059895833335]}, 'weight': {'score': [0.0029149585300021702, 0.006806784565642986, 0.1843041145440304, 0.0054289803533496975, 0.0010264673166804844], 'topk_tokens': [',', ' John', '<|eot_id|>', ',', 'Just', 'nes', ',', 'ot', ' to', '<|start_header_id|>', 'assistant', 'ian', ' back', '<|eot_id|>', ' went', '<|end_header_id|>', '.', '.', '<|begin_of_text|>', 'office'], 'evidence_proportions': [0.003725846608479818, 0.0006508827209472656, 0.004730105400085449, 0.005404472351074219, 0.0005159775416056315]}, 'saliency': {'score': [0.00019457163634123626, 0.0006817780531846084, 0.0031648224050348454, 0.0006653023908238211, 6.828291548622979e-05], 'topk_tokens': ['.\n', ' *\n\n', 'In', 'assistant', ' went', ' information', '.', ' John', 'ian', ',', 'Just', ',', '.', '<|eot_id|>', '.', '<|start_header_id|>', ' help', '<|eot_id|>', '<|end_header_id|>', 'office'], 'evidence_proportions': [0.00045439600944519043, 5.728006362915039e-05, 0.00022515654563903809, 0.00012946128845214844, 6.197889645894368e-05]}}, 'pred_res': "Mary's hand.<|eot_id|>", 'score': 0}
2025-01-23 22:19:45.823 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:19:45.824 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-3_pid-3_0-4-7-8-9.pkl | len: 10 |  size: 9.32 KB
Processing depth (0, 4, 7, 8, 9):   4%|▍         | 4/100 [00:57<22:53, 14.31s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Processing depth (0, 1, 4, 7, 9):   4%|▍         | 4/100 [01:08<22:53, 14.31s/it]2025-01-23 22:19:56.585 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:19:56.585 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:56.586 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the apple.
2025-01-23 22:19:56.588 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (476, 481) --> . Mary picked up the
2025-01-23 22:19:56.588 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-23 22:19:56.598 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1916, 1922) --> . Mary journeyed to the
2025-01-23 22:19:56.598 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary dropped the apple.
2025-01-23 22:19:56.612 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2927, 2931) -->  dropped the apple.
2025-01-23 22:19:56.612 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:19:56.624 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2335, 2341) -->  went back to the kitchen.
2025-01-23 22:19:56.625 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:19:56.627 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (426, 432) --> . Sandra journeyed to the
2025-01-23 22:19:56.627 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:19:56.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (916, 922) --> . John went back to the
2025-01-23 22:19:56.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John journeyed to the office.
2025-01-23 22:19:56.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-23 22:19:56.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-23 22:19:56.644 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2334, 2340) -->  Daniel went back to the kitchen
2025-01-23 22:19:56.644 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:19:56.655 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2315, 2320) -->  John moved to the bedroom
2025-01-23 22:19:56.655 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John took the milk.
2025-01-23 22:19:56.666 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2200, 2204) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:19:57.243 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary's hand.<|eot_id|>
2025-01-23 22:19:57.243 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4229])
your chose emoji: ['🙋🏿\u200d♂', '🧙🏼', '🤾🏼\u200d♂️', '↘️', '🧑🏽\u200d🦼\u200d➡', '🎌', '🧗\u200d♂', '\U0001fae2', '🕵🏾\u200d♂', '🇲🇰']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4231, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 180400.17it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 135.22it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 137.95it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 136.61it/s]
2025-01-23 22:20:00.418 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [1.1536684389467593, 0.5783842010606239, 1.0613172126538826, 0.5708393763649867, 0.4912763483384076], 'topk_tokens': [' tele', ' picked', ' went', ' Knowledge', ' ', '.', '202', ':', ' the', '26', ' Date', ' the', ' journey', 'user', ' Mary', ' bedroom', ' wield', ':', 'Mary', ' office'], 'evidence_proportions': [2.0686848958333335, 1.3556640625, 0.8050944010416666, 0.629547119140625, 0.768310546875]}, 'weight': {'score': [0.07912910646862453, 0.007502735041467553, 0.033655968579379, 0.0068321599431016185, 0.00806599241845748], 'topk_tokens': [' candidate', '<|start_header_id|>', ' first', ' the', 'office', 'ian', '8', 'assistant', ' it', ' John', 'In', 'cont', 'ot', '<|eot_id|>', '<|eot_id|>', ' divided', 'itol', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0003797908624013265, 0.004168272018432617, 0.35153937339782715, 0.0007904469966888428, 0.00016129016876220703]}, 'saliency': {'score': [0.0019622577561272513, 0.0002059084787010386, 0.0012664704611807158, 0.00018614823513538596, 0.00029605802367715277], 'topk_tokens': ['8', 'ian', ' the', '<|end_header_id|>', ' it', 'In', '<|eot_id|>', '<|start_header_id|>', 'cont', ' John', 'ot', '<|end_header_id|>', ' divided', 'system', 'itol', '\n\n', '***', '<|begin_of_text|>', 'assistant', '.'], 'evidence_proportions': [0.0004687805970509847, 0.0002470254898071289, 0.008070498704910278, 8.760392665863037e-05, 2.662340799967448e-05]}}, 25: {'grad': {'score': [0.8622753002025463, 0.9294745765997992, 0.9479536576704546, 0.9297633732402676, 1.2526137408088236], 'topk_tokens': ['.', '.', '.', ',', ' prepared', '.', '<|start_header_id|>', '.', ',', 'E', 'ine', '\n', '.', ' Gree', '\n\n', '\n', '.', 'ucci', '\n\n', '\n\n'], 'evidence_proportions': [1.0522054036458333, 0.83359375, 0.9510498046875, 0.3568572998046875, 0.9444173177083334]}, 'weight': {'score': [0.09523551993899876, 0.00749945634928924, 0.0472782597397313, 0.006616796500618239, 0.007001634906320011], 'topk_tokens': [' the', 'es', ' candidate', 'ian', ' first', ' the', ' it', '<|eot_id|>', '8', 'In', 'ot', 'assistant', 'cont', '<|eot_id|>', ' divided', ' John', 'itol', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00012668967247009277, 0.0033067703247070313, 0.42499057451883954, 0.0008988082408905029, 8.772810300191243e-05]}, 'saliency': {'score': [0.0015502108467949762, 0.00026315542237749056, 0.0024499152645920262, 0.00023752282318009144, 0.00018378040369819193], 'topk_tokens': [' it', 'es', '.', ' the', 'ot', '8', ' candidate', ' divided', '.', ' Republican', '<|start_header_id|>', 'itol', 'eward', '<|eot_id|>', 'office', ' John', '.', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [8.58306884765625e-06, 0.00014030933380126953, 0.00683740774790446, 4.217028617858887e-06, 1.0222196578979492e-05]}}, 26: {'grad': {'score': [0.6690741644965278, 0.7306508600951311, 0.6802830551609849, 0.7314479611126303, 0.6024385340073529], 'topk_tokens': [' pay', ' fire', ' that', ' take', ' of', ' than', ' of', '\n\n', ' use', ',', ' December', ' of', 'and', ' and', ' and', ' be', ' be', ' use', '202', 'ting'], 'evidence_proportions': [0.67864990234375, 0.4759521484375, 0.7947285970052084, 0.66400146484375, 0.6981608072916666]}, 'weight': {'score': [0.08183344425978484, 0.007476382434748499, 0.032280799114342895, 0.006798802377279643, 0.008536654360154095], 'topk_tokens': [' candidate', 'assistant', '8', ' the', '<|eot_id|>', ' it', 'ot', '<|start_header_id|>', 'In', ' John', 'cont', '<|end_header_id|>', ' divided', 'itol', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.000693043073018392, 0.006514739990234375, 0.3592264652252197, 0.003459945321083069, 0.0005954106648763021]}, 'saliency': {'score': [0.0016723760852107295, 0.00035910154453999775, 0.0012428724404537315, 0.0003436081731270841, 0.00041237035218407125], 'topk_tokens': ['office', ' divided', '8', 'user', ' Republican', 'eward', 'In', ' candidate', ' John', '-text', ' PA', 'assistant', '<|eot_id|>', '<|start_header_id|>', '.', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [2.847115198771159e-05, 0.000365447998046875, 0.006856769323348999, 0.00043307244777679443, 4.719694455464681e-05]}}, 27: {'grad': {'score': [0.6244619864004629, 0.45103543668569784, 0.44545121626420453, 0.44995698126292405, 0.39772863949046416], 'topk_tokens': ['.', '\n', ' nearly', ' river', ' the', ' The', ' for', ' the', ' the', 'user', ' of', ' ', ' ', ' city', '\n\n\n', ' the', ' river', ' the', '4', 'Today'], 'evidence_proportions': [0.7640889485677084, 1.225048828125, 0.250335693359375, 0.353912353515625, 0.5388387044270834]}, 'weight': {'score': [0.11565560985494543, 0.007493771269258893, 0.0449157790704207, 0.0064975363377671935, 0.009087765041519614], 'topk_tokens': [' candidate', 'es', 'eward', '<|start_header_id|>', '�', ' the', '8', 'ian', ' the', 'assistant', ' it', 'In', 'ot', ' John', 'cont', ' divided', 'itol', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0004940827687581381, 0.0041981697082519535, 0.5157074928283691, 0.0007320493459701538, 0.0002621610959370931]}, 'saliency': {'score': [0.0010085724018238209, 0.0002497101303421558, 0.0004719116471030495, 0.0002430398039496565, 0.0002017371794756721], 'topk_tokens': ['In', '<|begin_of_text|>', ' prepared', 'eward', '<|eot_id|>', 'itol', '-text', 'user', '<|eot_id|>', 'office', ' Joseph', 'UL', '.', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', 'assistant', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [8.183717727661133e-05, 0.00023794174194335938, 0.004110376040140788, 0.00016325712203979492, 3.9239724477132164e-05]}}, 28: {'grad': {'score': [0.5330465811270254, 0.18497671345057315, 0.5349514123165247, 0.179954644045193, 0.21568999570958755], 'topk_tokens': ['ed', ' PA', 'system', ',', '\n\n\n', 'ting', ' the', ',', ':', ',', ',', 'UL', 'Cut', ' the', ' journey', '202', '.', ' the', 'ed', ' to'], 'evidence_proportions': [1.4801839192708333, 0.3423828125, 0.1915257771809896, 0.4149627685546875, 0.1650390625]}, 'weight': {'score': [0.13647887000331171, 0.007382653222188857, 0.057991128979307235, 0.006146576129627068, 0.006008582080111784], 'topk_tokens': [' up', '.', ' the', 'ian', '8', '<|eot_id|>', 'In', ' the', 'assistant', ' it', '<|start_header_id|>', '<|eot_id|>', 'ot', 'cont', ' divided', ' John', 'itol', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0001279910405476888, 0.0018338441848754882, 0.6117639541625977, 0.0009643733501434326, 9.185075759887695e-05]}, 'saliency': {'score': [0.000593300218935366, 0.00012659350345388875, 0.001126907088539817, 0.00011565813312882629, 0.00017194800517138313], 'topk_tokens': [' first', ' divided', 'ian', ' Republican', '<|eot_id|>', '.', ' candidate', ' was', '�', '<|start_header_id|>', 'eward', 'es', '.', ' back', ' the', 'office', 'assistant', '.', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [7.351239522298177e-06, 6.996393203735351e-05, 0.002551605304082235, 7.140636444091797e-05, 4.9869219462076826e-06]}}, 29: {'grad': {'score': [0.9912538881655093, 0.6079529717413141, 0.9099398526278409, 0.6030825109854502, 0.8307225844439339], 'topk_tokens': ['prise', 'u', 'iously', ' and', ' office', ' *', ' the', 'cuts', ',', ' rebellion', '\n\n\n', 'ed', ' the', ' journey', '.', '<|start_header_id|>', 'Today', ' to', 'system', ' the'], 'evidence_proportions': [2.05517578125, 0.8840087890625, 0.5376993815104166, 0.863037109375, 0.5557352701822916]}, 'weight': {'score': [0.20269053953665275, 0.007476292724897889, 0.04099664001753836, 0.005947413289612701, 0.005440645358141731], 'topk_tokens': [' up', '�', '8', '.', ' the', ' the', 'ian', 'In', ' it', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'cont', ' John', 'ot', ' divided', 'itol', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005768140157063802, 0.0017728805541992188, 0.9086366494496664, 0.0014657527208328247, 0.0004393955071767171]}, 'saliency': {'score': [0.003004577424791124, 0.0002507480485195303, 0.0005547729405489835, 0.0002305163979330271, 0.0003595343407462625], 'topk_tokens': ['<|eot_id|>', 'ian', 'In', 'es', 'aining', ' it', 'asca', '\n', '�', ' the', ' divided', 'cont', '<|start_header_id|>', 'itol', '<|eot_id|>', '<|eot_id|>', '***', '.', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00013708074887593588, 0.00024100542068481446, 0.012983739376068115, 9.475648403167725e-05, 0.0001357694466908773]}}, 30: {'grad': {'score': [0.8187612957424588, 0.6092061615531198, 0.8488799586440577, 0.6059534106715295, 0.8278817709754495], 'topk_tokens': [' had', ' minutes', ' directly', ' very', '�', ' him', ' apple', ' lesser', ' celebrity', ' made', ' to', ' which', ' Republican', 'assistant', '.', ' Peter', ' Date', 'graph', ' office', ' streets'], 'evidence_proportions': [1.0029449462890625, 0.5804723739624024, 0.5658365885416666, 1.5438232421875, 0.6027018229166666]}, 'weight': {'score': [0.10155800095310917, 0.007135962403093886, 0.020541422294847893, 0.006418682322231193, 0.0035584113177131206], 'topk_tokens': ['<|end_header_id|>', ' it', ' the', '<|eot_id|>', 'UL', 'In', '<|start_header_id|>', 'cont', ' John', 'ot', ' divided', 'itol', 'office', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '.', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015166203180948894, 0.015924835205078126, 0.4300558567047119, 0.01660659909248352, 0.0010967652002970378]}, 'saliency': {'score': [0.0010782171178747107, 0.00025787485375581296, 0.0007376580527334502, 0.0002487686234279606, 8.69675594217637e-05], 'topk_tokens': [' every', 'system', '\n', 'of', '?\n', '.', ' Paul', '<|start_header_id|>', ' for', '<|eot_id|>', ' went', ' the', '***', 'UL', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'office', '<|eot_id|>', 'assistant'], 'evidence_proportions': [0.0004233221213022868, 0.0015050411224365235, 0.0013611316680908203, 0.0025709718465805054, 9.934107462565105e-05]}}, 31: {'grad': {'score': [0.9810836226851852, 1.0817810505790593, 0.8232070460464015, 1.0844786705029894, 1.1312655280618107], 'topk_tokens': ['.', ' of', ' office', ' to', ',', '.', 'sur', '.', '.', 'iously', ',', 'ible', ',', ',', ' ', '.', ',', ',', '<|start_header_id|>', ' Gree'], 'evidence_proportions': [1.1861165364583333, 0.9189453125, 0.6981201171875, 1.14349365234375, 1.0025227864583333]}, 'weight': {'score': [0.0656598409016927, 0.006602261371292478, 0.008518760854547674, 0.006204802936799976, 0.002053235383594737], 'topk_tokens': ['<|eot_id|>', 'Just', ' Mary', '?\n', ' PA', 'Today', 'user', ' divided', 'Answer', ' John', 'ot', 'itol', '<|eot_id|>', '<|start_header_id|>', 'assistant', '.', '<|eot_id|>', '<|end_header_id|>', 'office', '<|begin_of_text|>'], 'evidence_proportions': [0.004735628763834636, 0.015176773071289062, 0.26889554659525555, 0.012743353843688965, 0.0006952285766601562]}, 'saliency': {'score': [0.0017895124576709888, 0.000476479558509677, 0.0006099000121607925, 0.00046692435274419097, 0.00012744524899651022], 'topk_tokens': ['Question', '<|start_header_id|>', ' office', ' John', ' candidate', ' Mary', ' Republican', ' PA', 'eward', 'assistant', '<|eot_id|>', 'Just', 'system', '?\n', '.', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', 'office'], 'evidence_proportions': [0.0012493729591369629, 0.0015382885932922364, 0.004554351170857747, 0.0013229399919509888, 8.521477381388347e-05]}}, 'pred_res': "Mary's hand.<|eot_id|>", 'score': 0}
2025-01-23 22:20:00.425 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:20:00.425 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-3_pid-4_0-1-4-7-9.pkl | len: 10 |  size: 9.46 KB
Processing depth (0, 1, 4, 7, 9):   5%|▌         | 5/100 [01:12<22:49, 14.41s/it]Processing depth (0, 1, 4, 7, 9):   5%|▌         | 5/100 [01:12<22:57, 14.50s/it]
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:99 - Selected idx: 4
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:103 - Needle: [' Daniel picked up the apple.', ' John went back to the bedroom.', ' Sandra journeyed to the bedroom.', ' Daniel journeyed to the bathroom.', ' Mary got the football there.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Mary journeyed to the office.']
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel picked up the apple.', ' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Mary journeyed to the office.']
2025-01-23 22:20:00.641 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Processing depth (1, 3, 4, 9):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-23 22:20:11.440 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:20:11.442 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (407, 412) -->  Daniel picked up the apple
2025-01-23 22:20:11.442 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:20:11.449 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1391, 1397) --> . Daniel journeyed to the
2025-01-23 22:20:11.450 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:20:11.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1813, 1818) --> . Daniel went to the
2025-01-23 22:20:11.459 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary journeyed to the office.
2025-01-23 22:20:11.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3600, 3606) -->  Mary journeyed to the office
2025-01-23 22:20:11.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:20:11.478 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (133, 139) --> . John went back to the
2025-01-23 22:20:11.478 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-23 22:20:11.488 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2013, 2019) -->  to the ground. Sandra journey
2025-01-23 22:20:11.488 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary got the football there.
2025-01-23 22:20:11.505 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3369, 3374) --> . Mary got the football
2025-01-23 22:20:11.505 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:20:11.517 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2350, 2355) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:20:11.996 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:20:11.997 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4198])
your chose emoji: ['🕧', '🧔🏻\u200d♀', '👉🏻', '🧎\u200d➡️', '👩🏿\u200d🚒', '♣', '👋🏻', '👨🏽\u200d🎤', '🏄🏼\u200d♂️', '🐷']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4201, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 149130.81it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 98.14it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 101.14it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 99.47it/s]
2025-01-23 22:20:15.126 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.34098581834272906, 0.27973437837066323, 0.2955041365189986, 0.27932676077175805, 0.2799136528602013], 'topk_tokens': [' and', ' and', ' bedroom', ' When', ' at', ' than', 'when', ' at', ' in', ' was', ' and', ' to', ' and', ' bathroom', ' was', ' persons', ' men', ' bathroom', ' When', 'athroom'], 'evidence_proportions': [0.370904541015625, 0.3539079030354818, 0.3803401947021484, 0.2703361511230469]}, 'weight': {'score': [0.005402451211755926, 0.0075507941515041066, 0.002961608496579257, 0.007586451026433817, 0.04788645689304059], 'topk_tokens': ['.\n\n', '<|end_header_id|>', 'athroom', 'assistant', 'daily', 'ing', 's', '.', '.', '.', '.', ',', '�', '.', '.', '.', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0025636911392211913, 0.009584943453470865, 0.0028400421142578125, 0.005720933278401692]}, 'saliency': {'score': [9.18954610824585e-05, 9.073621033203372e-05, 6.294385953383012e-05, 9.087716010375639e-05, 0.0003393879303565392], 'topk_tokens': ['room', 'ing', 's', '<|begin_of_text|>', ' bathroom', 'b', '.', ' bathroom', '.', ',', '.', '.', '.', '.', '�', '.', '.', '�', '.', 'athroom'], 'evidence_proportions': [0.00015755295753479002, 0.00012092789014180501, 1.4203786849975587e-05, 7.289151350657146e-05]}}, 25: {'grad': {'score': [0.4033480557528409, 0.2983950674179139, 0.2841988910328258, 0.297914757130953, 0.34630721165583683], 'topk_tokens': [' Grow', 'ION', ' steam', ' long', ' the', ' propriet', ' large', ' old', ' exc', ' old', '�', ' fastest', ' Hoe', ' old', ' web', ' old', ' old', ' rest', 'old', 'vent'], 'evidence_proportions': [0.435687255859375, 0.33455403645833337, 0.499188232421875, 0.365325927734375]}, 'weight': {'score': [0.004247435114600442, 0.007495229512672996, 0.002433289181102406, 0.0075392069396761595, 0.05417240032782922], 'topk_tokens': ['daily', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', 's', 'ing', '.', '.', '�', '.', '.', ',', '.', '.', '.', '.', '.', '�', '<|begin_of_text|>'], 'evidence_proportions': [0.004074156284332275, 0.0056097110112508135, 0.004095268249511719, 0.0031563639640808105]}, 'saliency': {'score': [6.245618516748601e-05, 0.00010088023700137502, 5.653094161640514e-05, 0.00010131829657289654, 0.00042496782082777755], 'topk_tokens': ['room', 'MIN', 'ing', '.', 'ree', '<|end_header_id|>', '.', '<|begin_of_text|>', 'athroom', '.', '\n\n', '�', '.', '.', '.', '�', '.', '.', ',', '.'], 'evidence_proportions': [7.865428924560546e-05, 3.801286220550537e-05, 0.00011563301086425781, 2.9087066650390625e-05]}}, 26: {'grad': {'score': [0.34185179797085846, 0.34033495878724934, 0.2779535813765092, 0.340657070849072, 0.2651201394888071], 'topk_tokens': [' called', ' that', ' of', ' message', 'national', ' out', ' and', ' one', ' all', ' rival', ' patent', ' out', ' state', ' candidates', 'state', ' state', 'ierce', ' state', ' state', ' state'], 'evidence_proportions': [0.41497802734375, 0.3173821767171224, 0.33734436035156246, 0.30913909276326496]}, 'weight': {'score': [0.005918773737820712, 0.00749478071140579, 0.004741701212796298, 0.007517691440871337, 0.03668349339411809], 'topk_tokens': ['Answer', '.\n\n', 'assistant', 'ing', ' bogus', 's', '.', 'athroom', '.', '.', ',', '�', '.', '.', '.', '.', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00510702133178711, 0.006143490473429361, 0.006776857376098633, 0.005655447642008463]}, 'saliency': {'score': [0.00010913068597966975, 0.00012146065247056031, 7.204982367428866e-05, 0.00012178740192841876, 0.00027728676795959475], 'topk_tokens': ['b', ' dropped', ' bogus', ' Bench', 'room', '.', '�', '.', ':', 'Answer', '.', ' \n', '.', '�', '.', ' bathroom', ' bathroom', '.', '.', 'athroom'], 'evidence_proportions': [0.0001227259635925293, 0.00014765560626983643, 0.0001194596290588379, 5.0668915112813316e-05]}}, 27: {'grad': {'score': [0.2685847282409668, 0.3206936682556051, 0.2921139110218395, 0.3211206952797727, 0.229157227736253], 'topk_tokens': [' to', ' *', ' for', '.', ' states', ' *', ' state', ' *', '1', ' cap', ' bill', ' state', ' DAYS', ' state', 'itol', ' bill', ' bill', 'state', ' state', '1'], 'evidence_proportions': [0.2078826904296875, 0.31302976608276367, 0.2639892578125, 0.2785542805989583]}, 'weight': {'score': [0.004474569450725208, 0.007563484535135561, 0.002472454851323908, 0.007606775077570222, 0.05305616488823524], 'topk_tokens': ['.\n\n', ' *\n\n', ' bogus', 'daily', 'athroom', 'ing', 's', '.', '.', '.', '.', '�', ',', '<|begin_of_text|>', '.', '.', '.', '.', '�', '.'], 'evidence_proportions': [0.0034044265747070314, 0.005299210548400879, 0.0047369241714477536, 0.0043230851491292315]}, 'saliency': {'score': [0.0002606213092803955, 0.00010178344387634684, 4.629655317826705e-05, 0.00010123648177783077, 0.0003084191909203163], 'topk_tokens': ['.', ' apple', ' *\n\n', '<|begin_of_text|>', '.', '.', '�', ' bogus', '.', '<|end_header_id|>', ',', '.', '.', '.', ' bathroom', '�', '.', '.', ' bathroom', 'athroom'], 'evidence_proportions': [0.0003330826759338379, 0.00029041866461435956, 0.0003046870231628418, 0.00013371805349985758]}}, 28: {'grad': {'score': [0.3484080162915317, 0.270584495088595, 0.25685293024236505, 0.2702453026325335, 0.3457231961763822], 'topk_tokens': [' or', ' a', ' so', ' of', ' a', ' *', 'super', ' the', ' *', ' bathroom', ' of', ' most', ' the', ' and', ' the', ' One', ' *', 'pl', ' of', ' *'], 'evidence_proportions': [0.414337158203125, 0.2732563018798828, 0.4239185810089111, 0.30569330851236975]}, 'weight': {'score': [0.0028573762286793103, 0.007389929657236675, 0.0019397302107377486, 0.007442761161506878, 0.05298326382270226], 'topk_tokens': ['assistant', '\n\n', 'Answer', ' \n', 'athroom', 'ing', 's', '.', '.', '.', ',', '�', '<|begin_of_text|>', '.', '.', '.', '.', '.', '.', '�'], 'evidence_proportions': [0.0008956193923950195, 0.0015882651011149087, 0.002814626693725586, 0.005796909332275391]}, 'saliency': {'score': [0.00024745139208706945, 7.89202740861529e-05, 8.260661905462092e-05, 7.80088513809986e-05, 0.0003116327982682448], 'topk_tokens': ['b', ' \n', '<|end_header_id|>', '<|begin_of_text|>', ' dropped', ',', ' apple', ' bathroom', 'athroom', ' the', 'Answer', '�', '.', '.', '.', '.', '.', '.', '�', '.'], 'evidence_proportions': [2.492666244506836e-05, 1.5099843343098959e-05, 8.779168128967284e-05, 0.000798289974530538]}}, 29: {'grad': {'score': [0.43754161487926135, 0.24032748230750492, 0.3220741965553977, 0.23885114393126428, 0.14682219578669622], 'topk_tokens': [' up', ' to', ' the', ' the', ' house', ' the', ' It', ' to', ' the', ' the', ' to', ' the', 'E', ' bathroom', ' the', ' the', ' went', ' bathroom', 'athroom', ' the'], 'evidence_proportions': [0.3600799560546875, 0.30968729654947913, 0.5926513671875, 0.500689188639323]}, 'weight': {'score': [0.0025383450768210673, 0.007474967168359863, 0.0013555830175226386, 0.007533478627905755, 0.06021430584100577], 'topk_tokens': ['If', 'daily', '.', '\n\n', 'athroom', 's', 'ing', '.', '.', '<|begin_of_text|>', '.', '�', ',', '.', '.', '.', '.', '.', '�', '.'], 'evidence_proportions': [0.000605463981628418, 0.0007906556129455566, 0.006034660339355469, 0.0029831727345784507]}, 'saliency': {'score': [0.0002229024063457142, 6.381663303379784e-05, 0.00012412938204678622, 6.26555150421818e-05, 0.00022951364517211914], 'topk_tokens': [',', '.', 'If', '.', 'b', 'ing', '<|begin_of_text|>', '�', 's', '.', '.', '�', '.', '.', '.', '.', '.', '.', '.', 'athroom'], 'evidence_proportions': [4.4506788253784176e-05, 1.4603137969970703e-05, 0.00038321018218994145, 0.00044627487659454346]}}, 30: {'grad': {'score': [0.2280699990012429, 0.24056713355776527, 0.17640267989852212, 0.24097284800105293, 0.18393761561467098], 'topk_tokens': [' United', '8', ' It', ' up', ' up', ' item', ' gang', ' part', ' as', ' company', 'ien', ' It', ' as', ' war', 'ab', ' company', ' B', ' four', ' West', 'b'], 'evidence_proportions': [0.2124725341796875, 0.25137074788411456, 0.2482879638671875, 0.20091883341471353]}, 'weight': {'score': [0.006965149532664906, 0.007394028345592474, 0.003385771404613148, 0.007417510901903745, 0.04471699274503268], 'topk_tokens': [' bogus', '<|eot_id|>', '.\n\n', 'assistant', 'ing', '.', '\n\n', 'athroom', '<|end_header_id|>', '.', '.', '�', ',', '.', '<|begin_of_text|>', '.', '.', '.', '�', '.'], 'evidence_proportions': [0.00234370231628418, 0.0068327585856119795, 0.008157444000244141, 0.009955167770385742]}, 'saliency': {'score': [0.00013555179942737925, 0.00011819003026163654, 8.042563091624866e-05, 0.00011829800665421586, 0.0003656946695767916], 'topk_tokens': ['ANK', '.', '�', ',', ' bathroom', '<|eot_id|>', ' bathroom', '.', ' bogus', '<|eot_id|>', 'ing', 'b', '.', '<|end_header_id|>', '\n\n', '.', '�', '.', '.', 'athroom'], 'evidence_proportions': [7.803440093994141e-05, 0.00021722912788391113, 7.756352424621582e-05, 0.00015012919902801514]}}, 31: {'grad': {'score': [0.29505261507901276, 0.37347541414990476, 0.3436736193570224, 0.3740481687962851, 0.18190435262826773], 'topk_tokens': [' year', ' propriet', ' of', ' Moore', ' He', ' for', ' so', ' so', ' We', ' for', ' and', ' He', ' or', "'ve", ' now', ' scramble', ' governor', 'le', ' governor', ' so'], 'evidence_proportions': [0.28659210205078123, 0.36194864908854163, 0.260107421875, 0.2643280029296875]}, 'weight': {'score': [0.003265071998942982, 0.007083247202233286, 0.002692114223133434, 0.007126693144021253, 0.033235373863807094], 'topk_tokens': [',', ',', '�', '.', 'assistant', '?', '.', 'b', ' \n', '<|eot_id|>', '\n\n', '.', '<|end_header_id|>', '.', '.', '.', 'athroom', '<|begin_of_text|>', '.', '�'], 'evidence_proportions': [0.0014954090118408203, 0.001451889673868815, 0.005961108207702636, 0.004306276639302572]}, 'saliency': {'score': [7.489052685824308e-05, 0.00013258194843946937, 6.191838871348988e-05, 0.0001332612386941623, 0.00023252413823054386], 'topk_tokens': [' first', ' need', ' and', ' location', ' Where', 'Answer', '.\n\n', ' the', ' Do', 'b', '.', 'assistant', '�', ' \n', '?', '<|eot_id|>', ',', '\n\n', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [5.1140785217285156e-05, 3.406405448913574e-05, 6.518959999084473e-05, 0.00014359255631764728]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:20:15.134 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:20:15.134 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-4_pid-0_1-3-4-9.pkl | len: 10 |  size: 7.78 KB
Processing depth (1, 3, 4, 9):   1%|          | 1/100 [00:14<23:46, 14.41s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
Processing depth (2, 6, 7, 9):   1%|          | 1/100 [00:24<23:46, 14.41s/it]2025-01-23 22:20:25.745 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:20:25.750 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (945, 950) --> . Daniel picked up the
2025-01-23 22:20:25.750 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:20:25.762 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2414, 2420) -->  the senate. Daniel journeyed
2025-01-23 22:20:25.762 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:20:25.777 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2858, 2863) --> . Daniel went to the
2025-01-23 22:20:25.779 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary journeyed to the office.
2025-01-23 22:20:25.798 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3599, 3605) -->  Mary journeyed to the office
2025-01-23 22:20:25.798 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:20:25.799 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (137, 143) -->  John went back to the bedroom
2025-01-23 22:20:25.799 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-23 22:20:25.809 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2043, 2049) -->  Sandra journeyed to the bedroom
2025-01-23 22:20:25.809 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary got the football there.
2025-01-23 22:20:25.826 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3368, 3373) --> . Mary got the football
2025-01-23 22:20:25.826 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:20:25.838 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2383, 2388) -->  Mary moved to the bathroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:20:26.318 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:20:26.318 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4200])
your chose emoji: ['🏌🏻\u200d♂', '❤️\u200d🩹', '🙍🏼', '🌡️', '🏋🏼\u200d♀️', '◼️', '👨🏾\u200d❤\u200d👨🏽', '⛑', '🇰🇾', '🤛🏼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4203, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 222214.78it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.54it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.92it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.55it/s]
2025-01-23 22:20:29.347 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.20979291742498224, 0.2258544239350055, 0.2839272238991477, 0.22563219540507268, 0.21050511545209744], 'topk_tokens': [' It', ' and', ' for', ' it', 'e', ' announcement', 'Dub', ' it', ' so', ' and', ' Daily', ' It', ' dollar', ' We', ' grat', ' bathroom', 'ree', ' STR', ' Dub', ' Dub'], 'evidence_proportions': [0.14840240478515626, 0.1704565684000651, 0.33284301757812496, 0.19774627685546875]}, 'weight': {'score': [0.39218090881000867, 0.007458594478994728, 0.004614804278720509, 0.005438559006314692, 0.002553297512566865], 'topk_tokens': [' apple', 'user', '<|end_header_id|>', ':', 'Question', 'Answer', '<|start_header_id|>', '<|eot_id|>', '.\n\n', '\n\n', '<|eot_id|>', ' \n', '\n\n', 'b', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [1.7163505554199219, 0.0016158620516459148, 0.0019489645957946776, 0.004464536905288696]}, 'saliency': {'score': [0.0005894276228818027, 8.829698487744002e-05, 0.0011666823517192495, 7.994176677053552e-05, 7.914829609999016e-05], 'topk_tokens': [' garden', '\n\n', 'Question', 'rail', ':', '4', ' bogus', 'user', '<|end_header_id|>', 'assistant', ' apple', ' bedroom', ' bill', ' bathroom', 'athroom', '\n\n', '<|begin_of_text|>', '.', 'b', ' bathroom'], 'evidence_proportions': [0.00231621265411377, 3.9036075274149575e-05, 0.00012342333793640138, 8.916854858398438e-05]}}, 25: {'grad': {'score': [0.2691060846502131, 0.26779222000561353, 0.2821975187821822, 0.26770906982642007, 0.30133528495902445], 'topk_tokens': [' circ', ' Min', 'Min', ' Min', ' tele', ' Min', ' West', ' Min', ' old', ' Min', 'port', ' well', ' low', ' Min', '�', ' rest', ' Min', ' Min', 'istributed', 'rich'], 'evidence_proportions': [0.2418670654296875, 0.24144999186197916, 0.32869873046875, 0.26980082194010413]}, 'weight': {'score': [0.4440315487709912, 0.007385185835278094, 0.004438619722019543, 0.005091029660694299, 0.0021224070840807105], 'topk_tokens': [' garden', 'Question', '<|eot_id|>', 'ree', 'Minnesota', '<|eot_id|>', 'b', '<|eot_id|>', '\n\n', ' \n', ' bogus', ' apple', 'Answer', '.\n\n', 'athroom', 'assistant', '<|end_header_id|>', '\n\n', '.', '<|begin_of_text|>'], 'evidence_proportions': [1.9445642471313476, 0.0011377731959025064, 0.005452197790145874, 0.0019642015298207602]}, 'saliency': {'score': [0.0012603022835471413, 9.477135682996613e-05, 0.00025053186850114304, 8.778207776179019e-05, 4.229216433283108e-05], 'topk_tokens': [' bathroom', ' Peter', ' ty', ' pay', ' Paul', ' Paul', ' rest', ' Douglas', ' Daniel', '.\n\n', ' tie', 'assistant', 'Answer', ' bogus', 'ree', 'athroom', '<|begin_of_text|>', '.', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.004802846908569336, 2.7239322662353516e-05, 0.0006563663482666016, 4.45246696472168e-05]}}, 26: {'grad': {'score': [0.44225658069957385, 0.3643829178702341, 0.3555335131558505, 0.3640177979667647, 0.4234266494637105], 'topk_tokens': [' rates', ' state', ' message', ' strange', '�', ' first', ' three', ' late', '�', ' trials', ' *', ' Pioneer', ';', ' *', '�', 'state', ' state', ' state', ' three', ' state'], 'evidence_proportions': [0.35184020996093746, 0.4922434488932292, 0.4768310546875, 0.43880462646484375]}, 'weight': {'score': [0.3927499868653037, 0.007274266001782813, 0.008773355321450666, 0.005227272536038607, 0.005671045673427297], 'topk_tokens': ['Answer', ' garden', ' bogus', '<|eot_id|>', ' bathroom', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'b', 'user', '<|eot_id|>', '<|end_header_id|>', ' \n', '<|eot_id|>', '\n\n', '.\n\n', 'assistant', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [1.7202347755432128, 0.0013568600018819175, 0.003013515472412109, 0.002686182657877604]}, 'saliency': {'score': [0.0015301541848616166, 0.00010399533515029824, 0.0031543319875543766, 8.031586868082511e-05, 0.0001133555796608996], 'topk_tokens': ['Bridge', 'rail', '<|eot_id|>', ' bill', ' Bank', ' block', '\n\n', ' \n', ' Pioneer', ' bedroom', 'road', 'user', 'athroom', 'assistant', ' garden', ' bathroom', ' bogus', '.', '<|begin_of_text|>', ' bathroom'], 'evidence_proportions': [0.00640450119972229, 5.283951759338379e-05, 0.00021412968635559082, 4.220008850097656e-05]}}, 27: {'grad': {'score': [0.3728878714821555, 0.44101500346664585, 0.35214549844915216, 0.4418454737753844, 0.4654802920213386], 'topk_tokens': [' hopes', ' cap', ';', ' made', ' states', ' *', ' state', ' *', ' cap', ' states', ';', '9', ' *', ' *', ' bill', ' bill', ' bill', ' state', 'state', ' state'], 'evidence_proportions': [0.30965576171875003, 0.49811553955078125, 0.32741546630859375, 0.33824729919433594]}, 'weight': {'score': [0.45500556989149615, 0.007406917719962397, 0.008891644802960482, 0.0050313865000538366, 0.004389733520906363], 'topk_tokens': [' Pioneer', 'Question', '<|eot_id|>', ' bogus', '\n\n', ' garden', '<|eot_id|>', ' \n', ' bathroom', 'Minnesota', ' apple', '<|eot_id|>', '.\n\n', 'b', '\n\n', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [1.9881783485412599, 0.0014429092407226562, 0.007396697998046875, 0.003931641578674316]}, 'saliency': {'score': [0.0009756968779997392, 0.00012694312338882362, 0.0031231953339143233, 0.00010660406803104504, 0.00019196120660696457], 'topk_tokens': ['a', '<|eot_id|>', 'road', 'Minnesota', 'u', 'athroom', ' bill', 'assistant', ' apple', ' bogus', ' bedroom', 'Question', '<|end_header_id|>', ' Pioneer', '\n\n', ' bathroom', '<|begin_of_text|>', '.', 'b', ' bathroom'], 'evidence_proportions': [0.0037153422832489013, 6.192425886789958e-05, 0.00026922225952148433, 0.00019516050815582275]}}, 28: {'grad': {'score': [0.4765389182350852, 0.47052327241609415, 0.4014449553056197, 0.47085685711636166, 0.5031983959141062], 'topk_tokens': [' of', 'a', ' of', ' of', ' *', ' of', ' a', ' of', ' a', ' of', ' *', ' a', ' *', 'a', ' *', ' *', ' a', ' a', ' *', ';'], 'evidence_proportions': [0.391082763671875, 0.46791585286458337, 0.635107421875, 0.42423502604166663]}, 'weight': {'score': [0.4717726870016618, 0.00715007868205653, 0.002301996404474432, 0.004717994149013849, 0.0025357835328401026], 'topk_tokens': [' block', 'Answer', '\n\n', 'Just', ' apple', '<|eot_id|>', '.\n\n', ' garden', 'Question', ':', '<|eot_id|>', '.\n\n', ' \n', 'b', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [2.0690394401550294, 0.0013268391291300456, 0.0020627260208129885, 0.0025878747304280596]}, 'saliency': {'score': [0.0014446567405353892, 8.730679332089657e-05, 0.0008033717220479792, 7.633898200310031e-05, 4.992734140424586e-05], 'topk_tokens': [' Sh', ' Bank', ' dropped', '\n\n', ' before', '<|start_header_id|>', '<|eot_id|>', 'ree', '<|eot_id|>', ' apple', ' block', ' bathroom', 'athroom', 'assistant', ' garden', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', '.', '\n\n'], 'evidence_proportions': [0.006107413768768311, 1.9758939743041992e-05, 0.0001119077205657959, 9.454786777496338e-05]}}, 29: {'grad': {'score': [0.40321575511585583, 0.3171334585461649, 0.45015560496937146, 0.31597445451975387, 0.23285780380021281], 'topk_tokens': [' in', ' bathroom', ' new', 'constitutional', ' the', ' ', ' the', ' constitutional', ' in', ' bathroom', ' l', ' the', 'ATION', ' B', ' the', 'APER', ' B', 'E', 'b', ' bedroom'], 'evidence_proportions': [0.34524993896484374, 0.43834431966145837, 0.4120697021484375, 0.4090137481689453]}, 'weight': {'score': [0.5425931215286255, 0.00732243302150366, 0.005910960110751065, 0.0044984602532612645, 0.0026679964207891207], 'topk_tokens': ['.\n\n', ' apple', '\n\n', 'Question', ' the', '<|eot_id|>', '<|eot_id|>', ':', '<|eot_id|>', 'Answer', '.\n\n', 'b', ' \n', ':', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [2.378284072875976, 0.002650022506713867, 0.0021688222885131838, 0.0031473437945048017]}, 'saliency': {'score': [0.003279236229983243, 9.383250213140196e-05, 0.00019907138564369896, 7.642588096044476e-05, 9.162434891088685e-05], 'topk_tokens': [' versatile', '<|start_header_id|>', 'description', ':', ' apple', 'assistant', ' ', '<|eot_id|>', '.\n\n', '.\n\n', ' the', 'athroom', 'a', '<|end_header_id|>', 'a', '<|end_header_id|>', 'b', '\n\n', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.014237046241760254, 3.7585695584615074e-05, 3.4350156784057614e-05, 9.345014890034994e-05]}}, 30: {'grad': {'score': [0.23762096058238635, 0.261645327742089, 0.3383173509077592, 0.26136683569300717, 0.303646877630433], 'topk_tokens': [' Bench', ' no', ' brought', 'RI', ' bathroom', '8', 'ree', ' body', ' bend', ' bodies', 'body', ' business', ' bathroom', ' Bor', ' bogus', '9', ' balance', ' B', ' B', 'b'], 'evidence_proportions': [0.2393829345703125, 0.2625401814778646, 0.1702423095703125, 0.26738230387369794]}, 'weight': {'score': [0.3064143874428489, 0.0072220812063968215, 0.006943242116407914, 0.005640908742529981, 0.009574954189471345], 'topk_tokens': ['<|start_header_id|>', ' bogus', '.\n\n', 'Answer', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'Question', '<|eot_id|>', '.\n\n', 'b', ':', '<|eot_id|>', ' \n', '\n\n', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [1.3294040203094482, 0.004148642222086589, 0.004831695556640625, 0.007507681846618652]}, 'saliency': {'score': [0.0016369196501645174, 0.0001735654851240457, 0.0027868111025203357, 0.00015200136028307223, 0.0002358239978107054], 'topk_tokens': ['<|start_header_id|>', '<|start_header_id|>', '<|start_header_id|>', 'assistant', ' dry', 'Minnesota', '.\n\n', 'Bridge', ' bogus', '\n\n', 'athroom', ' bathroom', ' garden', ' bedroom', ' bill', '<|end_header_id|>', '<|begin_of_text|>', '.', ' bathroom', 'b'], 'evidence_proportions': [0.006335896253585815, 4.061063130696615e-05, 0.00022158622741699218, 0.0004968593517939249]}}, 31: {'grad': {'score': [0.2988001216541637, 0.33240637732720674, 0.25737068869850854, 0.3329810642169972, 0.2198534438859171], 'topk_tokens': [' printed', 'early', 'of', ' made', ' of', ' so', ' made', ' do', 'ences', ' of', ' so', ' scale', ' of', 'of', ' of', ' compelled', ' conducted', '185', ' made', ' five'], 'evidence_proportions': [0.30076637268066403, 0.2662340799967448, 0.2964935302734375, 0.3316497802734375]}, 'weight': {'score': [0.30839554288170556, 0.006882116847341185, 0.0032800381833856754, 0.005306245329632843, 0.00508025955798021], 'topk_tokens': ['<|start_header_id|>', ' apple', ':', '<|eot_id|>', ' Do', '?', 'Question', '.\n\n', 'Answer', ':', '<|eot_id|>', '.\n\n', ' \n', 'assistant', '<|end_header_id|>', 'b', '\n\n', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [1.346379280090332, 0.0023850599924723306, 0.0024164438247680662, 0.00440216064453125]}, 'saliency': {'score': [0.00022480704567649147, 0.00010436582559635037, 0.00025438449599526143, 0.00010293516495904834, 0.0002015366483090529], 'topk_tokens': [' Pioneer', ' person', ':', ' bathroom', 'Just', 'Bridge', 'Question', '.', ' bathroom', '<|start_header_id|>', ':', '<|end_header_id|>', '<|eot_id|>', '.\n\n', ' \n', '<|begin_of_text|>', 'assistant', '\n\n', 'b', 'athroom'], 'evidence_proportions': [0.0007632911205291749, 3.827114899953206e-05, 4.1502714157104496e-05, 0.00011535982290903728]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:20:29.355 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:20:29.355 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-4_pid-1_2-6-7-9.pkl | len: 10 |  size: 9.02 KB
Processing depth (2, 6, 7, 9):   2%|▏         | 2/100 [00:28<23:21, 14.30s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.18s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Processing depth (1, 3, 6, 9):   2%|▏         | 2/100 [00:40<23:21, 14.30s/it]2025-01-23 22:20:41.379 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:20:41.381 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (429, 434) --> . Daniel picked up the
2025-01-23 22:20:41.381 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:20:41.389 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1504, 1510) -->  tragedy. Daniel journeyed to
2025-01-23 22:20:41.389 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:20:41.401 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2403, 2408) -->  the senate. Daniel went
2025-01-23 22:20:41.401 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary journeyed to the office.
2025-01-23 22:20:41.419 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3582, 3588) -->  Mary journeyed to the office
2025-01-23 22:20:41.419 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:20:41.420 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (140, 146) --> . John went back to the
2025-01-23 22:20:41.420 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-23 22:20:41.430 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2040, 2046) --> . Sandra journeyed to the
2025-01-23 22:20:41.430 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary got the football there.
2025-01-23 22:20:41.446 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3355, 3360) -->  Mary got the football there
2025-01-23 22:20:41.447 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:20:41.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2371, 2376) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:20:41.935 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:20:41.935 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4185])
your chose emoji: ['⏲', '♟️', '👳', '🪖', '🏋🏾\u200d♂', '♂️', '🧑\u200d🎄', '🧑🏽\u200d🔧', '🍐', '🚶🏼\u200d♀']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4188, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 216480.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.57it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.93it/s]
2025-01-23 22:20:44.832 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.1385429555719549, 0.1738310616246381, 0.175208872014826, 0.17401108732554904, 0.23099972651554987], 'topk_tokens': [' IN', ' were', ' Jul', ' STR', 'ARR', '4', ' so', 'PA', ' Min', 'system', '�', ' would', ' thought', 'ANK', 'ol', ' EAR', 'ION', 'ION', ' MO', 'ting'], 'evidence_proportions': [0.21114044189453124, 0.06760692596435547, 0.07434539794921875, 0.20247904459635419]}, 'weight': {'score': [0.271883793852546, 0.007574516130609521, 0.0011964480985294688, 0.006205190210441365, 0.004259198904037476], 'topk_tokens': ['<|end_header_id|>', 'assistant', 'b', ' impressions', ' will', ' directly', ' newspaper', ' of', ' hour', 'the', 'athroom', ' Daniel', ' a', 's', 'able', ' If', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0007345795631408692, 0.7221204042434692, 0.3257915735244751, 0.00268171230951945]}, 'saliency': {'score': [0.00019534744999625465, 7.333124008424644e-05, 6.130879575555974e-05, 7.274729634804155e-05, 4.1397718282846305e-05], 'topk_tokens': ['\n\n', ' Project', ' Brown', ' Bank', '<|start_header_id|>', '***', ' newspaper', 'peak', 'the', ' printer', ' of', 's', 'assistant', 'able', '<|eot_id|>', ' a', 'athroom', ' If', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.25885009765625e-05, 0.00039684275786081946, 0.00024155378341674807, 0.00010764598846435547]}}, 25: {'grad': {'score': [0.2401393543590199, 0.2326574899405668, 0.3163278753107244, 0.2321735735565539, 0.3012304122631366], 'topk_tokens': [' the', '<|eot_id|>', ' Paul', ' const', ' printer', ' the', ' THE', ' John', ' *\n\n', ' *\n\n', ' the', '<|end_header_id|>', '<|start_header_id|>', ' the', ' P', ' the', ' the', '\n\n\n\n\n\n\n', '<|eot_id|>', '<|eot_id|>'], 'evidence_proportions': [0.321832275390625, 0.14398956298828125, 0.223455810546875, 0.28211466471354163]}, 'weight': {'score': [0.30177387866106903, 0.00756076687044947, 0.0011759237809614701, 0.006032721042402923, 0.004112144502309652], 'topk_tokens': ['4', ' will', ' directly', 'assistant', ' newspaper', ' hour', ' of', '\n\n', '<|end_header_id|>', 'athroom', ' Daniel', 'the', ' a', 's', 'ed', 'able', ' If', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0010146498680114746, 0.7693588733673096, 0.3952996015548706, 0.006883472204208374]}, 'saliency': {'score': [0.0006089508533477783, 0.00011205543704338038, 7.538091052662243e-05, 0.0001096121794238514, 6.234703155664298e-05], 'topk_tokens': [' proceedings', '<|eot_id|>', 'the', ' Mr', ' stere', 'ed', 's', ' newspaper', ' Alexander', ' members', '<|begin_of_text|>', ' If', ' mob', 'able', 'b', ' a', 'assistant', '\n\n', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [2.5480985641479492e-05, 0.0012785096963246663, 0.0007163047790527344, 0.0003361552953720093]}}, 26: {'grad': {'score': [0.3133675510233099, 0.3702408755064694, 0.2831916809082031, 0.3710049429340252, 0.35310824100787824], 'topk_tokens': [' would', ' for', ' for', ' *', '\n', '\n', ' H', '\n', ' arranged', '.', '\n', ' *', '!"', '.', ' They', ' quite', ' their', ' Guards', '!', "'"], 'evidence_proportions': [0.5011352539062499, 0.2274235486984253, 0.31549758911132814, 0.24106343587239584]}, 'weight': {'score': [0.18936495347456497, 0.007612000449908383, 0.0032920810309323397, 0.006670029711654288, 0.006840623342073881], 'topk_tokens': ['b', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' directly', ' will', ' newspaper', 'athroom', ' of', ' hour', 'the', ' Daniel', ' a', 's', 'able', ' If', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.017699193954467774, 0.48929131031036377, 0.21917991638183593, 0.00764759381612142]}, 'saliency': {'score': [0.0008380291136828336, 0.00012898630860200242, 4.933909936384721e-05, 0.00012564492271673725, 0.0002116641172995934], 'topk_tokens': ['ian', 'Bridge', ' Daniel', ' of', 'gro', ' Knowledge', ' a', '<|start_header_id|>', ' will', '.', 's', 'ed', '-column', ' returned', 'able', '.', 'fax', '<|begin_of_text|>', ' If', '<|end_header_id|>'], 'evidence_proportions': [0.00017293691635131834, 0.0019573618968327837, 0.0009917378425598144, 0.0001448492209116618]}}, 27: {'grad': {'score': [0.3423122059215199, 0.4246500884222779, 0.3799813010475852, 0.4253243516310762, 0.5044226279625525], 'topk_tokens': ['es', '\n', 'his', 'nes', '\n\n', 'nes', 'nes', 'human', 'nes', '\n', ' fr', '\n', 'nes', 'us', '\n', 'nes', '\n', '\n', 'nes', '\n'], 'evidence_proportions': [0.34137725830078125, 0.2155901590983073, 0.3872894287109375, 0.43233235677083337]}, 'weight': {'score': [0.31285811825232074, 0.0076142962819184135, 0.0014170923016288064, 0.006026693580233452, 0.003868898520102868], 'topk_tokens': ['b', 'assistant', '<|end_header_id|>', ' directly', ' had', ' impressions', ' newspaper', 'athroom', ' of', ' hour', ' Daniel', 'the', ' a', 's', 'ed', '.', 'able', '.', ' If', '<|begin_of_text|>'], 'evidence_proportions': [0.0022644758224487304, 0.8350874185562134, 0.3662627100944519, 0.00495302677154541]}, 'saliency': {'score': [0.0008847090330990878, 0.00017794344748558037, 7.701868360692805e-05, 0.00017472711107675634, 0.0003041510398571308], 'topk_tokens': ['<|start_header_id|>', '\n\n', '<|eot_id|>', 'ed', ' hour', '.', ' newspaper', ' If', 'daily', ' Alexander', '<|end_header_id|>', 'assistant', 's', ' of', '<|start_header_id|>', 'the', ' a', 'able', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [0.00038386583328247075, 0.0021488020817438764, 0.0005707442760467529, 0.0002996226151784261]}}, 28: {'grad': {'score': [0.7120813889936968, 0.7760234785398759, 0.7957097833806818, 0.7762584272038522, 0.7649970421424279], 'topk_tokens': [' a', ' a', ' A', ' a', '.', ' a', ' a', ' a', ' a', ' A', ' a', ' a', ' a', ' a', ' a', ' a', 'A', ' A', ' a', ' a'], 'evidence_proportions': [0.9408935546875, 0.4299704233805338, 0.671002197265625, 0.8377482096354167]}, 'weight': {'score': [0.3197805109349164, 0.0074921392096489415, 0.001879551193930886, 0.005864034180302877, 0.002547523150077233], 'topk_tokens': ['<|start_header_id|>', '4', 'assistant', ' newspaper', 'b', ' hour', ' of', '\n\n', 'the', ' Daniel', '<|end_header_id|>', ' a', 's', 'athroom', 'ed', '.', 'able', ' If', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005531132221221924, 0.8599357207616171, 0.37060462236404423, 0.003294706344604492]}, 'saliency': {'score': [0.001381986520507119, 0.00011391645971660742, 6.1925161968578e-05, 0.00010746044311744366, 7.050140545918391e-05], 'topk_tokens': [' hour', '4', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'the', ' Daniel', '.', 'athroom', 'ed', 'assistant', ' a', 'able', '.', 'b', 's', '\n\n', ' If', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [2.3639202117919923e-05, 0.0038712074359258013, 0.0012929677963256837, 9.890397389729817e-05]}}, 29: {'grad': {'score': [0.35027937455610797, 0.42318190726760235, 0.38709605823863635, 0.42376051354132105, 0.4913623516376202], 'topk_tokens': ['Anything', ' to', 'vent', ' different', ' early', ' either', ' as', 'ex', ' great', ' decided', ' very', ' when', 'ot', 'en', 'out', ' with', ' than', 'SP', 'APER', ' would'], 'evidence_proportions': [0.30892333984375, 0.3859100341796875, 0.38032226562500004, 0.32407633463541663]}, 'weight': {'score': [0.3304305835203691, 0.007491011332645799, 0.0018526315689086914, 0.005806497280201856, 0.003009871794627263], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', 'b', 'assistant', '\n\n', 'athroom', ' newspaper', '<|end_header_id|>', ' hour', ' of', ' Daniel', 'the', 's', ' a', 'ed', '.', ' If', 'able', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.001713132858276367, 0.8530638217926025, 0.42259750366210935, 0.004922787348429361]}, 'saliency': {'score': [0.0013467994603243742, 0.00013424209190075355, 6.062063303860751e-05, 0.00012819561747740593, 0.0002442409212772663], 'topk_tokens': ['<|eot_id|>', ' of', ' W', 'athroom', 'the', '<|eot_id|>', ' newspaper', '\n\n', 'ed', 's', ' *\n\n', '<|end_header_id|>', '.', ' If', ' hour', ' a', '.', 'assistant', '<|begin_of_text|>', 'able'], 'evidence_proportions': [7.405877113342285e-05, 0.0034942626953125, 0.0015984237194061278, 5.026658376057942e-05]}}, 30: {'grad': {'score': [0.3196352178400213, 0.3526667437330244, 0.44604561545632104, 0.3523463668970528, 0.3502375529362605], 'topk_tokens': ['Cut', ' of', ' went', ' of', ' By', ' Bor', ' Knowledge', ' Republicans', ' back', ' Alexander', ' had', ' a', ' an', ' James', ' M', 'istributed', ' trib', ' be', ' B', ' B'], 'evidence_proportions': [0.3044586181640625, 0.2766685485839844, 0.2622314453125, 0.42308553059895837]}, 'weight': {'score': [0.20808736844496292, 0.007410594818129808, 0.0039533214135603475, 0.006363580096872617, 0.005382721240703876], 'topk_tokens': ['<|start_header_id|>', ' of', 'assistant', 'the', '<|eot_id|>', '\n\n', 's', ' a', '<|start_header_id|>', '<|eot_id|>', ' Daniel', 'able', ' If', 'b', '<|end_header_id|>', 'athroom', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.005025577545166016, 0.5465066035588583, 0.23806753158569335, 0.013902823130289715]}, 'saliency': {'score': [0.0008568804372440685, 0.00019335112006980802, 0.00012137944048101252, 0.0001902106113760628, 0.000275255968937507], 'topk_tokens': [' but', ' spoken', ' mob', ' W', '.', '<|end_header_id|>', '\n\n', 'ed', '<|eot_id|>', ' B', '<|begin_of_text|>', 'b', '-text', 'assistant', '<|start_header_id|>', ' Alexander', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0002535581588745117, 0.0020548303922017417, 0.0004978299140930176, 0.00046090781688690186]}}, 31: {'grad': {'score': [0.4024810791015625, 0.5516404568181225, 0.4466556202281605, 0.5529896780330702, 0.6356940636268029], 'topk_tokens': ['ian', ' location', ' the', ' speech', ' return', 'Question', ' the', ' location', ' and', ' and', ' were', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', ' be', ' coach', ' only', '<|start_header_id|>'], 'evidence_proportions': [0.438958740234375, 0.27620697021484375, 0.4624420166015625, 0.4483896891276042]}, 'weight': {'score': [0.19262775507840244, 0.006949492830715526, 0.002579212188720703, 0.005986950457326234, 0.003200767131952139], 'topk_tokens': ['.\n\n', 'able', ' directly', ',', 'Question', '<|eot_id|>', ' If', '<|start_header_id|>', ' Daniel', ' set', 'assistant', '\n\n', '<|eot_id|>', 'ed', '<|end_header_id|>', 'b', '.', '.', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.002239561080932617, 0.4508268435796102, 0.2971316337585449, 0.005998929341634115]}, 'saliency': {'score': [0.0009484358809211037, 0.00014286481178478615, 8.769197897477584e-05, 0.0001388810373641349, 0.00012041800297223605], 'topk_tokens': ['Answer', ' was', '-text', '<|eot_id|>', 'assistant', ' unless', '.', 'Question', ' they', 'ulations', 'ed', '<|start_header_id|>', '.', '<|eot_id|>', '\n\n', ' set', '<|begin_of_text|>', 'b', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [0.0001392066478729248, 0.0025273760159810386, 0.0008318483829498291, 0.00014100968837738037]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:20:44.839 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:20:44.839 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-4_pid-2_1-3-6-9.pkl | len: 10 |  size: 8.83 KB
Processing depth (1, 3, 6, 9):   3%|▎         | 3/100 [00:44<23:59, 14.84s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (0, 3, 5, 9):   3%|▎         | 3/100 [00:54<23:59, 14.84s/it]2025-01-23 22:20:54.891 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:20:54.891 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-23 22:20:54.891 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:20:54.899 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1442, 1448) --> . Daniel journeyed to the
2025-01-23 22:20:54.899 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:20:54.909 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2086, 2091) --> . Daniel went to the
2025-01-23 22:20:54.909 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary journeyed to the office.
2025-01-23 22:20:54.928 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3647, 3653) --> . Mary journeyed to the
2025-01-23 22:20:54.928 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:20:54.929 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (145, 151) --> . John went back to the
2025-01-23 22:20:54.929 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-23 22:20:54.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1987, 1993) --> . Sandra journeyed to the
2025-01-23 22:20:54.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary got the football there.
2025-01-23 22:20:54.955 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3374, 3379) --> . Mary got the football
2025-01-23 22:20:54.956 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:20:54.967 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2335, 2340) -->  Mary moved to the bathroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:20:55.440 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:20:55.440 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4200])
your chose emoji: ['🙍🏻\u200d♀️', '👩🏾\u200d🏭', '💆🏻', '🇺🇦', '🌦', '🇬🇮', '🦹🏽\u200d♀️', '🪆', '🏇🏻', '🧑🏿\u200d⚖']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4203, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 239674.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.55it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.22it/s]
2025-01-23 22:20:58.479 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.15088774941184305, 0.13341329621327028, 0.10642927343195135, 0.13346359929834373, 0.13253475303080545], 'topk_tokens': [' the', 'asc', '.', '.', 'announcement', '\n', 'ot', ' announcement', '.', 'ian', ' the', '.', '\n', '.', ' conventions', 'ol', ' of', 'announcement', 'ian', '.'], 'evidence_proportions': [0.09492530822753907, 0.1850255330403646, 0.1547687530517578, 0.16015116373697916]}, 'weight': {'score': [0.028158094395290722, 0.007528706899348424, 0.019013968381014736, 0.00735882897737021, 0.001878386113181043], 'topk_tokens': ['assistant', 'Bridge', '<|eot_id|>', 'of', '\n\n', ' ', '.', 'b', '<|end_header_id|>', ' ', ' bathroom', 'athroom', ',', 'the', ' the', 'able', ',', 'ides', 'daily', '<|begin_of_text|>'], 'evidence_proportions': [0.07467575073242189, 0.02243645985921224, 0.016849952936172485, 0.0045384665330251055]}, 'saliency': {'score': [0.00035403533415360886, 4.3099433672930156e-05, 0.0002102296460758556, 4.057059152735687e-05, 1.0385442135938958e-05], 'topk_tokens': ['.', '\n\n', ' PA', ' ', ' boat', '<|end_header_id|>', ' the', '<|begin_of_text|>', 'Daniel', ' the', 'ides', ' apple', 'daily', ' Bench', ' bedroom', ' bedroom', 'Bridge', ' bathroom', 'b', 'athroom'], 'evidence_proportions': [0.0008436143398284912, 0.00043455759684244793, 0.00015239715576171873, 3.356238206227621e-05]}}, 25: {'grad': {'score': [0.11110773953524503, 0.11960326073823757, 0.11165341463955966, 0.11969025234214158, 0.1239527090271907], 'topk_tokens': ['cap', ' ty', ' composing', ' FR', ' great', ' very', ' New', ' step', ' wood', ' compos', ' great', '1', ' York', 'ig', 'UG', 'ANK', ' fr', ' Francis', ' web', 'ir'], 'evidence_proportions': [0.11607170104980469, 0.10885111490885417, 0.10372543334960938, 0.11537965138753255]}, 'weight': {'score': [0.01816452226855538, 0.007484110202104059, 0.013682805679061195, 0.0073948242280826625, 0.002699747014401564], 'topk_tokens': [' the', '<|eot_id|>', 'Daniel', '.', ' ', 'assistant', ' ', ' Bench', ' bathroom', 'athroom', '\n\n', '<|end_header_id|>', 'the', ',', 'able', ' the', ',', 'ides', 'daily', '<|begin_of_text|>'], 'evidence_proportions': [0.041692352294921874, 0.01681971549987793, 0.013535243272781373, 0.003760536511739095]}, 'saliency': {'score': [0.00045803324742750687, 4.17618958007827e-05, 0.00014868107709017667, 3.8994357516542595e-05, 2.156576113914376e-05], 'topk_tokens': [':', ' apple', ' Daniel', ' Bench', ' PA', '<|eot_id|>', 'athroom', ' Daniel', ' apple', ' bathroom', 'the', ',', ',', 'Daniel', 'able', '<|end_header_id|>', ' the', '\n\n', 'ides', 'daily'], 'evidence_proportions': [0.0009147584438323975, 0.0006187856197357178, 0.0003305494785308838, 2.291301886240641e-05]}}, 26: {'grad': {'score': [0.0700278715653853, 0.08354137456051203, 0.060654304244301536, 0.0837339238783407, 0.08327992994393875], 'topk_tokens': ['ol', '.', ' Proof', 'g', 'ian', ' block', ' the', ' Press', 'ian', 'ian', 'sur', ' Gutenberg', 'burn', ' trib', 'ab', ' Paul', 'super', ' res', ' Paul', 'b'], 'evidence_proportions': [0.06382598876953124, 0.08449554443359375, 0.07440261840820311, 0.0570828119913737]}, 'weight': {'score': [0.026616012508218937, 0.007406011014616605, 0.035611716183749115, 0.007155194581199879, 0.0053717342775259445], 'topk_tokens': [' Bench', ' ', ' bathroom', 'b', '<|eot_id|>', '<|start_header_id|>', '\n\n', ' the', ':', ' bathroom', '<|end_header_id|>', 'athroom', ',', ' the', 'the', 'able', ',', 'ides', 'daily', '<|begin_of_text|>'], 'evidence_proportions': [0.054736328125, 0.017935951550801597, 0.020369446277618407, 0.017067948977152504]}, 'saliency': {'score': [0.00023907152089205655, 4.651546903715059e-05, 0.0002802054990421642, 4.426074102538857e-05, 5.371134672591935e-05], 'topk_tokens': [' boat', '<|start_header_id|>', ' \n', '<|end_header_id|>', ',', ',', 'able', ' bathroom', 'the', 'ides', 'Daniel', 'daily', 'assistant', '<|begin_of_text|>', ' the', 'Bridge', 'athroom', ' the', ' bathroom', 'b'], 'evidence_proportions': [0.0004991710186004638, 0.0002528478701909383, 0.0001648426055908203, 7.04030195871989e-05]}}, 27: {'grad': {'score': [0.07221486351706764, 0.09355464538675871, 0.061568043448708275, 0.09383672772476552, 0.10080903323728647], 'topk_tokens': [' provide', 'ern', ' is', ' heard', ' Stephen', ' one', 'ad', ' o', ' o', 'g', ' business', ' o', ' would', ' news', ' o', ' would', 'sur', '202', ' instance', ' o'], 'evidence_proportions': [0.12885551452636718, 0.06636444727579752, 0.04659957885742187, 0.05221080780029297]}, 'weight': {'score': [0.021610392765565353, 0.007512967075417786, 0.018195499073375355, 0.00738188771279737, 0.003842275534103166], 'topk_tokens': [' apple', 'assistant', 'Daniel', ' bathroom', ' the', 'b', '\n\n', ' ', ':', ' bathroom', '<|end_header_id|>', 'athroom', ' the', ',', 'the', 'able', ',', '<|begin_of_text|>', 'ides', 'daily'], 'evidence_proportions': [0.05183868408203125, 0.0161893367767334, 0.015617716312408447, 0.0068351030349731445]}, 'saliency': {'score': [0.0005870136347683994, 6.064586828868502e-05, 0.00026337125084616923, 5.6789160118735905e-05, 7.519659711353814e-05], 'topk_tokens': ['Dub', ' PA', ' bedroom', ',', 'NEW', ' Daniel', ' bedroom', '.\n\n', ',', ':', 'the', ' bathroom', 'athroom', ' the', 'able', 'Daniel', 'daily', 'ides', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.0008451759815216064, 0.0008905977010726929, 0.0005740523338317871, 7.909536361694336e-05]}}, 28: {'grad': {'score': [0.15267458829012784, 0.1433251630740877, 0.1478713398629969, 0.14325165904809375, 0.15587772540192105], 'topk_tokens': [' fight', 'ian', ' of', ' hand', '1', 'ian', 'wing', 'ian', ' half', ' as', '1', 'time', ' else', ' other', ' it', 'de', ' Proof', 'hand', ' journey', 'UG'], 'evidence_proportions': [0.138177490234375, 0.16417948404947916, 0.1326202392578125, 0.16996256510416666]}, 'weight': {'score': [0.014126793904737993, 0.007323537132667979, 0.02273162386634133, 0.007206045053532044, 0.0017681032864015494], 'topk_tokens': ['assistant', ' garden', ' bathroom', ' ', ' the', 'b', ' the', '<|end_header_id|>', ' the', '\n\n', 'athroom', ':', ',', 'the', ' the', 'able', ',', '<|begin_of_text|>', 'ides', 'daily'], 'evidence_proportions': [0.006198501586914063, 0.006569981575012207, 0.02990351915359497, 0.015143245458602905]}, 'saliency': {'score': [0.0002071423964066939, 5.114609617123909e-05, 0.0004590641368519176, 4.816313981149999e-05, 1.51231217740187e-05], 'topk_tokens': [' the', ' the', 'athroom', ' bathroom', ' bathroom', ' the', '\n\n', ' the', '<|end_header_id|>', '<|begin_of_text|>', ' the', ',', ':', 'the', 'able', 'Bridge', ',', ' the', 'daily', 'ides'], 'evidence_proportions': [4.015564918518067e-05, 0.00010973711808522542, 0.0006149113178253174, 0.00010389586289723714]}}, 29: {'grad': {'score': [0.18226866288618607, 0.15281927021179997, 0.1640824404629794, 0.1526039116197436, 0.22061602749041656], 'topk_tokens': [' not', ' editors', ' value', ' which', ' is', ' work', ' Guards', ' ever', ' its', ' when', ' affairs', ' when', ' If', 'd', ' be', ' him', ' advantage', 'antics', ' writer', ' enough'], 'evidence_proportions': [0.23598022460937498, 0.14692179361979166, 0.22479858398437502, 0.13741429646809897]}, 'weight': {'score': [0.008266102183948864, 0.007388995540672446, 0.00884096459908919, 0.007376675351820018, 0.0012170285431306754], 'topk_tokens': [' was', ' before', 'b', ' the', '<|start_header_id|>', ' ', 'If', '<|end_header_id|>', 'athroom', ' the', ':', '\n\n', ',', ' the', 'the', ',', 'able', '<|begin_of_text|>', 'ides', 'daily'], 'evidence_proportions': [0.006242609024047852, 0.0029334823290506997, 0.011718380451202392, 0.012408067782719929]}, 'saliency': {'score': [6.687505678697066e-05, 5.21603749248206e-05, 9.899789636785334e-05, 5.1834780197070176e-05, 1.8920916229931275e-05], 'topk_tokens': ['<|eot_id|>', ' ', ' garden', ',', '<|start_header_id|>', ' the', ' before', ' was', '<|end_header_id|>', 'assistant', ':', 'athroom', 'b', ' the', 'ides', 'daily', 'the', 'able', ' the', '\n\n'], 'evidence_proportions': [4.960298538208008e-05, 1.3977289199829102e-05, 9.161829948425292e-05, 0.00011354684829711914]}}, 30: {'grad': {'score': [0.1015606468374079, 0.08938744395453675, 0.10348181291060014, 0.0892484955100894, 0.13663508642965289], 'topk_tokens': [' the', ' Online', ' senate', 'AP', 'ION', ' Hon', '�', 'RE', ' Project', ' Times', ' Hor', ' THE', ' EAR', ' FR', '�', ' im', 'UG', '�', 'b', ' Hon'], 'evidence_proportions': [0.09022388458251952, 0.1216202974319458, 0.10717077255249023, 0.086273193359375]}, 'weight': {'score': [0.031740741296248, 0.007197868815496255, 0.03712798790498213, 0.006909721228445227, 0.012650916825479535], 'topk_tokens': [' bathroom', ' the', ' bathroom', 'b', '<|eot_id|>', '<|eot_id|>', ',', 'the', 'assistant', ' the', 'able', ',', '\n\n', ':', ' the', '<|end_header_id|>', 'athroom', 'ides', '<|begin_of_text|>', 'daily'], 'evidence_proportions': [0.022919464111328128, 0.029720147450764973, 0.03812251091003418, 0.03579425811767578]}, 'saliency': {'score': [0.0008183839646252719, 0.00010210358611521425, 0.0010691217400810935, 9.319938613577914e-05, 9.778186456481022e-05], 'topk_tokens': [' to', 'Bridge', ' the', ' the', ' to', ' about', 'assistant', ' the', ' the', 'Daniel', 'daily', ' bathroom', ' the', '<|end_header_id|>', ' the', ' bathroom', ':', 'athroom', ' the', 'b'], 'evidence_proportions': [0.0005006194114685059, 0.0009994655847549438, 0.0011050403118133545, 0.0006632258494695028]}}, 31: {'grad': {'score': [0.14087164402008057, 0.16318253656242565, 0.12546048922972244, 0.16350009479210853, 0.1427298401718709], 'topk_tokens': [' a', ' a', ' o', 't', ' to', ' to', ':', ' o', ' veto', '\n', '\n', '\n', ' o', '\n', '\n', ' o', 'd', ' a', 'g', ' a'], 'evidence_proportions': [0.06951532363891601, 0.17187182108561197, 0.14026985168457032, 0.16983656088511148]}, 'weight': {'score': [0.009281505237926136, 0.0069116793897303404, 0.012524886564774946, 0.0068694512756377005, 0.004061711368276112], 'topk_tokens': [':', 'the', ',', 'assistant', '.\n\n', ':', ' the', 'b', '<|start_header_id|>', '?', 'able', '<|eot_id|>', ' \n', ',', '\n\n', 'ides', 'daily', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006073570251464844, 0.005321184794108073, 0.011843395233154298, 0.013780196507771809]}, 'saliency': {'score': [0.00014009529894048518, 7.700797435416513e-05, 0.0001336715438149192, 7.63745240851015e-05, 4.062234465755633e-05], 'topk_tokens': ['<|eot_id|>', ' the', ' the', '<|end_header_id|>', ' the', ' \n', 'able', ' ', '<|begin_of_text|>', 'assistant', ',', 'If', ' the', 'ides', 'daily', '<|start_header_id|>', '.\n\n', '?', 'b', 'athroom'], 'evidence_proportions': [7.600188255310058e-05, 0.00014126300811767578, 0.00022442340850830077, 0.00012206534544626872]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:20:58.487 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:20:58.487 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-4_pid-3_0-3-5-9.pkl | len: 10 |  size: 8.49 KB
Processing depth (0, 3, 5, 9):   4%|▍         | 4/100 [00:57<22:59, 14.37s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.27s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Processing depth (1, 4, 5, 9):   4%|▍         | 4/100 [01:09<22:59, 14.37s/it]2025-01-23 22:21:10.216 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:21:10.219 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (436, 441) --> . Daniel picked up the
2025-01-23 22:21:10.219 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:21:10.228 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1850, 1856) --> . Daniel journeyed to the
2025-01-23 22:21:10.228 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:21:10.239 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2156, 2161) -->  Daniel went to the garden
2025-01-23 22:21:10.240 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary journeyed to the office.
2025-01-23 22:21:10.259 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3675, 3681) --> . Mary journeyed to the
2025-01-23 22:21:10.259 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:21:10.260 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (140, 146) --> . John went back to the
2025-01-23 22:21:10.260 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-23 22:21:10.270 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2083, 2089) --> . Sandra journeyed to the
2025-01-23 22:21:10.270 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary got the football there.
2025-01-23 22:21:10.289 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3449, 3454) --> . Mary got the football
2025-01-23 22:21:10.290 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:21:10.301 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2418, 2423) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:21:10.804 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:21:10.805 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4237])
your chose emoji: ['🦻🏻', '🧑🏿\u200d❤\u200d🧑🏾', '👰🏽\u200d♂️', '🧜\u200d♀', '👿', '🧑🏽\u200d❤️\u200d💋\u200d🧑🏼', '👩🏻\u200d🦽\u200d➡', '🇳🇵', '🤾🏾\u200d♀', '👩🏼\u200d❤️\u200d💋\u200d👨🏻']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4240, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 180400.17it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.80it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.11it/s]
2025-01-23 22:21:13.744 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.1806429082697088, 0.15584047785345115, 0.18528535149314188, 0.15555605442894427, 0.1069163978099823], 'topk_tokens': ['PA', 'ION', 'RE', 'user', ' Sh', '.', 'EF', ' IN', ' EAR', ' Col', ' but', 'ioneer', 'reading', 'Cut', ' B', ' MO', ' Col', 'ol', 'ting', 'ANK'], 'evidence_proportions': [0.17497863769531252, 0.2091242472330729, 0.1610870361328125, 0.17317835489908853]}, 'weight': {'score': [0.0020341873168945312, 0.0074958664066386675, 0.0045980296351692896, 0.0075396960894191, 0.011252916489656154], 'topk_tokens': ['<|end_header_id|>', ' extortion', ' apple', 'ors', ' curs', ' luxury', ' will', ' to', ' item', ' waving', ' a', ' traveled', '❤', 'b', ' item', ' garden', '️', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004975974559783935, 0.0007957965135574341, 0.003723621368408203, 0.0031452079614003496]}, 'saliency': {'score': [3.306974064220082e-05, 6.542818726233716e-05, 6.358596411618319e-05, 6.560750440601171e-05, 5.4171452155480016e-05], 'topk_tokens': ['️', ' Hor', ' Project', '<|start_header_id|>', '.\n', ' set', '\n\n', ' John', '<|eot_id|>', ' power', '***', ' Brown', ' garden', ' Bank', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [1.0752677917480469e-05, 6.491939226786295e-06, 8.805394172668458e-05, 3.24249267578125e-05]}}, 25: {'grad': {'score': [0.17707998102361505, 0.18814010620117189, 0.31579728560014203, 0.18752877750887656, 0.2501987677354079], 'topk_tokens': [' the', ' the', ' I', 'LY', ' the', ' ST', ' ST', 'system', ' THE', ' the', ' the', '<|start_header_id|>', ' Col', ' STR', '\n\n\n\n', '\n', ' P', ' *\n\n', ' St', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.2167736053466797, 0.15954081217447919, 0.1969757080078125, 0.14496135711669922]}, 'weight': {'score': [0.0033814527771689677, 0.0074441007848055855, 0.0029654936356977983, 0.007488883342824742, 0.01362801629763383], 'topk_tokens': [' will', ' Newspaper', ' a', ' item', ' luxury', ' traveled', 'eward', ' waving', ' time', ' curs', ' item', '❤', 'b', ' garden', '\n\n', '️', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009239077568054199, 0.0010420183340708415, 0.0075961828231811525, 0.004256566365559896]}, 'saliency': {'score': [0.00019264763051813299, 9.863120767305482e-05, 8.487430485812101e-05, 9.821039989405978e-05, 0.00012086962278072651], 'topk_tokens': [' curs', ' tele', ' man', ' item', ' garden', 'Republicans', ' Southern', ' Stephen', ' Douglas', ' Daniel', '❤', ' senate', '<|eot_id|>', '️', '<|begin_of_text|>', '\n\n', 'assistant', 'athroom', 'b', '<|end_header_id|>'], 'evidence_proportions': [4.61876392364502e-05, 3.526608149210612e-06, 0.0007338523864746094, 5.281468232472738e-05]}}, 26: {'grad': {'score': [0.20186203176325018, 0.23363340125893647, 0.25984688238664105, 0.2336625417603892, 0.24475831710375273], 'topk_tokens': ['\n', 'Civil', '\n', ' *', ' first', '.', ' arranged', '\n', ' two', '!', ' *', ' *', '.', '\n', ' *', '\n', ' *', ' quite', ' John', '!"'], 'evidence_proportions': [0.2814041137695313, 0.0648813247680664, 0.375909423828125, 0.1275181770324707]}, 'weight': {'score': [0.004818336530165238, 0.007526381510608601, 0.005600128661502491, 0.007550679544986601, 0.013551629506624661], 'topk_tokens': ['<|eot_id|>', '<|start_header_id|>', 'b', ' will', ' time', 'eward', ' curs', ' extortion', ' traveled', ' a', ' waving', ' luxury', 'assistant', '<|end_header_id|>', ' garden', ' item', '❤', 'athroom', '️', '<|begin_of_text|>'], 'evidence_proportions': [0.005452513694763184, 0.002145091692606608, 0.007219123840332031, 0.004962444305419922]}, 'saliency': {'score': [0.00013078478249636564, 0.0001071938858279642, 0.0001515773209658536, 0.0001068374903799808, 0.0003480874001979828], 'topk_tokens': [' Foster', 'vent', 'port', 'asc', ' Foster', '�', '❤', 'out', ' Knowledge', 'outs', 'athroom', '�', 'ree', '️', 'nes', '<|end_header_id|>', ' composing', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.049773216247558e-05, 5.111098289489746e-06, 0.000444716215133667, 2.8421481450398765e-05]}}, 27: {'grad': {'score': [0.27654023603959516, 0.28882657896797614, 0.28642394325949927, 0.28890359458523324, 0.4370953761614286], 'topk_tokens': [' affairs', 'nes', ' spring', ' Foster', ' have', ' up', ' Col', ' vigorous', ' grat', ' Col', ' cap', 'fortunate', 'ATION', 'ayers', ' NEW', '♀', ' Papers', 'ERS', ' bogus', '�'], 'evidence_proportions': [0.2905609130859375, 0.20164108276367188, 0.485479736328125, 0.16563924153645834]}, 'weight': {'score': [0.0032933232459154997, 0.007499916598481952, 0.004275072704661976, 0.007538880282805236, 0.016305138285343464], 'topk_tokens': [' item', '<|end_header_id|>', 'ors', ' Newspaper', 'eward', ' a', ' traveled', ' extortion', ' time', 'b', ' curs', ' luxury', ' waving', ' item', 'assistant', '❤', ' garden', '️', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0014729201793670654, 0.0010902881622314453, 0.007338714599609376, 0.0036422014236450195]}, 'saliency': {'score': [0.00019073621793226763, 0.00018441250442333942, 0.00022138926115902987, 0.00018418547621900633, 0.0006981254197083986], 'topk_tokens': [' traveled', ' extortion', ' time', ' waving', '♀', '<|eot_id|>', ' Pennsylvania', '�', ' *\n\n', ' a', '�', '❤', '️', 'b', '<|end_header_id|>', '�', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [4.718899726867676e-05, 6.92903995513916e-06, 0.0007096171379089356, 6.176531314849854e-05]}}, 28: {'grad': {'score': [0.5520659360018644, 0.48629127358490565, 0.5725929953835227, 0.4854939236200004, 0.4724736213684082], 'topk_tokens': [' a', ' a', ' a', ' a', ' a', ' a', 'a', ' a', ' a', ' a', ' a', ' A', ' a', ' a', ' A', ' a', ' A', ' a', ' a', ' a'], 'evidence_proportions': [0.6285469055175781, 0.4955902099609375, 0.63551025390625, 0.4752705891927083]}, 'weight': {'score': [0.001524199138988148, 0.007344414378112217, 0.003327865492213856, 0.00739598940450652, 0.013595375877160292], 'topk_tokens': ['<|start_header_id|>', 'tele', ' a', 'ors', ' Newspaper', ' luxury', ' traveled', ' time', ' curs', ' waving', ' item', ' garden', '❤', 'assistant', '\n\n', 'b', '️', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000744938850402832, 0.0008209347724914551, 0.0021821022033691405, 0.002328594525655111]}, 'saliency': {'score': [2.5703148408369586e-05, 0.00011060351751885324, 5.57642091404308e-05, 0.00011133618503666925, 7.157944715940036e-05], 'topk_tokens': [' time', ' the', 'announcement', ' agreement', ' \n', ' then', '❤', '<|eot_id|>', ' curs', '<|eot_id|>', ' item', '️', ' garden', ' almost', '\n\n', 'assistant', 'b', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [1.9842386245727538e-05, 1.2899438540140787e-05, 6.213188171386719e-05, 1.3033548990885418e-05]}}, 29: {'grad': {'score': [0.36439280076460406, 0.3816820396567291, 0.40840669111772016, 0.3816325689521259, 0.5463369076068585], 'topk_tokens': [' entire', ' upper', 'antics', '�', ' up', '�', ' over', 'es', 'being', ' heard', '�', 'ot', 'en', '�', ' happened', 'ed', 'inen', 'ot', 'PA', 'ed'], 'evidence_proportions': [0.23819007873535156, 0.313812255859375, 0.62169189453125, 0.30572636922200525]}, 'weight': {'score': [0.002481139518997886, 0.007323057696504413, 0.0029575323516672306, 0.00737133313918591, 0.018546450596589308], 'topk_tokens': ['<|start_header_id|>', '<|eot_id|>', 'eward', ' time', ' extortion', ' Newspaper', ' luxury', ' waving', ' item', '<|eot_id|>', ' curs', ' garden', '\n\n', 'b', '❤', 'assistant', '<|end_header_id|>', '️', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010112941265106202, 0.0010222991307576497, 0.003249073028564453, 0.004524906476338705]}, 'saliency': {'score': [0.00013745914806019175, 0.00016489605999217842, 9.599057110873136e-05, 0.0001654011917409724, 0.0005227668354144463], 'topk_tokens': [' Col', 'APER', '<|eot_id|>', 'Republicans', ' luxury', '***', '!"', ' curs', '<|start_header_id|>', '\n\n', '❤', '♀', 'b', '<|eot_id|>', '️', '<|eot_id|>', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [3.9005279541015624e-05, 1.4026959737141926e-05, 0.0004540562629699707, 7.910529772440593e-05]}}, 30: {'grad': {'score': [0.4842348965731534, 0.40001183995660744, 0.49606938795609906, 0.39906661276367306, 0.2782450639284574], 'topk_tokens': ['.', ' bedroom', ' John', ' picked', 'Cut', ' up', ' James', ' Pioneer', ' Good', ' a', ' back', ' but', ' M', ' almost', ' but', ' trib', ' Sh', ' B', ' B', ' be'], 'evidence_proportions': [0.6178802490234375, 0.6162109375, 0.24936523437500002, 0.4366124471028646]}, 'weight': {'score': [0.0033315040848471904, 0.007352975629410654, 0.004276622425426136, 0.007390190153831067, 0.008352153576337375], 'topk_tokens': [' upper', '<|eot_id|>', ' senate', ' huge', ' \n', '***', '\n\n', ' item', ' garden', '❤', '<|start_header_id|>', '<|start_header_id|>', 'assistant', '️', '<|eot_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002026057243347168, 0.0021146933237711587, 0.006889247894287109, 0.0026714007059733076]}, 'saliency': {'score': [0.00020178339698097923, 0.0002054810031967343, 0.00015346164053136653, 0.00020577313220421396, 0.00028647539707330556], 'topk_tokens': [' Min', '<|end_header_id|>', ' Joseph', 'political', '<|eot_id|>', 'Republicans', ' Sh', 'b', ' huge', 'athroom', ' almost', '<|start_header_id|>', '-text', ' B', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.00018963217735290527, 2.8153260548909507e-05, 0.0005749940872192382, 7.45306412378947e-05]}}, 31: {'grad': {'score': [0.5021852579983798, 0.5720130848434736, 0.4703044024380771, 0.5729124659691911, 0.7493391220386212], 'topk_tokens': ['\n', '�', '\n', ' and', ' history', ',', ' speech', ',', ' and', '\n', ',', ' secured', '�', '.', '�', '<|eot_id|>', '\n', '<|end_header_id|>', ' the', '�'], 'evidence_proportions': [0.3975193023681641, 0.3862369855244955, 0.7057861328125, 0.5356877644856771]}, 'weight': {'score': [0.0026644901795820756, 0.006754240449869408, 0.0028191425583579325, 0.006796315440231556, 0.004042699933052063], 'topk_tokens': [' apple', ' they', ' item', ' will', 'Answer', ' garden', '️', ' \n', ' set', ' that', '<|eot_id|>', '.\n\n', '<|start_header_id|>', '\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0011707305908203124, 0.001798391342163086, 0.0026786327362060547, 0.004763603210449219]}, 'saliency': {'score': [6.041608073494651e-05, 0.0001699232689614566, 7.414275949651545e-05, 0.00017099961056041082, 0.0001390877251441662], 'topk_tokens': [' also', '<|eot_id|>', ' unless', ',', ' be', ' item', ' of', ' upper', 'assistant', '\n\n', ' will', '.\n\n', ' that', ' set', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [2.9516220092773437e-05, 2.7577082316080727e-05, 0.0001033961772918701, 8.318821589152017e-05]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:21:13.751 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:21:13.751 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-4_pid-4_1-4-5-9.pkl | len: 10 |  size: 9.04 KB
Processing depth (1, 4, 5, 9):   5%|▌         | 5/100 [01:13<23:15, 14.69s/it]Processing depth (1, 4, 5, 9):   5%|▌         | 5/100 [01:13<23:11, 14.65s/it]
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:99 - Selected idx: 5
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:103 - Needle: [' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-23 22:21:13.959 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.13s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.19s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Processing depth (1, 5, 7, 8, 9):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:21:23.759 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:21:23.761 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (436, 442) --> . Daniel journeyed to the
2025-01-23 22:21:23.762 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:21:23.772 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2125, 2129) -->  Daniel grabbed the milk
2025-01-23 22:21:23.772 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:21:23.786 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2864, 2869) --> . Daniel went to the
2025-01-23 22:21:23.786 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:21:23.802 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3309, 3313) -->  Daniel left the milk
2025-01-23 22:21:23.803 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary journeyed to the office.
2025-01-23 22:21:23.821 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3625, 3631) -->  Mary journeyed to the office
2025-01-23 22:21:23.821 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:21:23.830 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1766, 1772) --> . Sandra journeyed to the
2025-01-23 22:21:23.830 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:21:23.843 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2671, 2676) --> . Mary got the football
2025-01-23 22:21:23.843 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the bedroom.
2025-01-23 22:21:23.844 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (86, 92) --> . John went back to the
2025-01-23 22:21:23.844 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:21:23.850 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1295, 1300) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:21:24.389 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bedroom.<|eot_id|>
2025-01-23 22:21:24.389 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4231])
your chose emoji: ['👨🏿\u200d⚕', '🌶', '🧖🏾\u200d♂️', '🧝🏿', '👨🏿\u200d🤝\u200d👨🏽', '🧑🏼\u200d🌾', '👰🏻\u200d♂️', '👩🏿\u200d🦽\u200d➡', '9⃣', '🍼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4234, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 233016.89it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.86it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.48it/s]
2025-01-23 22:21:27.349 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.75616455078125, 0.6089617657560079, 0.7207971052689985, 0.6074952152198444, 0.623414982066435], 'topk_tokens': [' political', ' by', ' Charles', ' bill', ' state', ' between', ' very', ' by', ' business', '.', ' still', ' John', ' still', ' balance', ' John', ' by', ' bend', ' state', ' P', 'athroom'], 'evidence_proportions': [0.62811279296875, 0.89105224609375, 0.753955078125, 0.83184814453125, 0.7456766764322917]}, 'weight': {'score': [0.0019307231903076172, 0.0073748112400579295, 0.45987576110796496, 0.005029714823566361, 0.0017143512473386878], 'topk_tokens': ['<|start_header_id|>', '\n', '\n\n', ':', '<|eot_id|>', 'Answer', 'user', '\n\n', 'b', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0010946591695149739, 0.002628892660140991, 0.0024170756340026857, 0.0016067028045654297, 0.002112060785293579]}, 'saliency': {'score': [8.823037147521973e-05, 0.00010679382786865666, 0.002438692884011702, 9.465203354687231e-05, 6.362304967992446e-05], 'topk_tokens': ['<|start_header_id|>', ':', 'EF', 'Answer', 'user', ' ', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', '\n\n', '\n\n', '.', '.', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [4.749993483225505e-05, 0.00011504441499710083, 0.0001827120780944824, 7.815659046173096e-05, 3.906587759653728e-05]}}, 25: {'grad': {'score': [0.7767715454101562, 0.6747306495778224, 0.5673971392891624, 0.6746853462175506, 0.5221689953523524], 'topk_tokens': [' Daniel', ' to', ' no', ' old', ' some', ' down', ' late', ' now', ' prize', ' then', ' wait', ' rates', 'outs', ' prize', 'stage', ' rates', ' late', ' wait', ' at', 'old'], 'evidence_proportions': [0.6912027994791666, 1.0005474090576172, 0.873516845703125, 0.8155670166015625, 0.6066716512044271]}, 'weight': {'score': [0.0014465010166168212, 0.007165779524610805, 0.556971241127361, 0.004311053421778072, 0.0013224612264072194], 'topk_tokens': ['.\n\n', 'ENCES', '.\n\n', ' soon', 'MIN', '\n\n', ':', '?\n', 'Answer', 'b', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0007484753926595051, 0.0023079216480255127, 0.002390432357788086, 0.0011413171887397766, 0.000987092653910319]}, 'saliency': {'score': [0.00010014653205871582, 0.00010875322850289973, 0.004745880311185663, 8.443948873506687e-05, 4.625635988572064e-05], 'topk_tokens': ['\n\n', 'UL', ' Do', '<|eot_id|>', ' ', '<|eot_id|>', '.\n\n', '?\n', ' *\n\n', ' soon', 'ENCES', 'MIN', '<|start_header_id|>', 'assistant', 'athroom', '.', '.', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [7.311006387074788e-05, 0.00015914440155029297, 0.00018648505210876464, 5.8591365814208984e-05, 4.360576470692953e-05]}}, 26: {'grad': {'score': [0.4205741882324219, 0.43699877364600553, 0.376845966685902, 0.4374129070084278, 0.37071331248563877], 'topk_tokens': [' type', ' first', 'estead', ',\n', ' first', 'until', ' It', ' throughout', 'proc', ' time', ' time', ' time', ' question', ' service', ' bag', ' of', 'b', 'Johnson', '�', ' Fourth'], 'evidence_proportions': [0.3764165242513021, 0.39111328125, 0.508984375, 0.3903961181640625, 0.43081601460774743]}, 'weight': {'score': [0.0031562185287475585, 0.006953143262615393, 0.4382205605506897, 0.004709780458223002, 0.003504347801208496], 'topk_tokens': [' *\n\n', '<|eot_id|>', 'user', '?\n', '\n\n', '.\n\n', '\n\n', 'Answer', ':', '<|eot_id|>', 'b', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0019067327181498208, 0.004621744155883789, 0.004750871658325195, 0.002259969711303711, 0.002697308858235677]}, 'saliency': {'score': [8.248329162597656e-05, 9.656020026191203e-05, 0.004532140764323148, 7.333812008673916e-05, 8.563679807326373e-05], 'topk_tokens': ['4', '<|eot_id|>', ' bogus', ' bogus', '<|eot_id|>', 'RE', 'assistant', 'Answer', ':', '<|eot_id|>', '.\n\n', '\n\n', '\n\n', '<|end_header_id|>', '\n\n', '.', 'b', '<|begin_of_text|>', 'athroom', '.'], 'evidence_proportions': [8.145968119303386e-05, 5.37186861038208e-05, 0.00015015602111816408, 6.245076656341553e-05, 5.964438120524089e-05]}}, 27: {'grad': {'score': [0.3847135925292969, 0.4976510114133724, 0.5191549821333452, 0.4982123556016367, 0.5139873729032629], 'topk_tokens': [' offices', 'assistant', '<|end_header_id|>', ' West', '�', ' enemy', ' duration', ' conventions', '.', '<|start_header_id|>', ' work', 'Republicans', ' other', ' conventions', ' members', ' ', 'ers', '<|end_header_id|>', '.\n\n', '\n'], 'evidence_proportions': [0.3613300323486328, 0.4799652099609375, 0.448162841796875, 0.3115234375, 0.34051513671875]}, 'weight': {'score': [0.0035083866119384765, 0.007310541834986655, 0.6309161944822832, 0.004056596175167138, 0.003334173034219181], 'topk_tokens': ['.\n\n', 'RE', 'user', 'EF', '\n\n', 'RI', 'Answer', '?\n', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', ':', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0022843281428019204, 0.0038631558418273926, 0.005980777740478516, 0.0026612281799316406, 0.0030003786087036133]}, 'saliency': {'score': [0.00019194602966308595, 0.00016303858106069875, 0.0072295408357273445, 0.00012573604086062092, 0.00016606660450206083], 'topk_tokens': ['\n\n', '9', ' *\n\n', '?\n', 'RI', ' *\n\n', '<|eot_id|>', '.\n\n', 'EF', 'RE', '\n\n', ':', 'NEW', '<|begin_of_text|>', 'b', 'assistant', '.', 'athroom', '<|end_header_id|>', '.'], 'evidence_proportions': [0.0001375824213027954, 0.0002150014042854309, 0.0003714621067047119, 0.0001409277319908142, 0.00011535485585530599]}}, 28: {'grad': {'score': [0.9184765625, 0.8686869697408627, 0.8509306474165483, 0.8684829810310839, 0.934600830078125], 'topk_tokens': [' hand', ' sure', ' instead', ' so', '�', ' and', ' *', ' summer', ' that', ' half', ' summer', ' and', ' prev', ' under', 'half', ' Jul', 'ball', ' Dub', ' exc', ' *'], 'evidence_proportions': [0.9442545572916666, 1.027587890625, 1.0029052734375, 0.81146240234375, 0.8209431966145834]}, 'weight': {'score': [0.0035209083557128907, 0.007014290138860891, 0.6281825845891779, 0.003771307589702, 0.0018503988490385169], 'topk_tokens': ['If', 'us', ',', '.\n\n', '.\n\n', '.\n\n', '<|eot_id|>', 'Answer', '?\n', '<|eot_id|>', '<|start_header_id|>', 'b', ':', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0012181997299194336, 0.006321310997009277, 0.00509486198425293, 0.0025492608547210693, 0.0032928188641866045]}, 'saliency': {'score': [0.00021679043769836425, 0.0001189679076325854, 0.00687985664064234, 8.285968805344542e-05, 0.0001194613821366254], 'topk_tokens': [' was', ' Jul', ' milk', '.\n\n', '.\n\n', 'Just', ' bathroom', ' bathroom', ':', 'Answer', '?\n', 'assistant', 'b', '<|eot_id|>', '\n\n', 'athroom', '<|end_header_id|>', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [5.542238553365071e-05, 0.00044970959424972534, 0.0004053473472595215, 0.00013533979654312134, 0.00012004872163136801]}}, 29: {'grad': {'score': [0.4960650634765625, 0.3894598397321165, 0.4689351862127131, 0.38840572265158524, 0.5605798272525563], 'topk_tokens': [' fact', 'ideas', ' newspaper', ' December', ' editors', ' Wins', ' Hon', 'P', '�', '000', ' Do', ' *', 'pend', ' I', ' facts', ' extra', ' feature', ' work', ' to', ' H'], 'evidence_proportions': [0.40311876932779944, 0.5114974975585938, 0.54141845703125, 0.476226806640625, 0.5541540781656901]}, 'weight': {'score': [0.00197789192199707, 0.007195254683100395, 0.7283778569915078, 0.003437054735224259, 0.0012752094689537498], 'topk_tokens': ['\n\n', '.\n\n', '.\n\n', '.\n\n', ':', '<|eot_id|>', 'If', '<|eot_id|>', 'Answer', '<|start_header_id|>', 'b', '?\n', ':', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0006683468818664551, 0.00405120849609375, 0.0022166728973388675, 0.0008945167064666748, 0.0024284919102986655]}, 'saliency': {'score': [5.791187286376953e-05, 0.00012227396639552271, 0.0069612223993648185, 8.672397518772939e-05, 5.485695951125201e-05], 'topk_tokens': ['ENCES', ' P', 'us', '?\n', '      ', '\n\n', ':', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'b', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', '.', '.', 'athroom'], 'evidence_proportions': [2.9365221659342446e-05, 9.250640869140625e-05, 6.85572624206543e-05, 1.8514692783355713e-05, 8.07891289393107e-05]}}, 30: {'grad': {'score': [0.4362115478515625, 0.4070865881572538, 0.5013805736194957, 0.40641723261091345, 0.5429339913760914], 'topk_tokens': [' Bor', ' block', 'RI', ' board', '      ', ' bathroom', 'blue', ' an', 'body', ' body', ' went', ' bathroom', 'vent', ' based', 'ree', ' time', 'athroom', ' B', ' B', 'b'], 'evidence_proportions': [0.3808949788411458, 0.5350112915039062, 0.3845245361328125, 0.498565673828125, 0.4271647135416667]}, 'weight': {'score': [0.005679206848144531, 0.0071524760607114424, 0.415209791877053, 0.005017193228696792, 0.009364544644075282], 'topk_tokens': ['.\n\n', 'ANK', 'If', '�', '.\n\n', '.\n\n', 'Answer', '?\n', 'b', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', ':', 'assistant', '<|start_header_id|>', '\n\n', 'athroom', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0031227668126424155, 0.008068084716796875, 0.006396961212158204, 0.005758404731750488, 0.005992134412129721]}, 'saliency': {'score': [0.00016747117042541503, 0.00019610606672406815, 0.005021318793296814, 0.00017092364313268195, 0.0005274618373197668], 'topk_tokens': ['.\n\n', ' *\n\n', '.\n\n', ' bathroom', '�', ' bogus', 'Answer', '<|start_header_id|>', '�', ':', '<|eot_id|>', 'assistant', '?\n', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', '.', 'b', '.', 'athroom'], 'evidence_proportions': [0.00017885367075602213, 0.00024696439504623413, 0.0002048015594482422, 0.00014228373765945435, 8.877615133921304e-05]}}, 31: {'grad': {'score': [0.7177320861816406, 0.7012531206638226, 0.5573800693858754, 0.7019106876545486, 0.34805652394014247], 'topk_tokens': ['<|end_header_id|>', ' A', ' the', ' on', ' company', ' have', 'P', ' location', ' the', ' to', ' it', ' members', ' at', ' item', ' day', ' location', ' per', ' item', ' item', ' time'], 'evidence_proportions': [0.5541388193766277, 0.5816802978515625, 0.79080810546875, 0.922515869140625, 0.7746073404947916]}, 'weight': {'score': [0.0027236199378967286, 0.006839792740812378, 0.32403256676413794, 0.0051977252202868775, 0.002301793939927045], 'topk_tokens': [' Where', '.\n\n', ' was', ',', ':', '.\n\n', 'Answer', '<|eot_id|>', '<|start_header_id|>', 'b', ':', '<|end_header_id|>', '?\n', 'assistant', '<|eot_id|>', '.', '\n\n', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0012083550294240315, 0.0037824511528015137, 0.0027350425720214845, 0.002084672451019287, 0.0039494434992472325]}, 'saliency': {'score': [5.2834749221801755e-05, 0.00017670055971879918, 0.0014883117242292924, 0.00017054846983181426, 5.8744935428394993e-05], 'topk_tokens': [' write', '<|eot_id|>', '.', ':', 'Answer', '.', ' Where', ',', '.\n', '<|begin_of_text|>', ':', '<|start_header_id|>', '?\n', '<|end_header_id|>', '.', 'b', '\n\n', '<|eot_id|>', 'assistant', 'athroom'], 'evidence_proportions': [3.778437773386637e-05, 0.00011151283979415894, 5.608797073364258e-05, 2.769380807876587e-05, 4.2816003163655594e-05]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-23 22:21:27.357 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:21:27.357 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-5_pid-0_1-5-7-8-9.pkl | len: 10 |  size: 9.27 KB
Processing depth (1, 5, 7, 8, 9):   1%|          | 1/100 [00:13<21:57, 13.31s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Processing depth (1, 2, 3, 6, 9):   1%|          | 1/100 [00:24<21:57, 13.31s/it]2025-01-23 22:21:38.158 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:21:38.161 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (469, 475) --> . Daniel journeyed to the
2025-01-23 22:21:38.161 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:21:38.166 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1029, 1033) -->  Daniel grabbed the milk
2025-01-23 22:21:38.166 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:21:38.174 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1521, 1526) -->  tragedy. Daniel went to
2025-01-23 22:21:38.174 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:21:38.185 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2435, 2439) -->  left the milk.
2025-01-23 22:21:38.185 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary journeyed to the office.
2025-01-23 22:21:38.204 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3675, 3681) -->  cold. Mary journeyed to
2025-01-23 22:21:38.204 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:21:38.213 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1824, 1830) --> . Sandra journeyed to the
2025-01-23 22:21:38.213 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:21:38.227 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2757, 2762) --> . Mary got the football
2025-01-23 22:21:38.227 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the bedroom.
2025-01-23 22:21:38.227 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (86, 92) --> . John went back to the
2025-01-23 22:21:38.227 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:21:38.234 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1371, 1376) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:21:38.706 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the garden<|eot_id|>
2025-01-23 22:21:38.706 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4215])
your chose emoji: ['💇🏼\u200d♀️', '👰🏿\u200d♂️', '🍆', '🐸', '📯', '🦻🏻', '🕵🏿\u200d♀️', '👨\u200d🔧', '👩🏻\u200d🦽\u200d➡️', '🚵\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4218, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 200924.74it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.31it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.30it/s]
2025-01-23 22:21:41.573 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5017494201660156, 0.4550464642788348, 0.46369569951837714, 0.45472091715046037, 0.34944385030995245], 'topk_tokens': ['ing', ' EVENTS', ',', ' the', ' rebellion', ' made', ',', ' the', ',', 'S', ',', ' ago', 'action', ',', 'ulations', ',', ',', ' times', 'ences', ' bathroom'], 'evidence_proportions': [0.5467777252197266, 0.628143310546875, 0.61358642578125, 0.3616180419921875, 0.37268193562825525]}, 'weight': {'score': [0.0021420443058013917, 0.007491640594685004, 0.4320098960941488, 0.005284577129385049, 0.0012913372205651324], 'topk_tokens': ['NEW', '\n\n', ':', '\n\n', 'Answer', '<|end_header_id|>', '\n\n', 'user', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'b', 'athroom', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0015371441841125488, 0.0021200180053710938, 0.0019191086292266845, 0.0008049607276916504, 0.003838797410329183]}, 'saliency': {'score': [0.0001602482795715332, 0.00011755061861800605, 0.0013612630692395296, 0.0001107347194481399, 5.7152215985284336e-05], 'topk_tokens': ['Democratic', ':', ' bedroom', ' Grow', 'Bridge', 'user', ' Project', '<|end_header_id|>', 'b', 'Just', '<|eot_id|>', ' left', '\n\n', '<|eot_id|>', 'NEW', 'assistant', 'athroom', '<|begin_of_text|>', '<|start_header_id|>', '.'], 'evidence_proportions': [9.039541085561117e-05, 0.00011445581912994385, 0.00012732744216918944, 3.843754529953003e-05, 0.00036927064259847003]}}, 25: {'grad': {'score': [0.49121551513671874, 0.46688508512624466, 0.4449490633877841, 0.466854956075174, 0.523853606072025], 'topk_tokens': [' Republican', ' edition', ' for', ' state', 'Dub', '�', 'state', ' const', ' for', ' until', ' confirmed', ' await', ' to', 'ian', ' to', ' Democratic', 'ian', 'otyping', 'cont', ' to'], 'evidence_proportions': [0.6931304931640625, 0.5293312072753906, 0.348480224609375, 0.30170440673828125, 0.5091768900553385]}, 'weight': {'score': [0.0023656082153320312, 0.007479434784355182, 0.5062281909314069, 0.004879428318037857, 0.0015785093756689541], 'topk_tokens': ['\n\n', ' discarded', 'Civil', '<|end_header_id|>', ' bogus', 'Answer', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'user', 'b', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|eot_id|>', 'assistant', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0013526678085327148, 0.0023694857954978943, 0.003282773494720459, 0.00037723034620285034, 0.003937244415283203]}, 'saliency': {'score': [0.00021964311599731445, 0.00018528977125501566, 0.0017738315192135897, 0.00017670507883745486, 7.515325062516807e-05], 'topk_tokens': [' Times', ' report', 'rich', ' especial', 'NEW', ' paper', ' Senator', ' veto', ' Mary', ' Bank', ' God', ' Mary', '<|eot_id|>', 'MIN', 'Civil', '<|eot_id|>', '<|begin_of_text|>', 'assistant', '.', '<|end_header_id|>'], 'evidence_proportions': [0.0001159210999806722, 0.00012771785259246826, 0.0001887857913970947, 2.1204352378845215e-05, 0.0005426555871963501]}}, 26: {'grad': {'score': [0.28425132751464843, 0.3077277540312722, 0.33663489601828833, 0.3077159951099587, 0.27603541940882587], 'topk_tokens': [' paper', ' tour', ' circ', ' intercept', ' air', ' Press', ' paper', ' paper', ' air', ' cable', ' service', ' speech', ' trials', ' bathroom', ' Eagle', ' question', ' force', ' lin', ' bathroom', ' Square'], 'evidence_proportions': [0.28127288818359375, 0.3095588684082031, 0.34300155639648433, 0.18879318237304688, 0.28503831227620446]}, 'weight': {'score': [0.0027896404266357423, 0.007400901807094989, 0.42940008504824206, 0.005202699817933216, 0.0046855142151100045], 'topk_tokens': [' bogus', 'Bridge', '<|start_header_id|>', '\n\n', 'Answer', '\n\n', '\n\n', '<|end_header_id|>', 'user', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'b', '<|eot_id|>', 'athroom', '<|start_header_id|>', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0017421841621398926, 0.0022094249725341797, 0.004261302947998047, 0.0006889998912811279, 0.004397948582967122]}, 'saliency': {'score': [0.00010106801986694337, 0.00013899447766453692, 0.0019139037890867753, 0.00012986001508809257, 0.00020395586456077686], 'topk_tokens': [' bathroom', 'Bridge', 'far', ':', "'clock", 'user', '<|end_header_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', '<|eot_id|>', ' bathroom', 'assistant', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', '.', 'athroom'], 'evidence_proportions': [7.776419321695963e-05, 5.37186861038208e-05, 0.00018191933631896972, 1.8924474716186523e-05, 0.00014332433541615802]}}, 27: {'grad': {'score': [0.504754638671875, 0.7402093994950955, 0.6662999933416193, 0.7420104965835531, 0.6441498908443727], 'topk_tokens': ['itol', '--', ',', 's', ',', ' days', ' work', ',', ' work', ',', ' earth', ' use', ',', 'state', ',', 'RE', 'in', 'in', ';', ';'], 'evidence_proportions': [0.5903981526692709, 0.26862335205078125, 0.501788330078125, 0.5608291625976562, 0.5416208902994791]}, 'weight': {'score': [0.002358701229095459, 0.007482941538420488, 0.5161573439836502, 0.0048306445242603515, 0.004052401884742405], 'topk_tokens': [':', 'Minnesota', '\n\n', 'Minnesota', 'Just', 'NEW', '\n\n', '\n\n', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'user', 'b', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00183717409769694, 0.0018619894981384277, 0.002355921268463135, 0.0008719861507415771, 0.0042048295338948565]}, 'saliency': {'score': [0.000128018856048584, 0.00019581399720220444, 0.0020186630162325773, 0.00018660570185580604, 0.0003451808639194654], 'topk_tokens': ['Minnesota', 'Question', '\n\n', '<|start_header_id|>', '\n\n', 'RE', ' bathroom', 'until', 'NEW', 'Just', 'b', '<|end_header_id|>', 'Penn', '<|start_header_id|>', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '<|start_header_id|>', '.', 'user'], 'evidence_proportions': [0.00011561314264933267, 0.00013277679681777954, 0.0001017451286315918, 4.476308822631836e-05, 0.0002146512269973755]}}, 28: {'grad': {'score': [0.448182373046875, 0.38109523869058204, 0.41252933848987927, 0.380527334457906, 0.36456940139549365], 'topk_tokens': [' about', ' division', ' that', ' completed', ' than', ' that', ' ', 'u', 'a', ' secured', ' dropped', ' dropped', 'a', ' proc', ' decided', ' that', ' instance', ' a', '\n\n', 'a'], 'evidence_proportions': [0.5561726888020833, 0.5076828002929688, 0.52706298828125, 0.16108322143554688, 0.4261906941731771]}, 'weight': {'score': [0.002348531484603882, 0.007150884712403733, 0.5117414092475717, 0.004518198855516008, 0.0019483173239058342], 'topk_tokens': ['user', '\n', '.\n\n', ' During', ' milk', ':', 'Just', 'Answer', 'During', '<|eot_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'b', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.000879069169362386, 0.0019301027059555054, 0.004000335931777954, 0.0004029273986816406, 0.0040175120035807295]}, 'saliency': {'score': [7.896900177001954e-05, 0.00013034938070207708, 0.002133908596905795, 0.00012008954054787416, 4.649464634881503e-05], 'topk_tokens': [':', '.\n\n', '\n', 'Answer', '<|eot_id|>', 'Just', '<|start_header_id|>', 'Bridge', '\n\n', ' During', '<|eot_id|>', 'During', 'athroom', ' milk', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', 'b', '\n\n', '.'], 'evidence_proportions': [3.896653652191162e-05, 0.00011590123176574707, 0.00014753341674804686, 1.138448715209961e-05, 8.226931095123291e-05]}}, 29: {'grad': {'score': [0.6567027282714843, 0.4865301285043267, 0.602096904407848, 0.48490059504375216, 0.7007162121758945], 'topk_tokens': [' book', '�', '�', ' B', ' the', ' bathroom', ' the', '️', ' to', '�', ' the', '�', '<|end_header_id|>', '�', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.8746312459309895, 0.905120849609375, 0.500946044921875, 0.6144952774047852, 0.431097666422526]}, 'weight': {'score': [0.002097175121307373, 0.007363945005515448, 0.6052973514253442, 0.004241704369185783, 0.0026998856793279233], 'topk_tokens': ['.', '\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' milk', '\n\n', 'Answer', 'user', '<|eot_id|>', ':', 'b', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.003101309140523275, 0.0009902417659759521, 0.0010292649269104002, 0.0008410811424255371, 0.00355831782023112]}, 'saliency': {'score': [0.00015618085861206056, 0.00017974754965887526, 0.007344912398945202, 0.0001420960369741165, 0.00018010968747346297], 'topk_tokens': ['Saturday', '\n\n', 'A', '"The', 'Bridge', '<|start_header_id|>', '<|end_header_id|>', 'During', 'a', '<|start_header_id|>', 'b', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', '\n\n', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0004073580106099447, 6.207823753356934e-05, 3.364086151123047e-05, 3.094971179962158e-05, 0.00015334288279215497]}}, 30: {'grad': {'score': [0.39516273498535154, 0.3911632612001393, 0.3422773534601385, 0.39139713871767695, 0.3560812576957371], 'topk_tokens': [' had', '9', ' heard', 'a', ' based', ' bogus', ' bend', 'a', '<|start_header_id|>', 'ed', ' bogus', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', '<|eot_id|>', '<|end_header_id|>', 'ab', ' B', ' B'], 'evidence_proportions': [0.47458426157633465, 0.4157562255859375, 0.31970977783203125, 0.37195587158203125, 0.3803609212239583]}, 'weight': {'score': [0.00422400951385498, 0.007224271963308523, 0.3219657161019065, 0.005582146523410941, 0.009402097999185755], 'topk_tokens': ['\n\n', '.\n\n', ':', '<|start_header_id|>', 'NEW', ' *\n\n', ' *\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.002940734227498372, 0.00194549560546875, 0.006257319450378418, 0.0016689300537109375, 0.007035255432128906]}, 'saliency': {'score': [0.00021124839782714843, 0.0002353258003494422, 0.0013703934170983055, 0.00022948318406907354, 0.0004342664843020232], 'topk_tokens': ['Democratic', 'ANK', 'Answer', 'Bridge', '\n\n', 'system', ' bogus', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', 'user', '<|start_header_id|>', '<|begin_of_text|>', ' bathroom', '<|start_header_id|>', '.', 'b', 'assistant', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00017953415711720783, 9.705871343612671e-05, 0.0001941204071044922, 6.413459777832031e-05, 0.0004314382870992025]}}, 31: {'grad': {'score': [0.38845344543457033, 0.35940226179098506, 0.30592424219304865, 0.35951020637023823, 0.3733187136442765], 'topk_tokens': [',', ' Do', '202', ' *', ' as', ' assistance', 'ot', ' per', ' P', ' PA', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'user', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.5657246907552083, 0.33075904846191406, 0.296868896484375, 0.5167465209960938, 0.2404368718465169]}, 'weight': {'score': [0.0019417023658752442, 0.007003761343958255, 0.3361557871103287, 0.005297985009168498, 0.0029071107290793157], 'topk_tokens': [' place', ' milk', 'user', ' discarded', ' was', ' Where', 'Question', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'Answer', ':', '\n\n', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0017196337381998699, 0.0012191832065582275, 0.001330697536468506, 0.0007102489471435547, 0.003975590070088704]}, 'saliency': {'score': [6.482243537902832e-05, 0.00017365550418327409, 0.0013675066557797518, 0.00016801083896749456, 0.00015071336773858553], 'topk_tokens': ['Penn', ' You', '�', '\n\n', ' Where', 'Question', 'Bridge', '\n\n', '<|start_header_id|>', ' Pioneer', 'Answer', '<|begin_of_text|>', ':', '<|eot_id|>', 'assistant', '.', '<|eot_id|>', 'b', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [7.143119970957437e-05, 3.562122583389282e-05, 5.221962928771972e-05, 3.241002559661865e-05, 0.00010979175567626953]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-23 22:21:41.582 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:21:41.582 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-5_pid-1_1-2-3-6-9.pkl | len: 10 |  size: 9.93 KB
Processing depth (1, 2, 3, 6, 9):   2%|▏         | 2/100 [00:27<22:37, 13.85s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
Processing depth (0, 1, 2, 3, 9):   2%|▏         | 2/100 [00:37<22:37, 13.85s/it]2025-01-23 22:21:51.416 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:21:51.416 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-23 22:21:51.416 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:21:51.419 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (477, 481) -->  Daniel grabbed the milk
2025-01-23 22:21:51.419 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:21:51.424 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1032, 1037) -->  cable. Daniel went to
2025-01-23 22:21:51.424 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:21:51.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1501, 1505) -->  Daniel left the milk
2025-01-23 22:21:51.432 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary journeyed to the office.
2025-01-23 22:21:51.453 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3659, 3665) --> . Mary journeyed to the
2025-01-23 22:21:51.453 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:21:51.462 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1804, 1810) --> . Sandra journeyed to the
2025-01-23 22:21:51.462 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:21:51.475 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2708, 2713) --> . Mary got the football
2025-01-23 22:21:51.475 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the bedroom.
2025-01-23 22:21:51.476 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (93, 99) --> . John went back to the
2025-01-23 22:21:51.476 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:21:51.483 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1372, 1377) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:21:51.986 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:21:51.986 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4226])
your chose emoji: ['👩🏻\u200d❤️\u200d👩🏻', '🧙🏾\u200d♂️', '👨🏼\u200d❤\u200d👨🏼', '🖌', '🇸🇩', '🏇🏼', '🚏', '🏃🏻\u200d♂\u200d➡', '🤾\u200d♂️', '💮']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4229, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 110.27it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.32it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.96it/s]
2025-01-23 22:21:55.109 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3858475112915039, 0.40597384081407767, 0.42053673484108667, 0.40601754587612215, 0.5091115713119507], 'topk_tokens': [' into', ' made', ' related', 'ails', ' set', ' set', ' stake', '�', ' doctor', 'called', 'ages', ' Hor', 'un', ' traveled', 'G', 'LES', ' Daily', ' traveled', 'eward', 'eward'], 'evidence_proportions': [0.40577824910481775, 0.18984222412109375, 0.3880401611328125, 0.4573099613189697, 0.4471181233723958]}, 'weight': {'score': [0.0022796344757080077, 0.007408007686937016, 0.0015507502989335494, 0.007469478034334625, 0.0026394985616207123], 'topk_tokens': [' where', ' or', '<|eot_id|>', 'Answer', '\n\n', 'user', 'Just', '<|eot_id|>', ':', ' left', '<|eot_id|>', '\n\n', '\n\n', '?\n', 'b', 'assistant', ' dropped', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005988041559855143, 0.0015478059649467468, 0.0010296106338500977, 0.0009532570838928223, 0.0009850511948267617]}, 'saliency': {'score': [8.972644805908203e-05, 9.298986310681487e-05, 7.833946834910999e-05, 9.308644227010106e-05, 8.479170501232147e-05], 'topk_tokens': ['rail', '?\n', ' left', '<|eot_id|>', ' left', 'Just', 'Bridge', ' dropped', 'If', ' will', 'b', ' Associated', ' item', ' Pioneer', '\n\n', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', ' left', ' dropped'], 'evidence_proportions': [0.00023819009462992352, 3.679841756820679e-05, 3.504157066345215e-05, 7.184594869613647e-05, 3.403921922047933e-05]}}, 25: {'grad': {'score': [0.45656036376953124, 0.3719236058640858, 0.4160881042480469, 0.3711853136804217, 0.4423779249191284], 'topk_tokens': ['�', ' remin', 'most', ' journey', '3', ' printers', ' wait', ' July', 'street', ' meet', ' answer', '�', 'SP', ' county', ' candidates', ' leve', ' typ', ' old', 'SP', 'fast'], 'evidence_proportions': [0.371856689453125, 0.51513671875, 0.4478164672851563, 0.4475555419921875, 0.5155029296875]}, 'weight': {'score': [0.001236119270324707, 0.0073694922338284116, 0.001071603460745378, 0.007439288474166867, 0.0016685027629137039], 'topk_tokens': [' dropped', 'user', ' after', '\n\n', ' met', 'press', ' or', ' random', 'Times', '.\n\n', ' obtained', 'b', '?\n', ' dropped', '.\n\n', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009271701176961263, 0.002192363142967224, 0.001428985595703125, 0.0011329948902130127, 0.0008156001567840576]}, 'saliency': {'score': [9.210824966430664e-05, 0.0001039486541396254, 4.3938105756586246e-05, 0.00010433513003066075, 9.417273104190826e-05], 'topk_tokens': [' papers', 'Minnesota', ' Douglas', '�', 'had', 'ot', ' an', ' after', ' so', 'press', 'Times', ' happened', 'assistant', '.\n\n', 'athroom', ' met', '?\n', '<|end_header_id|>', '<|begin_of_text|>', ' obtained'], 'evidence_proportions': [8.81652037302653e-06, 0.00020985305309295654, 0.0001605987548828125, 0.0001190677285194397, 2.185503641764323e-05]}}, 26: {'grad': {'score': [0.3356658172607422, 0.3367411896928204, 0.39100469242442737, 0.3364621574715642, 0.28848484754562376], 'topk_tokens': [' two', ' method', ' three', 'ulations', ' of', ' to', ' three', ' first', ' At', ' By', ' account', ' MO', ' force', 'room', ' bathroom', ' method', ' three', ' at', 'room', ' bathroom'], 'evidence_proportions': [0.45949045817057294, 0.31873321533203125, 0.30838928222656253, 0.2643289566040039, 0.29341793060302734]}, 'weight': {'score': [0.0017536139488220214, 0.007277099932302789, 0.0009297647259452126, 0.007343510387617683, 0.003201106935739517], 'topk_tokens': ['Question', '<|start_header_id|>', ' after', ' left', ' that', '<|start_header_id|>', '<|eot_id|>', '\n\n', ' dropped', '\n\n', '<|eot_id|>', '\n\n', ' or', 'user', '.\n\n', '<|end_header_id|>', 'b', 'athroom', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0038873751958211264, 0.001633189618587494, 0.0006899893283843994, 0.0011179149150848389, 0.0010102887948354087]}, 'saliency': {'score': [2.644658088684082e-05, 9.325811219683323e-05, 3.279068253257058e-05, 9.397560902499287e-05, 0.0001390598714351654], 'topk_tokens': [' laughed', ' carrier', 'Question', '�', '<|eot_id|>', ' Pioneer', ' You', ' or', 'Bridge', ' Gal', '\n\n', '\n\n', ' context', 'Saturday', ' bathroom', 'b', ' Pioneer', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [2.586841583251953e-05, 5.1058828830718994e-05, 1.8149614334106445e-05, 1.7940998077392578e-05, 2.32011079788208e-05]}}, 27: {'grad': {'score': [0.8328797912597656, 0.935299645814244, 0.8592608191750266, 0.9363119247597066, 0.874999749660492], 'topk_tokens': ['.', '.', '.', '.', ',', '!', '.', '.', '      ', '      ', '.', '!', '      ', 'ors', '.', 'ors', '.', '      ', '.', '.'], 'evidence_proportions': [0.9980875651041667, 0.8366165161132812, 0.7828620910644531, 0.670501708984375, 0.8151143391927084]}, 'weight': {'score': [0.0021335864067077636, 0.007370752599341392, 0.0020053088665008545, 0.007430286056285039, 0.00394950844347477], 'topk_tokens': ['Bridge', ' where', ' *\n\n', ' that', 'If', '<|eot_id|>', ' or', '\n\n', ' unless', 'Just', '.\n\n', 'user', '\n\n', ' dropped', '\n\n', 'b', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0030357440312703448, 0.003098711371421814, 0.0014275550842285157, 0.0016089677810668945, 0.0015261173248291016]}, 'saliency': {'score': [9.62376594543457e-05, 0.00013629481510000888, 0.00010267577388069846, 0.00013671113451606972, 0.0002083607017993927], 'topk_tokens': ['itated', 'assistant', ' edition', '<|eot_id|>', ' context', 'athroom', 'press', ' dropped', 'Just', 'always', '\n\n', ' Newspaper', ' unless', 'b', ' You', '<|end_header_id|>', ' dropped', ' Pioneer', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [9.139378865559895e-05, 0.00022538751363754272, 0.00011435747146606445, 3.147125244140625e-05, 4.305938879648845e-05]}}, 28: {'grad': {'score': [0.3861004638671875, 0.3609493433315722, 0.4043398770419034, 0.36057072789517386, 0.3734858989715576], 'topk_tokens': [' Peter', 'nes', ' part', ' two', ' bathroom', '\n', ' moment', ',\n', ' one', 'nes', 'nes', ' of', ' trib', ' and', ' much', 'nes', '\n', ' two', ' one', ' mob'], 'evidence_proportions': [0.3458607991536458, 0.5424957275390625, 0.2732086181640625, 0.4737701416015625, 0.3577067057291667]}, 'weight': {'score': [0.0013123571872711182, 0.007113994563209387, 0.0013619065284729004, 0.007178936426232501, 0.003065963089466095], 'topk_tokens': ['Bridge', ' dropped', '<|eot_id|>', ' left', ' they', ' first', ' will', ' after', '.\n\n', ' or', 'Just', ' that', ' dropped', '.\n\n', 'b', '\n\n', '<|end_header_id|>', 'athroom', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0022455453872680664, 0.002430662512779236, 0.0003583073616027832, 0.0009178072214126587, 0.0006917069355646769]}, 'saliency': {'score': [4.3071508407592774e-05, 9.004161780143805e-05, 5.8981505307284266e-05, 9.048580125665505e-05, 0.00010965950787067413], 'topk_tokens': [' item', ' or', '.\n\n', '\u200d', '.\n\n', ' generally', ' about', ' *\n\n', ' will', ' after', '<|eot_id|>', ' based', 'As', 'Bridge', 'athroom', ' milk', ' dropped', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.569324811299642e-05, 0.0001317933201789856, 1.1926889419555664e-05, 3.586709499359131e-05, 2.2058685620625813e-05]}}, 29: {'grad': {'score': [0.9304156494140625, 0.6026393974269922, 0.732694452459162, 0.59999577774493, 0.550486969947815], 'topk_tokens': [' two', ' to', ' discover', ' the', ' the', ' up', ' bedroom', ' the', '.', ' out', ' the', ' bathroom', ' down', 'room', ' the', ' the', ' grabbed', ' bedroom', ' the', ' bathroom'], 'evidence_proportions': [1.2300211588541665, 1.5350379943847656, 0.70423583984375, 0.73504638671875, 0.5464579264322916]}, 'weight': {'score': [0.0010294771194458007, 0.0072748236522011565, 0.0007268054918809371, 0.007346605111513908, 0.0030445657670497895], 'topk_tokens': ['?\n', ' after', ' that', ' will', '\n\n', 'Question', ' or', '\n\n', ' dropped', '.', '\u200d', '.', 'b', '.\n\n', '.\n\n', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002301181356112162, 0.0011195540428161621, 0.00024743080139160155, 0.0005922615528106689, 0.0006409039100011189]}, 'saliency': {'score': [5.817532539367676e-05, 0.00012230860423072062, 1.6995451667092063e-05, 0.00012324600770449536, 0.00020629316568374634], 'topk_tokens': ['.\n\n', ' left', ' read', '�', 's', ' or', '\n', ' late', '.', '<|eot_id|>', 'As', 'assistant', ' text', ' dropped', '\u200d', 'a', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [9.900331497192383e-05, 0.00010804831981658936, 6.115436553955078e-06, 5.0358474254608154e-05, 3.269314765930176e-05]}}, 30: {'grad': {'score': [0.40185302734375, 0.3504867860402134, 0.3942205255681818, 0.34994965110424886, 0.514981746673584], 'topk_tokens': ['graph', 'UG', ' be', 's', ' Bor', ' o', 'ree', ' o', 'Sh', ' o', ' bathroom', '<|end_header_id|>', ' B', '<|eot_id|>', ' Sh', ' B', ' O', 'gro', ' o', '<|eot_id|>'], 'evidence_proportions': [0.4271697998046875, 0.49835968017578125, 0.40015869140625, 0.4051513671875, 0.31141153971354163]}, 'weight': {'score': [0.0040965735912323, 0.00721367912445826, 0.002338939092376015, 0.0072579574408227895, 0.00597991906106472], 'topk_tokens': ['\n\n', 'As', ' will', ' that', '?\n', ':', ' *\n\n', 'Question', '<|eot_id|>', '.\n', '.', ' dropped', '\n\n', 'b', '.\n\n', '.\n\n', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.011391381422678629, 0.0025826990604400635, 0.0006273210048675538, 0.00187605619430542, 0.002182404200236003]}, 'saliency': {'score': [0.0013067495822906494, 0.00019607539153262553, 4.572082649577748e-05, 0.0001902267415235055, 0.00029114559292793275], 'topk_tokens': [' late', '<|eot_id|>', '<|end_header_id|>', '.\n\n', 'assistant', ' random', '�', ' You', ' will', ' where', '.\n\n', ' dropped', 'As', ' bogus', 'un', ' milk', 'b', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005172957976659139, 0.00010763108730316162, 1.0591745376586913e-05, 0.0001997128129005432, 5.810956160227458e-05]}}, 31: {'grad': {'score': [0.5074227905273437, 0.5874921212835666, 0.5313987731933594, 0.5882658627773232, 0.4559436559677124], 'topk_tokens': [' five', '185', ' for', ' so', 'pl', ' Col', '185', ' for', '185', ' of', '185', ' ty', ' of', ' D', ' so', ' so', ' so', ' leve', ' fifty', ' o'], 'evidence_proportions': [0.4973023732503255, 0.3974342346191406, 0.4861000061035156, 0.4472770690917969, 0.6487350463867188]}, 'weight': {'score': [0.0015777826309204101, 0.006854696176835171, 0.001631589098410173, 0.00691371846147726, 0.003844182938337326], 'topk_tokens': ['.\n', ' that', ' after', ' ', '.\n', 'Question', ' dropped', '.\n\n', ' or', 'Answer', '.\n\n', ':', '?\n', '<|eot_id|>', '\n\n', 'assistant', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001895626386006673, 0.0017376542091369629, 0.0011687517166137696, 0.0009485483169555664, 0.0019137064615885417]}, 'saliency': {'score': [3.6923885345458985e-05, 0.00012910985501276512, 2.8951601548628375e-05, 0.00013018783942641738, 0.0002201661467552185], 'topk_tokens': ['�', ' place', ' where', ' left', 'Bridge', ' met', '?\n', ' milk', ' text', ' or', 'Question', '.\n\n', ' Pioneer', '<|begin_of_text|>', ' dropped', '\n\n', 'b', 'assistant', '<|eot_id|>', 'athroom'], 'evidence_proportions': [5.020697911580403e-05, 2.7008354663848877e-05, 2.8640031814575195e-05, 3.629177808761597e-05, 3.7575761477152504e-05]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:21:55.117 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:21:55.117 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-5_pid-2_0-1-2-3-9.pkl | len: 10 |  size: 9.29 KB
Processing depth (0, 1, 2, 3, 9):   3%|▎         | 3/100 [00:41<22:09, 13.71s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (1, 3, 4, 5, 6):   3%|▎         | 3/100 [00:51<22:09, 13.71s/it]2025-01-23 22:22:05.648 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:22:05.650 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (436, 442) --> . Daniel journeyed to the
2025-01-23 22:22:05.651 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:22:05.658 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1474, 1478) -->  Daniel grabbed the milk
2025-01-23 22:22:05.658 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:22:05.667 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1841, 1846) --> . Daniel went to the
2025-01-23 22:22:05.667 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:22:05.678 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2118, 2122) -->  Daniel left the milk
2025-01-23 22:22:05.678 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary journeyed to the office.
2025-01-23 22:22:05.690 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2423, 2429) -->  the senate. Mary journeyed
2025-01-23 22:22:05.690 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:22:05.703 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1759, 1765) --> . Sandra journeyed to the
2025-01-23 22:22:05.704 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:22:05.717 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2751, 2756) --> . Mary got the football
2025-01-23 22:22:05.717 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the bedroom.
2025-01-23 22:22:05.718 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (86, 92) --> . John went back to the
2025-01-23 22:22:05.718 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:22:05.724 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1290, 1295) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:22:06.237 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The garden.<|eot_id|>
2025-01-23 22:22:06.238 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4206])
your chose emoji: ['🐎', '👩🏾\u200d🔧', '💇🏽\u200d♀', '🙎🏾\u200d♂️', '🩴', '✋🏿', '\U0001fac4🏽', '👿', '🏋️\u200d♂️', '💪🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4209, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 226719.14it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.07it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.42it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.88it/s]
2025-01-23 22:22:09.268 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.32480072021484374, 0.2927440553584952, 0.27974943681196734, 0.29262018822409225, 0.2572377522786458], 'topk_tokens': [' every', ' during', ' during', ' men', ' for', ' drawing', ' the', ' of', ' Ear', 'possible', ' Democrats', ' every', ' EAR', 'u', ' IN', ' conditions', 'ulations', ' city', 'ences', ' Daily'], 'evidence_proportions': [0.306549072265625, 0.3883075714111328, 0.40437622070312496, 0.38008880615234375, 0.19754282633463544]}, 'weight': {'score': [0.0016389632225036622, 0.007479005597441518, 0.3689428053118966, 0.005603416329218412, 0.0027502864599227906], 'topk_tokens': [':', '\n\n', ' left', '\n\n', 'user', 'Answer', '\n\n', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'b', '<|eot_id|>', '<|eot_id|>', 'athroom', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.001241534948348999, 0.0029912590980529785, 0.0008444786071777344, 0.0017404556274414062, 0.001729269822438558]}, 'saliency': {'score': [8.604645729064941e-05, 0.0001151881356170212, 0.0003717690706253052, 0.00011400691538347412, 8.975913127263387e-05], 'topk_tokens': [' bedroom', ' traveled', ' bedroom', ' company', '\n\n', '***', '<|start_header_id|>', ' Pioneer', 'Question', ' cold', '\n\n', 'If', '<|start_header_id|>', '.', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [5.124509334564209e-05, 9.454041719436646e-05, 4.0298700332641596e-05, 0.00020837783813476562, 7.175405820210776e-05]}}, 25: {'grad': {'score': [0.5960768127441406, 0.501429374044087, 0.5035981264981356, 0.5008493888154734, 0.3410833994547526], 'topk_tokens': [' shore', ' where', ' confirmed', ' when', 'ing', ' res', ' until', ' as', ' question', ' for', ' was', ' at', ' to', ' from', ' to', ' from', 'as', ' pur', ' await', ' waiting'], 'evidence_proportions': [0.6116434733072917, 0.7280120849609375, 0.6472351074218751, 0.5499744415283203, 0.4806563059488932]}, 'weight': {'score': [0.0031267070770263673, 0.007489436651018233, 0.42219348658214917, 0.005323549851610476, 0.0033168007930119833], 'topk_tokens': ['\n\n', ' dropped', ' random', 'user', ' discarded', '\n\n', ' milk', 'Answer', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', '<|eot_id|>', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0012008845806121826, 0.00470578670501709, 0.0038119018077850345, 0.0014422610402107239, 0.004551778237024943]}, 'saliency': {'score': [0.00028759002685546875, 0.00017026660843964966, 0.0015532618219202216, 0.0001622514762539293, 0.00013472338517506917], 'topk_tokens': [' item', ' Senator', ' facts', ' Douglas', 'road', 'rail', ' Paul', ' paper', ' random', ' printers', 'Times', ' milk', 'athroom', 'paper', 'Answer', 'assistant', '.', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [8.792181809743246e-05, 0.0003043264150619507, 0.0004831850528717041, 0.00015325844287872314, 0.00040265917778015137]}}, 26: {'grad': {'score': [0.30367355346679686, 0.29456769998069615, 0.3496641679243608, 0.2942217680652917, 0.304230785369873], 'topk_tokens': [' tour', 'Times', ' the', ' Wright', ' from', ' force', ' it', ' journey', ' At', ' one', ' service', ' at', ' bedroom', ' at', ' Daily', ' trials', ' bathroom', ' bathroom', ' Press', 'UL'], 'evidence_proportions': [0.37045923868815106, 0.2555389404296875, 0.3120903015136719, 0.24981689453125, 0.2978680928548177]}, 'weight': {'score': [0.002864837646484375, 0.007431978056679488, 0.35890884832902387, 0.005601532925555821, 0.008657761414845784], 'topk_tokens': ['\n\n', 'paper', '<|start_header_id|>', ' Pioneer', '\n\n', 'Answer', '\n\n', 'user', 'assistant', 'b', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|end_header_id|>', 'athroom', '<|eot_id|>', '<|eot_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0019872188568115234, 0.0052416324615478516, 0.0026935100555419925, 0.001259148120880127, 0.003371159235636393]}, 'saliency': {'score': [0.00013335704803466798, 0.0001333190287963657, 0.0020555460994893856, 0.00012315806146426478, 0.00027965654929478965], 'topk_tokens': [' Pioneer', 'b', ' cash', ' bathroom', 'paper', 'athroom', 'ioneer', ' Married', ' Pioneer', 'user', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [7.833043734232584e-05, 0.00017298012971878052, 0.00015254616737365724, 9.541958570480347e-05, 0.0001712689797083537]}}, 27: {'grad': {'score': [0.6801239013671875, 0.7959723641397155, 0.7341736880215731, 0.7969948971632408, 0.640478515625], 'topk_tokens': ['\n', ' were', 'ank', ',', ' and', ',', ' and', ',', ' do', ' were', 'itol', ',', ' take', ' days', ' work', ',', ' Think', ',', ' producing', '--'], 'evidence_proportions': [0.7640482584635417, 0.542449951171875, 0.77161865234375, 0.4811363220214844, 0.7443949381510417]}, 'weight': {'score': [0.0031074941158294677, 0.007500039225492435, 0.48421629721468146, 0.0050065399347618755, 0.008279192447662353], 'topk_tokens': ['If', '\n\n', 'had', 'Answer', ' Minnesota', 'Minnesota', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.002259572347005208, 0.004371047019958496, 0.0030367136001586918, 0.001058913767337799, 0.004537751277287802]}, 'saliency': {'score': [0.00021831750869750978, 0.00018114369140438298, 0.002195583148436113, 0.00017027223213311287, 0.0006878465414047241], 'topk_tokens': ['<|eot_id|>', 'If', ' bathroom', '\u200d', ' Hale', '<|start_header_id|>', ' Chicago', ' Minnesota', '<|eot_id|>', 'had', '\n\n', 'assistant', 'athroom', 'user', '<|eot_id|>', 'b', '<|end_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00012250244617462158, 0.00020853430032730103, 0.00017855167388916015, 4.554539918899536e-05, 0.00046897431214650476]}}, 28: {'grad': {'score': [0.6505874633789063, 0.537534228411217, 0.6346674832430753, 0.5363417097950486, 0.5409858703613282], 'topk_tokens': [' a', ' o', ' a', ' a', ' a', ' a', ' a', ' a', 'a', ' a', '\n\n', ' a', ' a', 'a', ' a', ' a', ' o', ' a', ' a', ' a'], 'evidence_proportions': [0.7651316324869791, 0.735687255859375, 0.7050750732421874, 0.41646766662597656, 0.5899836222330729]}, 'weight': {'score': [0.0014830231666564941, 0.007276630650849092, 0.48038093610243365, 0.0048106396530523485, 0.003765206535657247], 'topk_tokens': [' discarded', '.\n\n', '\n\n', 'Question', 'through', '<|start_header_id|>', ' during', '<|start_header_id|>', 'Answer', '.\n\n', '<|eot_id|>', ' milk', '<|eot_id|>', '\n\n', 'assistant', 'b', 'athroom', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0006612539291381836, 0.004713833332061768, 0.0008011102676391601, 0.0005103200674057007, 0.0013676484425862632]}, 'saliency': {'score': [9.0712308883667e-05, 0.00012306482782270994, 0.0025206045670942826, 0.00011058595677741957, 0.00010423461596171061], 'topk_tokens': ['<|start_header_id|>', ' or', 'Question', 'always', ' dropped', ' left', '<|eot_id|>', 'Answer', '.\n\n', ' during', '<|eot_id|>', '<|end_header_id|>', 'athroom', 'through', 'b', 'assistant', '<|begin_of_text|>', ' milk', '.', '\n\n'], 'evidence_proportions': [4.269679387410482e-05, 0.0003439709544181824, 5.6529045104980474e-05, 1.4632940292358398e-05, 4.909435907999674e-05]}}, 29: {'grad': {'score': [0.46314727783203125, 0.31153754707639136, 0.3881029649214311, 0.31022215004095605, 0.40063088734944663], 'topk_tokens': [' the', ' the', 're', '<|eot_id|>', ' the', '-per', 'c', ' grabbed', '\n', ' STR', 'pl', ' went', ' to', ' l', ' bathroom', ' bathroom', ' the', 'SP', ' the', ' the'], 'evidence_proportions': [0.6653391520182292, 0.684722900390625, 0.43589630126953127, 0.267913818359375, 0.2661031087239583]}, 'weight': {'score': [0.0010082745552062989, 0.007411072672787062, 0.5613240748643875, 0.004521592592230673, 0.0051317885518074036], 'topk_tokens': ['\n\n', ' discarded', '.\n\n', '\n\n', ' milk', 'user', '<|end_header_id|>', 'Answer', '<|eot_id|>', '<|start_header_id|>', 'b', '\n\n', '<|start_header_id|>', 'athroom', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0010047952334086099, 0.001552581787109375, 0.0006906688213348389, 0.0004202648997306824, 0.001305560270945231]}, 'saliency': {'score': [8.900284767150879e-05, 0.0001464298981389911, 0.0037140196019952946, 0.0001279168521939763, 0.000281656285127004], 'topk_tokens': ['would', ' *\n\n', 'If', '\n\n', '<|eot_id|>', '"', ' milk', 'A', 'In', 'If', 'b', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00014501313368479413, 0.00021632760763168335, 3.4815073013305665e-05, 9.641051292419434e-06, 4.617373148600261e-05]}}, 30: {'grad': {'score': [0.32110015869140623, 0.3386424928584952, 0.3423206155950373, 0.33872842261677805, 0.3897905190785726], 'topk_tokens': ['ast', ' a', ' breaking', 'a', 'ided', 'blue', 'a', ' based', ' being', ' border', 'ball', ' be', ' bogus', ' board', ' o', ' Brown', 'ab', 'b', ' B', ' B'], 'evidence_proportions': [0.2934913635253906, 0.4182548522949219, 0.308013916015625, 0.28151702880859375, 0.32123311360677087]}, 'weight': {'score': [0.002763746976852417, 0.007247932508400201, 0.27497148513793945, 0.005859702446035672, 0.0165464719136556], 'topk_tokens': [':', '.\n\n', 'Question', 'Answer', '.\n\n', '\n\n', 'user', '<|end_header_id|>', '<|eot_id|>', '.\n\n', 'assistant', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|eot_id|>', 'b', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.002275725205739339, 0.004996895790100098, 0.002190744876861572, 0.0012722089886665344, 0.00323486328125]}, 'saliency': {'score': [0.00013318300247192383, 0.00021367898375950053, 0.0014723878015171397, 0.00020750906678245138, 0.0006218185027440389], 'topk_tokens': ['\u200d', 'assistant', ' milk', 'user', 'paper', ' bathroom', ' bathroom', '<|end_header_id|>', ' huge', '.\n\n', '\n\n', 'athroom', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', '<|begin_of_text|>', '<|eot_id|>', '.', '<|end_header_id|>'], 'evidence_proportions': [0.000206679105758667, 0.0001525580883026123, 8.055567741394043e-05, 8.979439735412598e-05, 0.00011955201625823975]}}, 31: {'grad': {'score': [0.35148139953613283, 0.40615301660133046, 0.40309771624478424, 0.40649756418289557, 0.3184466044108073], 'topk_tokens': [' do', '<|end_header_id|>', ' Col', ' of', ' mess', 'nes', ' five', 'nes', ' leve', ' manifest', 'ot', ' leve', '      ', '<|eot_id|>', '<|start_header_id|>', 'nes', 'user', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.42212677001953125, 0.361114501953125, 0.36737098693847653, 0.24153900146484375, 0.33446756998697913]}, 'weight': {'score': [0.0016158676147460937, 0.007070131725584081, 0.263031466440721, 0.005749902806563884, 0.006833598017692566], 'topk_tokens': [' or', ' place', ' dropped', ' left', ' milk', '.\n\n', 'Answer', ' Where', '<|eot_id|>', '<|start_header_id|>', 'Question', '.\n\n', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'assistant', 'b', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0012226104736328125, 0.0024910569190979004, 0.0011622071266174316, 0.0006559938192367554, 0.0024436314900716143]}, 'saliency': {'score': [5.11932373046875e-05, 0.0001278658835387734, 0.0009135509079152888, 0.00012417336686879955, 0.00034548441569010415], 'topk_tokens': [' item', '.\n\n', ' location', '\n', ' place', ' milk', '�', ' Where', ' Pioneer', '.', '.\n\n', 'Question', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', 'b', 'athroom', '.', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [3.026922543843587e-05, 6.066262722015381e-05, 7.283687591552734e-05, 1.917034387588501e-05, 6.911655267079671e-05]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-23 22:22:09.279 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:22:09.279 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-5_pid-3_1-3-4-5-6.pkl | len: 10 |  size: 9.88 KB
Processing depth (1, 3, 4, 5, 6):   4%|▍         | 4/100 [00:55<22:13, 13.89s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
Processing depth (0, 4, 5, 7, 8):   4%|▍         | 4/100 [01:05<22:13, 13.89s/it]2025-01-23 22:22:19.641 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:22:19.642 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-23 22:22:19.642 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:22:19.651 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1842, 1846) -->  Daniel grabbed the milk
2025-01-23 22:22:19.651 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:22:19.662 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2143, 2148) -->  Daniel went to the garden
2025-01-23 22:22:19.662 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:22:19.676 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2882, 2886) -->  Daniel left the milk
2025-01-23 22:22:19.676 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary journeyed to the office.
2025-01-23 22:22:19.693 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3325, 3331) --> . Mary journeyed to the
2025-01-23 22:22:19.693 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-23 22:22:19.702 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1778, 1784) --> . Sandra journeyed to the
2025-01-23 22:22:19.702 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:22:19.716 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2747, 2752) --> . Mary got the football
2025-01-23 22:22:19.716 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the bedroom.
2025-01-23 22:22:19.716 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (93, 99) --> . John went back to the
2025-01-23 22:22:19.716 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:22:19.723 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1295, 1300) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:22:20.220 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the garden<|eot_id|>
2025-01-23 22:22:20.220 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4232])
your chose emoji: ['🤶🏽', '🍄\u200d🟫', '🧑🏼\u200d❤\u200d💋\u200d🧑🏽', '👨🏿\u200d🦳', '🏒', '🦻🏻', '🇬🇩', '🧜🏼\u200d♂️', '👨🏼\u200d🦲', '👨🏾\u200d✈️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4235, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 188508.04it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.97it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.86it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.48it/s]
2025-01-23 22:22:23.394 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.2594659042358398, 0.14099909074191078, 0.22558834877881137, 0.13984755444435587, 0.12325092526369316], 'topk_tokens': ['RI', ' in', ' management', ' The', ' He', ' journey', ' prohibiting', ' alarm', ' P', ' had', ' the', '25', ' business', ' to', '10', ' B', ' Project', 'Daniel', ' the', ' bathroom'], 'evidence_proportions': [0.5819241205851237, 0.1459665298461914, 0.22422447204589846, 0.12208962440490723, 0.13362598419189453]}, 'weight': {'score': [0.0071636557579040526, 0.007540092558337215, 0.0076098008589311076, 0.007541973488947951, 0.00847634915695634], 'topk_tokens': ['<|eot_id|>', ' the', '?\n', ' dropped', '<|eot_id|>', ' the', '<|start_header_id|>', ' write', ' Do', ' discarded', ' milk', ' not', 'Question', 'Answer', ' was', 'b', 'assistant', 'athroom', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0017301936944325764, 0.006979703903198242, 0.003920412063598633, 0.01939857006072998, 0.007265845934549968]}, 'saliency': {'score': [5.964398384094238e-05, 6.144138970087946e-05, 6.86347484588623e-05, 6.141433173856398e-05, 4.494640716286593e-05], 'topk_tokens': [' moved', ' the', ' or', ' location', ' of', ' from', 'Question', ' milk', '<|start_header_id|>', ' was', ' discarded', ' dropped', '?\n', ' *\n\n', 'user', 'Answer', '<|end_header_id|>', 'assistant', 'athroom', 'b'], 'evidence_proportions': [5.7369470596313477e-05, 3.5531818866729736e-05, 2.5099515914916994e-05, 0.0001397654414176941, 5.3366025288899735e-05]}}, 25: {'grad': {'score': [0.3201951599121094, 0.294506309896602, 0.2474131150679155, 0.2946003473932217, 0.3335779101349587], 'topk_tokens': ['\n', ' to', '\n', ',', 'ting', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ' ', ':', '\n', '\n', '\n'], 'evidence_proportions': [0.51556396484375, 0.2610931396484375, 0.12708663940429688, 0.35082244873046875, 0.3047332763671875]}, 'weight': {'score': [0.006346495151519776, 0.007539300119159073, 0.011843163858760487, 0.007523811848366272, 0.009272633596908214], 'topk_tokens': [' item', 'graph', ' dropped', "'clock", ' discarded', ' the', ' moved', ' milk', ' write', ' Do', 'Answer', 'b', 'Question', ' not', ' was', '.', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008816917737325033, 0.007460832595825195, 0.0029651641845703128, 0.016596436500549316, 0.007052888472874959]}, 'saliency': {'score': [0.00010953783988952637, 0.00010119839148087935, 0.00017266788265921852, 0.00010077317299565021, 0.00010558096475379412], 'topk_tokens': [' Do', ' return', 'Answer', '.', ' speech', ' milk', ' *', 'graph', ' write', ' not', ' dropped', ' was', ' moved', '.', 'Question', '<|begin_of_text|>', 'athroom', 'assistant', 'b', '<|end_header_id|>'], 'evidence_proportions': [1.9649664560953777e-05, 5.895644426345825e-05, 0.00016678571701049806, 0.0001223236322402954, 0.00017691651980082193]}}, 26: {'grad': {'score': [0.17349517822265625, 0.1606147631000037, 0.10550091483376244, 0.1608273930627045, 0.1381743675054506], 'topk_tokens': [' hours', ' which', ' for', ' for', '.', ' When', ' Sh', ' both', ' four', ' first', ' Paul', ' for', ' ', ' when', ' for', ' who', ' for', ' for', ' for', ' St'], 'evidence_proportions': [0.21640777587890625, 0.15789794921875, 0.23325653076171876, 0.10413551330566406, 0.13741938273111978]}, 'weight': {'score': [0.007446918487548828, 0.007540528523739, 0.006984220309690995, 0.0075440096678683725, 0.00899889580039091], 'topk_tokens': [' an', 'b', ' discarded', ' dropped', ' write', ' the', ' milk', ' Do', 'Answer', '<|eot_id|>', ' moved', '<|eot_id|>', 'Question', ' was', ' not', '<|start_header_id|>', 'athroom', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0028282006581624346, 0.007963180541992188, 0.009085464477539062, 0.011970758438110352, 0.007340113321940104]}, 'saliency': {'score': [0.00012042045593261718, 8.59729233148388e-05, 5.391781980341131e-05, 8.593567975271055e-05, 0.00011395229849704476], 'topk_tokens': ['4', '<|begin_of_text|>', 'Question', '<|eot_id|>', 'speech', ' Foster', ' their', 'made', ' not', '<|start_header_id|>', 'athroom', 'and', 'u', 'were', '<|eot_id|>', ' Grow', ' arrest', 'assistant', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [8.829931418100992e-05, 7.12275505065918e-05, 0.00032045841217041013, 4.86522912979126e-05, 6.648401419321696e-05]}}, 27: {'grad': {'score': [0.3691675567626953, 0.3943047929964212, 0.35056807778098364, 0.39468460164985547, 0.40882772345875584], 'topk_tokens': [' the', ' the', ' clerk', ' grat', '�', ' between', ' between', 'for', 'ers', ' alarm', ' customary', ' time', ' and', ' Bor', ' their', ' by', ' bathroom', ' the', ' and', ' by'], 'evidence_proportions': [0.3004302978515625, 0.32108211517333984, 0.50894775390625, 0.3914337158203125, 0.33863417307535804]}, 'weight': {'score': [0.007244234085083008, 0.007541578804025121, 0.01221814751625061, 0.007518787282130324, 0.008958343156548433], 'topk_tokens': [' dropped', ' discarded', ' the', 'Answer', ' write', ' item', "'clock", ' the', ' moved', ' Do', ' milk', 'Question', ' not', ' was', 'b', '.', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0017638603846232097, 0.007433891296386719, 0.00485076904296875, 0.01902914047241211, 0.006736119588216146]}, 'saliency': {'score': [0.0002478659152984619, 0.00020942975103925506, 0.0002279986034740101, 0.0002091027646829884, 0.00024182886578315912], 'topk_tokens': [' which', ' moved', ' others', ' new', ' an', ' item', '.\n\n', ' location', ' discarded', 'b', ' different', 'Answer', 'assistant', ' the', '\n', '.', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [5.130469799041748e-05, 0.00019424408674240112, 0.0005471885204315185, 0.0002588704228401184, 0.00022340317567189535]}}, 28: {'grad': {'score': [0.40094497680664065, 0.36412233895366, 0.35450233112681995, 0.3639530634584035, 0.3137541482614916], 'topk_tokens': ['A', ' a', ' a', ' a', ' A', ' a', ' a', '.', ' an', ' a', ' A', 'Daniel', ' a', ' a', ' A', ' a', ' a', ' a', ' a', '202'], 'evidence_proportions': [0.5534566243489584, 0.3151216506958008, 0.494091796875, 0.21531295776367188, 0.35178120930989587]}, 'weight': {'score': [0.005172562599182129, 0.007443243835160977, 0.01610303196040067, 0.007411307754011072, 0.008812614305074825], 'topk_tokens': [' dropped', '<|start_header_id|>', ' item', "'clock", ' Do', ' write', ' the', ' milk', 'Answer', ' *\n\n', '❤', 'Question', ' not', ' was', '.', 'b', 'assistant', 'athroom', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0008928974469502767, 0.004745036363601685, 0.0019419193267822266, 0.01789069175720215, 0.0039506951967875166]}, 'saliency': {'score': [3.71086597442627e-05, 8.785120852581867e-05, 0.0002375041896646673, 8.736797025802597e-05, 5.657347135765608e-05], 'topk_tokens': [' others', ' the', ' be', '.', 'ot', ' speech', ' in', 'Question', 'the', 'ian', 'ors', 'athroom', 'graph', ' dropped', '\n\n', '<|begin_of_text|>', 'b', '.', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [1.5815099080403645e-05, 2.203136682510376e-05, 2.9826164245605467e-05, 9.113550186157227e-05, 3.8504600524902344e-05]}}, 29: {'grad': {'score': [0.3407408142089844, 0.2919752100704693, 0.2729089910333807, 0.2917842637274304, 0.32213512686795964], 'topk_tokens': [' than', 'await', ' by', ' and', ' Out', ' and', ' heard', 'up', ' and', ' and', ' and', ' and', ' was', '�', ' between', 'of', ' and', ' between', ' hidden', ' almost'], 'evidence_proportions': [0.451812744140625, 0.25439453125, 0.5543304443359375, 0.1983642578125, 0.204159418741862]}, 'weight': {'score': [0.004264345169067383, 0.007478380484732996, 0.026306331157684326, 0.007398661279405086, 0.008487301163895185], 'topk_tokens': [' item', 'Answer', "'clock", 'b', ' moved', ' milk', ' Buchanan', ' the', ' write', ' Do', '❤', '<|end_header_id|>', ' *\n\n', 'Question', 'assistant', ' was', ' not', '.', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0017061630884806316, 0.0038200020790100098, 0.001994752883911133, 0.013530910015106201, 0.0028323729832967124]}, 'saliency': {'score': [9.8801851272583e-05, 8.868869248922769e-05, 0.00026611712845889, 8.769627258451985e-05, 9.57493172135464e-05], 'topk_tokens': [' the', ' moved', 'Answer', ' effort', '<|start_header_id|>', ' Newspaper', 'Question', ' was', 'graph', ' *\n\n', 'assistant', ' not', '<|start_header_id|>', '<|eot_id|>', '.', '<|eot_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00011171897252400717, 4.3623149394989014e-05, 0.00010349154472351075, 7.133930921554565e-05, 0.00013707081476847333]}}, 30: {'grad': {'score': [0.22068260192871095, 0.21114606080398649, 0.17783728512850674, 0.21126410749375307, 0.23413234533265578], 'topk_tokens': ['\n', ',', '�', ',', "'", ',', '      ', ',', ' time', '�', '\n', '<|end_header_id|>', ' time', ':', '<|eot_id|>', '.', '�', '�', '!', ' B'], 'evidence_proportions': [0.20422840118408203, 0.13150787353515625, 0.38660736083984376, 0.1414337158203125, 0.21114857991536456]}, 'weight': {'score': [0.005717763900756836, 0.007326597643855612, 0.015072368762709877, 0.007295512132624159, 0.005616909542749095], 'topk_tokens': [' moved', '\n\n', ' *\n\n', 'UL', ' the', ' Do', 'Question', '<|eot_id|>', ' was', ' not', '<|start_header_id|>', '<|start_header_id|>', '.', '<|eot_id|>', 'assistant', 'b', '<|eot_id|>', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0022587378819783526, 0.006365537643432617, 0.006210517883300782, 0.006563425064086914, 0.007770538330078125]}, 'saliency': {'score': [0.00020742058753967285, 0.0001314911608712873, 0.0002288953824476762, 0.00013052623046503367, 0.00010410471017970595], 'topk_tokens': ['\n', '\n', '\n', '\n', ' B', '\n', '<|start_header_id|>', '\n', '<|end_header_id|>', ' four', '<|begin_of_text|>', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', 'UL', 'b', '<|start_header_id|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00033166011174519855, 8.124113082885742e-05, 0.00028701424598693843, 2.896040678024292e-05, 0.00021994610627492267]}}, 31: {'grad': {'score': [0.2851791000366211, 0.3380582374833973, 0.31391837380149146, 0.33850070535282695, 0.3254658898641897], 'topk_tokens': ['<|eot_id|>', ' and', ',', ',', ' first', ' and', ' by', ' was', ' set', ',', ',', ',', '<|start_header_id|>', ',', ' Wing', ' had', ' met', ' had', ' was', ' had'], 'evidence_proportions': [0.22717030843098956, 0.33882904052734375, 0.44287109375, 0.222320556640625, 0.21791696548461914]}, 'weight': {'score': [0.0030564022064208983, 0.006626449143591006, 0.009340887719934637, 0.006633501083598096, 0.0038304488326227942], 'topk_tokens': [' milk', ' That', 'Minnesota', ' discarded', ' dropped', ' not', '.\n', '?\n', 'Answer', '.', ' was', '<|eot_id|>', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0026197036107381186, 0.0030807852745056152, 0.003389596939086914, 0.0030603408813476562, 0.0031965573628743487]}, 'saliency': {'score': [6.371021270751954e-05, 0.00011445513300799985, 0.0001155002550645308, 0.00011475256145171245, 6.403756696124409e-05], 'topk_tokens': [' state', ' an', ' it', ' during', ' person', ' That', ' left', 'Answer', '\n', '?\n', '.\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', 'athroom', '<|eot_id|>', 'b', '<|end_header_id|>'], 'evidence_proportions': [0.00012137492497762044, 4.622340202331543e-05, 4.410743713378906e-05, 4.093348979949951e-05, 4.9223502477010093e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-23 22:22:23.401 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:22:23.402 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-5_pid-4_0-4-5-7-8.pkl | len: 10 |  size: 9.35 KB
Processing depth (0, 4, 5, 7, 8):   5%|▌         | 5/100 [01:09<22:07, 13.97s/it]Processing depth (0, 4, 5, 7, 8):   5%|▌         | 5/100 [01:09<22:01, 13.91s/it]
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:99 - Selected idx: 6
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:103 - Needle: [' Daniel got the apple.', ' John went back to the bedroom.', ' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' Mary journeyed to the office.', ' Mary moved to the bathroom.', ' John journeyed to the office.', ' Sandra journeyed to the bedroom.', ' Daniel went to the garden.', ' John moved to the bedroom.']
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel got the apple.', ' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' John moved to the bedroom.']
2025-01-23 22:22:23.601 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.17it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.07it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (1, 3, 4, 6):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:22:32.826 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel got the apple.
2025-01-23 22:22:32.829 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (470, 474) -->  Daniel got the apple
2025-01-23 22:22:32.829 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:22:32.836 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1467, 1473) --> . Daniel journeyed to the
2025-01-23 22:22:32.836 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:22:32.845 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1815, 1820) -->  Daniel went to the garden
2025-01-23 22:22:32.846 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:22:32.857 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2387, 2392) --> . John moved to the
2025-01-23 22:22:32.858 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:22:32.871 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2701, 2707) --> . John went back to the
2025-01-23 22:22:32.871 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:22:32.885 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2911, 2916) --> . Mary got the football
2025-01-23 22:22:32.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the office.
2025-01-23 22:22:32.887 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (328, 334) -->  John journeyed to the office
2025-01-23 22:22:32.887 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:22:32.900 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2524, 2529) --> . Mary moved to the
2025-01-23 22:22:32.900 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John journeyed to the office.
2025-01-23 22:22:32.902 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (327, 333) --> . John journeyed to the
2025-01-23 22:22:32.902 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-23 22:22:32.917 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2958, 2964) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:22:33.472 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Daniel's house.<|eot_id|>
2025-01-23 22:22:33.472 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4208])
your chose emoji: ['👗', '🦝', '💇🏽\u200d♂️', '🚆', '🎰', '🫑', '🧎🏻\u200d➡️', '⛹🏻\u200d♂', '🤾🏻\u200d♂️', '🧑🏻\u200d🦳']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4211, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 228261.44it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.95it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.01it/s]
2025-01-23 22:22:36.530 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.23995013236999513, 0.1844752702823984, 0.23308734332813935, 0.18381077479880276, 0.2934184225778731], 'topk_tokens': ['asc', ' press', ' conclusion', ' It', ' announced', '�', ' the', ' advantage', ' competitors', ' announcement', ' of', ' conventions', ' minds', 'the', ' conventions', 'announcement', 'announcement', ' in', '�', 'iscal'], 'evidence_proportions': [0.2691802978515625, 0.22762298583984375, 0.21377105712890626, 0.2575376510620117]}, 'weight': {'score': [0.018155531585216524, 0.007474804173679936, 0.006394715870127958, 0.007432251480413166, 0.0008788600800529359], 'topk_tokens': [' It', 'Bridge', ' office', '\n\n', 'In', '.', '<|eot_id|>', 'assistant', ':', ' apple', '<|start_header_id|>', '<|eot_id|>', '\n\n', '.', '<|end_header_id|>', ' Bench', 'b', 'athroom', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04823184013366699, 0.007466375827789307, 0.021413707733154298, 0.0036632955074310304]}, 'saliency': {'score': [0.0006897762417793274, 7.726599926111733e-05, 0.0002849478931987987, 7.262048825696881e-05, 1.3451727609785776e-05], 'topk_tokens': [' bedroom', ' office', ' \n', ' It', ' the', ' book', ':', '<|eot_id|>', ' garden', '<|eot_id|>', ' office', 'In', ' apple', '<|end_header_id|>', '<|begin_of_text|>', '.', ' bathroom', ' Bench', 'athroom', 'b'], 'evidence_proportions': [0.0018157213926315308, 0.00023945669333140054, 0.0009585261344909668, 6.0653686523437495e-05]}}, 25: {'grad': {'score': [0.3952330589294434, 0.5173699590265821, 0.44906947192023783, 0.5185162074181046, 0.28708509414915057], 'topk_tokens': [' the', ' the', ' a', ' the', ' large', ' the', ' for', ' the', ' the', ' the', ' no', ' for', ' first', ' inverted', ' York', ' old', ' a', 'antic', ' old', ' a'], 'evidence_proportions': [0.4077591896057129, 0.3576749165852865, 0.3594322204589844, 0.46608276367187496]}, 'weight': {'score': [0.014696694910526276, 0.007255936257772959, 0.0028669246855904072, 0.007256035180168712, 0.0010953573953537714], 'topk_tokens': ['RE', ' Paul', ' Daniel', '.', 'Answer', ' apple', '.', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', ':', 'assistant', '.', ' bathroom', ' Bench', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.04209589958190918, 0.007752736409505209, 0.012549304962158203, 0.003257471323013306]}, 'saliency': {'score': [0.0004374191164970398, 7.72712505846062e-05, 6.826572558459113e-05, 7.561217686118831e-05, 1.556485418289427e-05], 'topk_tokens': [' Daniel', 'RE', ' Moore', '.', 'assistant', ' river', ' It', '<|eot_id|>', 'b', 'In', ':', 'E', 'athroom', '.', ' apple', ' bathroom', ' Bench', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.001615956425666809, 0.00012934207916259766, 0.00026829242706298827, 3.3408403396606445e-05]}}, 26: {'grad': {'score': [0.1830831289291382, 0.15352910534839, 0.15988784677842083, 0.1533349081676741, 0.20023418608165922], 'topk_tokens': ['ers', 'ley', 'outs', 'and', 'hue', ' and', 'PA', '�', ' large', 'ed', ' large', '�', '�', 'ers', ' Moore', ' Eagle', ' when', ' proprietor', ' prepared', ' political'], 'evidence_proportions': [0.2663917541503906, 0.11997381846110027, 0.20658597946166993, 0.16866455078125]}, 'weight': {'score': [0.014810402691364289, 0.007130496265825368, 0.003080836113761453, 0.007126669062712401, 0.001390604745774042], 'topk_tokens': ['.\n\n', 'Bridge', 'Answer', '<|eot_id|>', '.', ' the', ' \n', ' apple', 'assistant', ' the', ' the', ' Bench', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.0295792818069458, 0.005770926674207051, 0.023173809051513672, 0.005479264259338378]}, 'saliency': {'score': [0.0004062548279762268, 7.43413234770482e-05, 3.886135185466093e-05, 7.303462368036251e-05, 3.098258896479531e-05], 'topk_tokens': [' Joseph', '185', ' bathroom', ' Moore', ' garden', ' the', '<|start_header_id|>', ' apple', 'Bridge', ' apple', 'assistant', '.', ' Bench', '\n\n', 'athroom', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.0011448413133621216, 0.00011199712753295898, 0.0005170941352844238, 5.7655572891235356e-05]}}, 27: {'grad': {'score': [0.24754104614257813, 0.2988740934155894, 0.246887768016142, 0.29954625988396594, 0.20109110029916916], 'topk_tokens': ['!"', ' the', ' one', ',\n', 'ers', ' work', ' for', ' from', 'ers', ' and', ',\n', 'Republicans', ' struggle', 'UG', ' step', ' execution', 'event', '--', ' conventions', ' conventions'], 'evidence_proportions': [0.22280502319335938, 0.23998006184895834, 0.20975952148437502, 0.31418457031249997]}, 'weight': {'score': [0.012980104982852935, 0.007361328027164021, 0.005297590704525218, 0.007351174438002593, 0.001420286912766714], 'topk_tokens': ['.', ' apple', ' bathroom', 'Answer', ' apple', ' garden', 'RE', 'NEW', ' Bench', 'assistant', '<|start_header_id|>', '.\n\n', 'b', '.', '\n\n', '<|end_header_id|>', ':', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.032418668270111084, 0.007595062255859375, 0.01319112777709961, 0.0036802828311920167]}, 'saliency': {'score': [0.00048327744007110597, 9.387267284171367e-05, 0.00036442893392899455, 8.9786310507926e-05, 2.6516025028531512e-05], 'topk_tokens': [' apple', ' the', 'E', ' Bench', ' the', 'assistant', ' the', 'In', '.', '.', 'Bridge', ':', '<|end_header_id|>', ' bathroom', '.\n\n', '<|begin_of_text|>', 'RE', 'NEW', 'athroom', 'b'], 'evidence_proportions': [0.0007989406585693359, 0.00011334816614786784, 0.0010244488716125487, 0.00013349056243896484]}}, 28: {'grad': {'score': [0.342156982421875, 0.3615512490074359, 0.3823184406056124, 0.36147470361830264, 0.3675018038068499], 'topk_tokens': ['ivery', ' L', '\n', ' the', '.', 'eward', 'nes', 'S', 'the', 'nes', 'nes', 'ien', 'E', ' *', '\n', '.', ' Peter', '.', 'nes', ','], 'evidence_proportions': [0.22553443908691406, 0.4661267598470052, 0.3568450927734375, 0.272003173828125]}, 'weight': {'score': [0.020852406322956086, 0.006958947445541741, 0.006826710175065433, 0.006893185330951389, 0.0012875213509514218], 'topk_tokens': [' the', ' the', '?', ' garden', 'Bridge', ' the', ' apple', ' the', ' before', ' the', 'assistant', ' garden', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', 'b', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0034437179565429688, 0.010829289754231771, 0.05037479400634766, 0.017284709215164184]}, 'saliency': {'score': [0.0005750030279159546, 6.337287278221781e-05, 0.00019294987706577077, 5.985153016775653e-05, 1.5367591191851903e-05], 'topk_tokens': [' People', ' the', '?', 'b', ' the', ' the', ' the', ' Bench', ' the', ' bathroom', '<|start_header_id|>', ' garden', 'athroom', ' apple', ':', '\n\n', ' garden', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>'], 'evidence_proportions': [0.00014413148164749146, 0.00015364090601603192, 0.0019691526889801026, 3.118515014648438e-05]}}, 29: {'grad': {'score': [0.3788120269775391, 0.45508397341737045, 0.38214680727790384, 0.45604748137444007, 0.428359864250062], 'topk_tokens': ['<|start_header_id|>', ' hidden', ' had', ' facts', ' engaged', ' not', ' every', 'pend', ' be', ' not', ' work', 'itated', ' also', ' take', ' which', ' producing', ' wholly', ' give', '�', ' enough'], 'evidence_proportions': [0.45406341552734375, 0.30919774373372394, 0.4234283447265625, 0.35753173828125]}, 'weight': {'score': [0.007332378625869751, 0.007148346802424547, 0.0030358039280947518, 0.007181097781798689, 0.001181216940047249], 'topk_tokens': [' \n', '.', ' the', ' the', 'Answer', ' the', ' apple', ' the', '?', '.\n\n', ' before', ' the', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>', 'athroom', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.005429565906524658, 0.002075592676798503, 0.015638303756713868, 0.006856846809387207]}, 'saliency': {'score': [0.000163152813911438, 9.113086625930108e-05, 0.00012488400234895593, 9.050829094535088e-05, 2.0340321555970207e-05], 'topk_tokens': [' Where', '\n\n', ' \n', ' of', '.', '<|eot_id|>', '<|eot_id|>', ' garden', '***', 'NEW', ' the', 'athroom', ' the', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'b', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.571412086486816e-05, 6.119906902313232e-05, 0.00044602155685424805, 8.057951927185059e-05]}}, 30: {'grad': {'score': [0.21342751979827881, 0.24590481423165333, 0.20072160047643325, 0.2464306201388809, 0.20291203449642847], 'topk_tokens': ['Republicans', ' the', ' the', ' at', ' the', ' the', ' the', ' The', '202', ' the', ' THE', ' time', ' the', ' have', ' day', ' Proof', ' Team', ' Chicago', ' the', ' for'], 'evidence_proportions': [0.19765281677246094, 0.23597113291422525, 0.18334617614746093, 0.22907629013061523]}, 'weight': {'score': [0.020874014496803282, 0.007193636707386532, 0.006466364159303553, 0.007133766539199495, 0.0035010907385084364], 'topk_tokens': ['.', 'NEW', ' the', ' before', ' \n', ' the', '.\n\n', '?', ' the', ' garden', ' Bench', 'assistant', 'b', '<|end_header_id|>', ' bathroom', '<|start_header_id|>', '\n\n', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.015558242797851562, 0.012932101885477701, 0.04312620162963867, 0.01240473985671997]}, 'saliency': {'score': [0.001064850389957428, 0.00019370210167914947, 0.00022601993644938748, 0.00018924653945933865, 3.260515985034761e-05], 'topk_tokens': ['?', 'ANK', ' Daniel', '.', '<|begin_of_text|>', ' garden', ' the', 'assistant', ' bathroom', '<|start_header_id|>', '.\n\n', '.', '<|end_header_id|>', 'Bridge', ' Bench', ' the', ':', 'athroom', ' bathroom', 'b'], 'evidence_proportions': [0.001108027994632721, 0.0009171068668365479, 0.0019619524478912353, 0.0003104984760284424]}}, 31: {'grad': {'score': [0.23088012933731078, 0.24702982708893148, 0.20900747355292826, 0.24741850978685223, 0.1400632569714198], 'topk_tokens': [' The', ' the', 'If', ' location', ' the', ' the', ' the', "'clock", ' had', ' have', ' prepared', ' the', ' the', ' company', ' December', ' the', ' the', ' people', ' hours', 'If'], 'evidence_proportions': [0.26665735244750977, 0.16721582412719727, 0.29336915016174314, 0.21616649627685547]}, 'weight': {'score': [0.002724963426589966, 0.006640266222785075, 0.0024658038335687972, 0.006693246202856582, 0.0013967922755650111], 'topk_tokens': [' was', ' the', ' bathroom', ' Where', '.\n\n', '<|eot_id|>', ' before', 'Answer', '?', ' \n', ' the', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'b', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002012014389038086, 0.002819021542867025, 0.0036063909530639647, 0.002301025390625]}, 'saliency': {'score': [9.474605321884156e-05, 3.7963721282803174e-05, 6.866542732014375e-05, 3.743942379808093e-05, 1.0816350815788148e-05], 'topk_tokens': [' the', ' Mary', ' People', ' bathroom', '.\n\n', ' garden', '?', ' the', ' \n', ' apple', ' the', ':', ' the', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', ' the', 'assistant', 'b', 'athroom'], 'evidence_proportions': [0.00010079145431518555, 0.00010720888773600261, 8.960962295532227e-05, 8.009076118469239e-05]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-23 22:22:36.537 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:22:36.537 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-6_pid-0_1-3-4-6.pkl | len: 10 |  size: 8.94 KB
Processing depth (1, 3, 4, 6):   1%|          | 1/100 [00:12<21:12, 12.85s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.19it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Processing depth (4, 5, 7, 9):   1%|          | 1/100 [00:23<21:12, 12.85s/it]2025-01-23 22:22:46.775 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel got the apple.
2025-01-23 22:22:46.784 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1834, 1838) -->  Daniel got the apple
2025-01-23 22:22:46.784 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:22:46.795 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2129, 2135) -->  Daniel journeyed to the bathroom
2025-01-23 22:22:46.795 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:22:46.809 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2931, 2936) --> . Daniel went to the
2025-01-23 22:22:46.809 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:22:46.827 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3607, 3612) -->  John moved to the bedroom
2025-01-23 22:22:46.827 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:22:46.841 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2790, 2796) -->  John went back to the bedroom
2025-01-23 22:22:46.841 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:22:46.857 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3081, 3086) -->  Mary got the football there
2025-01-23 22:22:46.857 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the office.
2025-01-23 22:22:46.858 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (328, 334) -->  John journeyed to the office
2025-01-23 22:22:46.858 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:22:46.871 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2611, 2616) --> . Mary moved to the
2025-01-23 22:22:46.871 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John journeyed to the office.
2025-01-23 22:22:46.873 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (327, 333) --> . John journeyed to the
2025-01-23 22:22:46.873 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-23 22:22:46.889 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3096, 3102) -->  Sandra journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:22:47.487 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Daniel got the apple.<|eot_id|>
2025-01-23 22:22:47.487 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4214])
your chose emoji: ['👬', '🎅🏼', '🙅\u200d♂', '👩🏾\u200d🚀', '🐞', '🏋\u200d♂', '🦹🏽\u200d♀', '🇨🇼', '💇🏿\u200d♂️', '🧑🏿\u200d🦯\u200d➡']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4217, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 222214.78it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.24it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 133.62it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.24it/s]
2025-01-23 22:22:50.545 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.4741645812988281, 0.37799810419044194, 0.428924560546875, 0.3771201726366859, 0.28662507430366846], 'topk_tokens': [' com', ' started', ' Proof', ' was', 'com', ' looked', ' interest', 'ab', ' and', ' EAR', ' men', ' was', ' ', ' rest', ' Bor', 'remark', ' bend', ' Ear', ' bathroom', ' bathroom'], 'evidence_proportions': [0.3752784729003906, 0.5771586100260417, 0.38592529296875, 0.5179199218749999]}, 'weight': {'score': [0.003440673649311066, 0.007424602413561945, 0.002891260911436642, 0.007480766762915122, 0.003182282050450643], 'topk_tokens': [' upper', 'Just', 'Answer', ' location', '.\n\n', ' Wood', '.\n\n', ' Pioneer', '\n\n', '.\n\n', ' \n', '<|eot_id|>', 'Bridge', '<|eot_id|>', 'b', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0002513676881790161, 0.0022758742173512774, 0.0011845171451568604, 0.009646034240722657]}, 'saliency': {'score': [0.00028097778558731077, 0.00014963564444187555, 0.00032151884892407586, 0.00014760084459193476, 0.00011963861576025037], 'topk_tokens': [' Good', 'Question', 'Good', ' location', '\n\n', ' charges', 'Just', ' offices', 'rail', ' breaking', 'ioneer', ' upper', '\n\n', ' Pioneer', 'b', '<|end_header_id|>', 'Bridge', ' Wood', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [1.3127923011779785e-05, 0.0005384137233098348, 2.6154518127441407e-05, 0.0004411578178405762]}}, 25: {'grad': {'score': [0.3761940002441406, 0.3521098914179215, 0.3133195989272174, 0.35231099345206984, 0.3682068810946699], 'topk_tokens': [' past', ' Wood', ' large', ' old', ' three', 'ad', ' the', 'fast', 'old', ' em', ' em', ' a', 'posit', ' old', 'posit', ' no', ' wood', ' the', ' the', 'vent'], 'evidence_proportions': [0.396759033203125, 0.3325411478678385, 0.3846939086914063, 0.40362548828125]}, 'weight': {'score': [0.0027758538722991942, 0.00727337458870381, 0.0012964185546426212, 0.007343796620768708, 0.002303229725879172], 'topk_tokens': [' tele', ' Bank', '\n\n', 'Bridge', '<|eot_id|>', 'itol', '.\n\n', 'Answer', ' bogus', ' bogus', '.\n\n', ' remains', 'b', '<|eot_id|>', ' \n', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0006522461771965027, 0.0013965219259262085, 0.0036852300167083737, 0.005220562219619751]}, 'saliency': {'score': [0.00011294782161712647, 0.00011951962381226773, 5.465395310345818e-05, 0.00012008096631719262, 8.858934692714526e-05], 'topk_tokens': ['edit', ' dropped', ' printers', 'polit', '.\n\n', ' person', ' Senator', '\n\n', ' Bench', ' rest', ' received', 'Bridge', ' remains', 'eward', 'paper', ' Bank', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [4.623830318450928e-05, 7.399916648864746e-05, 0.00014553070068359374, 0.00018047094345092775]}}, 26: {'grad': {'score': [0.33014698028564454, 0.30026893534308674, 0.29207195955164295, 0.30019234064648764, 0.2894927038662676], 'topk_tokens': [' *', ' voice', ' were', 'ball', ' message', ' message', '.', ' *', ' Daniel', 'ab', ' state', ' state', ' *', ' Senator', ' state', ' *', ' *', ' B', ' B', 'b'], 'evidence_proportions': [0.3433408737182617, 0.35495758056640625, 0.33592987060546875, 0.2840362548828125]}, 'weight': {'score': [0.001969897747039795, 0.007199646766812761, 0.001715745119487538, 0.007269559722951248, 0.004007540751194609], 'topk_tokens': [' bogus', ' bogus', '<|eot_id|>', '?', '.\n', '.\n\n', 'Bridge', '\n\n', ' garden', '<|eot_id|>', 'b', '<|eot_id|>', ' \n', '.\n\n', '<|end_header_id|>', '.\n\n', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004217103123664856, 0.0018094430367151897, 0.0013819098472595214, 0.003988981246948242]}, 'saliency': {'score': [0.0001872912049293518, 0.00010279392020126245, 6.664675824782428e-05, 0.00010268319663937322, 9.566070376962855e-05], 'topk_tokens': [' Hale', ' context', ' Wing', ' cap', ' Senator', 'ages', '�', ' bathroom', ' bathroom', ' custom', 'fortunate', 'assistant', 'b', '.\n\n', 'columns', ' garden', 'Bridge', '\n\n', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [6.57886266708374e-06, 0.00043423473834991455, 6.179213523864746e-05, 0.0001610279083251953]}}, 27: {'grad': {'score': [0.32031973004341124, 0.39282649191516483, 0.3577669929055607, 0.3934611683993737, 0.40309174164481787], 'topk_tokens': [' three', ' lin', ' three', ' especial', ' earlier', '0', ' else', '.', ' presidents', ' *', ' *', ' *\n\n', 'ers', '-per', ' *', ' state', 's', ' will', ' St', ' *\n\n'], 'evidence_proportions': [0.34821510314941406, 0.2565752665201823, 0.38197021484375, 0.312846302986145]}, 'weight': {'score': [0.0034738391637802123, 0.0073850415514520144, 0.0023425054900786456, 0.00744501519397907, 0.00436640044917231], 'topk_tokens': [' a', ' Francis', ' garden', 'Just', ' Pioneer', ' Hale', '<|eot_id|>', '.\n', 'If', ' \n', ' bogus', ' bogus', 'b', '<|eot_id|>', '.\n\n', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000491805374622345, 0.0028326958417892456, 0.0031042575836181642, 0.006998419761657715]}, 'saliency': {'score': [0.00026955604553222654, 0.00011916282280435035, 0.00022224555997287526, 0.00011759840351098322, 0.0002232193946838379], 'topk_tokens': [' prize', ' Pioneer', 'Minnesota', 'If', '.\n', ' bathroom', ' *\n\n', ' bathroom', ' voice', 'If', '�', ' luxury', ' Hale', 'Bridge', '<|end_header_id|>', '.\n\n', 'b', '\n\n', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [3.0077993869781494e-05, 0.000568459431330363, 0.00021639466285705566, 0.00015561580657958985]}}, 28: {'grad': {'score': [0.3144229888916016, 0.35768674181512333, 0.33379420112161073, 0.35808972558693447, 0.41214036250459973], 'topk_tokens': [' for', '�', 'nes', ' that', '�', ' so', ' something', 'material', ' had', ' most', ' so', ' of', ' so', 'sur', ' after', ' for', ' out', ' After', ' out', 'ball'], 'evidence_proportions': [0.296295166015625, 0.30024592081705725, 0.34663696289062496, 0.31372375488281246]}, 'weight': {'score': [0.003518444299697876, 0.0071475802650659715, 0.001120064188452328, 0.007214243312366531, 0.0035693641158117766], 'topk_tokens': [' bathroom', '�', '\n\n', '.\n', 'If', ' garden', 'Bridge', '?', 'Just', ' a', '.\n\n', '<|eot_id|>', ' \n', 'b', '.\n\n', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.000247366726398468, 0.007272134224573771, 0.0012800753116607665, 0.0038692474365234378]}, 'saliency': {'score': [0.0019374653697013856, 0.00011355091899612206, 0.00011979043483734131, 0.00010473745933897416, 8.023735405742258e-05], 'topk_tokens': [' full', ' bedroom', ' full', ' person', ' Square', 'just', ' person', ' bathroom', '<|end_header_id|>', ' bedroom', ' garden', 'Just', ' dropped', '.\n\n', ' block', 'Bridge', '<|begin_of_text|>', '\n\n', ' bathroom', 'athroom'], 'evidence_proportions': [7.733702659606934e-06, 0.005742266774177551, 4.230737686157226e-05, 0.0008106470108032226]}}, 29: {'grad': {'score': [0.8348903656005859, 0.5509193281101791, 0.7572012508616728, 0.5478703235165305, 0.35994195247042005], 'topk_tokens': [' forthcoming', ' a', ' on', ' the', ' a', ' on', 'national', ' Minnesota', ' the', ' senate', 'the', ' senate', ' the', ' l', ' the', ' an', ' the', ' senate', ' bathroom', ' bathroom'], 'evidence_proportions': [0.6603012084960938, 0.9988059997558594, 0.8094207763671875, 0.80333251953125]}, 'weight': {'score': [0.001702795922756195, 0.007266992863416954, 0.0007649826652863447, 0.007346827666575652, 0.0024195132048233695], 'topk_tokens': ['Just', '\n\n', ' will', 'Question', ' Do', '?', '.\n\n', '.\n', '.\n\n', 'If', '\n\n', ' \n', 'b', '.\n\n', ' a', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0003567337989807129, 0.0012442866961161294, 0.0011525571346282957, 0.0038800954818725584]}, 'saliency': {'score': [0.00012160539627075196, 8.274155090509333e-05, 3.148264744702508e-05, 8.297348119821395e-05, 0.0001204877659894418], 'topk_tokens': ['a', ' *', 'I', ' *', ' *', 'P', ' *', ' *', 'Answer', 'If', ' a', ' context', 'Just', '.\n\n', 'a', '<|end_header_id|>', 'b', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [9.357929229736328e-06, 9.988248348236084e-05, 2.423524856567383e-05, 0.0003348410129547119]}}, 30: {'grad': {'score': [0.4054588317871094, 0.3144862405886886, 0.3032720229204963, 0.31414077615841, 0.3863784955895465], 'topk_tokens': [' Buchanan', ' bill', ' block', ' bedroom', ' border', ' balance', ' based', '�', 'athroom', ' bogus', ' bathroom', ' bogus', ' Bor', 'RI', ' bathroom', ' bend', 'body', ' B', ' B', 'b'], 'evidence_proportions': [0.3468780517578125, 0.4661407470703125, 0.2996551513671875, 0.48530883789062496]}, 'weight': {'score': [0.0034128040075302123, 0.007283111926868738, 0.0022735210026011746, 0.007342620033957825, 0.010478498279184534], 'topk_tokens': ['his', '.\n', '.\n\n', '�', '.\n\n', ' a', 'Question', ' bogus', '<|eot_id|>', ' bogus', 'Answer', '<|eot_id|>', 'b', ' \n', '.\n\n', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007991045713424683, 0.0023220976193745932, 0.002129960060119629, 0.008095455169677735]}, 'saliency': {'score': [0.0006782397627830506, 0.00019198579756547017, 0.00019296828438253964, 0.00018964169863293785, 0.00032696905343428904], 'topk_tokens': [' boat', ' Hale', ' Wood', 'states', 'assistant', '�', '<|eot_id|>', ' bill', '<|end_header_id|>', ' joke', '\n\n', ' bill', ' bathroom', ' bathroom', 'Bridge', ' bogus', ' bogus', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [2.734363079071045e-05, 0.0012895166873931885, 0.00013120770454406737, 0.0010124564170837403]}}, 31: {'grad': {'score': [0.34076528549194335, 0.43917314775017785, 0.4001441282384536, 0.4399646788355882, 0.30138040625530743], 'topk_tokens': [' be', ' be', ' is', ' generally', ' so', ' be', ' for', 'being', ' who', ' and', ' formally', 'b', ' or', ' now', ' of', ' so', ' he', ' We', ' so', ' Do'], 'evidence_proportions': [0.3829002380371094, 0.29857730865478516, 0.33373260498046875, 0.364715576171875]}, 'weight': {'score': [0.0024275854229927063, 0.006652975184164936, 0.0014811315957237693, 0.006715514333151351, 0.006408169649649358], 'topk_tokens': ['.\n\n', '.\n', 'If', ' a', ' Do', '<|eot_id|>', '.\n\n', '<|start_header_id|>', ':', '.\n\n', '?', 'Answer', ' \n', 'assistant', 'b', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006075203418731689, 0.0024990588426589966, 0.001417839527130127, 0.004807615280151367]}, 'saliency': {'score': [0.00048665106296539304, 0.00011314369361774317, 8.173812838161692e-05, 0.00011160577428819123, 0.00022887146991232166], 'topk_tokens': [' genu', 'Question', ' Buchanan', 'If', ' boat', ' bathroom', 'Bridge', 'body', '�', ' Hale', ' person', '\n\n', ' \n', ' bathroom', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [1.3589859008789062e-05, 0.0012694448232650757, 1.800060272216797e-05, 0.0003943979740142822]}}, 'pred_res': 'Daniel got the apple.<|eot_id|>', 'score': 0}
2025-01-23 22:22:50.552 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:22:50.553 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-6_pid-1_4-5-7-9.pkl | len: 10 |  size: 8.86 KB
Processing depth (4, 5, 7, 9):   2%|▏         | 2/100 [00:26<22:06, 13.53s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
Processing depth (1, 2, 3, 4):   2%|▏         | 2/100 [00:37<22:06, 13.53s/it]2025-01-23 22:23:00.739 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel got the apple.
2025-01-23 22:23:00.741 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (417, 421) -->  Daniel got the apple
2025-01-23 22:23:00.741 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:23:00.746 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (927, 933) --> . Daniel journeyed to the
2025-01-23 22:23:00.746 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:23:00.754 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1505, 1510) --> . Daniel went to the
2025-01-23 22:23:00.754 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:23:00.763 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1840, 1845) --> . John moved to the
2025-01-23 22:23:00.763 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:23:00.777 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2727, 2733) --> . John went back to the
2025-01-23 22:23:00.777 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:23:00.791 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2975, 2980) --> . Mary got the football
2025-01-23 22:23:00.791 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the office.
2025-01-23 22:23:00.793 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (323, 329) -->  John journeyed to the office
2025-01-23 22:23:00.793 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:23:00.806 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2550, 2555) --> . Mary moved to the
2025-01-23 22:23:00.806 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John journeyed to the office.
2025-01-23 22:23:00.808 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (322, 328) --> . John journeyed to the
2025-01-23 22:23:00.808 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-23 22:23:00.823 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3119, 3125) -->  Sandra journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:23:01.378 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Daniel's house.<|eot_id|>
2025-01-23 22:23:01.379 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4224])
your chose emoji: ['🙅\u200d♀️', '🧔🏽\u200d♂', '👩🏻\u200d❤\u200d💋\u200d👨🏽', '🧒🏻', '🧡', '\U0001fa87', '🦸🏻\u200d♂', '🧑🏾\u200d🦼\u200d➡', '🤾🏻', '🧑\u200d✈']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4227, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 205855.41it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.96it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.49it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.55it/s]
2025-01-23 22:23:04.517 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.11921176910400391, 0.13701606174718975, 0.12078111311968635, 0.1372336687700028, 0.10414662542222422], 'topk_tokens': [' do', 'athroom', ' Ear', '202', ' use', ' of', ' perform', ' the', ' employ', ' contents', ' conventions', ' announcement', ' employ', ' announced', 'announcement', ' competitors', ' Proof', ' appear', 'announcement', ' conventions'], 'evidence_proportions': [0.11772918701171875, 0.13022104899088544, 0.0923248291015625, 0.13407363891601565]}, 'weight': {'score': [0.21085604578256606, 0.007478691878994503, 0.01660682523951811, 0.006429589647427469, 0.10639809807644615], 'topk_tokens': [' apple', ' garden', ' John', '<|eot_id|>', '<|eot_id|>', ':', ' Daniel', '<|start_header_id|>', 'assistant', '\n\n', 'b', 'itol', ' no', '<|end_header_id|>', 'athroom', ' bathroom', '.', '�', '�', '<|begin_of_text|>'], 'evidence_proportions': [0.05625772476196289, 0.6479301452636719, 0.013326549530029298, 0.007575279474258422]}, 'saliency': {'score': [0.00037308037281036377, 4.863808813291378e-05, 0.00038295195383184096, 4.4359267829220383e-05, 7.573939576933656e-05], 'topk_tokens': ['assistant', ' Bench', ' the', '�', '<|begin_of_text|>', ' Project', ' apple', '<|eot_id|>', ' garden', '.', ' cap', ' garden', 'itol', 'New', ' bedroom', ' John', ' book', ' bathroom', 'athroom', 'b'], 'evidence_proportions': [0.0006477311253547668, 0.000608752171198527, 7.416009902954102e-05, 0.00016947388648986815]}}, 25: {'grad': {'score': [0.09943165779113769, 0.12210888001888899, 0.1047814874088063, 0.12235874241843316, 0.09676305704478975], 'topk_tokens': [' proc', ' others', 'SP', ' great', ' FR', 'posit', ' considerable', 'NEW', ' compos', ' Hale', ' very', 'New', ' Think', ' curs', ' passed', ' of', ' fr', ' New', 'ir', ' wood'], 'evidence_proportions': [0.09589672088623047, 0.0757896105448405, 0.12987823486328126, 0.10018348693847656]}, 'weight': {'score': [0.23728907704353333, 0.007419900914470383, 0.005218690809081583, 0.0063361380630450196, 0.14890868339357496], 'topk_tokens': [' apple', '<|eot_id|>', ' cap', '<|start_header_id|>', '<|eot_id|>', ' bag', ' Bench', ':', 'assistant', ' no', ' Daniel', ' bathroom', 'athroom', 'itol', '<|end_header_id|>', '\n\n', '.', '�', '�', '<|begin_of_text|>'], 'evidence_proportions': [0.07686042785644531, 0.7282347679138184, 0.008579730987548828, 0.005206513404846192]}, 'saliency': {'score': [0.0010718181729316711, 4.75707179918508e-05, 5.931976963492001e-05, 4.2566065019250575e-05, 0.0004964301103278051], 'topk_tokens': ['<|eot_id|>', ' Think', ' It', ' Moore', '<|eot_id|>', 'RE', ' Mr', ' cap', ' apple', ' bag', '<|begin_of_text|>', 'Dub', ' bathroom', ' Daniel', 'itol', '.', '�', '<|end_header_id|>', '\n\n', '�'], 'evidence_proportions': [0.002358183264732361, 0.0019008467594782512, 6.22093677520752e-05, 5.750060081481934e-05]}}, 26: {'grad': {'score': [0.0931851625442505, 0.10068245376287298, 0.07512159908519071, 0.1009266461624449, 0.10912805871118474], 'topk_tokens': ['ian', ' Paul', ' bogus', ' proprietor', ' cold', ' Press', ' Press', 'b', 'ian', ' block', ' B', 'ers', 'enan', ' breaking', ' Gutenberg', ' Peter', ' air', 'burn', ' prepared', 'ian'], 'evidence_proportions': [0.12434959411621094, 0.08477465311686198, 0.10501556396484374, 0.06651582717895507]}, 'weight': {'score': [0.1973208397626877, 0.007388123276951692, 0.010271777124965893, 0.006454334980630772, 0.10599106172972088], 'topk_tokens': ['<|eot_id|>', ' Daniel', 'b', ' Bench', ' cap', ' the', ' \n', ' the', ' no', '\n\n', 'itol', '<|start_header_id|>', ':', '<|end_header_id|>', ' bathroom', 'athroom', '.', '�', '�', '<|begin_of_text|>'], 'evidence_proportions': [0.06073141098022461, 0.6008888880411783, 0.012139129638671874, 0.007492434978485108]}, 'saliency': {'score': [0.0008682295680046082, 5.23027021156746e-05, 0.00021294635884902056, 4.708333435945308e-05, 0.00013545457320877268], 'topk_tokens': [' John', ' garden', '�', 'itol', 'Dub', 'assistant', ' bag', '�', ' no', ' the', '<|begin_of_text|>', ' the', '.', ' cap', ' Daniel', '<|end_header_id|>', 'athroom', 'b', ':', ' bathroom'], 'evidence_proportions': [0.002420850098133087, 0.0011932353178660073, 8.21232795715332e-05, 2.2232532501220703e-05]}}, 27: {'grad': {'score': [0.22009701728820802, 0.23395739217613334, 0.209229076609892, 0.23422529781404627, 0.1325437268124351], 'topk_tokens': [',', ' and', ' *\n\n', ',', ',\n', ' *\n\n', ' *\n\n', ' they', ' of', ' on', ' and', ' *\n\n', ' lines', '.', ' they', ' but', ' of', ' *\n\n', 'action', ' one'], 'evidence_proportions': [0.2241346836090088, 0.21194585164388022, 0.1773223876953125, 0.26942291259765627]}, 'weight': {'score': [0.24043094515800476, 0.007453799332347299, 0.010162080035490148, 0.006315140223691654, 0.15668179566347146], 'topk_tokens': ['RE', 'Dub', 'NEW', ' bathroom', 'assistant', '.\n\n', 'itol', '<|start_header_id|>', 'b', ' no', '\n\n', ' Daniel', '<|end_header_id|>', ':', ' bathroom', 'athroom', '<|begin_of_text|>', '.', '�', '�'], 'evidence_proportions': [0.0664520263671875, 0.7413031260172526, 0.014469623565673828, 0.0045287847518920895]}, 'saliency': {'score': [0.0014264643192291259, 7.528203414184194e-05, 0.00023340214701259837, 6.751790053547929e-05, 0.0005132020274295082], 'topk_tokens': ['�', ' the', ' cap', 'New', ' bathroom', '<|begin_of_text|>', '.\n\n', ' bedroom', 'RE', 'NEW', ':', '<|end_header_id|>', ' Daniel', 'athroom', '.', 'Dub', '�', '�', ' bathroom', 'b'], 'evidence_proportions': [0.0027852728962898254, 0.002355078856150309, 0.0005700886249542236, 8.145570755004884e-05]}}, 28: {'grad': {'score': [0.1115983247756958, 0.13631955957424, 0.11139289070578183, 0.13664113432561753, 0.13260228120827977], 'topk_tokens': [',', ' the', ' the', ' the', ',', '.', 'the', ' B', 'E', 'en', ' adher', ' the', ' ', ' the', '.', ' B', 'ot', ' the', 'E', 'nes'], 'evidence_proportions': [0.13814353942871094, 0.10834757486979166, 0.1105377197265625, 0.09532365798950194]}, 'weight': {'score': [0.24488549828529357, 0.007288436898673035, 0.007862499531577615, 0.0061450246395665965, 0.1630156523064722], 'topk_tokens': [' bathroom', ' \n', ' apple', ' before', ' garden', ' bathroom', 'b', ' the', '<|start_header_id|>', ' the', ' the', ' the', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>', '.', '�', '�'], 'evidence_proportions': [0.004503965377807617, 0.8001989523569744, 0.00724325180053711, 0.008456826210021973]}, 'saliency': {'score': [0.0010634258389472962, 5.133049212253373e-05, 0.00011304809766657213, 4.5976956171181574e-05, 0.0008115779749954804], 'topk_tokens': [' garden', ' \n', ' garden', ' apple', ' the', ' the', '<|begin_of_text|>', '.\n\n', 'athroom', ' the', ' the', ':', ' bathroom', '<|end_header_id|>', ' the', 'Bridge', ' bathroom', '.', '�', '�'], 'evidence_proportions': [6.218254566192627e-05, 0.0033735732237497964, 8.972883224487305e-05, 6.594061851501466e-05]}}, 29: {'grad': {'score': [0.14656620025634765, 0.14211610893789184, 0.1722864543690401, 0.14184896454032936, 0.21430841276917276], 'topk_tokens': [' affairs', ' writer', ' to', ' be', ' joke', ' could', 'super', 'P', ' him', ' ST', ' those', ' wholly', ' could', 'b', 'S', '\n', ' goods', ' S', ' its', '\n'], 'evidence_proportions': [0.10604095458984375, 0.09303855895996094, 0.18377075195312498, 0.2060150146484375]}, 'weight': {'score': [0.28938060998916626, 0.007357033088129972, 0.005572355845395257, 0.006019915304337078, 0.18381969989100588], 'topk_tokens': [' a', 'b', ' was', 'assistant', ' \n', ' no', ' the', ' before', ' the', 'athroom', ' the', '<|start_header_id|>', ' the', '<|end_header_id|>', ':', '\n\n', '<|begin_of_text|>', '.', '�', '�'], 'evidence_proportions': [0.0021308064460754395, 0.9553204774856567, 0.0054590702056884766, 0.003974151611328125]}, 'saliency': {'score': [0.001536346971988678, 5.890801637088657e-05, 0.00010770822272581212, 5.142946709496438e-05, 0.00045084840134729313], 'topk_tokens': ['      ', '      ', ' was', '      ', 'If', ' before', '<|begin_of_text|>', 'b', 'assistant', ' the', 'athroom', ' a', '�', ' the', '<|start_header_id|>', '<|end_header_id|>', ':', '�', '.', '\n\n'], 'evidence_proportions': [3.007054328918457e-05, 0.004922514160474141, 0.00015195012092590333, 6.236433982849122e-05]}}, 30: {'grad': {'score': [0.16877422332763672, 0.15991723534624883, 0.1636470065397375, 0.1598443975843973, 0.1401575124716457], 'topk_tokens': ['time', ' the', '.', ' Chicago', '      ', ' the', ' THE', ' im', ' Times', ' the', ' the', ' the', ' the', 'UG', ' time', ' the', ' In', 'g', ' at', '7'], 'evidence_proportions': [0.11184215545654297, 0.16978327433268228, 0.17672882080078126, 0.2051544189453125]}, 'weight': {'score': [0.14839314222335814, 0.00734149182917295, 0.019755686030668372, 0.00656432537560608, 0.08034621612935126], 'topk_tokens': [' the', 'b', '?', ' \n', ' bathroom', ' the', ' the', ' the', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'assistant', '\n\n', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '�', '.', '�'], 'evidence_proportions': [0.019788503646850586, 0.44798596700032556, 0.021773529052734372, 0.01838507652282715]}, 'saliency': {'score': [0.0016411691904067993, 0.00015202715291406124, 0.0003958484705756692, 0.00014290355704769424, 0.0005175920226905919], 'topk_tokens': [' apple', ' the', '�', '<|start_header_id|>', ' the', 'NEW', ':', ' bend', ' the', ' the', ' bathroom', ' the', '<|end_header_id|>', '�', '�', '.', ' the', ' bathroom', 'athroom', 'b'], 'evidence_proportions': [0.0011079832911491394, 0.0032440274953842163, 0.0006512939929962158, 0.0011341631412506103]}}, 31: {'grad': {'score': [0.11855912208557129, 0.1393275769686964, 0.0927421906415154, 0.13980667407696062, 0.1499043959605543], 'topk_tokens': ['\n', 'D', ' to', ' o', ' o', ' to', 'D', ' l', 'b', ' to', 'u', ' m', 't', ' o', ':', 'u', ' o', ' a', 'g', 'd'], 'evidence_proportions': [0.1550912857055664, 0.09819348653157553, 0.1192657470703125, 0.1130655288696289]}, 'weight': {'score': [0.1059733510017395, 0.00691309938595485, 0.005448411492740407, 0.006450265538855305, 0.07010335258290737], 'topk_tokens': [' the', ':', '.\n\n', '<|eot_id|>', 'Answer', ' the', '?', 'b', '<|start_header_id|>', '<|eot_id|>', ' \n', ':', 'assistant', '\n\n', '.', '<|end_header_id|>', 'athroom', '�', '�', '<|begin_of_text|>'], 'evidence_proportions': [0.0043340325355529785, 0.3425081570943197, 0.004623222351074219, 0.0047931671142578125]}, 'saliency': {'score': [0.0006270453333854676, 7.686703544095077e-05, 6.821138017317829e-05, 7.430071057160353e-05, 0.00042065185836598844], 'topk_tokens': [' apple', 'Question', ' the', '<|begin_of_text|>', ':', ' the', '<|eot_id|>', ' the', ' the', 'assistant', '.\n\n', ' the', '?', '.', '�', '<|end_header_id|>', '<|start_header_id|>', '�', 'b', 'athroom'], 'evidence_proportions': [0.0002778396010398865, 0.0017785181601842244, 0.00010365843772888183, 4.802942276000977e-05]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-23 22:23:04.524 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:23:04.524 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-6_pid-2_1-2-3-4.pkl | len: 10 |  size: 8.71 KB
Processing depth (1, 2, 3, 4):   3%|▎         | 3/100 [00:40<22:12, 13.73s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.11it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Processing depth (2, 3, 6, 7):   3%|▎         | 3/100 [00:50<22:12, 13.73s/it]2025-01-23 22:23:13.984 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel got the apple.
2025-01-23 22:23:13.989 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (910, 914) -->  Daniel got the apple
2025-01-23 22:23:13.989 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:23:13.997 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1485, 1491) --> . Daniel journeyed to the
2025-01-23 22:23:13.997 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:23:14.009 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2447, 2452) --> . Daniel went to the
2025-01-23 22:23:14.009 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:23:14.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2932, 2937) --> . John moved to the
2025-01-23 22:23:14.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:23:14.040 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2790, 2796) --> . John went back to the
2025-01-23 22:23:14.041 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:23:14.056 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3085, 3090) -->  Mary got the football there
2025-01-23 22:23:14.056 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the office.
2025-01-23 22:23:14.058 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (328, 334) -->  John journeyed to the office
2025-01-23 22:23:14.058 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:23:14.071 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2561, 2566) --> . Mary moved to the
2025-01-23 22:23:14.071 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John journeyed to the office.
2025-01-23 22:23:14.072 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (327, 333) --> . John journeyed to the
2025-01-23 22:23:14.073 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-23 22:23:14.088 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3136, 3142) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:23:14.639 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The capitol.<|eot_id|>
2025-01-23 22:23:14.640 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4209])
your chose emoji: ['👳🏼\u200d♀️', '👭🏻', '🧑', '🇧🇾', '👩🏻\u200d🦼\u200d➡', '🇬🇾', '🤸🏻\u200d♂', '\U0001faf6🏻', '🧓', '🥌']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4212, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 248551.35it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.41it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.25it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.13it/s]
2025-01-23 22:23:17.515 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.14964475631713867, 0.16538104123426187, 0.15366021324606502, 0.16555257414670563, 0.24080078303813934], 'topk_tokens': [' in', ' It', ' announcement', 'the', 'remark', ' announced', ' conventions', ' expedition', '202', 'announcement', ' minds', ' J', 'athroom', ' the', 'announcement', 'ree', ' fight', 'ab', ' advantage', ' Do'], 'evidence_proportions': [0.1322917938232422, 0.13592020670572916, 0.14567184448242188, 0.18396949768066406]}, 'weight': {'score': [0.020777222514152528, 0.007474972425136585, 0.00431542273829965, 0.00743682420184946, 0.0007474604062736034], 'topk_tokens': [' down', ' the', ' bag', ' Mr', '<|eot_id|>', '.', 'assistant', 'itol', ':', '<|eot_id|>', '\n\n', '.', ' Daniel', '<|start_header_id|>', '<|end_header_id|>', ' Bench', 'athroom', ' bathroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.07560467720031738, 0.012309789657592773, 0.00587843656539917, 0.001974964141845703]}, 'saliency': {'score': [0.0007674917578697205, 8.721358840621775e-05, 0.00022627150311189538, 8.2804369433331e-05, 1.103617250919342e-05], 'topk_tokens': ['\n\n', '<|eot_id|>', 'assistant', ':', ' John', ' bag', ' garden', 'New', '<|eot_id|>', ' the', '<|start_header_id|>', '.', 'itol', ' Mr', ' Daniel', '<|begin_of_text|>', ' bathroom', 'athroom', ' Bench', 'b'], 'evidence_proportions': [0.0033240094780921936, 0.00021671255429585773, 0.00010137557983398437, 4.932880401611328e-05]}}, 25: {'grad': {'score': [0.26694498062133787, 0.40719732011032467, 0.3608153848087086, 0.4082512000021079, 0.266418918967247], 'topk_tokens': [' the', ' Bank', ' do', ' a', ' for', ' to', ' with', ' old', 'APER', 'SP', ' closely', ' very', ' considerable', 'antic', ' old', 'iously', ' no', ' inverted', ' York', ' a'], 'evidence_proportions': [0.3829803466796875, 0.1983826955159505, 0.25040245056152344, 0.2729339599609375]}, 'weight': {'score': [0.025233790278434753, 0.007229535781193776, 0.0015088284716886632, 0.007189713500909589, 0.0009147431701421738], 'topk_tokens': [' apple', ' Paul', '<|eot_id|>', '.', ' bag', '.', '<|eot_id|>', ' Mr', 'b', ':', 'assistant', 'itol', ' bathroom', '<|start_header_id|>', ' Daniel', 'athroom', ' Bench', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.11003494262695312, 0.006149689356486003, 0.003949654102325439, 0.0015779256820678711]}, 'saliency': {'score': [0.0014850065112113954, 8.884589528312936e-05, 4.010165438932531e-05, 8.252893806134583e-05, 1.4906283468008041e-05], 'topk_tokens': ['cont', ' People', ':', 'Dub', 'athroom', '.', 'itol', '.', 'b', ' Paul', ' apple', '.', ' bag', ' bathroom', '<|end_header_id|>', ' Daniel', ' Mr', ' Bench', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.007050536572933197, 0.00011681020259857178, 0.00013427734375, 2.5147199630737307e-05]}}, 26: {'grad': {'score': [0.1964799165725708, 0.16619135104013644, 0.17883894022773295, 0.16594224345403086, 0.24124810099601746], 'topk_tokens': ['�', ' afterward', ' and', ' Press', ' bend', ' and', ' Out', ' when', ' soon', ' printer', ' Press', 'ian', '�', ' proprietor', 'ed', ' Clean', ' Eagle', ' prepared', ' expedition', ' Press'], 'evidence_proportions': [0.2231597900390625, 0.13177847862243652, 0.286492919921875, 0.16276473999023439]}, 'weight': {'score': [0.01703835725784302, 0.007143057309664213, 0.0024396961226182826, 0.00713392029220293, 0.0010294518433511257], 'topk_tokens': [' cap', ' bag', ' \n', ' the', ' apple', ' the', ' garden', ' the', 'assistant', ' Daniel', 'itol', '\n\n', 'b', ' bathroom', ' Bench', 'athroom', '<|end_header_id|>', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.05888795852661133, 0.00678714116414388, 0.009408628940582274, 0.003489863872528076]}, 'saliency': {'score': [0.0003122925758361816, 0.00010255762399771274, 4.6619597603293025e-05, 0.00010200620356977646, 1.477263867855072e-05], 'topk_tokens': ['assistant', ' the', ' garden', '***', ' the', ' Red', ' Jackson', ' the', ' apple', ' bag', 'athroom', 'itol', '<|end_header_id|>', '<|start_header_id|>', ' Bench', ' bathroom', '\n\n', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.0009070783853530884, 0.0001358240842819214, 0.0002932488918304443, 6.726980209350585e-05]}}, 27: {'grad': {'score': [0.21321744918823243, 0.2327047096245196, 0.17551820418413946, 0.23326605796355246, 0.19365227222442627], 'topk_tokens': [' decision', 'Republicans', ' one', '\n', ' services', ',\n', 'ers', '!"', ' expected', 'ides', ' duration', ' work', ' speakers', ' printed', ' execution', '!"', '--', ' DAYS', 'event', ' step'], 'evidence_proportions': [0.21544218063354492, 0.15862337748209634, 0.20808944702148438, 0.28207855224609374]}, 'weight': {'score': [0.022115403413772584, 0.007354239792565675, 0.004201758433790768, 0.00730901639039497, 0.0011332170106470585], 'topk_tokens': ['Dub', ' bathroom', ' bag', ' Mr', ' apple', ' garden', 'assistant', '.', ' Bench', '.\n\n', 'itol', '\n\n', ' Daniel', 'b', '<|end_header_id|>', ':', ' bathroom', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.08685529232025146, 0.007851640383402506, 0.006087636947631836, 0.0034677743911743163]}, 'saliency': {'score': [0.0014060944318771361, 0.00010364564933441756, 0.00012302836951087504, 9.722237176422662e-05, 2.8030481189489365e-05], 'topk_tokens': [' Fourth', ' the', ' apple', 'RE', ' Bench', 'NEW', ' Mr', 'Dub', '.', 'New', '<|begin_of_text|>', 'Bridge', '<|start_header_id|>', '.\n\n', ' bathroom', ':', ' Daniel', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.005899950861930847, 0.0003957053025563558, 0.0003485023975372314, 8.106827735900878e-05]}}, 28: {'grad': {'score': [0.17087674140930176, 0.270904627960292, 0.26749108819400563, 0.2714136751183184, 0.2665811777114868], 'topk_tokens': [' of', '\n', 'ible', '\n', ' and', ' the', ' the', 'nes', 'en', 'nes', ' the', 'nes', '\n', ' Peter', 'nes', ' the', 'nes', 'S', ',', '.'], 'evidence_proportions': [0.21718978881835938, 0.18735361099243164, 0.11985130310058595, 0.16507949829101562]}, 'weight': {'score': [0.00960158258676529, 0.006944312895351892, 0.003604713608236874, 0.006958839346033384, 0.0010582548566162586], 'topk_tokens': [' the', ' was', ' garden', 'Bridge', ' \n', ' the', ' the', ' bathroom', ' the', ' apple', ' garden', ' before', 'assistant', '<|end_header_id|>', 'b', '\n\n', 'athroom', '<|start_header_id|>', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.012471199035644531, 0.007711331049601236, 0.009203892946243287, 0.009971880912780763]}, 'saliency': {'score': [0.00017223805189132692, 5.271121618980922e-05, 6.980054518755744e-05, 5.1996551952206096e-05, 1.2002885341644287e-05], 'topk_tokens': [' the', ' \n', ' bathroom', ' was', ' garden', ' the', ' below', ' Bench', ' on', ' of', ' bathroom', ' apple', 'athroom', ' garden', '<|start_header_id|>', ':', '<|end_header_id|>', '\n\n', 'Bridge', '<|begin_of_text|>'], 'evidence_proportions': [0.00036296993494033813, 0.00014941891034444174, 0.00013977885246276855, 7.949471473693848e-05]}}, 29: {'grad': {'score': [0.21679564714431762, 0.24253534387659142, 0.25432783014634075, 0.24256272468502674, 0.3023398369550705], 'topk_tokens': [' B', '\n', ' every', ' themselves', ':', ' force', ' give', 'an', ' be', ' advantage', ' wholly', ' their', ' engaged', ' extra', ' not', 'assistant', ' not', 'pend', '<|start_header_id|>', ' enough'], 'evidence_proportions': [0.21436601877212524, 0.17842388153076172, 0.23603744506835939, 0.24554367065429689]}, 'weight': {'score': [0.005363751947879791, 0.007136331098145343, 0.0018536360824809356, 0.007188053852723962, 0.001001683995127678], 'topk_tokens': [' Where', '.', '.\n\n', ' the', 'Answer', ' garden', ' \n', ' was', ' apple', ' the', ' the', ' before', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|start_header_id|>', ':', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00751948356628418, 0.005712985992431641, 0.005460721254348755, 0.003123116493225098]}, 'saliency': {'score': [0.00013562291860580444, 7.118540838120216e-05, 6.360779790317311e-05, 7.093742582991812e-05, 1.9376631826162338e-05], 'topk_tokens': [' Where', ' garden', 'Answer', '      ', '<|eot_id|>', '.', ' apple', ' \n', ' before', ' the', ' the', ' was', 'athroom', '<|end_header_id|>', 'assistant', 'b', ':', '<|begin_of_text|>', '<|start_header_id|>', '\n\n'], 'evidence_proportions': [8.827447891235352e-05, 0.00014949341615041095, 0.0001385211944580078, 0.00015395879745483398]}}, 30: {'grad': {'score': [0.18271484375, 0.22536189739520734, 0.17354550081140854, 0.22599073302694214, 0.22120802849531174], 'topk_tokens': ['.', ' the', '      ', ' message', ' day', '202', 'time', ' the', ' concerned', ' Proof', ' at', 'UL', ' have', ' the', ' THE', ' Team', ' time', ' The', '7', ' for'], 'evidence_proportions': [0.15074825286865234, 0.19002135594685873, 0.20391349792480468, 0.17832164764404296]}, 'weight': {'score': [0.019029346108436585, 0.007210349991337413, 0.004384454558877384, 0.0071766079332233335, 0.002990313805639744], 'topk_tokens': [' the', '.\n\n', '?', '.', ' Bor', ' \n', ' the', ' before', ' Daniel', ' Bench', ' garden', 'assistant', '<|end_header_id|>', ' bathroom', '\n\n', 'b', '<|start_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.051050543785095215, 0.015346209208170572, 0.010882842540740968, 0.0059786558151245115]}, 'saliency': {'score': [0.0016216188669204712, 0.00015433349868624073, 0.00012893448857700124, 0.00014748353692073556, 2.9732007533311844e-05], 'topk_tokens': ['New', ' the', ' before', 'ANK', ' Mr', '.\n\n', ' Fourth', ' the', '.', '<|end_header_id|>', 'assistant', 'Bridge', '<|begin_of_text|>', ':', ' Daniel', ' Bench', ' bathroom', '<|start_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.006657637655735016, 0.00035031139850616455, 0.0006061911582946777, 0.00013380050659179689]}}, 31: {'grad': {'score': [0.17362284660339355, 0.13702623796598865, 0.15348353631356182, 0.1367156366392533, 0.10724442126229405], 'topk_tokens': ["'clock", ' Democratic', ' Pennsylvania', ' had', ' made', ' m', ' method', '�', ' N', ' considerable', ' l', '�', ' interesting', ' When', 'D', 'D', ' O', ' company', ' prepared', 'g'], 'evidence_proportions': [0.19182658195495605, 0.1511133909225464, 0.24624319672584533, 0.11345085501670837]}, 'weight': {'score': [0.004327458143234253, 0.006588515619493505, 0.0024359524250030518, 0.0066333468600269, 0.0016206512227654457], 'topk_tokens': [' the', ' was', ' Where', 'Answer', '<|eot_id|>', '?', ' the', ' before', ' apple', ' \n', ' the', '<|eot_id|>', 'assistant', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008435428142547607, 0.003705441951751709, 0.0016776561737060546, 0.00443730354309082]}, 'saliency': {'score': [0.0003432855010032654, 3.5204163134267864e-05, 6.25366673750036e-05, 3.349879230657066e-05, 1.6876962035894394e-05], 'topk_tokens': [' business', '.\n\n', 'Question', 'Answer', '.', ' Daniel', '.', ' garden', ' the', ' the', '<|start_header_id|>', ' apple', ' \n', '<|end_header_id|>', ' the', ' Daniel', ' the', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0009937286376953125, 0.0002457549174626668, 0.00011702179908752441, 0.0001662313938140869]}}, 'pred_res': 'The capitol.<|eot_id|>', 'score': 0}
2025-01-23 22:23:17.522 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:23:17.522 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-6_pid-3_2-3-6-7.pkl | len: 10 |  size: 9.01 KB
Processing depth (2, 3, 6, 7):   4%|▍         | 4/100 [00:53<21:30, 13.44s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.12it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.10it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (1, 4, 5, 7):   4%|▍         | 4/100 [01:03<21:30, 13.44s/it]2025-01-23 22:23:27.066 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel got the apple.
2025-01-23 22:23:27.068 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (445, 449) -->  got the apple.
2025-01-23 22:23:27.068 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel journeyed to the bathroom.
2025-01-23 22:23:27.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1862, 1868) --> . Daniel journeyed to the
2025-01-23 22:23:27.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:23:27.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2161, 2166) -->  Daniel went to the garden
2025-01-23 22:23:27.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John moved to the bedroom.
2025-01-23 22:23:27.103 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2885, 2890) --> . John moved to the
2025-01-23 22:23:27.103 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the bedroom.
2025-01-23 22:23:27.116 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2723, 2729) --> . John went back to the
2025-01-23 22:23:27.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football there.
2025-01-23 22:23:27.131 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2977, 2982) --> . Mary got the football
2025-01-23 22:23:27.131 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the office.
2025-01-23 22:23:27.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (328, 334) -->  John journeyed to the office
2025-01-23 22:23:27.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:23:27.146 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2566, 2571) --> . Mary moved to the
2025-01-23 22:23:27.146 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John journeyed to the office.
2025-01-23 22:23:27.147 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (327, 333) --> . John journeyed to the
2025-01-23 22:23:27.148 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-23 22:23:27.163 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3121, 3127) -->  Sandra journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:23:27.735 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Daniel's house.<|eot_id|>
2025-01-23 22:23:27.739 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4244])
your chose emoji: ['🧜🏻\u200d♂️', '🧑🏽\u200d❤\u200d💋\u200d🧑🏻', '👩🏾\u200d💼', '🇧🇲', '👨🏾\u200d🤝\u200d👨🏽', '🏩', '🖕🏿', '🧎🏻', '🇬🇵', '👩🏽\u200d❤\u200d💋\u200d👨🏿']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4247, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 225197.53it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.53it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.93it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.15it/s]
2025-01-23 22:23:30.916 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.30301475524902344, 0.24046946166963445, 0.4221571754006779, 0.2386978689821927, 0.1854466139668166], 'topk_tokens': [' during', ' no', 'AILY', ' MO', ' IN', ' and', '.', 'EF', ' when', ' discover', ' first', ':', ' first', ' charges', ' instead', ' but', ' make', ' instead', ' Daily', '202'], 'evidence_proportions': [0.37717437744140625, 0.3216094970703125, 0.26108856201171876, 0.263299560546875]}, 'weight': {'score': [0.02218829095363617, 0.007519911731695719, 0.0013733436079586252, 0.007499786697535989, 0.0257698827319675], 'topk_tokens': ['ace', '<|end_header_id|>', 'iscal', 'ible', ' afternoon', '�', 'ided', 'ian', '�', '�', '�', '�', 'hour', '�', '.', 'fax', 'illage', ' anxious', 'ian', '<|begin_of_text|>'], 'evidence_proportions': [0.000661700963973999, 0.04054996371269226, 0.03869223594665527, 0.0008716106414794922]}, 'saliency': {'score': [8.343905210494995e-05, 0.00010020294440951885, 6.74601863412296e-05, 0.00010054840866432763, 0.000142471958892514], 'topk_tokens': ['\n\n', ' else', '?', ' news', ' anything', ' value', ' write', 'b', ' \n', 'assistant', ' before', ' the', ' charges', ' apple', '<|eot_id|>', ' block', 'athroom', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [2.148747444152832e-05, 0.0001169741153717041, 0.00013774633407592773, 3.8450956344604494e-05]}}, 25: {'grad': {'score': [0.3063774108886719, 0.3180426877612138, 0.4078153722426471, 0.317370384938672, 0.3941728129531398], 'topk_tokens': [' *\n\n', 'APER', ' the', '600', ' printer', 'hand', 'old', ' newspaper', ' paper', ' old', ' web', ' the', '\n', '\n', ' veto', ' old', ' this', ' old', ' the', ' tour'], 'evidence_proportions': [0.4282341003417969, 0.26682790120442706, 0.2599029541015625, 0.302825927734375]}, 'weight': {'score': [0.025015565752983093, 0.007525440213369824, 0.0006144546410616706, 0.007498054331821102, 0.026897100487140693], 'topk_tokens': ['�', ' hear', '�', '�', 'iscal', '�', 'hour', 'ian', '�', 'ided', 'fax', '\n\n', '�', ' one', '.', 'ian', 'illage', ' anxious', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009381473064422607, 0.049050360918045044, 0.0399817943572998, 0.000469517707824707]}, 'saliency': {'score': [0.00026763230562210083, 0.00010591103296883953, 3.8949882282930264e-05, 0.00010568261743586927, 0.00023132984084312362], 'topk_tokens': [' to', '<|start_header_id|>', ' fact', ' hear', 'c', ' not', 'iscal', ' one', ' was', '.', ' anxious', ' *\n\n', ' *\n\n', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', ' *\n\n', ' *\n\n', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [5.523115396499634e-05, 0.00046512981255849206, 0.00045583248138427737, 1.2356042861938477e-05]}}, 26: {'grad': {'score': [0.5342529296875, 0.5630407367056893, 0.5143019732306985, 0.5635732609361958, 0.48180764130871706], 'topk_tokens': [' time', 'body', ' After', ' value', ' remains', ' containing', ' doctor', 'system', ' there', ' paper', ' hidden', ' face', ' paper', ' looked', ' more', ' paper', ' bathroom', ' news', 'Great', ' Paul'], 'evidence_proportions': [0.8570404052734375, 0.4574381510416667, 0.43203125, 0.47042236328125]}, 'weight': {'score': [0.02062114477157593, 0.00752244086610252, 0.002595229183926302, 0.007499915495027996, 0.02243855023624921], 'topk_tokens': ['time', 'iscal', '�', 'ible', '�', 'ian', 'ided', '�', '�', '�', ' anxious', '�', 'hour', '�', '�', '.', 'fax', 'illage', 'ian', '<|begin_of_text|>'], 'evidence_proportions': [0.004307985305786133, 0.03828640778859456, 0.030950450897216798, 0.002144050598144531]}, 'saliency': {'score': [0.00028145164251327517, 0.00013734631491235097, 0.00012824377592872172, 0.00013673276370162473, 0.0004207588205433855], 'topk_tokens': [' fortunate', ' strife', 'eward', ' printers', '�', ' prize', ' Reporter', 'cery', '\u200d', ' impressed', '<|eot_id|>', 'Bridge', ' sounded', ' bogus', '�', '�', 'estead', '�', 'ot', '<|start_header_id|>'], 'evidence_proportions': [0.00018840283155441284, 0.0006737311681111654, 0.00013906359672546386, 2.754330635070801e-05]}}, 27: {'grad': {'score': [0.36493968963623047, 0.6689028882520014, 0.5155590281767004, 0.6715961759254753, 0.7905617145576862], 'topk_tokens': ['nes', '�', ' assistance', 'eward', ' before', '\n', 'a', 'cuts', ' breaking', '�', '�', 's', '�', '�', 'Bridge', ' bogus', 'port', 'icians', '�', 'ably'], 'evidence_proportions': [0.40616607666015625, 0.3660380045572917, 0.28091964721679685, 0.41466064453125]}, 'weight': {'score': [0.0264174222946167, 0.007528017626556476, 0.0011955885326161103, 0.007489266015736721, 0.027396829140306722], 'topk_tokens': ['�', 'ible', ' afternoon', 'iscal', '�', '�', 'time', '�', 'hour', '�', '�', '�', 'ided', 'fax', '.', 'ian', 'ian', 'illage', ' anxious', '<|begin_of_text|>'], 'evidence_proportions': [0.002347707748413086, 0.053300301233927413, 0.038358545303344725, 0.001472616195678711]}, 'saliency': {'score': [0.0003817126154899597, 0.00020674399440362084, 0.0001515798708971809, 0.00020635673177006304, 0.000506818595558706], 'topk_tokens': ['�', ' themselves', '�', ' could', ' century', ' laid', '\n', ' so', 'ot', '<|end_header_id|>', '<|end_header_id|>', '<|start_header_id|>', 'ences', ' had', ' him', ' occasionally', ' exclaimed', 'isc', ' sounded', 'ors'], 'evidence_proportions': [0.0002256631851196289, 0.000724633534749349, 0.0004126429557800293, 6.41167163848877e-05]}}, 28: {'grad': {'score': [0.46313982009887694, 0.5843656327490361, 0.6099314970128676, 0.5847365549689342, 0.5010521002490111], 'topk_tokens': [' a', 'a', ' a', ' a', 'ra', ' a', 'A', ' a', ' a', ' a', ' a', 'a', ' a', ' a', ' a', ' a', ' a', 'a', ' a', ' A'], 'evidence_proportions': [0.6299285888671875, 0.37491607666015625, 0.3679065704345703, 0.530810546875]}, 'weight': {'score': [0.025306236743927003, 0.007436732783271532, 0.0009594848927329568, 0.00740402025025584, 0.02581313132035612], 'topk_tokens': ['�', '�', 'athroom', ' the', '�', 'ible', 'time', ' hear', ' one', '�', 'ided', '<|end_header_id|>', 'ian', 'hour', 'fax', '.', 'illage', ' anxious', 'ian', '<|begin_of_text|>'], 'evidence_proportions': [0.0018108487129211426, 0.047624409198760986, 0.04094781875610352, 0.0016791582107543944]}, 'saliency': {'score': [0.00016075372695922852, 7.287690239231082e-05, 2.7484753552605124e-05, 7.282581643218958e-05, 0.0001108291173222089], 'topk_tokens': ['nes', ' bathroom', ' minds', 'ed', ' speech', "'s", '.', 'ed', '\n\n', '<|begin_of_text|>', ' one', ' had', ' of', '.', ' not', ' duration', ' enter', 'athroom', 'b', '<|end_header_id|>'], 'evidence_proportions': [1.8596649169921875e-05, 0.00033234556516011554, 0.00018622875213623045, 4.309415817260742e-05]}}, 29: {'grad': {'score': [0.5185775756835938, 0.806999046661614, 0.7276695475858801, 0.8090180384069361, 1.022823449337121], 'topk_tokens': [' not', ' sounded', ' left', ' of', 'cap', ' became', 'had', '�', 'SP', ' themselves', ' another', ' and', '�', '�', 'ing', '�', '�', 'ian', '�', '�'], 'evidence_proportions': [0.603729248046875, 0.2914276123046875, 0.5166992187499999, 0.72491455078125]}, 'weight': {'score': [0.026979857683181764, 0.007517595023920263, 0.0009134122553993674, 0.007478314547160063, 0.02959536693312905], 'topk_tokens': ['first', ' afternoon', '�', ' an', ' one', ' hear', 'hour', '�', ' result', '�', 'illage', 'fax', '.', '�', 'ian', 'ian', '.', 'ided', ' anxious', '<|begin_of_text|>'], 'evidence_proportions': [0.0012794137001037598, 0.051741858323415116, 0.04404568672180176, 0.0007599830627441406]}, 'saliency': {'score': [0.00023582428693771362, 0.00010806822428738114, 3.262214800890754e-05, 0.0001080706200280111, 0.00030423053587325897], 'topk_tokens': [' an', ' appear', ' afternoon', ' of', '9', ' to', ' him', '�', ' result', '.', 'athroom', ' hear', ' early', '�', '<|eot_id|>', 'ided', ' part', ' fact', ' the', 'first'], 'evidence_proportions': [5.970895290374756e-05, 0.0004550516605377197, 0.0003047823905944824, 4.4685602188110354e-05]}}, 30: {'grad': {'score': [0.54661865234375, 0.4659072304715093, 0.5709413640639361, 0.4646705529185431, 0.43609443818679966], 'topk_tokens': [' news', 'in', ' was', ' to', ' to', ' no', ' to', ' news', ' to', ' Republicans', ' to', ' to', ' to', ' no', ' to', ' is', ' to', ' to', ' B', ' trib'], 'evidence_proportions': [0.629364013671875, 0.6079610188802083, 0.4784423828125, 0.47498779296875]}, 'weight': {'score': [0.011979639530181885, 0.007348460760401759, 0.004721370690009173, 0.007347673135073294, 0.018240033978163595], 'topk_tokens': ['<|start_header_id|>', 'assistant', '�', 'fax', 'ided', '�', '\n\n', '�', '�', 'ian', '�', ' anxious', 'illage', '.', 'ian', '<|begin_of_text|>', '<|eot_id|>', 'b', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.006812334060668945, 0.023640950520833332, 0.010411596298217774, 0.0036879539489746097]}, 'saliency': {'score': [0.0003457695245742798, 0.0002290347842348304, 0.00021326717208413517, 0.00022860583217337909, 0.000505851675765683], 'topk_tokens': [',', '.\n', ' provided', '.\n\n', '<|end_header_id|>', 'ian', '\u200d', '.\n', ' bathroom', '<|eot_id|>', 'ot', 'assistant', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'b', '<|eot_id|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00021945685148239136, 0.0007655968268712361, 0.00020962953567504883, 7.916688919067383e-05]}}, 31: {'grad': {'score': [0.37527074813842776, 0.5796216044505681, 0.4763951582067153, 0.5814333660290402, 0.6272442027775929], 'topk_tokens': [' had', ' apple', ' two', ' late', ' it', ' about', ' they', ' absent', ' on', ' offices', '      ', 'ant', ',\n', 's', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', ' and', ' execution', '<|start_header_id|>'], 'evidence_proportions': [0.5289797782897949, 0.2659149169921875, 0.34398193359375, 0.4148193359375]}, 'weight': {'score': [0.011327838897705078, 0.007152077728982867, 0.0021167379968306597, 0.00717299028026327, 0.03139412944967097], 'topk_tokens': ['\n\n', '�', '�', 'hour', '�', '<|eot_id|>', '�', ' anxious', '�', 'illage', 'fax', '.', 'ian', 'ian', '�', '�', '<|end_header_id|>', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [0.0025238990783691406, 0.022958000500996906, 0.013457202911376955, 0.0022854328155517576]}, 'saliency': {'score': [0.00014545321464538575, 0.00018117704025458976, 7.643682115218219e-05, 0.00018219675023829258, 0.0003187602216547186], 'topk_tokens': [' result', 'fax', 'illage', '\n', '.', ' one', ' of', '\n', ' fact', '.', '�', '�', '\n', ' the', '.\n\n', 'b', '<|end_header_id|>', ',', 'athroom', '<|eot_id|>'], 'evidence_proportions': [0.00020150840282440186, 0.00013277928034464517, 0.00017660856246948242, 8.466243743896485e-05]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-23 22:23:30.922 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:23:30.922 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-6_pid-4_1-4-5-7.pkl | len: 10 |  size: 8.65 KB
Processing depth (1, 4, 5, 7):   5%|▌         | 5/100 [01:07<21:15, 13.43s/it]Processing depth (1, 4, 5, 7):   5%|▌         | 5/100 [01:07<21:21, 13.49s/it]
2025-01-23 22:23:31.133 | INFO     | __main__:<module>:99 - Selected idx: 7
2025-01-23 22:23:31.133 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-23 22:23:31.134 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:23:31.134 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:23:31.134 | INFO     | __main__:<module>:103 - Needle: [' John journeyed to the office.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Mary journeyed to the office.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-23 22:23:31.134 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-23 22:23:31.134 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
Processing depth (3, 5, 6, 8, 9):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:23:40.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:23:40.503 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1537, 1543) --> . Daniel journeyed to the
2025-01-23 22:23:40.503 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:23:40.514 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2118, 2122) -->  grabbed the milk.
2025-01-23 22:23:40.514 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:23:40.525 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2395, 2400) -->  the senate. Daniel went
2025-01-23 22:23:40.526 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:23:40.541 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3307, 3311) -->  Daniel left the milk
2025-01-23 22:23:40.542 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:23:40.560 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3662, 3667) --> . John moved to the
2025-01-23 22:23:40.560 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John journeyed to the office.
2025-01-23 22:23:40.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (328, 334) -->  Mary journeyed to the office
2025-01-23 22:23:40.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:23:40.578 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3234, 3240) --> . John went back to the
2025-01-23 22:23:40.578 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary moved to the bathroom.
2025-01-23 22:23:40.592 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2826, 2831) --> . Mary moved to the
2025-01-23 22:23:40.592 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:23:40.606 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2794, 2800) --> . Sandra journeyed to the
2025-01-23 22:23:40.606 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary got the football there.
2025-01-23 22:23:40.623 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3540, 3545) --> . Mary got the football
2025-01-23 22:23:40.624 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:23:40.625 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (327, 333) --> . Mary journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:23:41.102 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:23:41.102 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4223])
your chose emoji: ['🤙🏻', '🪦', '✅', '👩\u200d👧\u200d👧', '🧑\u200d🦳', '⬆', '⛹🏿\u200d♂️', '🚶\u200d♂\u200d➡️', '👴🏽', '🤳🏻']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4226, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 151146.09it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 106.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.65it/s]
2025-01-23 22:23:44.059 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.4009691874186198, 0.35292578217433745, 0.37895359712488513, 0.3524368216574993, 0.29690422117710114], 'topk_tokens': ['com', ' bathroom', ' It', 'remark', ' completion', ' Queen', ' admission', ' perpet', ' Min', ' im', ' rest', ' drawing', ' It', 'ree', ' attention', ' considerable', '-per', ' It', 'athroom', ' bend'], 'evidence_proportions': [0.4520111083984375, 0.3621826171875, 0.4384765625, 0.430633544921875, 0.30950927734375]}, 'weight': {'score': [0.002103759596745173, 0.00740169948500251, 0.0025214605471667122, 0.007472015867249293, 0.002913661766797304], 'topk_tokens': ['<|eot_id|>', ' Newspaper', 'Just', '?\n', ':', 'user', '\n\n', 'Question', '<|eot_id|>', '.\n\n', 'Answer', 'Bridge', 'b', '<|eot_id|>', '\n\n', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002065752943356832, 0.0007408857345581055, 0.001619982719421387, 0.0023296475410461426, 0.0035427331924438475]}, 'saliency': {'score': [0.00011103972792625427, 0.0001471347637894139, 0.0001469333382213817, 0.00014734424779373946, 0.00015095621347427368], 'topk_tokens': [' Herald', ' *\n\n', '\n\n', ' bathroom', ' Where', ' Wood', 'ioneer', ' not', ' journey', 'Just', ' Pioneer', '\n\n', ' Newspaper', '\n\n', 'b', '<|end_header_id|>', 'Question', 'assistant', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.00014536579449971518, 5.1252543926239014e-05, 6.868839263916016e-05, 0.00018459558486938477, 0.00010118484497070312]}}, 25: {'grad': {'score': [0.5542484919230143, 0.4262130311242901, 0.4408409174750833, 0.42535645742105205, 0.3526514172554016], 'topk_tokens': [' Marshall', ' the', ' in', ' idol', ' print', 'just', ' grat', 'in', ' im', ' printers', ' the', ' grabbed', 'In', 'in', 'out', ' ', 'ages', 'fast', ' printing', 'out'], 'evidence_proportions': [0.6082127888997396, 0.5844135284423828, 0.517425537109375, 0.663330078125, 0.4149169921875]}, 'weight': {'score': [0.002980350206295649, 0.007241493483262213, 0.0023119730107924517, 0.0073062418361199795, 0.0031913332641124725], 'topk_tokens': ['.\n\n', 'burn', ' dropped', 'Bridge', 'Just', '<|eot_id|>', ' not', 'Question', '\n\n', ':', '?\n', 'b', '<|eot_id|>', 'Answer', '.\n\n', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.001988723874092102, 0.0013413876295089722, 0.005006057024002075, 0.0049765706062316895, 0.0018587887287139894]}, 'saliency': {'score': [0.00011696666479110718, 0.00012933692405193382, 0.00018528892713434557, 0.00012895173166168857, 0.00010607903823256493], 'topk_tokens': [' dropped', ' Senator', 'tele', ' did', ' return', ' Mary', 'b', 'nes', 'Democratic', 'assistant', ' milk', 'Min', ' received', ' obtained', 'Answer', '\n\n', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.4974346160888672e-05, 8.323043584823608e-05, 0.00025715827941894535, 0.00018806755542755127, 5.727410316467285e-05]}}, 26: {'grad': {'score': [0.4209722677866618, 0.4321051045610506, 0.43665257622213927, 0.43213211368919563, 0.43547189235687256], 'topk_tokens': [' bend', ' cap', ' paying', 'pay', ' papers', 'ab', '�', ' breaking', ' bogus', 'ball', 'body', ' hopes', ' Press', 'en', 'RI', 'burn', ' bag', ' B', 'blue', 'b'], 'evidence_proportions': [0.35118865966796875, 0.5287246704101562, 0.39977226257324217, 0.3193960189819336, 0.5209716796875]}, 'weight': {'score': [0.002055636296669642, 0.007032587355592495, 0.0021263632704229918, 0.007101267404611189, 0.006843168288469315], 'topk_tokens': [':', '.', 'Just', '.\n\n', '?\n', '<|eot_id|>', 'Question', '<|eot_id|>', ':', 'Answer', '\n\n', '<|eot_id|>', 'b', 'Bridge', '.\n\n', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0017251521348953247, 0.0007762163877487183, 0.0020597696304321287, 0.002526998519897461, 0.0030945301055908202]}, 'saliency': {'score': [9.593740105628967e-05, 8.757128937798431e-05, 0.0001303571112015668, 8.717409537071916e-05, 0.0002715163864195347], 'topk_tokens': ['tele', ' Sandra', 'Question', 'ball', '�', ' bathroom', ' laughed', 'assistant', ' met', '<|eot_id|>', '<|eot_id|>', ' carrier', '\n\n', '<|end_header_id|>', 'athroom', ' bathroom', 'Bridge', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.099045276641846e-05, 2.9549002647399902e-05, 0.0001271843910217285, 0.0001122206449508667, 0.00014671087265014646]}}, 27: {'grad': {'score': [0.4940268596013387, 0.4629068658975871, 0.4758586883544922, 0.46262201901360805, 0.39029140770435333], 'topk_tokens': [' that', ' be', ' of', ' city', ',', ' office', ' of', ' city', ',', ',', ' is', ' luxury', ' business', ' business', ' copy', ',', ',', ' be', '\n\n', '\n\n\n\n'], 'evidence_proportions': [0.5187606811523438, 0.5547332763671875, 0.4346210479736328, 0.4379086494445801, 0.5200815200805664]}, 'weight': {'score': [0.0035676037271817527, 0.007269412716361397, 0.0027503230992485493, 0.007327592289951164, 0.006116372533142567], 'topk_tokens': ['If', '.\n\n', 'If', 'Minnesota', 'Just', '\n\n', ' *\n\n', 'Bridge', '?\n', 'Answer', ':', 'Question', '<|eot_id|>', '.\n\n', 'b', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002293507258097331, 0.0006371438503265381, 0.0046861529350280765, 0.005661308765411377, 0.004647374153137207]}, 'saliency': {'score': [0.0004172772169113159, 0.00012664077296620346, 0.00025755868238561293, 0.00012389929418142834, 0.00025152135640382767], 'topk_tokens': [' bathroom', 'those', 'Answer', '�', ' Alexander', ' *\n\n', ' Daniel', ' laughed', 'If', 'Question', 'If', ' *\n\n', 'Bridge', 'Just', '\n\n', 'Minnesota', 'b', '<|end_header_id|>', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0002012352148691813, 9.581446647644043e-06, 0.0007926225662231445, 0.0007298365235328674, 0.0003772914409637451]}}, 28: {'grad': {'score': [0.41113726298014325, 0.34988580966413274, 0.3356783249798943, 0.3496490101439024, 0.32302403450012207], 'topk_tokens': [' upper', ' old', ' so', ' out', 'material', ' out', 'old', 'blue', ' out', ' three', ' border', 'Dub', 'hom', 'ball', ' Dub', ' three', ' Out', ' Dub', ' out', ' du'], 'evidence_proportions': [0.3472035725911458, 0.4998970031738281, 0.4071044921875, 0.47223663330078125, 0.372003173828125]}, 'weight': {'score': [0.0014395999411741893, 0.007015883837951284, 0.0012467301943722892, 0.007095054192414897, 0.0038343388587236404], 'topk_tokens': [' milk', '\n', '\n\n', '�', ':', 'Question', '?\n', '.', '.\n\n', 'Answer', '.\n\n', 'Just', 'Bridge', '<|eot_id|>', 'b', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0009696632623672485, 0.0010475367307662964, 0.0009043335914611817, 0.001046895980834961, 0.003166604042053223]}, 'saliency': {'score': [4.688898722330729e-05, 9.035339188474194e-05, 3.3431193407844094e-05, 9.106800331950417e-05, 7.10352323949337e-05], 'topk_tokens': ['.\n\n', '.', ' dropped', 'graph', 'just', '\n', 'assistant', ' bedroom', ' bathroom', '<|end_header_id|>', ' block', 'Just', ' bedroom', ' bathroom', ' milk', 'b', 'Bridge', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.9744038581848145e-05, 7.866322994232178e-05, 4.8601627349853514e-05, 6.035715341567993e-05, 4.1556358337402346e-05]}}, 29: {'grad': {'score': [0.6910374959309896, 0.6396221516541942, 0.6726814718807445, 0.6390564162534396, 0.37699565291404724], 'topk_tokens': [' shore', ' Among', ' discover', ' WITH', ' STR', 'ER', ' on', ' material', ' steam', 'S', 'ION', ' AND', ' first', ' legislative', 'ed', ' to', ' arranged', 'APER', 'ATION', 'athroom'], 'evidence_proportions': [0.7250391642252604, 0.749267578125, 0.7237152099609375, 0.6649627685546875, 0.59183349609375]}, 'weight': {'score': [0.0012927042941252391, 0.00715713979393399, 0.0008837484261568855, 0.007242082634265043, 0.0030088070780038834], 'topk_tokens': ['<|eot_id|>', '.', 'a', '\n\n', '.\n\n', ' not', '?\n', ' Do', '�', '\n\n', ':', 'Question', 'Answer', '.\n\n', 'b', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00024019181728363037, 0.0010353028774261475, 0.0009670495986938477, 0.0007611662149429321, 0.0035125255584716798]}, 'saliency': {'score': [2.0759801069895428e-05, 0.0001211447197850485, 2.212384167839499e-05, 0.0001225305038313033, 9.058043360710144e-05], 'topk_tokens': ['\n\n', ' Where', ' ', ' *', 'If', ' the', 'S', 'L', '<|eot_id|>', 'If', ' Do', '.\n\n', 'assistant', 'Answer', 'a', 'b', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [6.432334582010905e-06, 1.757591962814331e-05, 2.2959709167480468e-05, 1.3358891010284424e-05, 4.422068595886231e-05]}}, 30: {'grad': {'score': [0.2165654500325521, 0.210547877328406, 0.18351908291087432, 0.21073371160510862, 0.2764396667480469], 'topk_tokens': [' based', ' Bor', ' bogus', ' business', 'ed', 'ree', ' business', ' bathroom', ' border', ' business', 'RI', ' bogus', ' balance', 'ball', ' bend', 'body', ' bathroom', ' B', ' B', 'b'], 'evidence_proportions': [0.1851348876953125, 0.234344482421875, 0.25415344238281246, 0.220977783203125, 0.1989410400390625]}, 'weight': {'score': [0.0037748217582702637, 0.007114042786698854, 0.0034561691915287692, 0.007163109247091864, 0.015324326232075691], 'topk_tokens': [' *\n\n', '.\n\n', '\n', '?\n', '.', ':', ':', 'Bridge', '.\n\n', '<|eot_id|>', '<|eot_id|>', 'Answer', '.\n\n', 'Question', 'b', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0025727550188700357, 0.0013048350811004639, 0.0023612499237060545, 0.004846543073654175, 0.007749485969543457]}, 'saliency': {'score': [0.00010773912072181702, 0.00021847789878818135, 0.00013661296928630154, 0.00021978335422883795, 0.00031937286257743835], 'topk_tokens': ['un', ' journey', ' bathroom', 'Answer', ' PA', ' Do', ' bill', 'burn', '\n', ' milk', '\n\n', ' bathroom', ' bogus', ' bogus', 'assistant', '<|end_header_id|>', 'Bridge', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [6.664295991261801e-05, 3.533065319061279e-05, 8.82267951965332e-05, 0.00023351609706878662, 0.00013387203216552734]}}, 31: {'grad': {'score': [0.2863304217656453, 0.33387420553197467, 0.2922661725212546, 0.33448738305902753, 0.18790147453546524], 'topk_tokens': [' generally', ' of', ' from', ' for', ' o', ' for', 'for', ' for', ',', ' be', ' most', ' of', 'ex', 'ely', ' o', 't', 'gro', ' ', 'g', 'b'], 'evidence_proportions': [0.3907623291015625, 0.2815399169921875, 0.269384765625, 0.21022486686706543, 0.2426746368408203]}, 'weight': {'score': [0.002368677407503128, 0.006492725774906735, 0.0020611040732439827, 0.006552623159425501, 0.006174528039991856], 'topk_tokens': ['.\n', 'Just', ' Do', '.\n\n', '<|start_header_id|>', 'Question', '.\n\n', ':', ' Where', '<|eot_id|>', '?\n', ':', 'Answer', 'assistant', '<|eot_id|>', 'b', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009838789701461792, 0.0006938129663467407, 0.002100849151611328, 0.001984328031539917, 0.005945634841918945]}, 'saliency': {'score': [4.64320182800293e-05, 7.560760332857666e-05, 3.441116389106302e-05, 7.61116563473004e-05, 0.0001847180537879467], 'topk_tokens': [' One', ' not', ' You', '�', ' Where', 'Just', '.\n\n', '�', 'Answer', ' return', '?\n', 'body', '<|eot_id|>', '<|end_header_id|>', 'Question', 'assistant', 'b', '\n\n', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [1.9763906796773277e-05, 2.0951032638549805e-05, 3.3903121948242185e-05, 4.2244791984558105e-05, 0.00011469721794128418]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:23:44.072 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:23:44.073 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-7_pid-0_3-5-6-8-9.pkl | len: 10 |  size: 9.26 KB
Processing depth (3, 5, 6, 8, 9):   1%|          | 1/100 [00:12<21:12, 12.85s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.14it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Processing depth (0, 2, 3, 6, 9):   1%|          | 1/100 [00:22<21:12, 12.85s/it]2025-01-23 22:23:53.633 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:23:53.633 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-23 22:23:53.633 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:23:53.641 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (953, 957) -->  Daniel grabbed the milk
2025-01-23 22:23:53.641 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:23:53.648 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1449, 1454) -->  Daniel went to the garden
2025-01-23 22:23:53.648 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:23:53.660 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2417, 2421) -->  Daniel left the milk
2025-01-23 22:23:53.660 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:23:53.678 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3688, 3693) --> . John moved to the
2025-01-23 22:23:53.679 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John journeyed to the office.
2025-01-23 22:23:53.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (323, 329) -->  Mary journeyed to the office
2025-01-23 22:23:53.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:23:53.697 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3226, 3232) --> . John went back to the
2025-01-23 22:23:53.697 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary moved to the bathroom.
2025-01-23 22:23:53.711 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2850, 2855) --> . Mary moved to the
2025-01-23 22:23:53.711 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:23:53.725 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2782, 2788) --> . Sandra journeyed to the
2025-01-23 22:23:53.725 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary got the football there.
2025-01-23 22:23:53.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3566, 3571) --> . Mary got the football
2025-01-23 22:23:53.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:23:53.745 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (322, 328) --> . Mary journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:23:54.242 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:23:54.242 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4248])
your chose emoji: ['👨🏾\u200d🦽\u200d➡️', '🗒', '👩🏾\u200d❤\u200d💋\u200d👩🏿', '👩🏻\u200d❤\u200d👩🏿', '🧎🏼\u200d♀\u200d➡️', '🐻\u200d❄️', '😝', '🔩', '🇳🇦', '🤸🏻\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4251, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 239674.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.74it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.17it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.94it/s]
2025-01-23 22:23:57.391 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3697207768758138, 0.31578215081056293, 0.3262017193962546, 0.3153889258266661, 0.2772516186317701], 'topk_tokens': [' P', 'had', ' gold', ' distributed', 'burn', ' B', ' bathroom', 'posit', ' distributed', ' machine', ' had', ' had', ' business', ' heading', ' business', 'ien', 'ree', 'RI', 'athroom', ' bend'], 'evidence_proportions': [0.37907282511393225, 0.3511505126953125, 0.342694091796875, 0.2964286804199219, 0.459014892578125]}, 'weight': {'score': [0.006120135386784871, 0.007342714109131376, 0.008278427755131441, 0.007342124465766796, 0.0020829733837856336], 'topk_tokens': ['user', '<|eot_id|>', ' based', 'Question', ' bathroom', ':', '\n\n', 'b', '<|start_header_id|>', '\n\n', ':', '.', 'Answer', 'assistant', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.018618981043497723, 0.0017645955085754395, 0.0028205156326293946, 0.0011097490787506104, 0.0019138813018798827]}, 'saliency': {'score': [0.0002385688324769338, 8.246627198306567e-05, 0.00015048945651334876, 8.102118499859574e-05, 8.54700468899159e-05], 'topk_tokens': ['Daniel', '\n\n', 'Question', 'user', ' not', 'ed', '<|start_header_id|>', ' based', '\n\n', ' ', '<|eot_id|>', ':', '\n\n', '<|eot_id|>', ' on', ' late', ' only', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0007865975300470988, 1.8984079360961914e-05, 9.506940841674805e-05, 4.6722590923309326e-05, 5.357861518859864e-05]}}, 25: {'grad': {'score': [0.4697751998901367, 0.40810708544221214, 0.4464275135713465, 0.4074433770465191, 0.41785987039630335], 'topk_tokens': ['to', ' to', ' message', ' from', ' late', '26', ' at', ' prize', 'message', ' rates', 'as', ' engaged', ' res', 'low', ' century', 'rate', ' late', 'eward', ' prize', 'eward'], 'evidence_proportions': [0.2584571838378906, 0.5256004333496094, 0.580963134765625, 0.6120147705078125, 0.453717041015625]}, 'weight': {'score': [0.002906231830517451, 0.007077475094397021, 0.00423779382425196, 0.007124376835755963, 0.0019182499205128531], 'topk_tokens': ['Question', ' bogus', ' Bench', 'ree', 'Daniel', ' prior', ' not', '?\n', '.', '<|start_header_id|>', 'b', ':', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.006670355796813965, 0.0015100538730621338, 0.0019625186920166015, 0.002013489603996277, 0.0011641323566436768]}, 'saliency': {'score': [0.00012528151273727417, 8.798298261440998e-05, 9.625974823446835e-05, 8.770237809401153e-05, 7.442443558339323e-05], 'topk_tokens': [' late', ' against', '<|start_header_id|>', ' Bench', ' write', ' Among', 'Answer', 'Question', ':', ' not', ' prior', 'ree', '?\n', 'assistant', '<|eot_id|>', 'Daniel', '<|begin_of_text|>', 'athroom', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.00016534825166066489, 6.179511547088623e-05, 0.00016222596168518065, 0.00017876923084259033, 4.825592041015625e-05]}}, 26: {'grad': {'score': [0.3950386047363281, 0.43510781240810986, 0.37217336542466106, 0.43584748142350693, 0.5165700162394663], 'topk_tokens': [' fight', 'ages', ' B', '185', 'Good', 'Johnson', ' res', 'gro', ' Jackson', ' effect', ' bag', ' lesser', ' bend', 'ab', '�', ' PA', '185', ' trib', 'RI', 'b'], 'evidence_proportions': [0.4093424479166667, 0.4724273681640625, 0.3307373046875, 0.36202239990234375, 0.40667724609375]}, 'weight': {'score': [0.00560833141207695, 0.006865495122255816, 0.007794355644899256, 0.006865159007606261, 0.003083058957303508], 'topk_tokens': ['.\n\n', 'Question', ' Buchanan', ' *\n\n', ' *\n\n', ' bathroom', '?\n', 'Answer', '\n\n', 'b', '<|start_header_id|>', '<|eot_id|>', '.', 'assistant', '<|eot_id|>', ':', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01768803596496582, 0.0012229159474372864, 0.001705169677734375, 0.001240164041519165, 0.00201871395111084]}, 'saliency': {'score': [6.292263666788737e-05, 7.529381134178576e-05, 0.0002772089313058292, 7.372734201514519e-05, 9.691179468390647e-05], 'topk_tokens': ['Daniel', ' *\n\n', ' occur', ' *\n\n', ' Southern', ' based', '?\n', ' *\n\n', '<|start_header_id|>', ' Buchanan', 'Question', ' *\n\n', ':', '<|end_header_id|>', '.', 'assistant', 'b', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [9.968380133310954e-05, 2.5890767574310303e-05, 7.60793685913086e-05, 4.114210605621338e-05, 5.270242691040039e-05]}}, 27: {'grad': {'score': [0.35190606117248535, 0.39466609878557984, 0.455563040340648, 0.39441705153774825, 0.40510087602593925], 'topk_tokens': ['\n\n\n\n', ' attract', '\n', 'ting', ' printing', ' news', ' tour', ',', ' during', ' air', ' time', ' business', ' could', ' composing', ' containing', ' time', ' attention', ' writing', ' admission', ' mailing'], 'evidence_proportions': [0.28955936431884766, 0.3723106384277344, 0.326983642578125, 0.33261871337890625, 0.450750732421875]}, 'weight': {'score': [0.008702064553896586, 0.007171308929795127, 0.005734149147482479, 0.007174200725077786, 0.0028389618637856473], 'topk_tokens': ['<|eot_id|>', 'Minnesota', 'Daniel', ' *\n\n', 'Question', ' Buchanan', ' bathroom', '.', '<|start_header_id|>', '?\n', '<|eot_id|>', 'Answer', 'b', '<|eot_id|>', 'assistant', ':', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02745989958445231, 0.0013465583324432373, 0.0024155139923095702, 0.0029906630516052246, 0.0029327392578125]}, 'saliency': {'score': [0.000408546378215154, 0.00010649974529559795, 0.00015186211642097025, 0.00010440305085049135, 8.561905850185437e-05], 'topk_tokens': [' By', ' PA', 'RI', '\n\n', ':', '<|eot_id|>', '<|eot_id|>', 'Minnesota', ' not', 'Question', '\n\n', ':', ' bathroom', '?\n', 'assistant', ' *\n\n', '<|begin_of_text|>', 'b', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.001481513182322184, 3.7789344787597656e-05, 3.9196014404296876e-05, 9.931623935699463e-05, 3.432631492614746e-05]}}, 28: {'grad': {'score': [0.299541433652242, 0.28552952499228124, 0.2594616833855124, 0.28566070095383406, 0.2555018542857652], 'topk_tokens': ['\n\n', ' pleasure', '***', ' Min', ' so', ',', ' ', ' Min', ' Hor', ' Dub', ' under', ' custom', ';', ' Min', '\n\n\n', ' use', '!"', '!"', ',', ' inside'], 'evidence_proportions': [0.4148457845052083, 0.23383331298828125, 0.10566539764404297, 0.33460426330566406, 0.3795684814453125]}, 'weight': {'score': [0.004289838174978892, 0.006891405181418136, 0.004086931838708765, 0.006929036877532293, 0.001358631956443358], 'topk_tokens': ['.\n\n', ' bathroom', ' *\n\n', 'Question', '.\n\n', ' based', ':', 'Answer', '<|start_header_id|>', '.', '<|eot_id|>', '<|eot_id|>', 'b', '?\n', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.011614203453063965, 0.0005578547716140747, 0.001552271842956543, 0.0014429688453674316, 0.0035012483596801756]}, 'saliency': {'score': [0.00041966264446576435, 0.00010855658226533655, 3.4778433687546675e-05, 0.00010737411423739345, 3.45489282286569e-05], 'topk_tokens': [' ', '?\n', 'Daniel', ' block', '<|eot_id|>', 'During', ' about', 'Just', 'Answer', 'As', ' Stephen', ' based', 'athroom', 'b', 'assistant', ' bathroom', ':', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.001562232772509257, 1.0736286640167236e-05, 2.4974346160888672e-05, 5.003809928894043e-05, 6.610751152038574e-05]}}, 29: {'grad': {'score': [0.36009061336517334, 0.3185743835779449, 0.36988143359913545, 0.3179207157468898, 0.5112685942917727], 'topk_tokens': [' *', '!', '.', ' *', ' *', 'b', ' *', ' o', ' *', ' ', ' Francis', ' *', ' Wins', ' to', '.', ' *', '�', ' the', ' o', ' H'], 'evidence_proportions': [0.388696034749349, 0.418621301651001, 0.2926116943359375, 0.39276123046875, 0.320281982421875]}, 'weight': {'score': [0.0025252848863601685, 0.00706083907488794, 0.0022665393703124102, 0.007125675585853891, 0.0012487876281309664], 'topk_tokens': ['\n\n', '.', ':', '.\n\n', 'Question', '\n\n', ' about', ' locations', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '?\n', 'b', 'assistant', ':', 'Answer', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00605471928914388, 0.0001824498176574707, 0.0006229639053344726, 0.001360177993774414, 0.002998638153076172]}, 'saliency': {'score': [0.00012467304865519205, 0.00010121648969495473, 5.816186175626867e-05, 0.00010143134777624966, 4.150726821985137e-05], 'topk_tokens': [' news', '.', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'Answer', '?\n', '\n\n', ' ', '<|eot_id|>', 'assistant', ':', 'b', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0002800275882085164, 1.989305019378662e-06, 1.1849403381347657e-05, 0.00015761703252792358, 0.0001228630542755127]}}, 30: {'grad': {'score': [0.5787461598714193, 0.5488343706581907, 0.6577804228838753, 0.5477797442053429, 0.26014292641971887], 'topk_tokens': [' body', ' bodies', ' first', ' grabbed', ' block', 'athroom', ' a', 'ab', ' both', 'body', ' border', 'ball', ' before', 'blue', ' bend', ' business', ' web', ' B', ' B', 'b'], 'evidence_proportions': [0.6391143798828125, 0.6023101806640625, 0.57078857421875, 0.6889801025390625, 0.4072235107421875]}, 'weight': {'score': [0.007276037087043126, 0.006850498139339457, 0.0034781799596898698, 0.006875407722779284, 0.005619652485579587], 'topk_tokens': ['.\n\n', ':', ' *\n\n', ' locations', ' *\n\n', ' bathroom', 'Daniel', 'Question', 'Answer', '?\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'b', '<|eot_id|>', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.019642750422159832, 0.0009468421339988708, 0.0026014566421508787, 0.002737879753112793, 0.005804443359375]}, 'saliency': {'score': [0.0005825795233249664, 0.00017668239875727053, 8.138663628522088e-05, 0.00017513184424604346, 0.0001256794072269054], 'topk_tokens': ['In', 'Minnesota', 'UL', ' only', '<|eot_id|>', ' not', '<|eot_id|>', 'Answer', 'Daniel', 'assistant', ':', '\n\n', ' locations', '<|eot_id|>', '?\n', ' bathroom', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.0021831591924031577, 2.8245151042938232e-05, 4.184246063232422e-05, 5.21242618560791e-05, 7.045269012451172e-05]}}, 31: {'grad': {'score': [0.4033323923746745, 0.41646070352472214, 0.3046381333295037, 0.41744258925194333, 0.4694465251451128], 'topk_tokens': [' P', 'ot', ' Tribune', 'ot', 'ot', ' and', ' minutes', ' item', ' time', ' second', '<|start_header_id|>', ' item', ' Press', ' location', ' location', ' location', ' question', 'user', 'assistant', ' item'], 'evidence_proportions': [0.4741083780924479, 0.3800811767578125, 0.358807373046875, 0.37847900390625, 0.40140991210937504]}, 'weight': {'score': [0.00305199126402537, 0.006567920692890735, 0.0042243021375992715, 0.006607049082390539, 0.0021079745185509156], 'topk_tokens': [' about', '.\n\n', ' Where', ':', '.\n\n', 'Question', '.', '.\n\n', '<|eot_id|>', '<|start_header_id|>', 'Answer', 'assistant', '?\n', ':', 'b', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0040234724680582685, 0.0009582340717315674, 0.001556539535522461, 0.002202749252319336, 0.005736064910888672]}, 'saliency': {'score': [0.00015584255258242288, 8.950431720982213e-05, 0.00016479194164276123, 8.85141200050374e-05, 4.17132056161259e-05], 'topk_tokens': ['UL', '.\n', 'Answer', 'Minnesota', '<|start_header_id|>', 'If', '<|eot_id|>', 'Question', ' bathroom', '.\n\n', '.', ':', '?\n', '\n\n', 'b', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'assistant'], 'evidence_proportions': [0.00047714014848073327, 1.4171004295349121e-05, 2.1594762802124024e-05, 1.9438564777374268e-05, 0.00012699365615844726]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:23:57.400 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:23:57.400 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-7_pid-1_0-2-3-6-9.pkl | len: 10 |  size: 9.47 KB
Processing depth (0, 2, 3, 6, 9):   2%|▏         | 2/100 [00:26<21:26, 13.13s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.43it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.16it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Processing depth (1, 2, 3, 4, 8):   2%|▏         | 2/100 [00:35<21:26, 13.13s/it]2025-01-23 22:24:06.974 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:24:06.976 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (469, 475) --> . Daniel journeyed to the
2025-01-23 22:24:06.976 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:24:06.982 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1029, 1033) -->  Daniel grabbed the milk
2025-01-23 22:24:06.982 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:24:06.989 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1519, 1524) -->  tragedy. Daniel went to
2025-01-23 22:24:06.989 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:24:06.999 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1898, 1902) -->  Daniel left the milk
2025-01-23 22:24:06.999 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:24:07.015 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3318, 3323) --> . John moved to the
2025-01-23 22:24:07.015 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John journeyed to the office.
2025-01-23 22:24:07.017 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (328, 334) -->  Mary journeyed to the office
2025-01-23 22:24:07.017 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:24:07.033 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3207, 3213) --> . John went back to the
2025-01-23 22:24:07.033 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary moved to the bathroom.
2025-01-23 22:24:07.047 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2772, 2777) --> . Mary moved to the
2025-01-23 22:24:07.047 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:24:07.061 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2720, 2726) --> . Sandra journeyed to the
2025-01-23 22:24:07.061 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary got the football there.
2025-01-23 22:24:07.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3553, 3558) --> . Mary got the football
2025-01-23 22:24:07.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:24:07.080 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (327, 333) --> . Mary journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:24:07.575 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:24:07.575 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4233])
your chose emoji: ['🇽🇰', '🪖', '🇹🇯', '🚶', '👩🏽\u200d🤝\u200d👩🏼', '🧘🏾\u200d♀', '🎤', '🦒', '🚶🏿\u200d♂\u200d➡', '🚶🏽\u200d♀️\u200d➡️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4236, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.61it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.53it/s]
2025-01-23 22:24:10.587 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3459459940592448, 0.3573463735319287, 0.3040868534761317, 0.35784528037473423, 0.6382501963022593], 'topk_tokens': ['�', 'rich', '�', ' ranks', 'que', ' state', ' leap', 'ISC', 'state', ' strife', '�', 'remark', 'fect', 'question', ' board', ' bathroom', ' bathroom', ' state', ' team', ' appear'], 'evidence_proportions': [0.31444422403971356, 0.2627983093261719, 0.33416748046875, 0.41839599609375, 0.40408477783203123]}, 'weight': {'score': [0.0015161111950874329, 0.007395203548067578, 0.0017658524653490853, 0.007474786160150859, 0.0021103266123178844], 'topk_tokens': ['<|eot_id|>', 'Question', 'user', ' Where', ' journey', ':', ' traveled', 'Bridge', '.\n\n', '\n\n', '<|eot_id|>', '<|eot_id|>', 'Answer', '?\n', '\n\n', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0025243908166885376, 0.0007919445633888245, 0.001247549057006836, 0.0012228190898895264, 0.001388704776763916]}, 'saliency': {'score': [5.618731180826823e-05, 0.0001051054249066469, 7.323131841771742e-05, 0.00010564581608304342, 8.974445832742227e-05], 'topk_tokens': ['\n\n', '.\n\n', ' bathroom', ' was', ' will', 'assistant', ' Where', 'rail', ' left', ' journey', ':', 'b', ' upper', ' discarded', ' remains', ' traveled', ' directly', 'Answer', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [5.821883678436279e-05, 4.803389310836792e-05, 5.666017532348632e-05, 7.308274507522583e-05, 4.628300666809082e-05]}}, 25: {'grad': {'score': [0.4439098834991455, 0.45349554794931096, 0.508952309103573, 0.45309931196739595, 0.296726858293688], 'topk_tokens': [' answer', '\n', ' received', ' wait', ' papers', ' cap', ' wait', ' determined', ' copies', ' pay', ' printer', ' paper', ' carrier', ' paper', ' paper', ' paper', ' paper', ' papers', ' paper', ' paper'], 'evidence_proportions': [0.2833859125773112, 0.6106948852539062, 0.40845947265625, 0.39038848876953125, 0.581378173828125]}, 'weight': {'score': [0.002234784265359243, 0.007340733345327116, 0.001912692013908835, 0.007414236500709465, 0.0019317168641734767], 'topk_tokens': [' Times', '.\n\n', ' milk', 'MIN', '\n\n', 'Times', '<|eot_id|>', ':', ' ', 'Answer', ' remains', '.\n\n', 'b', '?\n', ' discarded', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.003305355707804362, 0.002186596393585205, 0.0015642642974853516, 0.0021876394748687744, 0.0016968846321105957]}, 'saliency': {'score': [0.00013381863633791605, 0.0001340653884512169, 5.549104774699492e-05, 0.00013470623326564075, 8.343643433338887e-05], 'topk_tokens': [' sounded', '.\n\n', ' veto', ' distributed', 'papers', 'paper', 'burn', 'eward', 'press', ' veto', ' obtained', ' item', ' persons', ' remains', 'Times', ' printed', 'advance', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.00023159881432851157, 0.0001523047685623169, 0.00010165572166442871, 0.00011201947927474976, 5.1295757293701166e-05]}}, 26: {'grad': {'score': [0.5280933380126953, 0.4595328124769461, 0.49260885575238395, 0.4588698067124122, 0.3245468268523345], 'topk_tokens': [' state', ' Min', ' Republican', ' message', ' Press', ' three', ' Min', ' charges', ' was', ' method', ' Out', ' to', ' state', ' Red', '8', ' state', 'material', ' rates', ' trials', ' By'], 'evidence_proportions': [0.5207557678222656, 0.5921440124511719, 0.6237396240234374, 0.48956298828125, 0.42083587646484377]}, 'weight': {'score': [0.001170689860979716, 0.007193808740214203, 0.001520707326776841, 0.007274574729002737, 0.0037090222577790956], 'topk_tokens': ['far', 'Question', '.\n', '\n\n', 'rail', 'user', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|start_header_id|>', '\n\n', 'Bridge', '?\n', ':', '.\n\n', 'b', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0014715592066446939, 0.0008165836334228516, 0.0012446403503417968, 0.0014501214027404785, 0.000795435905456543]}, 'saliency': {'score': [5.338961879412333e-05, 0.0001369359796625809, 4.594904534956988e-05, 0.00013815634070314705, 0.00011155935558112892], 'topk_tokens': [' bedroom', ' bedroom', '\n\n', ' Marshall', ' Herald', ' laughed', 'room', ' bathroom', '<|eot_id|>', '.\n\n', '\n\n', ' Newspaper', ':', 'edit', ' bathroom', 'Bridge', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [7.179379463195801e-05, 5.8084726333618164e-05, 4.224181175231934e-05, 7.802993059158325e-05, 1.8984079360961914e-05]}}, 27: {'grad': {'score': [0.7970107396443685, 0.9145606800111765, 0.8391597972196692, 0.9158495331906199, 0.711869072269749], 'topk_tokens': ['.', ',', '.', '.', 'ors', ',', ',', '.', '.', 'itol', ',', '.', 'ors', ',', '.', '.', ',', ';', ',', ','], 'evidence_proportions': [0.8626912434895834, 0.7113800048828125, 0.8686126708984375, 0.5673542022705078, 0.898822021484375]}, 'weight': {'score': [0.0023436546325683594, 0.007334132144989215, 0.0024570331853978776, 0.007402488493702525, 0.003596373909228557], 'topk_tokens': ['Minnesota', 'user', 'RE', 'Question', 'rail', 'Answer', ' *\n\n', '\n\n', '.\n\n', 'Bridge', '\n\n', '?\n', 'NEW', ':', '\n\n', 'b', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0032734175523122153, 0.0021414458751678467, 0.0016444087028503417, 0.0030305981636047363, 0.0015393972396850586]}, 'saliency': {'score': [0.00026969735821088153, 0.00014404976446747892, 0.00013949941186343923, 0.0001433650276888039, 0.00015572559189152074], 'topk_tokens': ['Min', ' bedroom', ' bedroom', 'press', 'in', 'user', 'Times', 'Johnson', ' bathroom', 'Minnesota', 'then', 'Bridge', 'b', ' Newspaper', '<|end_header_id|>', ' bathroom', 'NEW', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00042729079723358154, 0.00030781328678131104, 0.0001727759838104248, 0.0003125593066215515, 0.00011272430419921874]}}, 28: {'grad': {'score': [0.6446172595024109, 0.6699044981354019, 0.6450635124655331, 0.6702519101124166, 0.6649148270890519], 'topk_tokens': ['!"', ' that', ' of', ',', ' of', ',', ' one', ',', ' *', ' one', ',', ' one', ',', ':', ' one', ',', ' *', ',', ' "', ';'], 'evidence_proportions': [0.5047741731007894, 0.6374282836914062, 0.681494140625, 0.672027587890625, 0.759375]}, 'weight': {'score': [0.0007103234529495239, 0.00705664601384524, 0.0014924301820642809, 0.007138382509714568, 0.002205009798745851], 'topk_tokens': ['Question', ':', '.\n', 'Bridge', ' was', ' Where', '\n\n', '.\n\n', ' milk', ' discarded', 'Answer', '<|eot_id|>', '?\n', ':', 'b', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008832216262817383, 0.0004248768091201782, 0.0007838010787963867, 0.0005586743354797363, 0.0007790446281433105]}, 'saliency': {'score': [1.9407520691553753e-05, 9.073521783601799e-05, 4.7071015133577236e-05, 9.15002842848811e-05, 9.981925423080857e-05], 'topk_tokens': [' ', ':', '\u200d', ' Square', 'being', 'b', 'always', ' block', ' bedroom', 'through', '<|eot_id|>', ' dropped', 'assistant', 'athroom', ' dropped', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', ' milk', '\n\n'], 'evidence_proportions': [1.475214958190918e-05, 2.8386712074279785e-05, 2.042651176452637e-05, 2.1323561668395996e-05, 1.52587890625e-05]}}, 29: {'grad': {'score': [0.6302274068196615, 0.49912063843580323, 0.6336642994600183, 0.49727261380295595, 0.34713910077069254], 'topk_tokens': [' the', ' the', ' the', ' the', ' the', ' out', ' in', ' in', ' the', ' down', ' up', ' out', ' bathroom', ' up', ' out', 'room', ' out', ' out', ' bathroom', ' the'], 'evidence_proportions': [0.647247314453125, 0.5924072265625, 0.5778900146484375, 0.7132492065429688, 0.6259796142578126]}, 'weight': {'score': [0.0007316246628761292, 0.007131897131151698, 0.0009414071545881383, 0.0072190400699841005, 0.0026625696871731734], 'topk_tokens': ['<|eot_id|>', ' Do', ' milk', 'user', '\n\n', 'I', '\n\n', '.\n\n', ':', 'Question', ' discarded', 'Answer', '?\n', ':', 'b', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009068300326665243, 0.0001413598656654358, 0.00022221207618713378, 0.0014342516660690308, 0.0009409010410308838]}, 'saliency': {'score': [3.341709574063619e-05, 0.00010954465530636853, 2.42845100515029e-05, 0.00011067579613170195, 0.000161955485472808], 'topk_tokens': ['\n\n', '�', '?\n', ' As', '.\n\n', 'If', 'in', '\u200d', '\u200d', 'Times', 'b', ' Do', 'a', ':', 'assistant', 'a', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [4.961093266805013e-05, 2.816319465637207e-06, 4.57763671875e-06, 4.610419273376465e-05, 5.7154893875122076e-05]}}, 30: {'grad': {'score': [0.4231414794921875, 0.4669662489994615, 0.45826900706571694, 0.46728877190370316, 0.5865145116238981], 'topk_tokens': [' glance', ' based', ' there', ' for', "'s", ' Sh', ' four', 'gro', ' made', 'tele', ' he', ' bogus', ' Bor', ' bend', ' no', '<|start_header_id|>', 'Sh', ' B', 'b', ' B'], 'evidence_proportions': [0.3532994588216146, 0.24574661254882812, 0.428155517578125, 0.5265426635742188, 0.5611328125]}, 'weight': {'score': [0.0018458391229311626, 0.007107123887347544, 0.002547765479368322, 0.007174450124833731, 0.008622099821631974], 'topk_tokens': [' *\n\n', '<|eot_id|>', '<|eot_id|>', ' Where', 'NEW', ':', ' *\n\n', '.\n', '\n\n', 'Answer', 'Question', '.\n\n', '.\n\n', '?\n', ':', 'b', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002603034178415934, 0.0005919933319091797, 0.0015323638916015627, 0.0019544661045074463, 0.002166855335235596]}, 'saliency': {'score': [8.197873830795288e-05, 0.0002476425667322391, 0.000118980512899511, 0.00024964123636184556, 0.0002657078407906197], 'topk_tokens': [' bathroom', '<|eot_id|>', ' and', ' B', ' Where', '\n', ' B', ' bogus', 'NEW', 'RI', ' bathroom', '<|start_header_id|>', 'un', 'Bridge', '<|end_header_id|>', ' bogus', ' milk', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [0.0001442184050877889, 4.208087921142578e-05, 5.6302547454833986e-05, 0.0001133456826210022, 3.979206085205078e-05]}}, 31: {'grad': {'score': [0.42313170433044434, 0.48234123683853797, 0.4506687837488511, 0.48293910476224394, 0.4387757262668094], 'topk_tokens': ['t', ' of', 'of', '186', '185', ' mess', ' fr', ' so', ' Col', ' ne', ' leve', ' of', ' so', ' propriet', '185', ' Cl', ' Do', ' so', ' so', ' formally'], 'evidence_proportions': [0.39278666178385413, 0.3940858840942383, 0.5078380584716797, 0.3653678894042969, 0.44428710937500004]}, 'weight': {'score': [0.0013492355744043987, 0.006717563238315024, 0.0023489471744088564, 0.006783952134941798, 0.005684184061514365], 'topk_tokens': [' Do', ' was', '<|start_header_id|>', '.\n', ' discarded', '<|eot_id|>', 'Question', ' Where', '.\n\n', ':', 'Answer', '?\n', ':', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010262628396352131, 0.0004430636763572693, 0.0007186353206634522, 0.0010776817798614502, 0.0033095836639404296]}, 'saliency': {'score': [4.205604394276937e-05, 0.00011368972496450567, 6.698773187749525e-05, 0.00011448127022766508, 0.0003107721741135056], 'topk_tokens': [' bathroom', ' place', ' directly', '�', '<|start_header_id|>', ' Where', '�', 'Question', '�', ' discarded', 'Bridge', ':', '?\n', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '\n\n', '<|eot_id|>', 'b', 'athroom'], 'evidence_proportions': [1.479188601175944e-05, 1.3567507266998291e-05, 1.5521049499511717e-05, 2.942979335784912e-05, 0.000134199857711792]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:24:10.599 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:24:10.600 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-7_pid-2_1-2-3-4-8.pkl | len: 10 |  size: 9.27 KB
Processing depth (1, 2, 3, 4, 8):   3%|▎         | 3/100 [00:39<21:16, 13.16s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (1, 2, 3, 5, 9):   3%|▎         | 3/100 [00:48<21:16, 13.16s/it]2025-01-23 22:24:20.108 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:24:20.111 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (445, 451) --> . Daniel journeyed to the
2025-01-23 22:24:20.111 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:24:20.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (910, 914) -->  Daniel grabbed the milk
2025-01-23 22:24:20.116 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:24:20.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1481, 1486) --> . Daniel went to the
2025-01-23 22:24:20.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:24:20.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2128, 2132) -->  Daniel left the milk
2025-01-23 22:24:20.134 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:24:20.152 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3630, 3635) -->  John moved to the bedroom
2025-01-23 22:24:20.152 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John journeyed to the office.
2025-01-23 22:24:20.153 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (329, 335) -->  Mary journeyed to the office
2025-01-23 22:24:20.153 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:24:20.170 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3219, 3225) --> . John went back to the
2025-01-23 22:24:20.170 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary moved to the bathroom.
2025-01-23 22:24:20.184 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2842, 2847) --> . Mary moved to the
2025-01-23 22:24:20.184 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:24:20.198 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2774, 2780) --> . Sandra journeyed to the
2025-01-23 22:24:20.198 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary got the football there.
2025-01-23 22:24:20.215 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3537, 3542) --> . Mary got the football
2025-01-23 22:24:20.215 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:24:20.217 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (328, 334) --> . Mary journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:24:20.713 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the office<|eot_id|>
2025-01-23 22:24:20.713 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4236])
your chose emoji: ['👩\u200d🦼\u200d➡', '🧑🏼\u200d🎓', '🧔🏽\u200d♀', '👨🏻\u200d🦽\u200d➡️', '🎏', '🍎', '👩🏾\u200d❤️\u200d💋\u200d👩🏿', '🏎️', '📂', '🚽']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4239, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 228261.44it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.38it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.31it/s]
2025-01-23 22:24:23.843 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3846413294474284, 0.2781129311941569, 0.33727096108829274, 0.27702035655304735, 0.27076676604035615], 'topk_tokens': [' out', ' the', 'were', ' the', ' regular', ' the', ' the', ' the', 'asca', ' so', ' Associated', 'old', ' milk', ' competitors', ' competitors', 'being', ' work', ' EAR', 'ex', ' whistle'], 'evidence_proportions': [0.4060872395833333, 0.469696044921875, 0.30735473632812504, 0.3239154815673828, 0.416729736328125]}, 'weight': {'score': [0.011211817463239035, 0.007430728355312325, 0.37952059156754436, 0.004383179566073834, 0.0022483135198617912], 'topk_tokens': ['user', ' Where', ' milk', ' Bench', ' bathroom', '?\n', '\n\n', ':', '\n\n', 'b', 'Answer', '<|eot_id|>', '<|eot_id|>', 'assistant', ' bathroom', '<|end_header_id|>', 'athroom', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.004343152046203613, 0.028861284255981445, 0.006875205039978027, 0.013815522193908691, 0.00758829116821289]}, 'saliency': {'score': [0.00027774274349212646, 7.824384960877387e-05, 0.0005680620670318604, 7.311546098270704e-05, 5.406721845849768e-05], 'topk_tokens': [' Bench', '<|begin_of_text|>', 'wing', 'user', ' traveled', ' prior', ' milk', '<|eot_id|>', ' to', ' ', '.', '.', '\n\n', 'assistant', '<|end_header_id|>', '\n\n', ' Where', 'Answer', 'athroom', 'b'], 'evidence_proportions': [9.394685427347818e-05, 0.000719510018825531, 0.00010361075401306152, 0.0003439858555793762, 0.00026602149009704587]}}, 25: {'grad': {'score': [0.5435012181599935, 0.34517328973704, 0.4197322621065028, 0.34342852165698434, 0.2834656455300071], 'topk_tokens': [' part', ' the', ' the', ' the', ' be', ' grabbed', ' York', ' the', ' the', ' charges', ' the', ' celebrity', ' evidently', ' of', ' to', ' journey', ' the', ' majority', ' duration', 're'], 'evidence_proportions': [0.401826540629069, 0.6355743408203125, 0.6252510070800781, 0.5092949867248535, 0.5854675292968751]}, 'weight': {'score': [0.009681801001230875, 0.007358616268502623, 0.44911079546984506, 0.0037529404669166097, 0.0022581134523664203], 'topk_tokens': [' prior', ' discarded', ' bathroom', ' to', 'b', ' Daniel', 'Answer', ' bathroom', '<|eot_id|>', '<|eot_id|>', ':', ' Bench', '?\n', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0040273865063985195, 0.027801454067230225, 0.007617163658142089, 0.011527419090270996, 0.002559518814086914]}, 'saliency': {'score': [0.0008672339220841726, 7.472238779124238e-05, 0.0012729378307566924, 6.0429251727177406e-05, 3.301284529946067e-05], 'topk_tokens': [' Daniel', ' directly', ' Daniel', ' Daniel', 'b', ' prior', ' Ear', ' Do', ' Bench', '<|eot_id|>', '<|eot_id|>', ':', '?\n', '<|begin_of_text|>', ' Daniel', 'assistant', '\n\n', '.', '.', '<|end_header_id|>'], 'evidence_proportions': [0.0006296634674072266, 0.002636723220348358, 0.0006029188632965089, 0.0007164254784584045, 0.0001216888427734375]}}, 26: {'grad': {'score': [0.3002281188964844, 0.3012544056468765, 0.2424175879534553, 0.30173875931432104, 0.35276305211054815], 'topk_tokens': [' ty', '.', ' looked', ' Paul', '�', ' Min', ' wait', ' I', ' Press', 'b', ' Eagle', 'ors', "'clock", ' instead', '.', ' prepared', 'Johnson', ' directly', 'ig', ' trib'], 'evidence_proportions': [0.24461841583251953, 0.3203125, 0.39274749755859373, 0.24974441528320312, 0.29875984191894533]}, 'weight': {'score': [0.012129207452138266, 0.007218375074382547, 0.3618328746627359, 0.004306451380751801, 0.003598343242298473], 'topk_tokens': [' bathroom', ' prior', '<|start_header_id|>', '.\n\n', '<|eot_id|>', 'b', ' Bench', '<|eot_id|>', '\n\n', ',', 'Answer', 'assistant', ' bathroom', ':', '?\n', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.005471030871073404, 0.028868436813354492, 0.011081171035766602, 0.014681577682495117, 0.005733776092529297]}, 'saliency': {'score': [0.00036385531226793927, 7.330086130554818e-05, 0.0004754916710012099, 6.83623790398821e-05, 8.416253250914734e-05], 'topk_tokens': [' Moore', 'Question', '.\n\n', 'If', '\n\n', 'assistant', ' Daniel', ' Bench', '<|start_header_id|>', ' bathroom', '.', '.', ':', ',', '<|end_header_id|>', '?\n', ' bathroom', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0003198285897572835, 0.0008826926350593567, 0.00038082003593444826, 0.0002116188406944275, 0.00010644197463989259]}}, 27: {'grad': {'score': [0.4897518952687581, 0.5781949478153013, 0.5065738453584558, 0.5792850580149306, 0.5419535574974952], 'topk_tokens': ['.', ' The', '\u200d', ' him', ' concerned', '.', '.', 'ers', ',', '!"', ' departing', '.', ' be', ' work', '.', '.', ' other', ',', '.', '\n'], 'evidence_proportions': [0.47086334228515625, 0.30298280715942383, 0.5556472778320313, 0.5641975402832031, 0.5363815307617188]}, 'weight': {'score': [0.01402809222539266, 0.007359570842724022, 0.4119369633057538, 0.0040312580331265425, 0.0035042197673351735], 'topk_tokens': [' discarded', ' bathroom', ' prior', ' bend', ',', '<|eot_id|>', ' to', ' Daniel', 'b', 'Answer', 'assistant', '?\n', '\n\n', ' bathroom', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.006808360417683919, 0.03357052803039551, 0.012613773345947266, 0.016979694366455078, 0.006110858917236328]}, 'saliency': {'score': [0.0005115382373332977, 0.0001629993976863889, 0.00301701356382931, 0.00013778977946099182, 9.730767894100834e-05], 'topk_tokens': ['<|eot_id|>', ' was', ' unless', 'Answer', '<|start_header_id|>', '?\n', ' bathroom', '\n\n', '\n\n', ' to', ',', 'assistant', ' bathroom', '<|begin_of_text|>', 'athroom', ':', '.', 'b', '.', '<|end_header_id|>'], 'evidence_proportions': [0.0003268520037333171, 0.001228831708431244, 0.0006300628185272217, 0.00045187026262283325, 8.853673934936524e-05]}}, 28: {'grad': {'score': [0.2759510278701782, 0.34015876714225424, 0.21944954816032858, 0.3415089463309448, 0.3587547896744369], 'topk_tokens': [' summer', ' from', ' morning', 'nes', 'nes', 'nes', 'E', ' Date', 'S', '.', '\n', 'nes', '.', ' the', 'nes', ' season', ' instead', ' Min', 'nes', ' half'], 'evidence_proportions': [0.2962346076965332, 0.24269866943359375, 0.2534141540527344, 0.15935897827148438, 0.3940231323242187]}, 'weight': {'score': [0.011263693372408548, 0.007077089540518013, 0.4144884987789042, 0.0037399772692741718, 0.001875413702679919], 'topk_tokens': ['<|eot_id|>', ' milk', '.\n\n', '.\n\n', '<|eot_id|>', ' was', ' bathroom', 'Answer', ' discarded', 'b', ',', 'assistant', '?\n', '\n\n', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.0021387338638305664, 0.01482391357421875, 0.013210797309875488, 0.02376270294189453, 0.0074191570281982415]}, 'saliency': {'score': [0.0004467504719893138, 7.562184283181614e-05, 0.0022629289066090305, 5.570423286573022e-05, 3.699668042071454e-05], 'topk_tokens': ['.', ' Daniel', ' strange', ' directly', '.\n\n', ' Daniel', ' Daniel', ',', 'Answer', ' bathroom', '\n\n', 'b', 'athroom', ' bathroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', ':', '.', '.'], 'evidence_proportions': [0.0001301268736521403, 0.0005876347422599792, 0.0005236804485321046, 0.0009317994117736816, 0.00024902224540710447]}}, 29: {'grad': {'score': [0.5443005561828613, 0.2601953476322909, 0.5521672192741843, 0.2561901889044655, 0.3319329596185065], 'topk_tokens': [' body', ' H', 'us', ' John', ' Sandra', 'ed', ' com', 'nes', ' journey', 'b', 'nes', ' John', 'ed', ' river', ' Bench', ' bathroom', ' a', 'ed', ' l', ' m'], 'evidence_proportions': [0.5153910319010416, 0.4929351806640625, 0.4553428649902344, 0.668670654296875, 0.6095458984375]}, 'weight': {'score': [0.006368232270081838, 0.007196568126637971, 0.511079795220319, 0.0031037315658563863, 0.0022732085221773618], 'topk_tokens': [' discarded', 'Question', ' the', ' was', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ' was', 'Answer', '?\n', 'b', 'assistant', ',', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.0006926854451497395, 0.009896278381347656, 0.0030952095985412598, 0.0187605619430542, 0.003715610504150391]}, 'saliency': {'score': [0.00013064717253049216, 0.00011180856228437196, 0.005058271043440875, 7.147566321591275e-05, 8.006722896130054e-05], 'topk_tokens': [' milk', ' the', ' the', '***', ' ', '<|start_header_id|>', 'nes', ' Do', ' a', '\n\n', '\n\n', 'b', 'assistant', '<|end_header_id|>', ',', 'athroom', '<|begin_of_text|>', ':', '.', '.'], 'evidence_proportions': [9.665886561075847e-06, 0.00017631053924560547, 7.622241973876953e-05, 0.00037323683500289917, 9.964704513549804e-05]}}, 30: {'grad': {'score': [0.4389325777689616, 0.3793119991632755, 0.4752863540368922, 0.3781892959936416, 0.36734983518526154], 'topk_tokens': [' of', ' *', 'ad', '      ', 'vent', ' fight', '      ', ' to', ' an', '202', ' an', ' Proof', '      ', '202', '      ', '      ', ' ', '      ', ' at', 'b'], 'evidence_proportions': [0.49987030029296875, 0.4255409240722656, 0.5094593048095704, 0.47069406509399414, 0.280584716796875]}, 'weight': {'score': [0.013451973597208658, 0.007260898285020098, 0.3478815029649174, 0.004455424387242286, 0.009281455696403207], 'topk_tokens': ['.\n\n', ' or', '<|eot_id|>', ' was', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', 'Answer', '<|eot_id|>', 'assistant', ',', '?\n', '\n\n', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.004563132921854655, 0.023818016052246094, 0.01162266731262207, 0.021872997283935547, 0.010918235778808594]}, 'saliency': {'score': [0.0003844040135542552, 0.00014931604698542914, 0.0007631235262926887, 0.00014297508417889993, 0.00024132024158130992], 'topk_tokens': [' lin', '\n\n', 'ANK', 'Bridge', ' web', ' or', ' milk', '<|eot_id|>', '<|eot_id|>', ' bathroom', '?\n', '.', ',', '.', '<|begin_of_text|>', 'assistant', '<|end_header_id|>', 'b', 'athroom', ':'], 'evidence_proportions': [0.0002161264419555664, 0.0008677244186401367, 0.00036116838455200193, 0.00042098015546798706, 0.00019365549087524414]}}, 31: {'grad': {'score': [0.2654151916503906, 0.3523053967691193, 0.29149669759413777, 0.353298666488038, 0.15289452788117644], 'topk_tokens': [' cold', ' night', ' paper', '1', ' expected', ' part', ' day', ' happened', ' location', ' company', ' Paul', 'user', ' location', ' that', ' location', ' location', ' question', ' location', ' location', ' location'], 'evidence_proportions': [0.18900553385416669, 0.18100738525390625, 0.261614990234375, 0.29404449462890625, 0.40552978515625]}, 'weight': {'score': [0.004728821416695912, 0.006982723703134676, 0.35858236603877125, 0.004136444299992577, 0.0023446439148543715], 'topk_tokens': ['.\n\n', 'Question', '.\n\n', ' was', ' Where', '<|start_header_id|>', '<|eot_id|>', 'Answer', ',', 'b', '<|eot_id|>', '?\n', 'assistant', '<|end_header_id|>', ':', '\n\n', 'athroom', '<|begin_of_text|>', '.', '.'], 'evidence_proportions': [0.001860111951828003, 0.0068846940994262695, 0.0042125463485717775, 0.005762636661529541, 0.006135797500610352]}, 'saliency': {'score': [0.00013882294297218323, 8.152467823725991e-05, 0.0011212247259476606, 7.274090402635539e-05, 3.6613894747449204e-05], 'topk_tokens': [' the', ' milk', 'If', ' bathroom', ' John', '\n\n', '<|eot_id|>', 'Question', 'If', ',', '?\n', '<|begin_of_text|>', 'Answer', '<|start_header_id|>', 'athroom', '.', '.', ':', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [4.582603772481283e-05, 0.00012837350368499756, 0.00010699629783630372, 9.215623140335083e-05, 0.00032793879508972166]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-23 22:24:23.852 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:24:23.852 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-7_pid-3_1-2-3-5-9.pkl | len: 10 |  size: 9.22 KB
Processing depth (1, 2, 3, 5, 9):   4%|▍         | 4/100 [00:52<21:06, 13.20s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.20it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (0, 1, 4, 5, 6):   4%|▍         | 4/100 [01:02<21:06, 13.20s/it]2025-01-23 22:24:33.315 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-23 22:24:33.315 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-23 22:24:33.316 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel grabbed the milk.
2025-01-23 22:24:33.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (452, 456) -->  Daniel grabbed the milk
2025-01-23 22:24:33.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went to the garden.
2025-01-23 22:24:33.327 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1811, 1816) --> . Daniel went to the
2025-01-23 22:24:33.327 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the milk.
2025-01-23 22:24:33.337 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2114, 2118) -->  Daniel left the milk
2025-01-23 22:24:33.338 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:24:33.349 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2419, 2424) -->  the senate. John moved
2025-01-23 22:24:33.350 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John journeyed to the office.
2025-01-23 22:24:33.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (335, 341) -->  Mary journeyed to the office
2025-01-23 22:24:33.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the bedroom.
2025-01-23 22:24:33.368 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3260, 3266) --> . John went back to the
2025-01-23 22:24:33.368 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary moved to the bathroom.
2025-01-23 22:24:33.382 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2863, 2868) --> . Mary moved to the
2025-01-23 22:24:33.382 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-23 22:24:33.396 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2832, 2838) --> . Sandra journeyed to the
2025-01-23 22:24:33.396 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Mary got the football there.
2025-01-23 22:24:33.414 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3549, 3554) --> . Mary got the football
2025-01-23 22:24:33.414 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:24:33.415 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (334, 340) --> . Mary journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:24:33.910 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the garden<|eot_id|>
2025-01-23 22:24:33.910 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4225])
your chose emoji: ['⛹🏾\u200d♂', '🦹🏾\u200d♀️', '🇧🇲', '😶\u200d🌫️', '👩🏻\u200d❤\u200d💋\u200d👩🏿', '⚾', '🐸', '🤌', '😕', '🧚\u200d♀️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4228, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 203360.19it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.66it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.41it/s]
2025-01-23 22:24:36.898 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5691604614257812, 0.3710281386506253, 0.3899659549488741, 0.3697333996816219, 0.398059064691717], 'topk_tokens': [' the', ' an', ' the', ' time', ' with', ' or', ' election', ' to', ' that', ' came', ' cable', ' milk', ' Ch', ' get', ' to', ' printed', ' a', ' print', ' prepared', ' cable'], 'evidence_proportions': [0.6342061360677084, 0.76904296875, 0.469775390625, 0.6071014404296875, 0.40023193359375]}, 'weight': {'score': [0.0019570800165335336, 0.007496389896100494, 0.3098200664800756, 0.005063281606713073, 0.0017370617750919227], 'topk_tokens': ['\n\n', '\n\n', '<|end_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '<|start_header_id|>', 'b', 'user', ' journey', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'athroom', '<|end_header_id|>', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00510398546854655, 0.00022149831056594849, 0.00024500489234924316, 0.001661539077758789, 0.0015177667140960694]}, 'saliency': {'score': [0.00019156187772750854, 9.184036062027671e-05, 0.00031489572104285746, 8.944774703156176e-05, 0.00013048404997045344], 'topk_tokens': [' bedroom', ' cash', ' Team', '.', ' *', 'Civil', '.', '<|eot_id|>', 'user', 'b', '<|eot_id|>', '<|end_header_id|>', 'system', '\n\n', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.00041399896144866943, 1.3276934623718262e-05, 1.2934207916259766e-05, 0.0001217573881149292, 0.00030173659324645997]}}, 25: {'grad': {'score': [0.6528292496999105, 0.5036961781944773, 0.4809074401855469, 0.5030236658146627, 0.4972544294415098], 'topk_tokens': [' for', ' for', ' for', 'MIN', 'RE', 'ulations', ' tour', ' doubtful', 'Dub', ' trials', 'otyping', ' trib', ' for', ' awarded', ' shore', ' for', ' for', ' wife', ' for', 'for'], 'evidence_proportions': [0.990875244140625, 0.8245620727539062, 0.3993732452392578, 0.5379180908203125, 0.4551727294921875]}, 'weight': {'score': [0.000981042782465617, 0.0074784649692849385, 0.35299037659869476, 0.004698736704796624, 0.0014764752351876461], 'topk_tokens': [' Bank', 'Civil', ' secret', 'b', '\n\n', '\n\n', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', ' journey', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0017182032267252603, 0.0004012584686279297, 0.0006135106086730957, 0.0012157559394836426, 0.0007400393486022949]}, 'saliency': {'score': [4.8664708932240806e-05, 0.0001895789166272758, 0.0012772556613473332, 0.00018152158609111246, 9.259491255789092e-05], 'topk_tokens': ['papers', ' Bank', ' Associated', ' facts', ' acquaintance', ' Douglas', '<|eot_id|>', ' secret', '<|eot_id|>', 'RE', 'b', 'user', 'athroom', '.', 'ed', '.', '<|begin_of_text|>', 'assistant', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [5.5198868115743e-05, 1.905113458633423e-05, 3.9297342300415036e-05, 6.892532110214233e-05, 5.767345428466797e-05]}}, 26: {'grad': {'score': [0.2599951426188151, 0.3071073100830771, 0.23785753811106963, 0.3079430856864801, 0.3843188430323745], 'topk_tokens': [' strange', 'then', ' no', ' but', 'then', ' news', ' news', ' about', ' by', '\n\n', ' bathroom', ' new', ' by', ' news', 'blue', ' about', ' about', ' ty', ' ', ' of'], 'evidence_proportions': [0.40879058837890625, 0.1760711669921875, 0.21969451904296874, 0.2102794647216797, 0.2286529541015625]}, 'weight': {'score': [0.0025585368275642395, 0.007355686167894723, 0.30560006990152244, 0.0049515668722651274, 0.007234419837142482], 'topk_tokens': ['\n\n\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'b', 'user', '<|eot_id|>', '<|start_header_id|>', 'athroom', 'assistant', '<|eot_id|>', ' journey', '<|end_header_id|>', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.006018797556559245, 0.0003550797700881958, 0.0006176114082336425, 0.002109706401824951, 0.0024689793586730956]}, 'saliency': {'score': [0.00030415629347165424, 0.00017111239533663472, 0.0051254077869303085, 0.00012995194045188044, 0.0005406850215160486], 'topk_tokens': ['S', ' bedroom', 'rail', '<|start_header_id|>', ' cash', ' Charles', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'b', '.', '<|eot_id|>', '.', 'user', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'ed', ' journey'], 'evidence_proportions': [0.0010818243026733398, 2.6404857635498047e-05, 2.865791320800781e-05, 7.61374831199646e-05, 5.1069259643554685e-05]}}, 27: {'grad': {'score': [0.6266605059305826, 0.7667139896042751, 0.6087794584386489, 0.7688077684214932, 0.7431424458821615], 'topk_tokens': [',', '\n', '\n', ',', ' in', ',', ',', ' use', ',', '\n\n\n', 'In', ' in', ',', ' in', ',', ' in', ' in', '\n\n', 'in', '\n'], 'evidence_proportions': [0.5519053141276041, 0.5725250244140625, 0.78521728515625, 0.5637731552124023, 0.65142822265625]}, 'weight': {'score': [0.003320696453253428, 0.007450561649751979, 0.35264725807835073, 0.004659777257939895, 0.006769481030377475], 'topk_tokens': ['If', 'could', '\n\n', '\n\n', '\n\n', 'Question', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ' journey', 'b', 'user', '<|end_header_id|>', 'assistant', 'athroom', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.008277495702107746, 0.0003149956464767456, 0.001049637794494629, 0.0025072991847991943, 0.0026988744735717773]}, 'saliency': {'score': [0.0010185490051905315, 0.000221098798733335, 0.001161128282546997, 0.00020884464827658746, 0.0011638457124883478], 'topk_tokens': ['.', ' *\n\n', 'could', '.', '<|begin_of_text|>', '<|start_header_id|>', '�', '<|start_header_id|>', '<|eot_id|>', ' Charles', ' *\n\n', 'Question', '<|eot_id|>', 'RE', 'ed', ' bathroom', 'athroom', 'assistant', 'b', 'user'], 'evidence_proportions': [0.003358493248621623, 1.0855495929718018e-05, 6.252527236938477e-05, 0.00015948712825775146, 0.0006600439548492432]}}, 28: {'grad': {'score': [0.665682315826416, 0.6730503165282048, 0.5815595739028033, 0.6738386899733143, 0.6625218246922349], 'topk_tokens': ['event', ' contract', ' contract', '50', ' "', '!"', '.', '800', '.', '--', '.', '"', ' proc', ';', ' ', '.', ' so', '!', '600', '\n\n'], 'evidence_proportions': [0.5326995849609375, 0.6837377548217773, 0.7823486328125, 0.6377944946289062, 0.716461181640625]}, 'weight': {'score': [0.0009637859960397085, 0.007187249193579494, 0.38859683099915, 0.0041132533507381415, 0.0018039395411809285], 'topk_tokens': [':', 'user', ' ', '\n\n', ',', ',', ',', '<|start_header_id|>', '<|eot_id|>', ' journey', '<|eot_id|>', 'assistant', 'athroom', 'b', '\n\n', '<|end_header_id|>', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0015333096186319988, 0.00011454522609710693, 0.0002915620803833008, 0.002007320523262024, 0.0007971465587615968]}, 'saliency': {'score': [4.614144563674927e-05, 0.00010869327083215921, 0.0011398888686124015, 0.00010064545152284545, 5.5099978591456555e-05], 'topk_tokens': [' proprietor', '\n', ' contract', 'still', ' *\n\n', ' December', '<|start_header_id|>', '\n', '<|eot_id|>', 'Question', ' twenty', 'ed', 'assistant', 'athroom', '.', '.', '<|begin_of_text|>', '<|end_header_id|>', 'b', '\n\n'], 'evidence_proportions': [0.0001183102528254191, 8.970499038696289e-06, 2.231001853942871e-05, 3.512948751449585e-05, 2.1916627883911136e-05]}}, 29: {'grad': {'score': [1.1015548706054688, 0.6411397856366781, 0.8243313957663143, 0.6369962660242899, 0.48668737122506806], 'topk_tokens': [' body', ' manager', ' the', ' the', ' persons', ' the', ' Pioneer', ' B', ' Hill', '185', ' the', ' bathroom', ' book', ' J', ' the', ' the', ' L', ' the', ' B', ' the'], 'evidence_proportions': [1.58154296875, 1.206787109375, 1.1716552734375, 0.81451416015625, 0.60091552734375]}, 'weight': {'score': [0.0021408324440320334, 0.007325503323139057, 0.4354543878751643, 0.003864604048877597, 0.003295874053781683], 'topk_tokens': ['-text', ',', ' ', '\n\n', ',', ',', ',', 'b', '<|start_header_id|>', 'assistant', ' journey', '<|eot_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', 'ed', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.004805962244669596, 0.00012896955013275146, 0.0004625678062438965, 0.00365525484085083, 0.0010188937187194825]}, 'saliency': {'score': [0.0003086676200230916, 0.00019320543915343487, 0.005499131539288689, 0.00014927916103701512, 0.0003060034730217674], 'topk_tokens': ['assistant', ',', ',', 'A', '-text', '<|start_header_id|>', 'athroom', ',', '.', ',', ' journey', 'If', 'b', '<|eot_id|>', '<|eot_id|>', 'ed', '\n\n', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0008450845877329509, 5.677342414855957e-06, 4.168152809143066e-05, 0.0003771185874938965, 0.00011958479881286621]}}, 30: {'grad': {'score': [0.6416292190551758, 0.5287519182477679, 0.51362486446605, 0.5282256028063291, 0.6274748137502959], 'topk_tokens': [' boats', ' Online', ' delegates', ' By', 'ioneer', ' Articles', ' Brown', ' Published', ' border', ' Bor', ' below', ' be', ' balance', ' Bench', ' bodies', ' be', ' boys', ' bend', ' B', ' B'], 'evidence_proportions': [0.6023674011230469, 0.665374755859375, 0.785284423828125, 0.749725341796875, 0.4396148681640625]}, 'weight': {'score': [0.006139076004425685, 0.007067609532553854, 0.18725950577679804, 0.005603763089477301, 0.011447568734486898], 'topk_tokens': ['Civil', ',', ' *\n\n', ',', ',', ' large', ' papers', 'Question', '<|eot_id|>', 'athroom', '<|start_header_id|>', '<|eot_id|>', 'ed', 'assistant', '\n\n', 'b', '<|end_header_id|>', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.014609893163045246, 0.0004420056939125061, 0.0027609348297119142, 0.006550788879394531, 0.003580522537231445]}, 'saliency': {'score': [0.0013594155510266621, 0.00028533472299801366, 0.001426872085122501, 0.00026984546396086257, 0.0015640525203762632], 'topk_tokens': ['***', 'ed', '�', 'S', ' large', '<|eot_id|>', ' Where', '�', 'Answer', '.', ' papers', '�', 'Civil', '<|start_header_id|>', '.', '\n\n', '<|begin_of_text|>', ' bathroom', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.005169729391733806, 4.1581690311431885e-05, 7.845163345336914e-05, 0.00017524510622024536, 6.960630416870117e-05]}}, 31: {'grad': {'score': [0.3882931073506673, 0.4530385575885466, 0.33562155330882354, 0.4543685501427959, 0.8395471139387651], 'topk_tokens': ['�', 'ot', 'cap', 'Just', '�', 'ot', ' it', '�', ' others', 'ot', 'ot', 'ot', ' anything', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.45259857177734375, 0.23510360717773438, 0.3710540771484375, 0.464630126953125, 0.3898475646972656]}, 'weight': {'score': [0.0023976080119609833, 0.0069194207692213966, 0.24572672037517324, 0.004998335713962857, 0.0021420608867298474], 'topk_tokens': ['?\n', 'Answer', 'If', ' during', ',', '<|start_header_id|>', 'Question', '<|eot_id|>', ',', ',', 'ed', '<|eot_id|>', 'assistant', '\n\n', '<|end_header_id|>', 'b', 'athroom', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.004939476648966471, 0.0003065839409828186, 0.0011654376983642578, 0.001962125301361084, 0.002600741386413574]}, 'saliency': {'score': [0.00018354877829551697, 0.00023422829362730317, 0.001948822070570553, 0.00022054007299226536, 0.00013479951656225956], 'topk_tokens': [' and', ' facts', '<|begin_of_text|>', '.\n', ' three', 'Penn', 'ed', ',', ' facts', 'Just', 'Question', '<|eot_id|>', '<|eot_id|>', ',', 'assistant', '.', '.', '<|end_header_id|>', 'b', 'athroom'], 'evidence_proportions': [0.00046346088250478107, 9.700655937194824e-06, 6.372332572937011e-05, 8.205324411392212e-05, 0.00018775463104248047]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-23 22:24:36.905 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:24:36.905 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-7_pid-4_0-1-4-5-6.pkl | len: 10 |  size: 9.45 KB
Processing depth (0, 1, 4, 5, 6):   5%|▌         | 5/100 [01:05<20:48, 13.15s/it]Processing depth (0, 1, 4, 5, 6):   5%|▌         | 5/100 [01:05<20:51, 13.18s/it]
2025-01-23 22:24:37.116 | INFO     | __main__:<module>:99 - Selected idx: 8
2025-01-23 22:24:37.116 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the kitchen? 
2025-01-23 22:24:37.116 | INFO     | __main__:<module>:101 - Answer: hallway
2025-01-23 22:24:37.116 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:24:37.117 | INFO     | __main__:<module>:103 - Needle: [' Mary journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Sandra picked up the milk.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Sandra travelled to the hallway.', ' Mary got the football there.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-23 22:24:37.117 | INFO     | __main__:<module>:104 - Real Needle: [' Sandra picked up the milk.', ' Sandra travelled to the hallway.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-23 22:24:37.117 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.25s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Processing depth (3, 6, 8, 9):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:24:46.951 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra picked up the milk.
2025-01-23 22:24:46.958 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1453, 1458) --> . Sandra picked up the
2025-01-23 22:24:46.959 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-23 22:24:46.971 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2423, 2428) -->  the senate. Sandra travelled
2025-01-23 22:24:46.971 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:24:46.981 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2118, 2123) -->  back to the kitchen.
2025-01-23 22:24:46.981 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:24:46.992 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2115, 2121) --> . Daniel went back to the
2025-01-23 22:24:46.992 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:24:47.009 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3300, 3306) --> . Mary journeyed to the
2025-01-23 22:24:47.009 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-23 22:24:47.020 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2115, 2121) --> . Daniel went back to the
2025-01-23 22:24:47.020 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the bedroom.
2025-01-23 22:24:47.023 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (659, 664) -->  Republican. John moved to
2025-01-23 22:24:47.023 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:24:47.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (566, 571) --> . Mary moved to the
2025-01-23 22:24:47.026 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the bedroom.
2025-01-23 22:24:47.027 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (182, 188) --> . John went back to the
2025-01-23 22:24:47.027 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John journeyed to the office.
2025-01-23 22:24:47.044 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3301, 3307) -->  Mary journeyed to the office
2025-01-23 22:24:47.044 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Mary got the football there.
2025-01-23 22:24:47.056 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2534, 2539) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:24:47.529 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:24:47.529 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4216])
your chose emoji: ['🧑\u200d🍼', '✈', '🇬🇫', '👨🏽\u200d✈', '👩🏻\u200d🚒', '🧛🏼\u200d♀', '☸️', '💂🏾\u200d♂️', '🏳\u200d⚧️', '🌡']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4219, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 222214.78it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.16it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.20it/s]
2025-01-23 22:24:50.548 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.26429821196056547, 0.3333961864759866, 0.29235221178103715, 0.3341299620176858, 0.2974482923746109], 'topk_tokens': [' living', ' measure', ' was', ' ST', 'little', ' one', 'road', '.', ' Bor', ' under', ' under', ' conducted', ' inside', 'St', ' conditions', 'ors', 'hall', ' generally', '.', ' ST'], 'evidence_proportions': [0.28553466796875004, 0.23515624999999998, 0.240673828125, 0.2905731201171875]}, 'weight': {'score': [0.0759570144471668, 0.007506412136297594, 0.04279595154982347, 0.006829866167156993, 0.004707889631390572], 'topk_tokens': ['.', ' past', ' century', 's', ' the', '.', 'the', ' the', 'ides', 's', 'cap', ' the', '.', '1', 'the', 'fect', 'able', 'ing', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0027898311614990234, 0.004128742218017577, 0.0017067909240722657, 0.25866174697875977]}, 'saliency': {'score': [0.00039370287032354447, 0.00016998986153558522, 0.0003404709009023813, 0.00016726162548849404, 0.00013259844854474068], 'topk_tokens': [' \n', '.', 's', ' the', ' hallway', 'the', 'ing', 'able', ' the', '1', '.', '\n\n', 's', 'cap', 'hall', '.', 'the', 'ides', ' the', 'fect'], 'evidence_proportions': [6.432533264160157e-05, 0.00015154480934143066, 0.00016524791717529297, 0.001060361663500468]}}, 25: {'grad': {'score': [0.5763135637555804, 0.6534199593727779, 0.7385953267415365, 0.6530105809117487, 0.48625171184539795], 'topk_tokens': ['RI', ' producing', 'same', ' time', ' and', 'old', ' Gal', ' Project', ' December', '600', '.', ' ST', ' perform', ' Gal', ' balance', ' late', ' so', ' old', 'ig', ' summer'], 'evidence_proportions': [0.5882568359375, 0.5823913574218751, 0.601422119140625, 0.5403722127278646]}, 'weight': {'score': [0.06935612786383856, 0.00754936025673305, 0.038453489542007446, 0.006947484045654686, 0.004055635072290897], 'topk_tokens': [' past', ' century', '.', 's', '.', ' the', 'ides', ' the', 'the', 'cap', '.', ' the', 'the', '1', 's', '.', 'able', 'fect', 'ing', '<|begin_of_text|>'], 'evidence_proportions': [0.0034192562103271484, 0.002668285369873047, 0.0004742562770843506, 0.2372782826423645]}, 'saliency': {'score': [0.0006109291598910378, 0.00022763678546651445, 0.0004450136270278539, 0.00022366303295778467, 0.00021738326177001], 'topk_tokens': ['<|begin_of_text|>', '\n\n', '.', '.', 's', '.', 'way', 'ides', 'able', 'the', ' the', '.', ' the', 'ing', '1', 'the', 'cap', 'fect', ' the', 's'], 'evidence_proportions': [8.351802825927735e-05, 8.701086044311523e-05, 1.366138458251953e-05, 0.0019847601652145386]}}, 26: {'grad': {'score': [0.2894080025809152, 0.2897962817481186, 0.29723431513859677, 0.2897284939530435, 0.3584420084953308], 'topk_tokens': ['ot', 'nes', 'ot', ' fore', ' genu', ' enabling', '�', ' Col', ' engaged', ' gun', 'en', ' Gen', ' Col', '�', 'ages', 'ot', ' Hale', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.22972564697265624, 0.3648941040039062, 0.25746307373046873, 0.3028589884440104]}, 'weight': {'score': [0.0435938835144043, 0.007526159229988338, 0.024674641780364208, 0.0071832371262524905, 0.011680837720632553], 'topk_tokens': ['<|end_header_id|>', '\n\n', ' century', 's', ' the', '.', 'the', ' the', 'cap', 's', 'ides', ' the', '.', 'the', '1', 'fect', 'able', 'ing', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00698404312133789, 0.011621856689453125, 0.0035170555114746095, 0.13414279619852704]}, 'saliency': {'score': [0.00025267969994317917, 0.00014348175181515212, 9.951988855997722e-05, 0.00014334262119631666, 0.0004153978079557419], 'topk_tokens': [' past', '.', ' the', ' the', 's', 's', 'the', ' century', 'cap', ' the', '.', 'ides', 'edit', ' Hale', '�', ' senate', 'graph', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.0002136528491973877, 0.0006187379360198975, 9.326338768005371e-05, 0.00011300047238667806]}}, 27: {'grad': {'score': [0.5726783389136905, 0.6422145470083255, 0.5408966846955128, 0.6435157388814168, 0.44466954469680786], 'topk_tokens': [',', '      ', ',', ' not', ' have', ' This', ' one', ' than', ',', ',', ' generally', '      ', ' some', ' consisted', '      ', ',', ' This', ';', ' for', '      '], 'evidence_proportions': [0.676361083984375, 0.47601318359375, 0.6309326171875, 0.5182851155598959]}, 'weight': {'score': [0.0518147831871396, 0.007554969959390138, 0.030069964054303292, 0.007120360474541772, 0.007197447121143341], 'topk_tokens': [' past', ' century', '.', 's', ' the', 'the', '.', '<|begin_of_text|>', ' the', '.', 'cap', 's', '.', 'ides', ' the', 'ing', 'able', '1', 'fect', 'the'], 'evidence_proportions': [0.002864265441894531, 0.0033165931701660155, 0.0010304927825927734, 0.17534228165944415]}, 'saliency': {'score': [0.00023468193553742909, 0.00010642973018726712, 0.0003053630009675637, 0.00010391669968166842, 0.00026829540729522705], 'topk_tokens': [' past', ' Hale', 's', ' the', '.', 'way', 'ides', ' rejo', ' the', 'the', '1', '.', ' the', 'fect', '<|begin_of_text|>', 'able', ' hallway', 'ing', 'the', '.'], 'evidence_proportions': [7.43567943572998e-05, 0.00010224580764770508, 3.0177831649780274e-05, 0.0006490697463353474]}}, 28: {'grad': {'score': [0.2033938453311012, 0.22002966953810144, 0.1851376264523237, 0.22044086020621695, 0.3566333204507828], 'topk_tokens': ['ANK', ' and', 'ot', 'ot', ' slow', ' so', ' res', 'ian', '�', ' half', 'e', ' Hill', ' Hale', ' Sh', 'x', ' half', ' hallway', ' o', ' hallway', 'hall'], 'evidence_proportions': [0.24771728515625002, 0.18033905029296876, 0.214007568359375, 0.17682520548502606]}, 'weight': {'score': [0.05117995398385184, 0.007475936828164813, 0.02823306046999418, 0.007060617717248541, 0.0026504751294851303], 'topk_tokens': ['hall', ' century', '.', 's', ' the', '.', 'the', ' the', '<|begin_of_text|>', 'ides', '.', 'cap', '.', ' the', 's', 'fect', 'able', 'ing', 'the', '1'], 'evidence_proportions': [0.0017938613891601562, 0.003134346008300781, 0.00243229866027832, 0.1729960838953654]}, 'saliency': {'score': [0.0002880607332502093, 0.00010337812495361603, 0.00015598382705297225, 0.00010195231173983787, 7.066130638122559e-05], 'topk_tokens': ['the', '<|eot_id|>', ' century', ' past', '.', ' the', '.', ' the', '<|end_header_id|>', 'hall', 'way', 'ides', 'ing', 'cap', 'fect', ' the', 'able', 'the', '1', 's'], 'evidence_proportions': [4.5067071914672846e-05, 4.58836555480957e-05, 0.00016058087348937988, 0.0007986028989156087]}}, 29: {'grad': {'score': [0.2961396716889881, 0.39070696852223574, 0.31712664090670073, 0.39187444772649255, 0.3252354562282562], 'topk_tokens': [' his', ' six', ' typ', ' miles', ' pur', '\n', ' now', 'nes', '\n', '\n', '\n', 'nes', 'pos', ' be', ' offices', '\n', '\n', '\n', 'Penn', '\n'], 'evidence_proportions': [0.2421173095703125, 0.3022857666015625, 0.3351104736328125, 0.30356089274088544]}, 'weight': {'score': [0.06602317946297782, 0.007540261132311951, 0.03626216604159428, 0.006975631275037126, 0.0025679003447294235], 'topk_tokens': ['.', ' century', ' past', 's', '.', 'the', '<|begin_of_text|>', ' the', 'cap', ' the', '.', '.', 'ides', 's', 'fect', 'the', ' the', 'ing', 'able', '1'], 'evidence_proportions': [0.001292991638183594, 0.003075408935546875, 0.0012729406356811524, 0.22638001044591266]}, 'saliency': {'score': [7.511888231549944e-05, 0.00011106055476253535, 8.953534639798678e-05, 0.00011144388206419563, 0.00014273496344685555], 'topk_tokens': ['fect', ' century', '.', '.', '.', 'hall', '1', 'able', 'ing', 'the', 'the', 's', 's', ' past', 'ides', ' the', 'cap', 'way', ' the', ' the'], 'evidence_proportions': [3.993511199951172e-05, 6.558895111083985e-05, 3.9750337600708006e-05, 0.0001418540875116984]}}, 30: {'grad': {'score': [0.286152340116955, 0.2358377533067485, 0.22531790611071464, 0.2356823476125025, 0.2748992443084717], 'topk_tokens': [' or', ' Hon', 'E', ' com', '<|eot_id|>', ' S', ' out', ' discover', ' senate', ' S', ' senate', ' the', ' S', ' favor', ' Bor', ' or', '�', ' Hor', ' the', ' for'], 'evidence_proportions': [0.339599609375, 0.37337646484375, 0.2788084030151367, 0.1750461260477702]}, 'weight': {'score': [0.04570545469011579, 0.007491324687631358, 0.028391511012346316, 0.007102384077697225, 0.012149181216955185], 'topk_tokens': ['<|eot_id|>', 'way', '<|eot_id|>', '.', ' the', 'the', 's', 'ides', 'cap', '<|end_header_id|>', ' the', '.', 'hall', '1', 'the', '<|begin_of_text|>', 'fect', '.', 'ing', 'able'], 'evidence_proportions': [0.00426015853881836, 0.00664215087890625, 0.003938007354736328, 0.1476021607716878]}, 'saliency': {'score': [0.00029272976375761486, 0.00015656805117567106, 0.00031283115729307517, 0.0001544152122473711, 0.0004825172945857048], 'topk_tokens': [' the', ' hallway', 's', '.', ' \n', '�', 'the', 'ides', '.', '<|end_header_id|>', 'cap', ' the', '<|eot_id|>', '<|eot_id|>', '1', 'ing', 'hall', 'the', 'fect', 'able'], 'evidence_proportions': [0.00010295510292053222, 0.00013413429260253907, 0.00010398030281066895, 0.0007403294245402018]}}, 31: {'grad': {'score': [0.41047595796130953, 0.473445526144747, 0.41410534198467547, 0.4743199257874724, 0.34944039583206177], 'topk_tokens': [' an', ' leve', ' the', ' employ', ' of', 'd', ' W', ' the', ' pay', 'D', ' leve', "'s", ' injunction', ' had', 'f', ' D', 'ab', 'pend', ' it', 'E'], 'evidence_proportions': [0.4391387939453125, 0.38360595703125, 0.40589599609375, 0.4127985636393229]}, 'weight': {'score': [0.05949083964029948, 0.007098499251982984, 0.03319006164868673, 0.006589287883474667, 0.004414410330355167], 'topk_tokens': ['assistant', 'ides', ' the', 's', ':', ':', '.', ' \n', '\n\n', '<|eot_id|>', 'the', '<|end_header_id|>', '1', 'way', 'able', 'fect', 'hall', '<|begin_of_text|>', '.', 'ing'], 'evidence_proportions': [0.002232646942138672, 0.005122661590576172, 0.0011450767517089842, 0.20113428433736166]}, 'saliency': {'score': [0.0003343877338227772, 0.00019075572278777071, 0.00026759810936756624, 0.00018930991242269564, 0.0001244405284523964], 'topk_tokens': [' the', ' read', ' set', 's', ' hallway', '.', '?', ' write', 'assistant', 'the', '1', 'able', ':', '<|end_header_id|>', 'ing', ' \n', 'fect', '<|eot_id|>', 'way', 'hall'], 'evidence_proportions': [0.00011866092681884767, 6.510615348815918e-05, 2.7477741241455078e-05, 0.0009943197170893352]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-23 22:24:50.556 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:24:50.556 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-8_pid-0_3-6-8-9.pkl | len: 10 |  size: 7.94 KB
Processing depth (3, 6, 8, 9):   1%|          | 1/100 [00:13<22:01, 13.35s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.42it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.19it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Processing depth (2, 3, 8, 9):   1%|          | 1/100 [00:22<22:01, 13.35s/it]2025-01-23 22:24:59.973 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra picked up the milk.
2025-01-23 22:24:59.978 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (917, 922) --> . Sandra picked up the
2025-01-23 22:24:59.978 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-23 22:24:59.985 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1411, 1416) --> . Sandra travelled to the
2025-01-23 22:24:59.986 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:24:59.996 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2106, 2111) -->  back to the kitchen.
2025-01-23 22:24:59.996 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:25:00.007 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2103, 2109) --> . Daniel went back to the
2025-01-23 22:25:00.007 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:25:00.024 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3273, 3279) --> . Mary journeyed to the
2025-01-23 22:25:00.024 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-23 22:25:00.034 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2103, 2109) --> . Daniel went back to the
2025-01-23 22:25:00.034 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the bedroom.
2025-01-23 22:25:00.042 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (667, 672) -->  Republican. John moved to
2025-01-23 22:25:00.042 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:25:00.045 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (574, 579) --> . Mary moved to the
2025-01-23 22:25:00.045 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the bedroom.
2025-01-23 22:25:00.046 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (209, 215) --> . John went back to the
2025-01-23 22:25:00.046 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John journeyed to the office.
2025-01-23 22:25:00.063 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3274, 3280) -->  Mary journeyed to the office
2025-01-23 22:25:00.063 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Mary got the football there.
2025-01-23 22:25:00.075 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2428, 2433) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:25:00.574 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:25:00.574 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4228])
your chose emoji: ['🦠', '\U0001faf5🏽', '🇹🇯', '🩺', '👩🏾\u200d❤️\u200d👨🏻', '🙏', '👩🏿\u200d🤝\u200d👨🏻', '👳🏽\u200d♂️', '⛎', '🚶\u200d♂\u200d➡']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4231, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.04it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.97it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.07it/s]
2025-01-23 22:25:03.843 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5500909714471727, 0.5012557575872282, 0.48106110401642627, 0.5011987093969147, 0.6412551152078729], 'topk_tokens': [' the', '�', ' printing', ' place', ' as', ' election', ' of', ' the', ' a', ' company', ' veto', ' of', 'Answer', '�', ' make', ' Sandra', ' Sh', ' bathroom', ' print', ' right'], 'evidence_proportions': [0.599151611328125, 0.48792724609375, 0.55814208984375, 0.5543009440104167]}, 'weight': {'score': [0.0009511056400480724, 0.007453600189923057, 0.0015902427526620717, 0.007541162723032756, 0.0028944944864825197], 'topk_tokens': ['***', ' Knowledge', ' line', '\n\n', '\n', '<|end_header_id|>', '<|eot_id|>', ' ', 'user', '<|start_header_id|>', '\n\n', 'way', '\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.000798487663269043, 0.0005365967750549317, 0.0012797892093658447, 0.001149808367093404]}, 'saliency': {'score': [4.538184120541527e-05, 0.00013127746916752678, 8.438795040815305e-05, 0.0001321483632981477, 9.585171937942505e-05], 'topk_tokens': [' convention', 'way', ' veto', ' Buchanan', ' fight', ' line', ' printers', '<|start_header_id|>', 'system', ' Knowledge', '\n\n', 'Just', 'assistant', 'user', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.824498176574707e-05, 1.4847517013549806e-05, 0.00012770891189575195, 2.483526865641276e-05]}}, 25: {'grad': {'score': [0.44058481852213544, 0.4509046243961977, 0.5110268226036658, 0.4503944230519791, 0.5828359503495065], 'topk_tokens': ['nes', '\n', ' means', '\n', ' stere', ' there', ' up', '\n', ' states', ' always', '\n', ' under', 'ERS', '�', '\n', '\n', ' remin', ' separate', '�', 'otype'], 'evidence_proportions': [0.53525390625, 0.334619140625, 0.45563964843750004, 0.4374529520670573]}, 'weight': {'score': [0.00045187558446611675, 0.007364757459402028, 0.0013890755482209034, 0.007455436460579136, 0.002639634436682651], 'topk_tokens': ['�', '<|start_header_id|>', ' tele', 'proc', '\n\n', '<|eot_id|>', ' Republicans', 'user', '\n\n', ' Daniel', 'Times', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'assistant', '<|eot_id|>', 'way', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000449138879776001, 0.0003012418746948242, 0.00019494295120239257, 0.0007937947909037273]}, 'saliency': {'score': [2.6306935719081332e-05, 0.00013382835355500356, 9.149847886501215e-05, 0.00013476549450141067, 9.849863617043746e-05], 'topk_tokens': [' Daniel', ' printers', 'fortunate', ' veto', 'Min', ' senate', 'men', '<|start_header_id|>', 'Times', ' Republicans', '\n\n', '<|eot_id|>', ' milk', '<|eot_id|>', 'assistant', '\n\n', '<|begin_of_text|>', 'way', '<|end_header_id|>', 'hall'], 'evidence_proportions': [1.3136863708496092e-05, 9.113550186157227e-06, 4.887580871582031e-06, 6.94592793782552e-05]}}, 26: {'grad': {'score': [0.4139353434244792, 0.36660624508371986, 0.34843112260867387, 0.36653789665681286, 0.42957526759097453], 'topk_tokens': [' of', ' One', 'AP', ' exc', '.', ' circ', ' air', ' Atlantic', '--', ' Eagle', ' the', 'daily', 'ol', '.', ' H', ' Jul', ' Hale', ' Daily', ' Buchanan', 'ORE'], 'evidence_proportions': [0.32748107910156254, 0.4955230712890625, 0.42344512939453127, 0.41006596883138025]}, 'weight': {'score': [0.0018730646088009788, 0.007446615218665408, 0.0018328802707867746, 0.007527166699311396, 0.01003459095954895], 'topk_tokens': ['�', '<|start_header_id|>', ' senate', 'tele', 'way', ' Daniel', '<|start_header_id|>', 'user', '<|end_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '<|eot_id|>', '❤', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0007805407047271728, 0.0005511939525604248, 0.0036190152168273926, 0.0024301012357076006]}, 'saliency': {'score': [8.698588325863792e-05, 0.000133420742873857, 6.235333589407113e-05, 0.00013431903127571058, 0.0004519400627989518], 'topk_tokens': ['tele', '<|eot_id|>', '�', ' tele', '\n', '<|eot_id|>', 'ate', '�', 'fortunate', '<|start_header_id|>', '<|start_header_id|>', ' exc', '<|end_header_id|>', ' Daniel', '<|start_header_id|>', 'hall', '❤', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [9.638071060180664e-05, 3.693699836730957e-05, 0.0001139223575592041, 9.841720263163248e-05]}}, 27: {'grad': {'score': [0.636687505812872, 0.587056400927492, 0.7507124680739182, 0.5852762906850313, 0.5422177691208688], 'topk_tokens': [' printer', ' Pioneer', '�', ' containing', ' available', ' attract', '�', ' Press', ' true', ' other', ' text', ' printer', 'void', ' manager', ' money', ' attempted', 'ioneer', ' doctor', ' Pioneer', 'itor'], 'evidence_proportions': [0.5409790039062501, 0.6314483642578126, 0.73931884765625, 0.635284423828125]}, 'weight': {'score': [0.0010638974961780367, 0.007413119859544416, 0.0018433783298883683, 0.00749716531370104, 0.007780305256969051], 'topk_tokens': ['�', ' tele', 'proc', ' *\n\n', ' Min', '\n\n', '\n\n', '<|start_header_id|>', 'user', '\n\n', '<|eot_id|>', 'Just', '<|eot_id|>', '<|start_header_id|>', 'way', '�', 'assistant', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0003893554210662842, 0.0009052872657775879, 0.0012231588363647461, 0.0016254733006159465]}, 'saliency': {'score': [8.397017206464495e-05, 0.00024790085959564115, 0.00018296868373186162, 0.00024933334564139476, 0.0008866904597533377], 'topk_tokens': [' president', 'user', 'Min', 'latest', 'RE', 'way', '<|eot_id|>', 'event', '<|eot_id|>', ' Min', 'assistant', '<|start_header_id|>', ' *\n\n', ' John', 'Just', '<|start_header_id|>', '�', 'hall', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [9.965896606445312e-06, 3.6144256591796874e-05, 0.0001033782958984375, 0.00016932189464569092]}}, 28: {'grad': {'score': [0.5359364464169457, 0.40104683587748907, 0.5044678296798315, 0.3994006837366068, 0.3691447283092298], 'topk_tokens': [' expected', ' item', ' to', ' get', ' print', ' I', 'to', ' to', ' contract', ' over', ' attempted', ' one', ' papers', ' provide', ' interest', ' await', ' two', ' to', ' contract', ' one'], 'evidence_proportions': [0.6552062988281251, 0.3743621826171875, 0.5403671264648438, 0.5674978892008463]}, 'weight': {'score': [0.0017336692128862654, 0.0070398567810996265, 0.0015788582655099721, 0.0071176340242165, 0.002351888700535423], 'topk_tokens': ['user', '<|start_header_id|>', ' \n', ' first', 'tele', ' got', '\n\n', 'Just', '\n', '\n\n', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'assistant', 'way', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00024337172508239746, 0.0004152059555053711, 0.0022265791893005373, 0.003663544853528341]}, 'saliency': {'score': [8.83340835571289e-05, 0.00012042184468843896, 6.118263953771346e-05, 0.00012113730188926271, 0.00012854761198947303], 'topk_tokens': ['�', ' during', 'assistant', '�', '<|eot_id|>', 'During', 'Today', '<|eot_id|>', ':', '\n\n', '<|start_header_id|>', '\n', '\n\n', 'Just', '\n\n', '\n\n', 'way', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [3.49879264831543e-06, 6.836652755737305e-06, 0.00011816620826721191, 0.00020208458105723065]}}, 29: {'grad': {'score': [0.4913330078125, 0.48709770749010284, 0.48098481007111377, 0.48713354102943873, 0.5471476504677221], 'topk_tokens': ['posit', ' newspaper', ' await', 'UL', ' Eagle', 'SP', ' proceedings', ' PA', 'pay', ' INCIDENT', ' hallway', 'ARR', ' press', ' Cl', ' Papers', ' Republican', ' FR', 'AP', 'E', ' STR'], 'evidence_proportions': [0.420111083984375, 0.56600341796875, 0.5568695068359375, 0.43384552001953125]}, 'weight': {'score': [0.0012877782185872395, 0.007254634785950818, 0.001434627251747327, 0.007339095174766107, 0.002877838125354365], 'topk_tokens': ['\n', ' ', ' be', '<|eot_id|>', 'During', 'Just', ',\n', '<|start_header_id|>', 'user', '\n\n', '\n\n', '\n\n', 'hall', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00032253265380859374, 0.000541985034942627, 0.0014855563640594482, 0.0025488287210464478]}, 'saliency': {'score': [4.2591776166643414e-05, 0.0002183468241523665, 4.225434401096442e-05, 0.00022087822255400038, 0.00015635866867868523], 'topk_tokens': ['Just', '\n\n\n\n', '<|start_header_id|>', '50', ' *\n\n', ';', ' *', 'user', ' not', '<|start_header_id|>', '\n\n', 'assistant', 'P', '\n\n', '<|start_header_id|>', '\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.638599395751953e-06, 4.5013427734375e-05, 5.018115043640137e-05, 6.504356861114502e-05]}}, 30: {'grad': {'score': [1.131549290248326, 0.8175942356269204, 0.9918183057736127, 0.8143845029770114, 0.7382057340521562], 'topk_tokens': [' all', ' instead', ' occurred', 'ARR', ' almost', ' held', ' hung', 'New', ' moved', ' DAYS', ' press', ' York', ' arranged', 'APER', 'NEW', 'S', ' INCIDENT', 'Penn', 'LY', 'ERS'], 'evidence_proportions': [1.1197708129882813, 0.92996826171875, 1.1703216552734375, 1.27703857421875]}, 'weight': {'score': [0.0024552331084296817, 0.006991220292569446, 0.0019675477957114195, 0.007061030639786996, 0.0067850535637453985], 'topk_tokens': ['�', ' John', 'proc', 'papers', 'ate', '<|start_header_id|>', 'Question', ' location', 'P', 'Just', ' company', 'hall', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001173311471939087, 0.0008703231811523437, 0.004042333364486695, 0.0035216758648554487]}, 'saliency': {'score': [0.00024830869265965054, 0.00035825209030443487, 0.00018222438983428173, 0.0003604515368805261, 0.00073729533898203], 'topk_tokens': [' milk', ' Col', 'St', '.', '�', 'Times', 'fortunate', ' location', 'proc', '<|eot_id|>', 'assistant', '<|start_header_id|>', '�', '<|eot_id|>', 'papers', '<|start_header_id|>', 'Question', 'way', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [8.998513221740722e-05, 6.912350654602051e-05, 0.0004545450210571289, 0.000357702374458313]}}, 31: {'grad': {'score': [0.7718280610584077, 1.1682913664581365, 0.9899137448041867, 1.17195534551781, 1.330307910316869], 'topk_tokens': ['ot', 'and', 'ot', 'ian', ' and', 'ot', 'ation', ' afterward', 'ern', 'ot', 'ot', 'ot', '\u200d', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [0.389971923828125, 0.9569793701171876, 0.7749999999999999, 0.93310546875]}, 'weight': {'score': [0.0012654832431248256, 0.006533284864648807, 0.0011345331485454852, 0.0066102868188516435, 0.0015585104885854218], 'topk_tokens': [' will', ' dropped', ' that', ' be', 'If', ' they', '?', '<|start_header_id|>', 'Just', 'Answer', ' \n', '<|eot_id|>', '\n\n', 'assistant', 'Question', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0005782544612884521, 0.0004827320575714111, 0.0017183005809783937, 0.0021131187677383423]}, 'saliency': {'score': [0.00010773539543151855, 0.0004052316655055537, 8.85916061890431e-05, 0.0004096901608267953, 0.00011185556650161743], 'topk_tokens': [' all', 'Just', ' Paul', 'system', ' item', ' company', '\n\n', 'Question', 'assistant', 'Answer', ':', ' \n', ' dropped', 'Times', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', 'hall'], 'evidence_proportions': [1.353621482849121e-05, 9.244680404663086e-06, 0.00011840462684631348, 0.00025941928227742517]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 0}
2025-01-23 22:25:03.850 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:25:03.850 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-8_pid-1_2-3-8-9.pkl | len: 10 |  size: 9.5 KB
Processing depth (2, 3, 8, 9):   2%|▏         | 2/100 [00:26<21:45, 13.32s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (0, 2, 3, 6):   2%|▏         | 2/100 [00:36<21:45, 13.32s/it]2025-01-23 22:25:14.091 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra picked up the milk.
2025-01-23 22:25:14.092 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-23 22:25:14.092 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-23 22:25:14.096 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (942, 947) --> . Sandra travelled to the
2025-01-23 22:25:14.097 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:25:14.104 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1462, 1467) --> . Sandra went to the
2025-01-23 22:25:14.104 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:25:14.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2107, 2113) --> . Daniel went back to the
2025-01-23 22:25:14.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:25:14.131 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3296, 3302) -->  Mary journeyed to the office
2025-01-23 22:25:14.131 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-23 22:25:14.142 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2107, 2113) --> . Daniel went back to the
2025-01-23 22:25:14.142 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the bedroom.
2025-01-23 22:25:14.146 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (785, 790) --> . John moved to the
2025-01-23 22:25:14.146 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:25:14.149 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (658, 663) --> . Mary moved to the
2025-01-23 22:25:14.149 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the bedroom.
2025-01-23 22:25:14.151 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (316, 322) --> . John went back to the
2025-01-23 22:25:14.151 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John journeyed to the office.
2025-01-23 22:25:14.168 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3297, 3303) -->  journeyed to the office.
2025-01-23 22:25:14.168 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Mary got the football there.
2025-01-23 22:25:14.180 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2512, 2517) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:25:14.718 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The hallway.<|eot_id|>
2025-01-23 22:25:14.718 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4227])
your chose emoji: ['👩🏾\u200d⚕️', '👰🏼\u200d♂', '🇩🇰', '🎐', '✂️', '👩🏽\u200d❤️\u200d💋\u200d👨🏽', '🧑🏿\u200d❤\u200d🧑🏼', '💆', '🪱', '🪰']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4230, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.16it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.40it/s]
2025-01-23 22:25:17.720 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.25245357695079984, 0.25274098353464836, 0.20920357337364784, 0.2531496153175116, 0.2790614978687183], 'topk_tokens': ['ly', ' line', ' lin', ' lyn', ' not', 'hall', ' or', ' Hale', ' L', ' but', ' landing', ' learned', ' line', ' John', ' or', ' generally', ' all', ' hallway', ' all', ' hallway'], 'evidence_proportions': [0.4094635009765625, 0.19405517578124998, 0.28294677734374996, 0.14486630757649738]}, 'weight': {'score': [0.001198659340540568, 0.0073993633261245475, 0.07064426480195461, 0.006839090814407495, 0.0009879228231069203], 'topk_tokens': ['user', '<|eot_id|>', ' traveled', 'Question', '<|start_header_id|>', 'Just', ' Where', '\n\n', ' \n', '<|start_header_id|>', 'assistant', '\n\n', 'Answer', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', 'hall', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0025386810302734375, 0.0010763585567474365, 0.0008274495601654052, 0.0004932334025700886]}, 'saliency': {'score': [3.578123592195057e-05, 0.00012556284172878762, 0.00018893755399263822, 0.0001254222661757069, 3.724243189837482e-05], 'topk_tokens': [' directly', 'If', ' Hill', '\n\n', '.', '<|start_header_id|>', ' ground', 'Question', '***', ' kitchen', ' Where', ' hallway', 'Just', ' traveled', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [4.998445510864258e-05, 4.270076751708984e-05, 2.2917985916137695e-05, 2.8898318608601887e-05]}}, 25: {'grad': {'score': [0.34501121157691594, 0.33150414000166223, 0.3515005356226212, 0.3312491021282095, 0.40710423443768473], 'topk_tokens': [' bathroom', 'body', '�', ' bogus', ' George', ' bend', ' boats', ' Adams', ' adher', 'ball', ' balance', ' body', ' goods', ' boats', ' boats', 'wing', '9', ' ', ' bag', ' item'], 'evidence_proportions': [0.336279296875, 0.370306396484375, 0.2883171081542969, 0.3784535725911458]}, 'weight': {'score': [0.0014824512458982923, 0.007425596809838117, 0.10098919730920058, 0.006580471063403488, 0.0007470173610223306], 'topk_tokens': [' remains', ' write', 'RE', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'user', ' Do', ' \n', ' milk', 'Answer', '<|eot_id|>', '<|eot_id|>', 'way', 'hall', 'assistant', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00273747444152832, 0.0013516545295715332, 0.000684666633605957, 0.0012104163567225137]}, 'saliency': {'score': [5.69806212470645e-05, 0.0001576006694324755, 0.0009854015631553454, 0.00015036536635254784, 2.5099999195820578e-05], 'topk_tokens': [' remains', ' Times', ' Mary', 'Times', ' item', ' minds', ' printers', ' item', 'istributed', '<|eot_id|>', ' milk', 'assistant', 'Answer', ' person', 'way', ' write', '<|end_header_id|>', '.', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0001216113567352295, 5.742907524108887e-05, 2.283453941345215e-05, 3.120303153991699e-05]}}, 26: {'grad': {'score': [0.30890037899925593, 0.3388301623910313, 0.3158410879281851, 0.3391958936512899, 0.4131487833487021], 'topk_tokens': ['�', ' were', ' Col', ' evidence', 'ages', '�', '3', 'ree', ' all', ' George', ' Gree', ' Gen', 'hall', ' pen', '�', ' room', ' Hill', ' Hale', ' hallway', ' hallway'], 'evidence_proportions': [0.2328369140625, 0.4182159423828125, 0.296771240234375, 0.29129791259765625]}, 'weight': {'score': [0.0020912133512042816, 0.007381308896039958, 0.10258473646946442, 0.0065175578962984706, 0.004893348829166309], 'topk_tokens': ['<|end_header_id|>', '.\n', ' \n', 'user', '<|start_header_id|>', '.\n\n', '\n\n', 'Question', '<|eot_id|>', ' kitchen', '<|start_header_id|>', 'Answer', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'way', 'assistant', 'hall', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.004880666732788086, 0.0016379117965698241, 0.0011557340621948242, 0.0009239862362543742]}, 'saliency': {'score': [0.00010753387496584938, 0.0001277807333790664, 0.0006392216071104392, 0.00012309943600524243, 0.00018995154548335719], 'topk_tokens': [' Gal', ' Col', 'way', ' Hale', '<|eot_id|>', 'Question', '\n\n', 'Just', 'far', 'rail', 'Answer', '\u200d', '<|start_header_id|>', 'polit', ' editor', ' kitchen', ' hallway', '<|begin_of_text|>', '.', 'hall'], 'evidence_proportions': [0.00021832585334777833, 0.0001404285430908203, 7.65383243560791e-05, 1.3624628384908041e-05]}}, 27: {'grad': {'score': [0.3382771809895833, 0.2387247865645316, 0.2527330349653195, 0.2380924311759089, 0.2573398899387669], 'topk_tokens': [' court', ' hand', ' room', ' small', ' back', 'wing', ' back', '30', ' devil', ' court', '600', ' back', ' back', ' scale', ' Bank', '600', '600', '800', ' state', ' state'], 'evidence_proportions': [0.40419311523437496, 0.23225097656250002, 0.324151611328125, 0.38347371419270837]}, 'weight': {'score': [0.0012337423506237212, 0.0074362132566195005, 0.12256084115077288, 0.006390743568646822, 0.003636412926622339], 'topk_tokens': [' \n', '<|start_header_id|>', 'RE', ' milk', 'Just', '<|eot_id|>', 'If', ' kitchen', 'Question', 'user', '\n\n', '<|eot_id|>', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'way', 'hall', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.002209687232971191, 0.0011322736740112306, 0.0008436083793640138, 0.0008301238218943279]}, 'saliency': {'score': [5.439349583217076e-05, 0.00011730399114865783, 0.0005481159075712546, 0.0001135916303959396, 0.00013830895359451706], 'topk_tokens': [' Newspaper', '<|end_header_id|>', 'Question', 'first', '<|eot_id|>', ' John', ' Mary', ' first', 'Answer', 'If', ' second', ' editor', 'assistant', 'way', 'hall', ' first', ' Paul', ' first', '.', '<|begin_of_text|>'], 'evidence_proportions': [4.485845565795898e-05, 6.226301193237304e-05, 4.4965744018554686e-05, 6.363789240519206e-05]}}, 28: {'grad': {'score': [0.19672729855492002, 0.1920888986429706, 0.17078668643266726, 0.19226476911732332, 0.24586177516627955], 'topk_tokens': [' take', ' Hor', ' attempted', ' Herald', 'ION', '.', ' hour', ' half', ' wholly', ' local', ' all', 'hours', ' all', ' half', 'half', ' Hill', ' Hale', 'hall', ' hallway', ' hallway'], 'evidence_proportions': [0.1778564453125, 0.2109222412109375, 0.1726707458496094, 0.22067101796468097]}, 'weight': {'score': [0.0006820516926901681, 0.007287495299715804, 0.1901211142539978, 0.005610805413991713, 0.0009839659607088244], 'topk_tokens': [' person', '.\n', ' \n', ' milk', 'Just', 'Question', '<|eot_id|>', '.\n\n', ' directly', '.', ' kitchen', '<|eot_id|>', 'Answer', '\n\n', 'assistant', 'hall', '<|end_header_id|>', 'way', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0009592533111572265, 0.0004058003425598145, 0.0008614897727966308, 0.0005317280689875284]}, 'saliency': {'score': [1.4384587605794271e-05, 7.75880354234231e-05, 0.00019327493814321665, 7.682436232944186e-05, 1.8555570293117215e-05], 'topk_tokens': ['<|start_header_id|>', ' first', '<|eot_id|>', 'assistant', '<|eot_id|>', ' person', 'Just', ' dropped', ' always', ' kitchen', ' also', ' directly', 'Question', ' milk', '<|begin_of_text|>', 'Answer', '.', 'way', '\n\n', 'hall'], 'evidence_proportions': [2.3174285888671872e-05, 7.760524749755859e-06, 1.4287233352661132e-05, 1.2661019961039226e-05]}}, 29: {'grad': {'score': [0.35100682576497394, 0.39434577815639776, 0.35961170685596955, 0.39488888290002766, 0.24100948024440455], 'topk_tokens': [' of', '30', 'ra', ',', 'ot', 'inen', 'ig', 'u', ',', ' Collection', '7', ' for', '10', 'ir', ' so', ' type', ' ty', ' pur', ' pur', ' copy'], 'evidence_proportions': [0.4162841796875, 0.27910842895507815, 0.3524169921875, 0.3553492228190104]}, 'weight': {'score': [0.0006217658519744873, 0.007442077564573176, 0.20562539192346427, 0.00562291144610023, 0.002172840205398766], 'topk_tokens': [' Where', ' hallway', ' \n', '<|start_header_id|>', '.\n\n', ' Do', ' kitchen', '.', ' milk', '<|eot_id|>', '\n\n', 'Question', '<|eot_id|>', 'hall', 'Answer', 'assistant', 'way', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0014753103256225585, 0.0002643883228302002, 0.0003079712390899658, 0.00046978890895843506]}, 'saliency': {'score': [2.21686703818185e-05, 9.330930298383637e-05, 0.0018180669882358648, 7.75367378902664e-05, 0.00010103671937375455], 'topk_tokens': [' twenty', '.', '<|eot_id|>', ' Do', 'hall', '<|start_header_id|>', ' \n', ' milk', 'If', '      ', 'way', 'Question', '<|eot_id|>', 'Answer', '.', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [3.6704540252685544e-05, 9.787082672119142e-06, 1.4603137969970703e-05, 2.6678045590718586e-05]}}, 30: {'grad': {'score': [0.25035167875744047, 0.35807558605293294, 0.31408295264610875, 0.3590295224452762, 0.5080083898595862], 'topk_tokens': [' James', ' D', 'd', ' gang', ' air', 'P', 'D', ' all', ' great', ' S', ' lin', ' ', 'which', 'ied', ' W', ' S', 'S', ' hallway', ' S', ' S'], 'evidence_proportions': [0.1453155517578125, 0.2709808349609375, 0.3295654296875, 0.2546793619791667]}, 'weight': {'score': [0.0016360140982128325, 0.007242122496837138, 0.06738760532476963, 0.006707842987504223, 0.00808774377848651], 'topk_tokens': ['.\n\n', '.\n\n', '.\n', '<|start_header_id|>', ' Where', ' \n', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'Question', '.', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', 'way', 'hall', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.003475856781005859, 0.0009905219078063965, 0.0011586427688598632, 0.0010385314623514812]}, 'saliency': {'score': [7.784082776024228e-05, 0.0001611563612665127, 0.0006979887302105243, 0.00015655520150987366, 0.0004906461045548722], 'topk_tokens': [' locations', ' second', '�', 'D', '.\n\n', '.', '.\n', '�', 'user', ' kitchen', ' \n', ' milk', 'Answer', '<|end_header_id|>', 'Question', 'assistant', 'way', '.', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00023320317268371582, 3.0028820037841798e-05, 2.5790929794311526e-05, 3.159046173095703e-05]}}, 31: {'grad': {'score': [0.26198214576357887, 0.3177873760250443, 0.2669070806258764, 0.3185442684365691, 0.3322776072734111], 'topk_tokens': [' imported', ' I', ' wield', 'ed', ' We', ' I', 't', '\u200d', ' D', 'ided', '\u200d', ' inverted', 'I', ' W', ' I', 'SP', '\u200d', 'f', 'D', 'D'], 'evidence_proportions': [0.1931427001953125, 0.274334716796875, 0.27152099609375, 0.3011054992675781]}, 'weight': {'score': [0.001095504987807501, 0.00676277873082082, 0.042261323103537925, 0.006459318183594756, 0.0026997231148384714], 'topk_tokens': [' milk', ' the', '<|eot_id|>', '.\n', ' Do', ' Where', ':', '.', '.\n\n', 'Question', ' \n', 'Answer', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', '.', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002278423309326172, 0.0008605003356933593, 0.0007760167121887208, 0.00057181715965271]}, 'saliency': {'score': [6.239754813058036e-05, 0.00013144296675427305, 0.00019631783167521158, 0.00013118393415455625, 7.964388744251148e-05], 'topk_tokens': [' was', 'If', ' item', ' dropped', ' item', 'Bridge', ' Hill', ' PA', '.', ' the', ' item', ' Where', ' item', ' write', 'assistant', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'way', 'hall'], 'evidence_proportions': [9.130239486694336e-05, 3.614425659179688e-05, 7.541179656982422e-05, 4.9342711766560875e-05]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-23 22:25:17.729 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:25:17.730 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-8_pid-2_0-2-3-6.pkl | len: 10 |  size: 9.02 KB
Processing depth (0, 2, 3, 6):   3%|▎         | 3/100 [00:40<21:56, 13.57s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (1, 4, 5, 6):   3%|▎         | 3/100 [00:50<21:56, 13.57s/it]2025-01-23 22:25:27.446 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra picked up the milk.
2025-01-23 22:25:27.449 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (469, 474) --> . Sandra picked up the
2025-01-23 22:25:27.449 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-23 22:25:27.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1840, 1845) --> . Sandra travelled to the
2025-01-23 22:25:27.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:25:27.469 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2112, 2117) --> . Sandra went to the
2025-01-23 22:25:27.469 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:25:27.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2116, 2122) -->  the kitchen. Daniel went back
2025-01-23 22:25:27.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:25:27.497 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3315, 3321) --> . Mary journeyed to the
2025-01-23 22:25:27.497 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-23 22:25:27.508 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2114, 2120) -->  went to the kitchen. Daniel
2025-01-23 22:25:27.508 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the bedroom.
2025-01-23 22:25:27.511 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (728, 733) --> . John moved to the
2025-01-23 22:25:27.511 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:25:27.515 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (657, 662) --> . Mary moved to the
2025-01-23 22:25:27.515 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the bedroom.
2025-01-23 22:25:27.517 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (309, 315) --> . John went back to the
2025-01-23 22:25:27.517 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John journeyed to the office.
2025-01-23 22:25:27.533 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3316, 3322) -->  Mary journeyed to the office
2025-01-23 22:25:27.534 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Mary got the football there.
2025-01-23 22:25:27.546 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2519, 2524) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:25:28.079 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The hallway.<|eot_id|>
2025-01-23 22:25:28.079 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4237])
your chose emoji: ['💪🏾', '👩🏿\u200d❤\u200d💋\u200d👩🏿', '🔎', '🏃🏼\u200d♀️\u200d➡', '👩🏾\u200d❤️\u200d👨🏼', '\U0001faf2🏼', '\U0001fac3🏽', '🔺', '🏊🏽\u200d♂️', '⛓\u200d💥']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4240, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 195083.91it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.37it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.75it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.79it/s]
2025-01-23 22:25:30.968 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.21863891964867002, 0.21669187725714917, 0.24954350789388022, 0.21637558503584428, 0.23734532524557675], 'topk_tokens': [' day', ' MO', ' man', ' made', ' perform', ' Daily', 'AILY', ',', ' back', ' daily', ' nineteenth', ' of', ' very', ' excessive', ' DAYS', ' make', ' afford', ' IN', ' em', ' method'], 'evidence_proportions': [0.22573585510253907, 0.1764404296875, 0.2185394287109375, 0.24797312418619794]}, 'weight': {'score': [0.0026879097734178814, 0.007419848667000825, 0.001399841064061874, 0.007499789100799834, 0.002607228826074039], 'topk_tokens': ['\n\n', 'fax', '<|start_header_id|>', ' part', ' block', '-four', '❤', '<|start_header_id|>', ' propriet', ' possibly', '\n\n', ' people', ' Democratic', 'assistant', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.005927103757858276, 0.0019569158554077148, 0.0016053080558776855, 0.0014999111493428547]}, 'saliency': {'score': [0.00013599651200430735, 7.893348218135115e-05, 4.540192775237255e-05, 7.895965610394637e-05, 3.465449108796961e-05], 'topk_tokens': [' Sandra', ' inverted', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'system', ' very', 'user', ' line', ' Project', '<|end_header_id|>', '\n\n', '***', '<|eot_id|>', 'way', 'assistant', ' Bank', ' block', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0004191279411315918, 2.16066837310791e-05, 5.7917833328247066e-05, 6.044407685597738e-05]}}, 25: {'grad': {'score': [0.30299159458705355, 0.2961908664343492, 0.3397026062011719, 0.2957507293190112, 0.28927755916819853], 'topk_tokens': [' FR', ' was', ' attract', ' was', ' revisit', ' Pioneer', ' was', 'RI', ' Jackson', ' first', 'the', ' and', ' by', ' the', ' method', ' Mary', 'PA', ' Written', 'system', ' of'], 'evidence_proportions': [0.28455810546875004, 0.1975616455078125, 0.30589599609374996, 0.4037907918294271]}, 'weight': {'score': [0.002387454112370809, 0.007351706612784907, 0.001597442688086094, 0.007430334745696857, 0.001690496416652904], 'topk_tokens': [' as', '<|start_header_id|>', ' connected', 'fax', '❤', ' one', '-four', 'assistant', ' people', ' rates', ' Democratic', ' propriet', '<|eot_id|>', ' possibly', '<|eot_id|>', 'way', '\n\n', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.005078124999999999, 0.0017181932926177979, 0.0013899326324462889, 0.0015342136224110923]}, 'saliency': {'score': [5.940596262613932e-05, 7.593962100316894e-05, 7.330072231781788e-05, 7.604730614064412e-05, 2.0023654488956227e-05], 'topk_tokens': [' Minnesota', ' actions', ' Douglas', '\n\n', ' Associated', ' corner', ' grat', ' Min', '<|eot_id|>', 'assistant', ' Dub', ' Min', '<|eot_id|>', ' Min', ' rates', 'hall', ' tele', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00014911293983459472, 1.8399953842163085e-05, 3.4528970718383786e-05, 3.9552648862202964e-05]}}, 26: {'grad': {'score': [0.2877840314592634, 0.24034865757204452, 0.28426725436479616, 0.23970057907287012, 0.2548766192267923], 'topk_tokens': [',', ' marched', '.', 'S', ' Team', ' very', ' crack', 'cap', 'lyn', ' parade', '<|eot_id|>', ' meeting', '3', ' Paul', ' *', ' be', ' enabling', '-per', ' Marshall', ' hallway'], 'evidence_proportions': [0.26379241943359377, 0.19869079589843752, 0.2889068603515625, 0.3810857137044271]}, 'weight': {'score': [0.010199438957940964, 0.0074845716638385126, 0.0027214670792604103, 0.007515372827863008, 0.005152549463159898], 'topk_tokens': [' picked', '-four', ' part', 'fax', 'way', '<|eot_id|>', 'fax', ' propriet', '<|eot_id|>', '<|start_header_id|>', '❤', '<|start_header_id|>', ' possibly', '<|eot_id|>', ' people', ' Democratic', 'assistant', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.025162720680236818, 0.0036773204803466795, 0.006785821914672852, 0.006009817123413086]}, 'saliency': {'score': [0.0004024477232070196, 0.0001286492313978807, 6.836882004371056e-05, 0.0001278361136263067, 0.00023096589481129366], 'topk_tokens': ['\n\n', ' seen', 'port', ' Democratic', '<|eot_id|>', ' senator', '<|start_header_id|>', '<|eot_id|>', 'nes', 'assistant', ' picked', 'ye', ' Knowledge', ' grat', '<|end_header_id|>', '❤', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010949611663818358, 5.9092044830322264e-05, 0.0003007948398590088, 0.00019619365533192953]}}, 27: {'grad': {'score': [0.28790065220424105, 0.3686416338074882, 0.36408610221667165, 0.3690897740815815, 0.4263570224537569], 'topk_tokens': [' the', ' the', ',', ' the', ' be', ' the', ' things', ' that', ',', '\n', ' the', ';', ' notified', ' and', ',', ' the', '\n', ' work', ' the', ','], 'evidence_proportions': [0.256158447265625, 0.3072540283203125, 0.30020751953125, 0.2879689534505208]}, 'weight': {'score': [0.004294577099028088, 0.007463266039794346, 0.002109231092990973, 0.007529139205029136, 0.0033477632438435275], 'topk_tokens': ['fax', ' the', ' connected', ' part', '<|eot_id|>', '<|eot_id|>', 'fax', '<|start_header_id|>', ' offices', ' propriet', '-four', 'assistant', ' possibly', '❤', ' people', 'way', ' Democratic', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.004239189624786377, 0.0016325950622558595, 0.005955994129180908, 0.0051745374997456866]}, 'saliency': {'score': [0.0007243667330060687, 0.00015415056555900933, 0.00031524132459591597, 0.00014978284328177785, 0.00010691495502696318], 'topk_tokens': ['<|begin_of_text|>', ' the', ' *\n\n', ' offices', ' Joseph', '<|eot_id|>', 'RE', '-four', 'ences', ' Democratic', 'isc', '<|start_header_id|>', ' grat', ' the', '<|eot_id|>', ' *\n\n', '<|start_header_id|>', '<|end_header_id|>', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.00038580298423767094, 5.270242691040039e-05, 0.0013359665870666503, 0.0010565569003423056]}}, 28: {'grad': {'score': [0.6934799920944941, 0.6477414976875737, 0.7141074156149839, 0.6468925074527138, 0.6555289773380055], 'topk_tokens': ['ORE', ' would', 'ARR', ' Daniel', '\n\n', '\n', '\n', ' W', '.', '.', '\n', 'PA', '.', '.', '.', ' FR', '.', '.\n', ' MO', 'com'], 'evidence_proportions': [0.745166015625, 0.6475341796875, 0.62847900390625, 0.7428639729817709]}, 'weight': {'score': [0.0013979261829739525, 0.007239898870576102, 0.001105948900565123, 0.007326479127817747, 0.001655137538909912], 'topk_tokens': [' very', ' fore', ' newspaper', ' as', '-four', ' connected', 'Just', ' people', ' Democratic', ' possibly', '<|start_header_id|>', ' propriet', '<|eot_id|>', 'assistant', '<|eot_id|>', '\n\n', 'way', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011697649955749513, 0.0005549907684326172, 0.0012845098972320557, 0.0023850202560424805]}, 'saliency': {'score': [6.462846483503069e-05, 0.00011574906420032933, 5.461466618073292e-05, 0.00011657628288679716, 2.6541597702923944e-05], 'topk_tokens': [' kitchen', ' first', ' very', ' connected', ' hopes', '<|eot_id|>', 'Answer', 'Question', 'pleasant', '<|start_header_id|>', ' rates', ' Wins', '<|eot_id|>', ' possibly', 'assistant', 'Just', 'way', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010671019554138185, 9.334087371826172e-06, 4.453659057617188e-05, 9.238223234812418e-05]}}, 29: {'grad': {'score': [0.33424849737258183, 0.43905962098319573, 0.32331202580378604, 0.44066612572190866, 0.5191673727596507], 'topk_tokens': ['<|start_header_id|>', ' cable', ' for', ' INCIDENT', ' for', 'UG', ' to', 'str', ' down', '-per', ' a', ' history', 'cap', ' old', ' up', ' FR', 'AP', ' time', 'PA', ' STR'], 'evidence_proportions': [0.2776214599609375, 0.537890625, 0.37841796875, 0.17492802937825522]}, 'weight': {'score': [0.003530763444446382, 0.007313649159557414, 0.0017577111721038818, 0.007384491786717228, 0.002584318203084609], 'topk_tokens': ['\n\n', ' propriet', '<|start_header_id|>', 'During', '<|start_header_id|>', ' hopes', ' Democratic', '\n\n', ' *\n\n', ' people', ' possibly', '<|eot_id|>', '<|start_header_id|>', 'assistant', 'hall', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0025414943695068358, 0.0021311759948730467, 0.0035096168518066405, 0.005539099375406901]}, 'saliency': {'score': [0.0008169809977213541, 0.00016411397395268927, 0.00030557925884540263, 0.00015951412859145533, 0.00017768109545988195], 'topk_tokens': [' John', ' W', '      ', '\n', ' *', ' people', ' prof', 'During', 'assistant', ' a', ' the', ' *\n\n', '<|end_header_id|>', 'hall', '<|eot_id|>', 'system', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00012903809547424314, 0.0002900004386901855, 0.0014071881771087647, 0.0013375778992970784]}}, 30: {'grad': {'score': [1.0028519403366816, 0.6386787558501621, 0.9501460148737981, 0.6339431434156793, 0.6464926775764016], 'topk_tokens': [' The', 'v', ' the', '\n', 'Min', ' held', ' the', ' perform', ' Marshall', '.', ' him', ' picked', ' Jackson', ' S', ' compos', ' leading', ' John', ' awarded', ' of', '600'], 'evidence_proportions': [1.0952728271484375, 0.8379638671875, 1.0020263671875, 1.0639292399088542]}, 'weight': {'score': [0.006065028054373605, 0.007226933173413546, 0.003983650452051408, 0.007263030789115213, 0.003091718870050767], 'topk_tokens': ['.', 'this', ' Joseph', 'ANK', ' excited', ' rates', ' tele', 'Question', 'Just', '***', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'hall', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0056744575500488285, 0.0029284477233886717, 0.006557369232177734, 0.008594036102294922]}, 'saliency': {'score': [0.0004175291174934024, 0.00023324722951313236, 0.0004394994332240178, 0.00023039704874942176, 0.00016875722829033348], 'topk_tokens': [' Jackson', 'this', ' a', 'ANK', '<|eot_id|>', '-text', ' of', '<|start_header_id|>', ' excited', ' John', '<|begin_of_text|>', '<|end_header_id|>', ' tele', 'Question', ' Joseph', 'assistant', '<|end_header_id|>', 'hall', '<|start_header_id|>', 'way'], 'evidence_proportions': [0.00032773613929748535, 0.0001406252384185791, 0.0004458248615264893, 0.0006995300451914469]}}, 31: {'grad': {'score': [0.513567016238258, 0.877307791080115, 0.5930907420622997, 0.8817869827507786, 0.9781228458180147], 'topk_tokens': [' and', 'D', ',', ' and', ' and', 'and', 'ing', ',', ' and', 'and', ' and', ' in', ' D', 'D', ' the', '\n', ' before', '\n', '\n', '<|start_header_id|>'], 'evidence_proportions': [0.4023946762084961, 0.6897705078125, 0.48328857421875004, 0.4846064249674479]}, 'weight': {'score': [0.00437451544262114, 0.006510455878275746, 0.002595454454421997, 0.0065577142047539855, 0.0027816214982201073], 'topk_tokens': [',', ' Democratic', 'If', ' they', ' \n', 'Answer', ',', '.\n\n', ' Where', 'Just', '<|eot_id|>', '<|start_header_id|>', 'assistant', '\n\n', 'Question', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.002315855026245117, 0.001957988739013672, 0.006077766418457031, 0.006684462229410808]}, 'saliency': {'score': [0.0002109025205884661, 0.00025413022288736306, 0.00013941526412963867, 0.0002554177025858865, 0.00011202447554644417], 'topk_tokens': [' course', ',', 'through', 'PA', ' rates', ' day', 'Just', ' were', ' Where', ' set', ' they', '<|eot_id|>', '<|end_header_id|>', 'Question', 'assistant', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', 'way', 'hall'], 'evidence_proportions': [8.426308631896972e-05, 7.210969924926758e-05, 0.0002788186073303222, 0.0003754993279774984]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-23 22:25:30.975 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:25:30.976 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-8_pid-3_1-4-5-6.pkl | len: 10 |  size: 9.44 KB
Processing depth (1, 4, 5, 6):   4%|▍         | 4/100 [00:53<21:30, 13.44s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.34it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Processing depth (0, 5, 6, 9):   4%|▍         | 4/100 [01:02<21:30, 13.44s/it]2025-01-23 22:25:40.029 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra picked up the milk.
2025-01-23 22:25:40.029 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-23 22:25:40.029 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-23 22:25:40.040 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2130, 2135) -->  Sandra travelled to the hallway
2025-01-23 22:25:40.040 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:25:40.050 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2138, 2143) -->  back to the kitchen.
2025-01-23 22:25:40.051 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel went back to the hallway.
2025-01-23 22:25:40.061 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2132, 2138) -->  to the hallway. Daniel went
2025-01-23 22:25:40.061 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary journeyed to the office.
2025-01-23 22:25:40.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3265, 3271) --> . Mary journeyed to the
2025-01-23 22:25:40.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-23 22:25:40.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2133, 2139) -->  the hallway. Daniel went back
2025-01-23 22:25:40.089 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the bedroom.
2025-01-23 22:25:40.093 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (726, 731) --> . John moved to the
2025-01-23 22:25:40.093 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary moved to the bathroom.
2025-01-23 22:25:40.096 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (638, 643) -->  Republican. Mary moved to
2025-01-23 22:25:40.097 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the bedroom.
2025-01-23 22:25:40.098 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (202, 208) --> . John went back to the
2025-01-23 22:25:40.098 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John journeyed to the office.
2025-01-23 22:25:40.114 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3266, 3272) -->  Mary journeyed to the office
2025-01-23 22:25:40.114 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Mary got the football there.
2025-01-23 22:25:40.127 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2497, 2502) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:25:40.639 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bedroom.<|eot_id|>
2025-01-23 22:25:40.639 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4210])
your chose emoji: ['🧑🏼\u200d⚖', '🖐🏿', '🍃', '🤼', '🕡', '❣', '🏊🏿\u200d♂', '🍙', '🥧', '👩🏼\u200d❤\u200d👩🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4213, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 216480.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.59it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.67it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.61it/s]
2025-01-23 22:25:43.698 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5888408479236421, 0.5175240292787948, 0.4936967507386819, 0.5173871669314609, 0.6046404252972519], 'topk_tokens': ['-four', '�', 'Min', ' Min', 'Min', 'Min', ' hung', ' Min', ' Min', ' Min', '\n', 'Min', 'Min', ' hand', ' hallway', ' hallway', ' hopes', 'hall', 'way', ' em'], 'evidence_proportions': [0.9279541015625, 0.6753204345703125, 0.20513839721679689, 0.5539321899414062]}, 'weight': {'score': [0.0011137880030132475, 0.0073701497242509, 0.0008015357531034029, 0.007463470104944554, 0.0031059760796396355], 'topk_tokens': [' directly', ':', 'In', 'user', '<|start_header_id|>', ' return', '\n\n', '\n\n', '.\n\n', ':', ' \n', '<|eot_id|>', 'hall', 'Answer', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'assistant', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.001944279670715332, 0.0011429250240325927, 0.0006720125675201416, 0.000765576958656311]}, 'saliency': {'score': [7.395801090058827e-05, 0.00020967767353497008, 7.758232263418344e-05, 0.00021160443288976245, 0.00014157776246991074], 'topk_tokens': ['<|start_header_id|>', 'If', ':', 'In', ' ', 'user', ' directly', ' \n', '\n\n', 'assistant', '\n\n', 'If', '<|eot_id|>', ' return', '<|begin_of_text|>', 'In', '.\n\n', 'Just', 'way', 'hall'], 'evidence_proportions': [9.70602035522461e-05, 8.837580680847168e-05, 4.0978193283081055e-05, 7.01745351155599e-05]}}, 25: {'grad': {'score': [1.166259765625, 0.9294231350507358, 1.0727296486879005, 0.9268797872843239, 0.8299156322813871], 'topk_tokens': ['nes', 'nes', 'ot', 'nes', 'ot', 'ot', ' news', 'rich', ' cap', ' information', ' air', ' Sandra', ' N', 'through', 'a', ' Proof', 'nes', ' issued', ' proof', '\n\n'], 'evidence_proportions': [1.1466552734375, 1.3109375, 1.0599609375, 1.1506144205729167]}, 'weight': {'score': [0.0008247040566943941, 0.007335403201530814, 0.000409094186929556, 0.007433368704447022, 0.0038093135022280507], 'topk_tokens': [' directly', '.\n\n', ' Do', '\n\n', '<|start_header_id|>', '.\n\n', ' not', '?', '<|eot_id|>', ':', 'hall', ' return', ' \n', 'Answer', 'way', '<|eot_id|>', '<|end_header_id|>', 'assistant', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0012781620025634766, 0.0002362549304962158, 0.0013693034648895263, 0.0004833638668060303]}, 'saliency': {'score': [0.00013850842203412737, 0.00023016264255839685, 3.2858970837715344e-05, 0.0002324789396516105, 0.00016488892990246154], 'topk_tokens': ['<|eot_id|>', ' *\n\n', ' dispatch', '?', 'hall', ' facts', ' Ear', ' directly', ' Do', '.\n\n', 'way', ' not', ' write', ' \n', ' return', 'Answer', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.00010384321212768554, 1.2981891632080079e-05, 0.00041667819023132323, 4.019339879353841e-05]}}, 26: {'grad': {'score': [0.6211420694986979, 0.476145264657014, 0.44612988447531676, 0.47569394438863233, 0.5980496322899534], 'topk_tokens': [' method', ' Hale', ' streets', 'Democratic', ' air', ':', 'street', ' Democratic', ' Atlantic', 'street', '�', ' panic', ' morning', ' secret', '3', ' Stewart', ' Hill', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.29835357666015627, 0.789459228515625, 0.6255859375, 0.7461649576822917]}, 'weight': {'score': [0.001142534471693493, 0.0069556785201392544, 0.0005213855168758294, 0.007045496351139648, 0.005260957437649108], 'topk_tokens': [':', '<|start_header_id|>', ' kitchen', '.', '<|eot_id|>', '.\n\n', '?', '\n\n', '.\n\n', 'Answer', ' \n', '<|eot_id|>', ':', '\n\n', 'hall', 'assistant', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0020143151283264164, 0.0011297345161437988, 0.0006599009037017822, 0.0008289118607838949]}, 'saliency': {'score': [7.962329047066825e-05, 0.00022757935490032118, 3.155301778744429e-05, 0.00023016835189055112, 0.000192654760260331], 'topk_tokens': [' *\n\n', ' *\n\n', '<|eot_id|>', '<|start_header_id|>', '.\n\n', 'As', ' *\n\n', 'Bridge', ' kitchen', '.', '\n\n', '<|eot_id|>', ':', '\n\n', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'way', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.00012351274490356444, 8.016824722290039e-05, 4.354119300842285e-05, 7.266302903493245e-05]}}, 27: {'grad': {'score': [0.4810097104027158, 0.38625486542769405, 0.43934125166672927, 0.38527720570994833, 0.3763618636549565], 'topk_tokens': [' block', ' paper', ' boat', ' method', ' paper', '600', 'rail', ' management', ' steam', ' Bench', ' bedroom', ' printed', ' hand', ' mob', ' loud', '8', '600', ' context', '240', '800'], 'evidence_proportions': [0.48023223876953125, 0.5879425048828125, 0.394580078125, 0.46457163492838544]}, 'weight': {'score': [0.0009465728487287249, 0.0072593944756910575, 0.000788734509394719, 0.007352080484083007, 0.005346878578788356], 'topk_tokens': [' *\n\n', '.\n\n', 'Just', '.', '.\n\n', '<|eot_id|>', ':', '?', '<|eot_id|>', 'If', ' \n', ':', '\n\n', 'assistant', 'Answer', 'hall', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0018427371978759766, 0.0006503701210021972, 0.0006881296634674072, 0.0006619741519292196]}, 'saliency': {'score': [5.387266476949056e-05, 0.00014020806391109572, 4.685459992824456e-05, 0.00014152129012763917, 0.00025076092335215785], 'topk_tokens': ['Another', ' *\n\n', '<|eot_id|>', ' *', ' anything', 'In', ' \n', 'As', '<|eot_id|>', 'Anything', '\n\n', 'If', 'RE', 'assistant', 'Answer', '<|begin_of_text|>', 'hall', ' *\n\n', 'way', '<|end_header_id|>'], 'evidence_proportions': [0.0001137852668762207, 3.3187866210937504e-05, 4.826188087463379e-05, 2.5858481725056965e-05]}}, 28: {'grad': {'score': [0.47157069614955355, 0.44714772704092987, 0.39190008701422274, 0.44754304984607324, 0.535395438211006], 'topk_tokens': ['hour', ' house', '�', '.', ' ', '\n', ' ', ' ', '\n', '\n', ' Hill', ' hallway', '\n', '\n', '.', '\n', ' hallway', ' Hale', '.', 'hall'], 'evidence_proportions': [0.29190673828125, 0.500390625, 0.526715087890625, 0.5513203938802083]}, 'weight': {'score': [0.0010807201975867862, 0.007046449617340139, 0.00047210240975404397, 0.0071383542306102275, 0.004071126381556193], 'topk_tokens': ['.', ':', 'During', '.\n\n', '.\n\n', 'Answer', ' kitchen', ' \n', '?', ':', 'Just', '.\n\n', 'hall', '<|eot_id|>', 'assistant', '.', '<|end_header_id|>', '\n\n', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0028225898742675784, 0.0002835094928741455, 0.0009731173515319825, 0.00038317342599232995]}, 'saliency': {'score': [0.00011212485177176339, 0.00011822388173847394, 1.4143876540355193e-05, 0.00011923211670886343, 0.00015085203605785704], 'topk_tokens': ['.\n\n', ':', '.\n', '?', 'From', ' before', '.\n\n', 'During', 'As', '<|eot_id|>', 'Answer', '<|eot_id|>', 'Just', 'assistant', 'way', 'hall', '.', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0004197835922241211, 8.195638656616211e-06, 2.5588274002075193e-05, 1.4464060465494793e-05]}}, 29: {'grad': {'score': [0.35056432088216144, 0.3834151298111871, 0.3085628411708734, 0.3842841657477346, 0.4222244463468853], 'topk_tokens': [' heard', ' had', ' I', 'u', '100', ' type', '186', ' *', 'P', '1', '50', 'ien', ' ', '25', '***', ' Hoe', '8', ' the', '9', '26'], 'evidence_proportions': [0.512286376953125, 0.24956626892089845, 0.32984161376953125, 0.31722990671793616]}, 'weight': {'score': [0.002981130565915789, 0.007254625425015044, 0.0003512073785830767, 0.007341063348408236, 0.004223785902324475], 'topk_tokens': [' kitchen', ' milk', '.\n\n', '.\n\n', '?', '\n\n', '.', '.\n\n', ':', ':', ' \n', 'hall', 'If', 'Answer', '<|end_header_id|>', 'assistant', '\n\n', '<|eot_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.009695482254028321, 0.0005785644054412842, 0.0014198362827301026, 0.000689054528872172]}, 'saliency': {'score': [0.00011909291857764834, 0.00011315035644574712, 9.758350176688953e-06, 0.00011409124121331835, 0.00024745757119697435], 'topk_tokens': ['.\n\n', '\n\n', '.', '<|end_header_id|>', 'Answer', '�', '<|start_header_id|>', '.\n\n', 'assistant', '"The', ':', ' \n', 'hall', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', '<|eot_id|>'], 'evidence_proportions': [0.00039703249931335446, 1.9407272338867185e-05, 5.6755542755126956e-05, 2.249578634897868e-05]}}, 30: {'grad': {'score': [0.6727049237205869, 0.6355763364511631, 0.7193979116586539, 0.6346014407695474, 0.5323019696955096], 'topk_tokens': ['reading', 's', '\n', ' air', ' air', ' county', ' first', ' S', 'lyn', ' an', ' lyn', 'city', ' S', 's', ' *', ' S', ' lin', ' military', 'hall', 'd'], 'evidence_proportions': [0.41293640136718746, 0.7467315673828125, 0.8542516708374024, 0.6762008666992188]}, 'weight': {'score': [0.003196741853441511, 0.006906998347081624, 0.000996245787693904, 0.0069812662825939195, 0.01394965878704138], 'topk_tokens': [' kitchen', '<|start_header_id|>', '.\n\n', '�', 'If', '.\n\n', ':', '<|eot_id|>', '?', ':', 'Answer', '<|end_header_id|>', '.', 'assistant', 'hall', ' \n', '\n\n', 'way', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.009456133842468262, 0.0007176339626312256, 0.0016574680805206297, 0.001329233249028524]}, 'saliency': {'score': [0.00024348639306567965, 0.00021865394487251952, 4.3953840549175555e-05, 0.00022016895153193538, 0.001099652888482077], 'topk_tokens': ['assistant', '�', '�', 'Answer', ':', '\n', '�', ' *\n\n', 'In', ' locations', '.', '\n\n', 'andra', '<|end_header_id|>', 'way', '�', '<|eot_id|>', ' \n', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0008908331394195557, 4.7320127487182616e-05, 2.0384788513183594e-05, 5.342066287994385e-05]}}, 31: {'grad': {'score': [0.30171457926432294, 0.40951524322966115, 0.2582018436529698, 0.4114813006644705, 0.5505906573513097], 'topk_tokens': [' second', ' *', ' item', ' *', ' *', 'D', ' item', ' *', ' the', ' in', '\u200d', ' second', 'f', ' the', ' the', ' it', 'I', ' the', ' an', ' an'], 'evidence_proportions': [0.5364303588867188, 0.2647247314453125, 0.17894935607910156, 0.23924732208251953]}, 'weight': {'score': [0.0011210924103146507, 0.0067439401854309765, 0.001041491062213213, 0.0068259231661877634, 0.005765949947792187], 'topk_tokens': ['.\n\n', 'Just', '<|start_header_id|>', '<|eot_id|>', '.\n\n', '.', 'If', 'Answer', '.\n\n', ':', 'assistant', '?', ':', '<|end_header_id|>', ' \n', '\n\n', '<|eot_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0021341800689697265, 0.0006184041500091553, 0.0009687185287475586, 0.0008227378129959106]}, 'saliency': {'score': [7.169729187375023e-05, 0.00013917215555771893, 5.609255570631761e-05, 0.00014029353204015764, 0.0002524016196267647], 'topk_tokens': ['\u200d', '.', 'If', '.', ' return', ' dispatch', '<|eot_id|>', '\n\n', 'If', '?', 'If', ':', ' write', '<|end_header_id|>', '<|begin_of_text|>', ' \n', '<|eot_id|>', 'way', 'assistant', 'hall'], 'evidence_proportions': [5.353689193725586e-05, 8.947849273681641e-05, 2.686977386474609e-05, 0.00010936955610911052]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-23 22:25:43.706 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:25:43.706 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-8_pid-4_0-5-6-9.pkl | len: 10 |  size: 8.64 KB
Processing depth (0, 5, 6, 9):   5%|▌         | 5/100 [01:06<20:52, 13.19s/it]Processing depth (0, 5, 6, 9):   5%|▌         | 5/100 [01:06<21:07, 13.34s/it]
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:99 - Selected idx: 9
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:101 - Answer: hallway
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:103 - Needle: [' Mary got the football there.', ' Sandra travelled to the hallway.', ' Mary moved to the bathroom.', ' Daniel went back to the kitchen.', ' Sandra picked up the milk.', ' John went back to the bedroom.', ' John moved to the bedroom.', ' Sandra went to the kitchen.', ' Mary journeyed to the office.', ' John journeyed to the office.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:104 - Real Needle: [' Sandra travelled to the hallway.', ' Sandra picked up the milk.', ' Sandra went to the kitchen.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-23 22:25:43.911 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Processing depth (0, 2, 5, 6, 8):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:25:53.577 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-23 22:25:53.578 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-23 22:25:53.578 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra picked up the milk.
2025-01-23 22:25:53.582 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (925, 930) --> . Sandra picked up the
2025-01-23 22:25:53.582 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:25:53.593 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2146, 2151) -->  Sandra went to the kitchen
2025-01-23 22:25:53.593 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra left the milk.
2025-01-23 22:25:53.605 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2361, 2365) -->  left the milk.
2025-01-23 22:25:53.605 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:25:53.618 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2612, 2618) --> . Daniel went back to the
2025-01-23 22:25:53.618 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary got the football there.
2025-01-23 22:25:53.620 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (435, 440) --> . Mary got the football
2025-01-23 22:25:53.620 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:25:53.633 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2523, 2528) --> . Mary moved to the
2025-01-23 22:25:53.633 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-23 22:25:53.646 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2612, 2618) --> . Daniel went back to the
2025-01-23 22:25:53.646 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John went back to the bedroom.
2025-01-23 22:25:53.658 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2363, 2369) -->  milk. John went back to
2025-01-23 22:25:53.658 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:25:53.679 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3299, 3304) --> . John moved to the
2025-01-23 22:25:53.679 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:25:53.683 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (808, 814) --> . Mary journeyed to the
2025-01-23 22:25:53.683 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  John journeyed to the office.
2025-01-23 22:25:53.687 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (809, 815) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:25:54.221 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:25:54.221 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4259])
your chose emoji: ['🥤', '👨🏻\u200d🎨', '🤙🏻', '👩🏽\u200d❤\u200d💋\u200d👨🏿', '💇🏻\u200d♂', '🧠', '🤾🏼\u200d♀', '👩🏿\u200d🤝\u200d👩🏽', '🚵🏿\u200d♂️', '🧑🏽\u200d🦰']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4262, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.91it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.10it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.57it/s]
2025-01-23 22:25:57.190 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.18965831756591797, 0.16877203108502464, 0.20169879228640825, 0.1683417545607568, 0.14903323805850485], 'topk_tokens': [' up', ' ', ' past', ' Sandra', '800', ' Mary', ' five', ' Papers', ' take', ' day', ' use', ' hallway', ' the', '240', ' room', 'ting', '�', 'Cut', 'user', ' em'], 'evidence_proportions': [0.2705846786499024, 0.21492919921874998, 0.125543212890625, 0.17967605590820312, 0.16124471028645831]}, 'weight': {'score': [0.0042490363121032715, 0.007464659264366261, 0.0028827587763468423, 0.007526375484671009, 0.015736012031202732], 'topk_tokens': ['pleasant', '-four', '�', ' offices', ' Pioneer', ' journey', ' Newspaper', 'itol', ' directly', '<|start_header_id|>', ' republic', '❤', ' discarded', '<|eot_id|>', 'way', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.004875612258911133, 0.0016803383827209474, 0.006423234939575195, 0.005273222923278809, 0.0033728480339050293]}, 'saliency': {'score': [7.189631462097168e-05, 7.825098312947181e-05, 4.205948267227564e-05, 7.862505059744302e-05, 0.00010800134876500006], 'topk_tokens': [' Buchanan', '<|start_header_id|>', ' would', ' great', ' location', ' and', ' location', ' discarded', '\n\n', ' location', 'system', '<|begin_of_text|>', ' republic', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', ' of', 'way', 'assistant', 'hall'], 'evidence_proportions': [0.00014690160751342773, 1.842379570007324e-05, 5.81204891204834e-05, 0.00010048598051071167, 4.6372413635253906e-05]}}, 25: {'grad': {'score': [0.25439048767089845, 0.28617906861325376, 0.3630775060409155, 0.28565397939550247, 0.24274448726488196], 'topk_tokens': [' which', ' their', ' but', ' attract', ' man', ' the', ' this', 'ISC', ' block', ' platform', ' boat', 'ides', ' revisit', 'the', ' use', ' method', ' body', ' trib', ' web', 'rich'], 'evidence_proportions': [0.1963024139404297, 0.18631591796875002, 0.22998809814453125, 0.2994537353515625, 0.3498191833496094]}, 'weight': {'score': [0.003409780263900757, 0.007463901829462217, 0.00297429851996593, 0.007529754037230057, 0.015996726634709732], 'topk_tokens': [' journey', 'itol', ' luxury', ' offices', 'street', ' possibly', '�', ' Newspaper', ' directly', 'way', ' Pioneer', '-four', ' discarded', '<|eot_id|>', '❤', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0025791645050048826, 0.0013479888439178467, 0.00516200065612793, 0.0052002668380737305, 0.0031662782033284507]}, 'saliency': {'score': [4.056692123413086e-05, 8.983317114052667e-05, 5.2602627338507235e-05, 9.047243923388986e-05, 0.0001036809838336447], 'topk_tokens': [' dropped', '❤', 'user', '<|end_header_id|>', '<|eot_id|>', '\n\n', ' Pioneer', '-known', 'system', '-four', '<|start_header_id|>', ' Min', 'way', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', 'hall', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [2.4306774139404294e-05, 1.6629695892333984e-05, 5.9223175048828126e-05, 9.337812662124634e-05, 2.331038316090902e-05]}}, 26: {'grad': {'score': [0.15992820739746094, 0.1522483950990402, 0.16826228606395233, 0.15205388889249136, 0.15077808628911557], 'topk_tokens': [' at', ' com', ' politically', '***', ' met', ' enabling', ' Pa', ' half', 'com', 'system', 'material', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', ' Team', '<|eot_id|>', ' room', ' day', ' Paul', ' hallway'], 'evidence_proportions': [0.26442680358886717, 0.12639617919921875, 0.12975311279296875, 0.17992782592773438, 0.11260223388671875]}, 'weight': {'score': [0.00623218297958374, 0.007460900731402236, 0.004238272324586526, 0.007498156675104302, 0.012497703018395798], 'topk_tokens': [' journey', ' Newspaper', ' discarded', ' directly', 'itol', ' offices', ' Pioneer', 'way', '❤', ' luxury', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006883382797241211, 0.00455535650253296, 0.006543445587158202, 0.008971452713012695, 0.0050013065338134766]}, 'saliency': {'score': [0.00016989588737487794, 7.93387958668924e-05, 3.548004688360752e-05, 7.920696259453616e-05, 8.40014089708743e-05], 'topk_tokens': [' *\n\n', '\n', 'assistant', 'user', ' devil', '\n', ' type', ' tie', '\n', '\n', ' for', '<|begin_of_text|>', ' *\n\n', '<|start_header_id|>', ' reached', '\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0005529463291168213, 0.00010578036308288573, 6.150007247924804e-05, 8.252263069152832e-05, 5.269547303517659e-05]}}, 27: {'grad': {'score': [0.30223297119140624, 0.2569356315967232, 0.24549002525133964, 0.2567722073846229, 0.18860153529954993], 'topk_tokens': ['\n', ' the', ' thought', '.', ' tie', '\n', ' Min', '\n', ',', ' of', '\n', '\n', '\n', ' of', ' to', '\n', '\n', ' Press', ' the', ' other'], 'evidence_proportions': [0.37366638183593753, 0.32958984375, 0.23645782470703125, 0.3683319091796875, 0.23065439860026044]}, 'weight': {'score': [0.0039693105220794675, 0.007469368338976484, 0.003097204061654898, 0.007530829952182288, 0.016803916057814724], 'topk_tokens': [' discarded', '<|start_header_id|>', 'estead', 'eward', ' states', ' republic', 'street', ' offices', ' luxury', '�', ' Newspaper', '-four', 'itol', 'way', ' Pioneer', '❤', 'assistant', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0033748626708984372, 0.0022984445095062256, 0.00517725944519043, 0.005843102931976318, 0.003601253032684326]}, 'saliency': {'score': [0.0001228511333465576, 0.00017710445437393296, 8.738881502396021e-05, 0.00017826101534590373, 0.0002637845666512199], 'topk_tokens': [' states', "'clock", ' republic', ' then', ' luxury', ' for', '<|eot_id|>', '�', 'itol', ' Pioneer', '❤', '-four', '\n', '<|end_header_id|>', ' Newspaper', '<|begin_of_text|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.00011693835258483887, 0.00014030933380126953, 0.00014767646789550783, 0.00017903000116348267, 5.5089592933654785e-05]}}, 28: {'grad': {'score': [0.3659283447265625, 0.2953439226513007, 0.3570582561003856, 0.2943502424330073, 0.2674426099528437], 'topk_tokens': [',', '\n', 'ot', 'ORE', 'ot', 'rich', ' one', ' wonderful', ' getting', ' type', ' MO', ' const', ' got', ' favor', 'com', ' FR', ' com', ' trib', ' got', 'ern'], 'evidence_proportions': [0.5107177734374999, 0.45909423828124996, 0.3075347900390625, 0.3344154357910156, 0.2373021443684896]}, 'weight': {'score': [0.0031006228923797607, 0.007307974512394691, 0.0022243979649666026, 0.007380257331796803, 0.010560528091762377], 'topk_tokens': ['<|eot_id|>', 'hours', ' forthcoming', ' republic', ' luxury', '<|start_header_id|>', ' possibly', ' Pioneer', '<|eot_id|>', 'itol', 'street', ' Newspaper', '❤', '-four', '\n\n', 'assistant', 'way', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002732515335083008, 0.0013485372066497803, 0.0037469863891601562, 0.004771590232849121, 0.0032148361206054688]}, 'saliency': {'score': [4.493355751037598e-05, 0.00014388209522637555, 2.9921531677246094e-05, 0.00014553006459781817, 8.451420327891474e-05], 'topk_tokens': ['estead', '<|eot_id|>', 'cery', ' possibly', 'Today', ' speech', ' *\n\n', '❤', ' waving', 'tele', 'street', '-four', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'way', '<|begin_of_text|>', '\n\n', 'hall', '<|end_header_id|>'], 'evidence_proportions': [3.566741943359375e-05, 2.923011779785156e-05, 4.734396934509277e-05, 9.318441152572632e-05, 3.1565626462300614e-05]}}, 29: {'grad': {'score': [0.27852386474609375, 0.3421143594900245, 0.28860879555726665, 0.34299012875204826, 0.2985289718793786], 'topk_tokens': [' for', ' a', '\n', '\n', '\n', '\n', ' other', ' a', ' a', ' thought', ' a', ' tie', '\n', '\n', '\n', ' to', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.29285125732421874, 0.128741455078125, 0.30135498046875, 0.26209259033203125, 0.383331298828125]}, 'weight': {'score': [0.004933712482452393, 0.007267099207748778, 0.0035022031038235393, 0.007315971436473288, 0.009432583075502644], 'topk_tokens': [' in', '\n\n', '-four', ' possibly', '�', 'street', ' luxury', ' Newspaper', ' idol', ' Pioneer', ' offices', '<|start_header_id|>', '❤', '<|eot_id|>', '<|eot_id|>', 'way', 'assistant', 'hall', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004296255111694336, 0.0018417954444885254, 0.003725576400756836, 0.00900721549987793, 0.0063326358795166016]}, 'saliency': {'score': [0.0004053449630737305, 0.00014067177255631388, 0.00031156570483476686, 0.00013750795809185578, 0.0001193437239398127], 'topk_tokens': ['\n', '      ', ' *\n\n', '      ', '<|start_header_id|>', ' to', '<|start_header_id|>', '.', ' idol', '      ', ' *\n\n', ' *\n\n', 'way', ' S', ' *', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'hall', 'assistant'], 'evidence_proportions': [0.0002129554748535156, 2.306103706359863e-05, 0.0002942204475402832, 0.0006201937794685364, 0.0008336106936136882]}}, 30: {'grad': {'score': [0.91173828125, 0.8088578466292784, 0.9878606551732773, 0.8065822105171454, 0.772168667420097], 'topk_tokens': ['str', ' M', 'Min', ' of', ' conducted', 'of', ' Press', ' of', ' the', ' Minnesota', ' INCIDENT', ' awarded', 'antic', ' trials', 'S', ' *', 'action', ' leading', 'constitutional', '.'], 'evidence_proportions': [1.052203369140625, 1.0380523681640625, 0.838531494140625, 0.8234519958496094, 0.8092854817708334]}, 'weight': {'score': [0.005779275894165039, 0.007164992731202661, 0.005120196403601231, 0.007192241415743489, 0.005292619052140609], 'topk_tokens': [' the', ' in', 'Question', '\n\n', ' for', ' and', ' to', '<|end_header_id|>', ' second', '<|eot_id|>', '***', 'assistant', '<|start_header_id|>', '<|start_header_id|>', 'hall', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015301942825317382, 0.0024456739425659183, 0.006060028076171875, 0.018758833408355713, 0.003211180369059245]}, 'saliency': {'score': [0.0006338131427764893, 0.00020479924490842278, 0.00032436465605711326, 0.00020113359496047123, 5.99428363468336e-05], 'topk_tokens': [' in', 'Question', ' the', ' course', ' the', ' for', ' and', ' second', ' to', '<|eot_id|>', ' S', 'assistant', '<|start_header_id|>', '***', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>', 'hall', 'way', '<|end_header_id|>'], 'evidence_proportions': [7.518529891967774e-05, 0.00018599629402160645, 0.0015932381153106689, 0.0015220940113067627, 8.080899715423584e-05]}}, 31: {'grad': {'score': [0.308953857421875, 0.35858181506353387, 0.26499704214242786, 0.35974677577934244, 0.4058427396027938], 'topk_tokens': ['\n', '\n', '\n', 'G', '\n', '\n', '<|eot_id|>', '\n', '\n', '\n', ',', ',', '<|end_header_id|>', ',', ',', '\n', ',', ',', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.25458984375, 0.1578094482421875, 0.38256225585937503, 0.4254188537597656, 0.3412272135416667]}, 'weight': {'score': [0.0036129283905029295, 0.006328082655249264, 0.003310900468092698, 0.0063722820268352015, 0.0036116806061371512], 'topk_tokens': [',', ' the', ' the', ' discarded', ' the', 'Answer', ' to', ' and', ' dropped', 'Question', ' that', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.0026447296142578123, 0.002668476104736328, 0.003734636306762695, 0.008558511734008789, 0.001808325449625651]}, 'saliency': {'score': [0.00017808079719543456, 0.00014319655090808196, 9.057842768155612e-05, 0.000143477637294135, 5.797815063725347e-05], 'topk_tokens': [' the', ' in', ' to', '<|start_header_id|>', ' second', 'two', ' lengthy', ' arranged', 'Question', ',', '.\n\n', ' and', ' the', 'assistant', ' that', '<|eot_id|>', '<|begin_of_text|>', 'way', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.00023946762084960937, 0.000145721435546875, 0.0001471996307373047, 0.000382058322429657, 4.36405340830485e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:25:57.198 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:25:57.199 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-9_pid-0_0-2-5-6-8.pkl | len: 10 |  size: 9.92 KB
Processing depth (0, 2, 5, 6, 8):   1%|          | 1/100 [00:13<21:47, 13.20s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.26it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Processing depth (4, 5, 6, 8, 9):   1%|          | 1/100 [00:23<21:47, 13.20s/it]2025-01-23 22:26:07.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-23 22:26:07.550 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1779, 1784) -->  Sandra travelled to the hallway
2025-01-23 22:26:07.550 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra picked up the milk.
2025-01-23 22:26:07.560 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2082, 2087) --> . Sandra picked up the
2025-01-23 22:26:07.560 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:26:07.572 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2335, 2340) --> . Sandra went to the
2025-01-23 22:26:07.572 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra left the milk.
2025-01-23 22:26:07.588 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3308, 3312) -->  Sandra left the milk
2025-01-23 22:26:07.588 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:26:07.601 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2589, 2595) --> . Daniel went back to the
2025-01-23 22:26:07.602 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary got the football there.
2025-01-23 22:26:07.604 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (470, 475) --> . Mary got the football
2025-01-23 22:26:07.604 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:26:07.617 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2500, 2505) --> . Mary moved to the
2025-01-23 22:26:07.617 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-23 22:26:07.629 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2339, 2345) -->  the kitchen. John went back
2025-01-23 22:26:07.629 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John went back to the bedroom.
2025-01-23 22:26:07.640 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2339, 2345) -->  the kitchen. John went back
2025-01-23 22:26:07.641 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:26:07.657 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3260, 3265) --> . John moved to the
2025-01-23 22:26:07.657 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:26:07.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (803, 809) --> . Mary journeyed to the
2025-01-23 22:26:07.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  John journeyed to the office.
2025-01-23 22:26:07.665 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (804, 810) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:26:08.143 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the kitchen<|eot_id|>
2025-01-23 22:26:08.143 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4224])
your chose emoji: ['🔜', '🤙🏿', '\U0001fac5🏽', '👳🏼\u200d♀', '🙅🏿\u200d♀️', '🙇🏿\u200d♀', '🦕', '🎪', '⭕', '🧏🏻\u200d♀️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4227, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 200924.74it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.15it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.47it/s]
2025-01-23 22:26:11.254 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.2923546600341797, 0.27910033371083587, 0.29658992473895734, 0.2788568909512441, 0.3620184536637931], 'topk_tokens': ['half', ' secret', ' candidate', ' carrier', ' Sandra', ' president', ' old', ' senate', ' tele', ' papers', ' tele', ' idol', ' dollars', ' tele', ' hallway', ' who', '�', ' day', '�', '�'], 'evidence_proportions': [0.34202423095703127, 0.2728763580322266, 0.2251373291015625, 0.4390525817871094, 0.22541109720865884]}, 'weight': {'score': [0.005700268745422363, 0.00752906221742181, 0.004895639725220509, 0.007564715187394387, 0.00763859368603805], 'topk_tokens': ['<|end_header_id|>', 'assistant', ' world', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'system', ' of', ' Adams', '<|start_header_id|>', '<|eot_id|>', ' directly', 'tele', 'user', ' Newspaper', ' extortion', '<|end_header_id|>', 'ors', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00729891061782837, 0.003191971778869629, 0.0030168890953063965, 0.00836271047592163, 0.006919503211975098]}, 'saliency': {'score': [8.008837699890137e-05, 7.796076727669263e-05, 7.288578229072767e-05, 7.79955340726084e-05, 0.00010837717302914323], 'topk_tokens': ['tele', ' Adams', '\n\n', ' Newspaper', ' extortion', ' of', '<|end_header_id|>', ' Project', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'ors', ' acquaintance', 'user', '<|eot_id|>', '<|start_header_id|>', 'hall', 'system', '<|end_header_id|>', '***'], 'evidence_proportions': [8.774995803833007e-05, 4.274249076843262e-05, 2.428889274597168e-05, 0.0001326426863670349, 0.00011628866195678711]}}, 25: {'grad': {'score': [0.3090361022949219, 0.34305712752007184, 0.3311204176682692, 0.3433732594717531, 0.30088766689958246], 'topk_tokens': [' of', '\n', 'city', ' Good', ' and', ' an', ' crack', ' bend', ' country', ' Bor', ' Bench', ' or', ' composing', 'MIN', ' method', ' the', 'up', ' up', ' remin', '\n'], 'evidence_proportions': [0.3996864318847656, 0.2923248291015625, 0.24341430664062502, 0.4198455810546875, 0.22823206583658853]}, 'weight': {'score': [0.006612696647644043, 0.007530470054595781, 0.004637278807468903, 0.007563085666849383, 0.008738441713925066], 'topk_tokens': ['-known', '<|start_header_id|>', '\n\n', 'system', ' of', 'street', ' Adams', ' day', '<|eot_id|>', 'assistant', ' extortion', 'user', '<|eot_id|>', ' directly', 'tele', ' Newspaper', 'ors', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.004287803173065186, 0.003266274929046631, 0.004371249675750733, 0.009798064827919006, 0.011083086331685385]}, 'saliency': {'score': [0.00015240907669067383, 0.00010052405831829041, 6.984288875873272e-05, 0.00010049990269818789, 0.0001611293389879424], 'topk_tokens': ['<|eot_id|>', ' acquaintance', '<|start_header_id|>', ' secret', '<|start_header_id|>', '\n\n', 'user', ' tie', ' Southern', ' Min', 'RE', ' Newspaper', 'tele', '<|eot_id|>', 'PA', 'system', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [7.929205894470215e-05, 4.908442497253418e-05, 3.44395637512207e-05, 0.00032261013984680176, 0.0002842843532562256]}}, 26: {'grad': {'score': [0.21345077514648436, 0.19753247512156782, 0.1953299106695713, 0.19745751536009892, 0.19868199578646956], 'topk_tokens': [' Gal', ' enabling', ' half', 'half', ' day', ' material', 'ching', 'material', ' a', 'ball', '<|eot_id|>', '\n', ' Team', ' Hale', ' Herald', ' cap', 'room', '-per', ' Paul', ' hallway'], 'evidence_proportions': [0.29612274169921876, 0.1802032470703125, 0.09051742553710937, 0.27521705627441406, 0.23353068033854166]}, 'weight': {'score': [0.011057651042938233, 0.007520518179957343, 0.005745381116867065, 0.007515906607506225, 0.009910184761573529], 'topk_tokens': [' acquaintance', 'assistant', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'tele', ' extortion', '<|end_header_id|>', ' Newspaper', ' Adams', ' directly', '<|start_header_id|>', 'user', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'ors', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.008377182483673095, 0.005578947067260743, 0.007471561431884766, 0.023834228515625, 0.012327651182810467]}, 'saliency': {'score': [0.00030296206474304197, 8.95774544963381e-05, 5.429524641770583e-05, 8.862655150784399e-05, 0.00011624806913836249], 'topk_tokens': [' secret', ' Ear', ' contract', '\n\n', ' Pioneer', ' state', ' directly', ' fall', '<|start_header_id|>', '<|start_header_id|>', ' Knowledge', 'hall', 'assistant', ' acquaintance', ' Hale', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0005107879638671875, 9.012818336486816e-05, 7.42197036743164e-05, 0.0005950257182121277, 0.0003030449151992798]}}, 27: {'grad': {'score': [0.279361572265625, 0.40026465606147976, 0.2854258219401042, 0.4020665517198103, 0.28153996632016937], 'topk_tokens': [' work', '\n', 'that', '\n', ',', '\n', 'action', '\n', '\n', ' as', '\n', ',', '\n', ' on', ',', '\n', '\n', '\n', '\n', '--'], 'evidence_proportions': [0.46427001953125, 0.23346328735351562, 0.13292388916015624, 0.26769065856933594, 0.293331782023112]}, 'weight': {'score': [0.007601348161697388, 0.00752894580011704, 0.005966784862371592, 0.007543145708244007, 0.011019114790291622], 'topk_tokens': ['<|eot_id|>', 'user', '<|start_header_id|>', ' news', ' world', '�', ' combined', ' luxury', 'street', 'assistant', ' of', ' directly', 'tele', ' Adams', ' Newspaper', ' extortion', '<|end_header_id|>', 'ors', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.005706924200057984, 0.004623091220855712, 0.005143117904663086, 0.010110616683959961, 0.01203759511311849]}, 'saliency': {'score': [0.0003377330303192139, 0.00013613595109699972, 0.00012813241053850224, 0.0001350002823728167, 0.0002647725672557436], 'topk_tokens': [' secret', '\n\n', 'graph', ' extortion', ' Adams', ' Southern', ' Daniel', '<|eot_id|>', 'tele', 'RE', 'ors', '<|start_header_id|>', 'user', ' Pioneer', ' Daniel', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [8.535981178283692e-05, 0.00014829635620117188, 0.00011780261993408204, 0.00048060715198516846, 0.0007939338684082031]}}, 28: {'grad': {'score': [0.5457962036132813, 0.4661159005491114, 0.5690483191074469, 0.4646731016299715, 0.43987628509258403], 'topk_tokens': ['ANK', 'PA', 'RI', '�', 'd', ' trib', ' press', 'de', ' mess', ' It', '�', ' MO', ' FR', 'g', ' Gus', ' du', ' com', ' Fletcher', ' com', 'com'], 'evidence_proportions': [0.6947967529296875, 0.4705718994140625, 0.5628402709960938, 0.4565086364746094, 0.5296376546223959]}, 'weight': {'score': [0.0052132165431976315, 0.0073143738168391995, 0.0033769546410976313, 0.007363878568147141, 0.006457945396160257], 'topk_tokens': ['0', ' about', '<|eot_id|>', '<|eot_id|>', '�', 'street', ' extortion', ' of', ' directly', '\n\n', 'tele', ' Adams', ' Newspaper', '<|start_header_id|>', 'assistant', 'way', 'ors', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.004476612806320191, 0.0034318804740905763, 0.0038838624954223634, 0.006038069725036621, 0.00786939263343811]}, 'saliency': {'score': [8.704900741577149e-05, 0.00012441562201809256, 3.98403558975611e-05, 0.00012543234090921885, 7.839346754139867e-05], 'topk_tokens': ['Question', ' directly', '�', ' from', 'Answer', ' world', '<|start_header_id|>', 'assistant', ' president', '<|eot_id|>', 'ierce', 'ors', 'ing', '<|start_header_id|>', 'Just', '\n\n', 'way', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [9.536147117614747e-05, 2.396106719970703e-05, 2.0617246627807616e-05, 0.0001950785517692566, 0.0001160353422164917]}}, 29: {'grad': {'score': [0.4223931121826172, 0.3721699729325615, 0.34921614329020184, 0.3720834057634048, 0.40957580763718177], 'topk_tokens': [' sound', ' got', ' up', 'out', ' com', ' be', ' not', ' an', ' of', ' ', ' times', ' to', ' of', ' hallway', ' to', ' to', ' of', ' a', ' in', ' were'], 'evidence_proportions': [0.4184051513671875, 0.36383056640625, 0.4764434814453125, 0.5572052001953125, 0.33960183461507165]}, 'weight': {'score': [0.009706752300262451, 0.007330587859522628, 0.008471676936516395, 0.0073056283147180994, 0.008251658801374764], 'topk_tokens': ['?\n', ',', '<|start_header_id|>', '0', ' world', ' the', '<|start_header_id|>', '<|eot_id|>', ' extortion', 'tele', 'way', ' Newspaper', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'ors', '<|end_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.00856117010116577, 0.006024217605590821, 0.017023324966430664, 0.006479740142822266, 0.009784380594889322]}, 'saliency': {'score': [0.00023080348968505858, 0.00015212327192544318, 0.00044942360657912033, 0.00014886559272883404, 0.00029686247480326686], 'topk_tokens': [' secret', '<|start_header_id|>', '      ', '<|eot_id|>', ' Daniel', '2', '\n', ' Pioneer', 'user', ' *', '      ', '      ', ' Newspaper', ' John', 'assistant', 'ors', '<|begin_of_text|>', '<|end_header_id|>', 'system', 'hall'], 'evidence_proportions': [0.00011807084083557128, 5.642175674438477e-05, 0.00022780895233154297, 0.0006338804960250854, 0.00020384291807810467]}}, 30: {'grad': {'score': [0.8042628479003906, 0.5933638483123743, 0.8105550912710336, 0.5900626392167491, 0.49602621999280205], 'topk_tokens': [' had', 'first', ' at', ' imported', ' both', ' represented', ' city', ' efforts', ' United', ' of', ' Daniel', ' him', ' extraordinary', ' Stephen', ' Becker', ' Hon', ' Min', ' Sh', ' hallway', ' had'], 'evidence_proportions': [1.180908203125, 0.6828163146972657, 0.662200927734375, 0.5159759521484375, 0.9021733601888021]}, 'weight': {'score': [0.009730247259140014, 0.007014356946618049, 0.007154095631379347, 0.006996738146109104, 0.010458744805434654], 'topk_tokens': [',', ',', ',', '<|end_header_id|>', ' Daniel', '***', '<|eot_id|>', 'ors', ' old', ' secret', '?\n', '<|eot_id|>', 'hall', '<|eot_id|>', '<|start_header_id|>', 'way', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0044872224330902094, 0.004181599617004395, 0.00821373462677002, 0.011368274688720703, 0.018895049889882404]}, 'saliency': {'score': [0.00037467241287231446, 0.00020864969707702895, 0.0002244756771967961, 0.00020750442176606257, 0.000530888294351512], 'topk_tokens': [' Daniel', ' state', ',', 'RE', '<|eot_id|>', '<|eot_id|>', '-text', ' Southern', 'Good', '�', '?\n', 'way', 'hall', '<|start_header_id|>', ' secret', '<|begin_of_text|>', '<|start_header_id|>', 'assistant', ' old', '<|end_header_id|>'], 'evidence_proportions': [0.00014597177505493164, 0.00019121170043945312, 0.0003075778484344482, 0.00017733126878738403, 0.0009056131045023601]}}, 31: {'grad': {'score': [0.5417620849609375, 0.5432880546907344, 0.440741954705654, 0.5442578954648548, 0.47272586822509766], 'topk_tokens': [' management', ' and', ' old', ' as', 'nes', ' own', ' and', ' delivered', ' and', 'out', ' and', ' D', ' printed', ' and', ' attempted', 'D', 'd', 'f', 'nes', '<|start_header_id|>'], 'evidence_proportions': [0.27268218994140625, 0.6235687255859376, 0.69981689453125, 0.8008880615234375, 0.3933601379394531]}, 'weight': {'score': [0.00413832426071167, 0.006264567459788705, 0.004847311056577242, 0.006290613359308804, 0.00484243343616354], 'topk_tokens': [' else', ' left', ' dropped', 'Answer', 'Question', 'ors', ' and', ',', ' directly', 'Just', '?\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'way', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.0017423033714294434, 0.0015495061874389647, 0.0039158821105957035, 0.003794759511947632, 0.00870676835378011]}, 'saliency': {'score': [0.00018900513648986817, 0.00016059860259273357, 0.0002401578120696239, 0.0001596826831795635, 0.00016437419529618887], 'topk_tokens': [' dropped', ' It', ' Pioneer', ' return', 'system', '<|eot_id|>', ' Do', '-text', '<|end_header_id|>', 'Just', ' set', ' the', '?\n', 'assistant', ' dropped', ' it', '<|start_header_id|>', '<|begin_of_text|>', 'way', 'hall'], 'evidence_proportions': [0.00012148618698120117, 4.370808601379394e-05, 0.00022050738334655762, 0.0004016011953353882, 0.0001983692248662313]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-23 22:26:11.260 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:26:11.260 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-9_pid-1_4-5-6-8-9.pkl | len: 10 |  size: 9.97 KB
Processing depth (4, 5, 6, 8, 9):   2%|▏         | 2/100 [00:27<22:23, 13.71s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.00it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (0, 1, 5, 8, 9):   2%|▏         | 2/100 [00:37<22:23, 13.71s/it]2025-01-23 22:26:21.271 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-23 22:26:21.271 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-23 22:26:21.271 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra picked up the milk.
2025-01-23 22:26:21.273 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (429, 434) --> . Sandra picked up the
2025-01-23 22:26:21.274 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:26:21.284 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2134, 2139) -->  Sandra went to the kitchen
2025-01-23 22:26:21.284 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra left the milk.
2025-01-23 22:26:21.301 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3337, 3341) -->  Sandra left the milk
2025-01-23 22:26:21.301 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:26:21.315 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2705, 2711) -->  went back to the kitchen.
2025-01-23 22:26:21.315 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary got the football there.
2025-01-23 22:26:21.317 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (478, 483) --> . Mary got the football
2025-01-23 22:26:21.317 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:26:21.330 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2573, 2578) --> . Mary moved to the
2025-01-23 22:26:21.330 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-23 22:26:21.344 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2704, 2710) -->  Daniel went back to the kitchen
2025-01-23 22:26:21.344 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John went back to the bedroom.
2025-01-23 22:26:21.356 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2458, 2464) --> . John went back to the
2025-01-23 22:26:21.356 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:26:21.373 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3289, 3294) --> . John moved to the
2025-01-23 22:26:21.373 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:26:21.377 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (887, 893) --> . Mary journeyed to the
2025-01-23 22:26:21.377 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  John journeyed to the office.
2025-01-23 22:26:21.382 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (888, 894) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:26:21.876 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - kitchen<|eot_id|>
2025-01-23 22:26:21.876 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4252])
your chose emoji: ['🔒', '👩🏻\u200d❤\u200d💋\u200d👩🏼', '👨\u200d✈️', '👩🏻\u200d⚕', '👩🏽\u200d🎤', '⛪', '🧖🏼\u200d♀️', '🙅🏾\u200d♀', '👨🏻\u200d🎨', '👨\u200d⚖']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4255, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 176602.27it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.15it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.03it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.06it/s]
2025-01-23 22:26:25.053 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5889992523193359, 0.5193263554825206, 0.49744041149432844, 0.5191144083087241, 0.6377082151525161], 'topk_tokens': [' convention', 'ly', 'ace', ' hopes', ' H', ' hung', 'enan', ' Hoe', 'estead', 'hand', 'out', ' Hoe', ' hallway', 'ANK', ' hidden', ' hand', 'hand', 'hall', ' hand', 'way'], 'evidence_proportions': [0.7185192108154297, 0.7564697265625, 0.492144775390625, 0.6438522338867188, 0.385650634765625]}, 'weight': {'score': [0.016129159927368165, 0.007311768089142025, 0.06880279993399596, 0.006686956579262532, 0.0035765914356007296], 'topk_tokens': ['andra', 'ot', 'Answer', '\n\n', ' the', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'hall', ' hallway', '<|eot_id|>', 'assistant', 'nes', 'way', ',', '<|end_header_id|>', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.06574745178222656, 0.00435519814491272, 0.003850352764129639, 0.00435832142829895, 0.0026717831691106158]}, 'saliency': {'score': [0.0008863961696624756, 0.0001571481432393631, 0.0004944358116541153, 0.0001496593768998849, 0.00012840558500850903], 'topk_tokens': [' *', 'user', ' ', 'In', 'assistant', ' *', 'Question', 'Answer', ' Where', ' *', 'andra', 'ian', ' the', ' hallway', '\n\n', 'tele', '<|end_header_id|>', '<|begin_of_text|>', 'way', 'hall'], 'evidence_proportions': [0.0039057612419128415, 5.7864189147949216e-05, 0.00014914870262145994, 0.00021488219499588013, 0.00012275079886118573]}}, 25: {'grad': {'score': [1.12386474609375, 0.7858979096283784, 1.1149953206380208, 0.7808194152497073, 0.858554817648495], 'topk_tokens': [' reached', ' at', ' still', ' at', '<|end_header_id|>', 'still', ' at', ' at', ' Ramsey', ' at', ' at', ' at', ' custom', ' laid', ' double', ' curs', ' At', ' laid', '\n\n', ' customary'], 'evidence_proportions': [0.812213134765625, 0.896533203125, 1.28125, 1.2418975830078125, 1.3631744384765625]}, 'weight': {'score': [0.0035313665866851807, 0.007302507250345412, 0.0898934740286607, 0.006556439679893779, 0.0043843416606678684], 'topk_tokens': ['\n\n', 'andra', 'hall', '<|start_header_id|>', 'Answer', '�', 'ot', '<|eot_id|>', ' the', 'assistant', '<|eot_id|>', 'way', ',', 'nes', '\n\n', '<|end_header_id|>', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.008201026916503906, 0.0039572477340698246, 0.001131284236907959, 0.004142329096794128, 0.0008778423070907593]}, 'saliency': {'score': [9.968638420104981e-05, 0.0001774048973334522, 0.0006293883690467248, 0.0001736624987248844, 0.00013724039582645192], 'topk_tokens': ['nes', ' PA', 'S', ' return', ' OF', 'hall', ' Do', 'MIN', ',', 'Answer', 'way', '<|eot_id|>', 'ian', '<|eot_id|>', ' the', '<|start_header_id|>', 'tele', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00021135210990905762, 6.820559501647949e-05, 1.7601251602172852e-05, 0.00020619481801986694, 3.026425838470459e-05]}}, 26: {'grad': {'score': [0.5633758544921875, 0.6032909352967097, 0.57129884377504, 0.6038267426432757, 0.8319949430577895], 'topk_tokens': [' until', ':', ' morning', ' Democratic', '�', '�', ' secret', '�', ' method', 'Democratic', '3', '�', '�', '�', '�', '�', '�', '�', '�', '�'], 'evidence_proportions': [0.62276611328125, 0.568402099609375, 0.48155975341796875, 0.4843120574951172, 0.630584716796875]}, 'weight': {'score': [0.01171473503112793, 0.006998940052072814, 0.0774589784634419, 0.0063151327572697185, 0.007119547619539149], 'topk_tokens': ['.\n\n', 'Answer', ' the', ':', '?\n', '\n\n', ' hallway', 'assistant', '<|start_header_id|>', 'hall', '<|eot_id|>', ',', 'nes', '<|eot_id|>', 'way', '<|end_header_id|>', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.037273597717285153, 0.005432844161987305, 0.004183125495910644, 0.006946533918380737, 0.005105733871459961]}, 'saliency': {'score': [0.0008135271072387695, 0.00020736076296706598, 0.0004452314132299179, 0.0002015313394483249, 0.00030088003943948185], 'topk_tokens': ['<|start_header_id|>', ':', '�', '<|eot_id|>', 'Answer', 'ot', '\n\n', 'tele', ' the', '\n\n', ' the', ' hallway', 'nes', ',', 'ian', '<|end_header_id|>', '<|eot_id|>', 'way', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0032966017723083498, 0.00020980238914489744, 9.146332740783691e-05, 0.0002899244427680969, 0.00019819041093190512]}}, 27: {'grad': {'score': [0.480328369140625, 0.5166660546415981, 0.3873885717147436, 0.5180858265269886, 0.568959269804113], 'topk_tokens': ['.', '.', '.', '.', '.', '.', '.', ' one', '.', '.', '.', '.', ' constitutional', '.', '.', ' Bank', ' mob', '.', '.', ' Bench'], 'evidence_proportions': [0.6637924194335938, 0.4737030029296875, 0.40127105712890626, 0.2643890380859375, 0.5428034464518229]}, 'weight': {'score': [0.007400467395782471, 0.007297431596157554, 0.10247289828765087, 0.0064111469156615235, 0.006633736105526195], 'topk_tokens': ['?\n', 'RE', '<|start_header_id|>', '\n\n', ' hallway', ' the', ':', '<|eot_id|>', 'Answer', 'assistant', 'hall', '<|eot_id|>', 'nes', '<|end_header_id|>', ',', 'way', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.025206565856933594, 0.002749180793762207, 0.0020755648612976072, 0.005240082740783691, 0.002315799395243327]}, 'saliency': {'score': [0.00039929509162902834, 0.00012483183561564894, 0.0004884081009106758, 0.00011981130215183587, 0.00031224664519814883], 'topk_tokens': ['?\n', 'As', 'If', '"The', ',', 'In', 'nes', '<|begin_of_text|>', ' Where', ' hallway', 'NEW', ':', '\n\n', '<|end_header_id|>', 'ian', 'tele', 'RE', ' the', 'way', 'hall'], 'evidence_proportions': [0.001775306463241577, 4.398822784423828e-05, 4.094839096069336e-05, 0.00011350959539413452, 3.785391648610433e-05]}}, 28: {'grad': {'score': [0.561341552734375, 0.46738608300896006, 0.5111061487442408, 0.46641877942823673, 0.6954212861902573], 'topk_tokens': ['.', ' *\n\n', ' trib', ' election', ' *\n\n', '      ', ',', ' pur', '.', '�', ' pleasure', '�', '\n\n', '<|end_header_id|>', '.\n\n', ' so', 'ex', ' make', '.', 'hall'], 'evidence_proportions': [0.59241943359375, 0.5264892578125, 0.583203125, 0.49274444580078125, 0.5920003255208333]}, 'weight': {'score': [0.003562459945678711, 0.00706447439944562, 0.12027381627987592, 0.006031877412570743, 0.003619256790946512], 'topk_tokens': ['.', '.\n\n', ' the', 'hall', '<|eot_id|>', '<|start_header_id|>', 'Answer', ':', '\n\n', 'assistant', '<|eot_id|>', '?\n', 'nes', 'way', ',', '<|end_header_id|>', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.007946014404296875, 0.001320958137512207, 0.0013528823852539064, 0.003300398588180542, 0.003793438275655111]}, 'saliency': {'score': [0.0001682555675506592, 0.0001315468486691754, 0.0014737385969895583, 0.00011883790184108376, 0.0001461470828336828], 'topk_tokens': ['nes', 'In', ':', '<|start_header_id|>', 'From', 'During', ',', 'way', 'Just', ':', '<|eot_id|>', 'assistant', '?\n', 'hall', 'ian', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'tele', ' the'], 'evidence_proportions': [0.000531548261642456, 4.0411949157714844e-05, 5.1361322402954104e-05, 6.601214408874512e-05, 0.0001376221577326457]}}, 29: {'grad': {'score': [0.5110824584960938, 0.5032523346843787, 0.4329024094801683, 0.5038602788475072, 0.6846575568704044], 'topk_tokens': ['SP', '186', ' had', 'SP', '186', '50', ' had', ' *', 'P', '186', ' P', ' *', '26', ' Hoe', '186', 'u', ' H', '25', '1', '***'], 'evidence_proportions': [0.5611068725585937, 0.4647308349609375, 0.5765380859375, 0.3745231628417969, 0.5445149739583334]}, 'weight': {'score': [0.0037720179557800292, 0.007227801716285643, 0.13711940478055906, 0.006039689588990481, 0.0030949042123906752], 'topk_tokens': ['<|start_header_id|>', 'Answer', ' hallway', ' the', 'If', '<|eot_id|>', 'hall', '?\n', ':', 'assistant', '\n\n', '<|end_header_id|>', 'nes', '<|eot_id|>', ',', 'way', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.015371417999267578, 0.0004149138927459717, 0.0007425069808959961, 0.0011994540691375732, 0.0011430730422337851]}, 'saliency': {'score': [3.545880317687988e-05, 0.00010402945738141321, 0.0012650589148203533, 9.363435299463074e-05, 8.07046890258789e-05], 'topk_tokens': ['Answer', 'As', 'hall', 'Question', '?\n', '<|start_header_id|>', ',', '<|eot_id|>', 'nes', 'assistant', ':', 'If', '<|end_header_id|>', 'way', '<|eot_id|>', 'ian', '\n\n', '<|begin_of_text|>', ' the', 'tele'], 'evidence_proportions': [9.716153144836427e-05, 5.8710575103759766e-06, 1.3464689254760743e-05, 2.024322748184204e-05, 3.716846307118734e-05]}}, 30: {'grad': {'score': [0.4784234619140625, 0.4509195437582623, 0.4592393728402945, 0.450678056932184, 0.543084716796875], 'topk_tokens': ['      ', 'cont', ' lyn', ' body', '      ', ' wield', '�', ' galaxy', '      ', ' gang', ' genu', 's', ' web', '186', '.', 'lyn', ' military', ' lin', 'ex', ' time'], 'evidence_proportions': [0.350201416015625, 0.5420257568359375, 0.47144775390625, 0.356292724609375, 0.6195068359375]}, 'weight': {'score': [0.007880263328552246, 0.006976401063446152, 0.0724254663173969, 0.006361962957616582, 0.013945662274080164], 'topk_tokens': [':', '<|start_header_id|>', '.\n\n', 'Answer', 'nes', '?\n', ':', 'assistant', ',', '<|start_header_id|>', 'hall', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', 'way', 'ian', ' the', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.015483665466308593, 0.004096484184265137, 0.004152441024780274, 0.010168731212615967, 0.006278117497762044]}, 'saliency': {'score': [0.0004926764965057373, 0.00025978248071726565, 0.0013645589351654053, 0.0002481125613380747, 0.0006685453302720014], 'topk_tokens': [' Where', ' printers', ' *\n\n', '�', '\n\n', 'In', ':', ' hallway', ',', ' location', 'andra', 'nes', '<|eot_id|>', 'way', '<|begin_of_text|>', '<|end_header_id|>', 'ian', ' the', 'tele', 'hall'], 'evidence_proportions': [0.001391524076461792, 0.00021225214004516602, 9.484291076660155e-05, 0.0007234513759613037, 0.00015500187873840332]}}, 31: {'grad': {'score': [0.525078125, 0.5389436141304348, 0.3953449542705829, 0.5403626036228698, 0.454076867945054], 'topk_tokens': [' time', ' enough', ' time', ' convention', ' the', ' D', 'I', '1', ' would', ' morning', ' the', ' value', ' the', ' first', 'ford', ' an', ' the', '      ', ' second', ' it'], 'evidence_proportions': [0.37275390625, 0.697509765625, 0.499969482421875, 0.45764923095703125, 0.5741984049479166]}, 'weight': {'score': [0.0025507307052612307, 0.006766170676530598, 0.05122181085439829, 0.006377627615768225, 0.005662244908949908], 'topk_tokens': ['Question', 'If', ':', 'Answer', ',', '<|start_header_id|>', 'nes', '<|eot_id|>', '?\n', ':', 'assistant', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'hall', 'ian', ' the', 'way', 'tele', '<|begin_of_text|>'], 'evidence_proportions': [0.00533294677734375, 0.0016673922538757322, 0.0010501742362976074, 0.0019701719284057617, 0.0026058355967203775]}, 'saliency': {'score': [0.0003634023666381836, 0.00011312881031551876, 9.30497279533973e-05, 0.00011182273665864836, 0.00017024664317860324], 'topk_tokens': ['If', ' hallway', ' the', ' discarded', '<|start_header_id|>', 'nes', ',', 'Answer', 'assistant', 'ian', '<|eot_id|>', '\n\n', ' hallway', ':', 'tele', '<|begin_of_text|>', '<|end_header_id|>', 'hall', 'way', '<|eot_id|>'], 'evidence_proportions': [0.0016012489795684814, 9.396076202392579e-05, 2.8622150421142577e-05, 4.576146602630615e-05, 4.71423069636027e-05]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-23 22:26:25.061 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:26:25.061 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-9_pid-2_0-1-5-8-9.pkl | len: 10 |  size: 9.24 KB
Processing depth (0, 1, 5, 8, 9):   3%|▎         | 3/100 [00:41<22:13, 13.75s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.16it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Processing depth (0, 3, 7, 8, 9):   3%|▎         | 3/100 [00:50<22:13, 13.75s/it]2025-01-23 22:26:34.086 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-23 22:26:34.086 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-23 22:26:34.086 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra picked up the milk.
2025-01-23 22:26:34.093 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1475, 1480) --> . Sandra picked up the
2025-01-23 22:26:34.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:26:34.106 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2581, 2586) -->  back to the kitchen.
2025-01-23 22:26:34.107 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra left the milk.
2025-01-23 22:26:34.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3324, 3328) -->  Sandra left the milk
2025-01-23 22:26:34.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:26:34.136 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2578, 2584) --> . Daniel went back to the
2025-01-23 22:26:34.136 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary got the football there.
2025-01-23 22:26:34.139 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (472, 477) --> . Mary got the football
2025-01-23 22:26:34.139 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:26:34.151 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2489, 2494) -->  Mary moved to the bathroom
2025-01-23 22:26:34.151 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-23 22:26:34.164 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2578, 2584) --> . Daniel went back to the
2025-01-23 22:26:34.164 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John went back to the bedroom.
2025-01-23 22:26:34.176 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2352, 2358) --> . John went back to the
2025-01-23 22:26:34.176 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:26:34.193 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3276, 3281) --> . John moved to the
2025-01-23 22:26:34.193 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:26:34.197 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (807, 813) --> . Mary journeyed to the
2025-01-23 22:26:34.197 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  John journeyed to the office.
2025-01-23 22:26:34.201 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (808, 814) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:26:34.698 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the kitchen<|eot_id|>
2025-01-23 22:26:34.699 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4231])
your chose emoji: ['🙏🏽', '👩🏼\u200d❤️\u200d💋\u200d👨🏾', '🔻', '☮', '📎', '⛹\u200d♀', '👩🏻\u200d❤\u200d👩🏽', '💖', '👏🏽', '🐙']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4234, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 234646.38it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.25it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.15it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.89it/s]
2025-01-23 22:26:37.690 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.7793402099609374, 1.0297632735463658, 0.8756732451610076, 1.0327057406199065, 0.7158854007720947], 'topk_tokens': [' hour', ' him', ' newspaper', ' wood', ' him', ' printing', ' Wood', ' with', ' hallway', ' distant', ' hung', ' connected', ' day', ' half', ' bathroom', ' offices', ' printed', 'hall', ' column', ' conducted'], 'evidence_proportions': [1.0230926513671874, 0.80897216796875, 0.74747314453125, 0.88067626953125, 0.5105183919270833]}, 'weight': {'score': [0.001563563346862793, 0.007435182833592735, 0.24922737784874746, 0.005209018536608854, 0.0006833956576883793], 'topk_tokens': [' left', '\n\n', '\n\n', ' directly', ':', 'Just', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'hall', 'user', '<|end_header_id|>', 'way', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'assistant', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.003059005737304688, 0.001500439643859863, 0.001056373119354248, 0.0007164031267166138, 0.001357396443684896]}, 'saliency': {'score': [0.0002503490447998047, 0.00017114300377593043, 0.0010447165904900967, 0.00016249803473337662, 2.9258430004119873e-05], 'topk_tokens': [' waving', ' will', '\n\n', '<|end_header_id|>', 'If', ' night', 'If', ' left', 'user', ' directly', 'way', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'Just', '<|begin_of_text|>', 'hall', '.', 'assistant'], 'evidence_proportions': [0.0008322834968566894, 8.240342140197754e-05, 0.00013182759284973143, 2.03549861907959e-05, 0.0001574556032816569]}}, 25: {'grad': {'score': [0.786024169921875, 0.7423605146304469, 0.7189116844764123, 0.7423180477224666, 0.8562288135290146], 'topk_tokens': ['\n', '\n', '\n', '\n', '\n', '\n', '.\n\n', '�', '\n', '\n', '.\n\n', '?\n', '\n', '\n', '\n', '<|start_header_id|>', '\n', '\n', '\n', ',\n'], 'evidence_proportions': [0.5006591796875, 1.0072021484375, 0.586224365234375, 0.854949951171875, 0.9600626627604166]}, 'weight': {'score': [0.0011724972724914552, 0.007443253927541669, 0.2941326651817713, 0.004799580516860925, 0.0007887068204581738], 'topk_tokens': [' remains', '<|eot_id|>', ' ', '<|start_header_id|>', '?\n', '<|start_header_id|>', '\n\n', ' ', 'hall', 'user', '<|end_header_id|>', '<|eot_id|>', ' directly', 'way', '<|eot_id|>', '\n\n', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.001381075382232666, 0.0013195693492889403, 0.00039501190185546874, 0.0009912550449371338, 0.001644854744275411]}, 'saliency': {'score': [5.4095983505249026e-05, 0.0002183936356822443, 0.0006649005107390574, 0.00021520267001730648, 3.317045047879219e-05], 'topk_tokens': ['.\n\n', ' item', ' ', '<|start_header_id|>', ' ', '<|eot_id|>', ' Senator', '?\n', ' Senator', 'user', '<|end_header_id|>', ' Douglas', 'way', 'hall', ' directly', 'assistant', '.', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [4.4250488281250005e-05, 8.75532627105713e-05, 1.4770030975341798e-05, 4.748255014419556e-05, 7.160007953643799e-05]}}, 26: {'grad': {'score': [0.608121109008789, 0.5607379089845596, 0.5671875782502003, 0.56039351639416, 0.46001723408699036], 'topk_tokens': [' at', ' panic', ' at', ' at', ' at', ' at', ' air', ' Moore', ' hallway', ' there', ' text', ' context', ' secret', ' Hale', 'street', 'street', ' Hill', ' hallway', ' circ', 'hall'], 'evidence_proportions': [0.6869415283203125, 0.8349609375, 0.45179023742675783, 0.4691162109375, 0.5763498942057292]}, 'weight': {'score': [0.0031137752532958986, 0.007272596003370686, 0.26077245214046574, 0.004926667017616528, 0.002074708230793476], 'topk_tokens': [' the', '.\n\n', '\n\n', '<|start_header_id|>', '\n\n\n', '.\n\n', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'hall', 'way', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', 'user', '<|start_header_id|>', '<|end_header_id|>', 'assistant', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0065456867218017575, 0.0036562919616699215, 0.0013293862342834474, 0.0017236173152923584, 0.0022155145804087324]}, 'saliency': {'score': [0.0002736306190490723, 0.00014396413745438653, 0.001019435051159981, 0.0001349989030000975, 0.00010500522330403328], 'topk_tokens': ['Saturday', '\n', ' hallway', ' Daniel', '\n\n\n', '\n\n', '<|eot_id|>', ',', '<|start_header_id|>', '<|start_header_id|>', '\n\n', ',', 'user', '.\n\n', 'way', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0006689667701721191, 0.0003122210502624512, 5.099177360534668e-05, 0.00012869387865066528, 0.00019418199857076007]}}, 27: {'grad': {'score': [0.792203369140625, 0.764094796448763, 0.7721707026163737, 0.7638507497682274, 0.7103295922279358], 'topk_tokens': ['\n', ' when', ' in', ' in', ' in', ' When', 'in', ' in', ' in', ';', ' in', ' Hill', '\n', ' part', ',\n', ' in', ' Sh', 'Sh', ' in', ' IN'], 'evidence_proportions': [1.084521484375, 0.7057830810546875, 0.8651367187500001, 0.5991172790527344, 0.688568115234375]}, 'weight': {'score': [0.0024435067176818846, 0.007433993556221308, 0.2893583308427762, 0.00482721010700976, 0.001436810940504074], 'topk_tokens': ['\n\n', ' location', '\n\n', '<|eot_id|>', 'Question', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ' left', '<|eot_id|>', 'Just', 'way', 'hall', 'user', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.003083038330078125, 0.0026098966598510744, 0.00124969482421875, 0.0018740743398666382, 0.00314637025197347]}, 'saliency': {'score': [0.0001413869857788086, 0.00022391012720640504, 0.0005814371964870355, 0.00022106109191473726, 6.902869790792465e-05], 'topk_tokens': [' Senator', ' item', ' Daniel', 'Question', '\n', ' the', ' and', ' the', ' location', '<|end_header_id|>', '<|start_header_id|>', '.', '\n\n', '<|end_header_id|>', 'way', '<|begin_of_text|>', '<|end_header_id|>', ' left', 'assistant', 'user'], 'evidence_proportions': [6.579756736755372e-05, 0.0002148449420928955, 4.834532737731933e-05, 3.789365291595459e-05, 0.0002896934747695923]}}, 28: {'grad': {'score': [0.9231109428405762, 1.0922247842059813, 0.9586106324807192, 1.094488284913756, 0.9997850656509399], 'topk_tokens': ['\n', '\n', '\n', ' year', ' He', ' *', ' *', ' large', ' he', ' *', ' the', 'a', '\n', ' large', ' the', ' Wood', 'hall', '.', ' Hill', ' he'], 'evidence_proportions': [0.8287795066833495, 1.0122314453124999, 1.057421875, 1.059823989868164, 0.7243855794270833]}, 'weight': {'score': [0.0021441316604614256, 0.007051778278882401, 0.2790524302384792, 0.004537309631169271, 0.0010943184606730938], 'topk_tokens': [':', ' the', '\n', '.\n\n', 'Question', '\n', 'Just', '.\n\n', '<|eot_id|>', 'user', '\n\n', '<|start_header_id|>', 'hall', '\n\n', '<|eot_id|>', 'way', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0030518054962158205, 0.0029362201690673827, 0.0016515970230102537, 0.0009711682796478271, 0.0019200841585795085]}, 'saliency': {'score': [9.21773910522461e-05, 0.00015765165995470351, 0.0016726782688727747, 0.00014387487793426148, 4.798592999577522e-05], 'topk_tokens': ['\n', ' based', 'assistant', 'through', '\n', ' the', ' At', ',', ' *\n\n', '\n', ' directly', '\n\n', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'way', 'hall', '<|begin_of_text|>', '.', '<|end_header_id|>'], 'evidence_proportions': [6.204843521118164e-05, 0.00015606284141540528, 8.280873298645019e-05, 5.082041025161743e-05, 9.942551453908284e-05]}}, 29: {'grad': {'score': [0.6636700439453125, 0.4729037061861567, 0.48152414957682294, 0.4716794002542107, 0.5272035896778107], 'topk_tokens': [' various', ' Date', 'itol', 'able', '202', ' Wright', ' type', ' grat', '<|start_header_id|>', ' capable', ' contents', 'isc', '<|start_header_id|>', '<|end_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '�', '<|eot_id|>'], 'evidence_proportions': [0.7384277343750001, 0.6642578125, 0.6485626220703125, 0.550933837890625, 0.688629150390625]}, 'weight': {'score': [0.002781827449798584, 0.007371265481652434, 0.33163650524921906, 0.004366083611115563, 0.001447831280529499], 'topk_tokens': ['If', '<|end_header_id|>', 'Just', '<|eot_id|>', '\n\n', 'If', '<|eot_id|>', '<|start_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', 'hall', '<|start_header_id|>', 'way', '<|eot_id|>', 'user', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.006454563140869141, 0.002226030826568604, 0.0011775672435760498, 0.0010756701231002808, 0.002658699949582418]}, 'saliency': {'score': [0.00024355411529541015, 0.000160610585714997, 0.0024919609228769937, 0.00013830932638913892, 7.184082642197609e-05], 'topk_tokens': ['Just', 'New', '\n\n', ' the', '<|eot_id|>', ' was', '<|end_header_id|>', 'hall', 'of', 'way', 'If', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'If', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0007736742496490479, 8.182525634765626e-05, 3.2454729080200195e-05, 7.674843072891235e-05, 0.00022368133068084717]}}, 30: {'grad': {'score': [0.6004910278320312, 0.695046390178466, 0.7079789577386318, 0.6954923167789011, 0.9834331274032593], 'topk_tokens': [' ', ' Gray', ' lowered', ' S', ' all', 'made', ' ahead', '0', 'ails', '�', ' less', ' hidden', 'called', ' dropped', ' moved', ' D', ' dollars', '7', 'hall', ' Among'], 'evidence_proportions': [0.31219482421875, 0.634332275390625, 0.6008399963378906, 0.87200927734375, 0.6312338511149088]}, 'weight': {'score': [0.005062975883483887, 0.007174414173859609, 0.1526325788253393, 0.005826672575742506, 0.005184636451303959], 'topk_tokens': ['\n', ' the', ' left', ':', ' location', ' and', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', 'hall', 'user', 'way', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.005946731567382813, 0.006350326538085937, 0.002633213996887207, 0.003560096025466919, 0.006280442078908285]}, 'saliency': {'score': [0.00037601828575134277, 0.00040134771370245817, 0.0011164599504226293, 0.0003948114687304417, 0.00038999784737825394], 'topk_tokens': [' morning', '\n', ' *\n\n', ' directly', ' journey', '\n', '<|eot_id|>', 'way', 'Question', '<|end_header_id|>', '<|eot_id|>', 'user', '<|begin_of_text|>', '<|start_header_id|>', ' location', '.', '<|start_header_id|>', 'hall', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [0.0007990539073944092, 0.0002652883529663086, 7.36236572265625e-05, 0.00034462660551071167, 0.00038868685563405353]}}, 31: {'grad': {'score': [0.4748381042480469, 0.6287146970580421, 0.38917854504707533, 0.631877484767557, 0.8278213441371918], 'topk_tokens': ['user', '202', ' absent', 'f', '�', '<|eot_id|>', 'ast', '9', '�', 'D', '<|end_header_id|>', '\u200d', 'SP', ' ', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.2709747314453125, 0.62742919921875, 0.5563072204589844, 0.4617652893066406, 0.4583892822265625]}, 'weight': {'score': [0.002889404296875, 0.006883580672273559, 0.15369203151800695, 0.0055344978968302405, 0.0038785329088568687], 'topk_tokens': [' location', ' Where', ' the', '.\n', '?\n', ':', 'Answer', '.\n', '<|eot_id|>', 'Question', ':', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'hall', '<|end_header_id|>', 'way', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.004424858093261719, 0.0027211666107177734, 0.0012980461120605468, 0.002796947956085205, 0.003137826919555664]}, 'saliency': {'score': [0.0005727922916412354, 0.00021597774709008693, 0.00043353667626014125, 0.000211803847365528, 0.00018001161515712738], 'topk_tokens': [' place', 'system', 'Just', ' location', 'Question', ' fall', ' Hill', '\n\n', ' thought', ' item', '.', '<|eot_id|>', ' hallway', ' item', '<|eot_id|>', 'assistant', 'way', '<|start_header_id|>', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.0017216742038726808, 0.00017917156219482422, 5.410909652709961e-05, 0.00018965452909469604, 0.0006310691436131796]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-23 22:26:37.698 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:26:37.699 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-9_pid-3_0-3-7-8-9.pkl | len: 10 |  size: 9.81 KB
Processing depth (0, 3, 7, 8, 9):   4%|▍         | 4/100 [00:53<21:17, 13.31s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.18it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Processing depth (0, 3, 5, 6, 8):   4%|▍         | 4/100 [01:02<21:17, 13.31s/it]2025-01-23 22:26:46.825 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-23 22:26:46.826 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-23 22:26:46.826 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra picked up the milk.
2025-01-23 22:26:46.833 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1461, 1466) --> . Sandra picked up the
2025-01-23 22:26:46.833 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Sandra went to the kitchen.
2025-01-23 22:26:46.844 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2109, 2114) --> . Sandra went to the
2025-01-23 22:26:46.844 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra left the milk.
2025-01-23 22:26:46.859 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2405, 2409) -->  Sandra left the milk
2025-01-23 22:26:46.859 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel went back to the hallway.
2025-01-23 22:26:46.873 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2668, 2674) --> . Daniel went back to the
2025-01-23 22:26:46.873 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary got the football there.
2025-01-23 22:26:46.876 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (488, 493) --> . Mary got the football
2025-01-23 22:26:46.876 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:26:46.889 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2547, 2552) --> . Mary moved to the
2025-01-23 22:26:46.889 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-23 22:26:46.902 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2668, 2674) --> . Daniel went back to the
2025-01-23 22:26:46.902 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  John went back to the bedroom.
2025-01-23 22:26:46.916 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2408, 2414) -->  milk. John went back to
2025-01-23 22:26:46.917 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John moved to the bedroom.
2025-01-23 22:26:46.933 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3297, 3302) --> . John moved to the
2025-01-23 22:26:46.933 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Mary journeyed to the office.
2025-01-23 22:26:46.937 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (824, 830) --> . Mary journeyed to the
2025-01-23 22:26:46.937 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  John journeyed to the office.
2025-01-23 22:26:46.942 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (825, 831) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:26:47.475 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:26:47.475 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4232])
your chose emoji: ['🎓', '🏃🏿\u200d♀️\u200d➡', '▶️', '🤾🏾', '\U0001faf3🏻', '🖇', '📹', '🏃🏼', '👩🏾\u200d❤️\u200d💋\u200d👩🏾', '⚙️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4235, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 226719.14it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.75it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.62it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.73it/s]
2025-01-23 22:26:50.617 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.667083740234375, 0.8141024502840909, 0.5816932091346154, 0.8171567365837966, 0.7553201528695913], 'topk_tokens': [' return', ' rest', ' report', ' get', ' Min', ' hallway', ' Min', ' Min', ' Hill', ' measure', ' hand', ' steam', 'hand', ' second', 'hand', ' measure', ' steam', 'out', 'out', 'hall'], 'evidence_proportions': [0.594464111328125, 0.51922607421875, 0.48582763671875, 1.017669677734375, 0.7681376139322917]}, 'weight': {'score': [0.0020033204555511474, 0.00733205374185141, 0.010619324751389332, 0.007333256034535545, 0.0016688649470989522], 'topk_tokens': [' directly', '<|start_header_id|>', 'Answer', ':', ' return', '?\n', '<|start_header_id|>', 'Question', '.\n\n', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', 'assistant', 'way', '\n\n', 'hall', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.005105280876159668, 0.000489497184753418, 0.0005850732326507568, 0.002705097198486328, 0.00139389435450236]}, 'saliency': {'score': [0.000538172721862793, 0.0002518120735285555, 0.0003922872054271209, 0.00024878221345845243, 0.00015681110895597017], 'topk_tokens': [' Stewart', 'ford', ' Where', 'Just', 'If', ' have', '<|eot_id|>', ' Hor', ' Wood', ' Do', ' hallway', '\n\n', ' return', 'way', '<|end_header_id|>', ' directly', '\n\n', 'Question', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0021374285221099853, 5.846619606018067e-05, 7.443428039550781e-05, 0.0002830103039741516, 0.00016177197297414142]}}, 25: {'grad': {'score': [1.0315306091308594, 0.8946992948457793, 0.9473810440454727, 0.893386571020338, 0.7799900935246394], 'topk_tokens': ['old', ' stere', ' charges', ' bag', ' long', ' tele', ' item', ' tele', ' old', ' bathroom', ' Daniel', ' custom', ' item', '000', ' Cl', 'iring', ' Col', ' war', ' item', '800'], 'evidence_proportions': [1.1272064208984376, 0.6596031188964844, 0.9173034667968749, 1.329833984375, 1.1580607096354167]}, 'weight': {'score': [0.0017011594772338868, 0.007345607176581409, 0.010111469488877516, 0.007353577102811123, 0.0011467085434840275], 'topk_tokens': ['man', 'Question', '<|eot_id|>', ' ', 'Times', '<|eot_id|>', 'Minnesota', ' Do', '<|eot_id|>', ' directly', ' return', '\n\n', 'way', '.\n\n', 'hall', 'assistant', '.', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016750097274780273, 0.0006612241268157959, 0.0006988823413848877, 0.0023202896118164062, 0.0030120412508646646]}, 'saliency': {'score': [0.00010926961898803711, 0.0002623273180676745, 0.00023852861844576322, 0.0002634672345774433, 7.936908648564265e-05], 'topk_tokens': ['paper', 'hall', ' Grow', '.', 'polit', ' Southern', '\n\n', ' Ear', ' Times', ' Marshall', ' write', ' rates', ' directly', 'way', 'edit', '.\n\n', 'Times', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [6.243586540222168e-05, 4.415512084960937e-05, 7.767081260681153e-05, 0.00019081681966781616, 0.00017452736695607504]}}, 26: {'grad': {'score': [0.775423583984375, 0.6596955534699676, 0.7445479172926682, 0.6582085136828791, 0.8981570317195012], 'topk_tokens': ['es', ' voice', ' week', ' pen', ' enraged', ' engaged', '�', '�', ' tie', '�', 'ages', ' veto', ' veto', ' Hill', 'ye', ' hallway', ' Hale', '�', 'hall', ' hallway'], 'evidence_proportions': [0.843621826171875, 0.689453125, 0.73719482421875, 0.8547821044921875, 0.7691853841145834]}, 'weight': {'score': [0.003403693437576294, 0.007274252070727568, 0.024795091305023585, 0.007133626617764547, 0.00257685688825754], 'topk_tokens': [':', '?\n', '<|end_header_id|>', 'Question', '<|start_header_id|>', ' inaugur', ' Hale', '\n\n', '<|eot_id|>', ' Southern', '<|eot_id|>', '.\n\n', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'way', 'hall', 'assistant', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.009758090972900391, 0.001479703187942505, 0.0008693933486938477, 0.0020881593227386475, 0.002700626850128174]}, 'saliency': {'score': [0.0010346436500549316, 0.0003119210284885015, 0.004944908313262157, 0.0002642694893743002, 0.00014185767907362717], 'topk_tokens': ['<|eot_id|>', ' Chicago', ' senate', 'far', ' Gal', ' Square', 'cap', ' Hon', 'rail', 'Question', '\n\n\n', '\n\n', 'edit', ' hallway', ' Hale', ' inaugur', 'polit', '<|begin_of_text|>', '.', 'hall'], 'evidence_proportions': [0.00439079999923706, 8.532404899597168e-05, 7.979869842529297e-05, 0.00018243491649627686, 0.00039278964201609295]}}, 27: {'grad': {'score': [0.7069432067871094, 0.6737147307592976, 0.609045664469401, 0.6741202406333351, 0.6308441748985878], 'topk_tokens': [' acquaintance', '      ', 'stage', ' doctor', ' absolutely', 'ages', ' capable', '      ', ' state', ' doctor', ' board', ' Date', ' candidate', ' bathroom', ' editor', '      ', '      ', 'icians', ' late', ' late'], 'evidence_proportions': [0.7542228698730469, 0.593603515625, 0.7268432617187499, 0.6300811767578125, 0.7966512044270833]}, 'weight': {'score': [0.0027726149559020997, 0.007344647647357747, 0.012220790752997765, 0.007326458061207287, 0.003058709089572613], 'topk_tokens': [':', 'polit', 'itol', '<|end_header_id|>', 'Minnesota', '<|eot_id|>', 'road', '<|eot_id|>', ' *\n\n', '<|eot_id|>', '.\n\n', 'Question', ' inaugur', '\n\n', 'assistant', '<|end_header_id|>', 'hall', '.', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.005066013336181641, 0.0011181771755218507, 0.0008882462978363037, 0.0029148459434509277, 0.0037156343460083008]}, 'saliency': {'score': [0.0003378021717071533, 0.00021234844341750972, 0.00038177309892116446, 0.00021001233582415473, 0.00021817363225496732], 'topk_tokens': ['If', ' *\n\n', 'ford', ' *\n\n', 'street', ' pur', 'If', '<|end_header_id|>', 'until', '\n\n', 'rail', '<|eot_id|>', 'assistant', 'road', 'itol', 'Minnesota', 'polit', 'hall', '<|begin_of_text|>', 'way'], 'evidence_proportions': [0.0008187532424926759, 2.7215480804443357e-05, 6.276965141296386e-05, 0.00027319788932800293, 0.00046809514363606774]}}, 28: {'grad': {'score': [0.504356689453125, 0.4298322549254723, 0.32908116854154146, 0.4303276233037465, 0.46031268193171576], 'topk_tokens': ['A', ' room', ' ', ' huge', '.', ' hidden', ' house', ' Hor', 'stage', ' lines', 'D', ' hung', ' Hale', '.', 'd', ' hallway', ' Hill', ' hallway', 'hall', ' all'], 'evidence_proportions': [0.604119873046875, 0.5699096679687501, 0.37811279296875, 0.5275955200195312, 0.45630391438802087]}, 'weight': {'score': [0.0013452863693237305, 0.00724229688768263, 0.01969854189799382, 0.007161172866478323, 0.002050584554672241], 'topk_tokens': [' about', 'Bridge', '?\n', '<|eot_id|>', ' return', ':', ':', 'Just', '.', ' directly', 'Answer', 'Question', '<|eot_id|>', 'hall', 'assistant', 'way', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0018731355667114258, 0.0006964683532714843, 0.0007011890411376953, 0.00135117769241333, 0.001978913942972819]}, 'saliency': {'score': [4.5278072357177734e-05, 0.0001476396917593099, 0.00042452261998103216, 0.0001456642916836461, 6.917806772085336e-05], 'topk_tokens': ['<|eot_id|>', ' milk', 'assistant', 'through', ' about', ' inaugur', ' directly', 'Answer', 'Bridge', '.', 'just', 'ographical', ' made', 'Just', '<|begin_of_text|>', 'way', '.', '<|end_header_id|>', 'hall', '\n\n'], 'evidence_proportions': [8.257627487182618e-05, 1.2731552124023437e-05, 1.906752586364746e-05, 2.7768313884735107e-05, 7.483363151550293e-05]}}, 29: {'grad': {'score': [0.5582696533203125, 0.45581576404774204, 0.49338472806490385, 0.45485039918835973, 0.4535617534930889], 'topk_tokens': [' DAYS', ' to', ' so', '0', '0', ' of', '️', ' of', '4', '185', ' a', '\n', '�', 'D', ' pur', 'APER', 'D', ' pur', 'P', ' the'], 'evidence_proportions': [1.121728515625, 0.35302734375, 0.51806640625, 0.4145050048828125, 0.3891016642252604]}, 'weight': {'score': [0.0024670469760894776, 0.007345840472116662, 0.017786110058808938, 0.007277463421893617, 0.002326444479135367], 'topk_tokens': [' directly', 'If', ' hallway', '<|start_header_id|>', ' Hale', 'Question', '<|eot_id|>', ':', '\n\n', '.', '.\n\n', '<|eot_id|>', ':', 'hall', 'assistant', 'way', '<|end_header_id|>', '\n\n', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.006516742706298827, 0.0002957582473754883, 0.0007186472415924072, 0.0015094876289367676, 0.002997080485026042]}, 'saliency': {'score': [0.00016345381736755372, 0.00018414553813416392, 0.0011195991283808, 0.00017552281049080417, 0.00015559517420255222], 'topk_tokens': [' twenty', '<|end_header_id|>', 'man', ' St', 'I', 'Answer', ':', ' the', '<|start_header_id|>', ' left', '.', '<|eot_id|>', 'hall', 'If', ':', '<|end_header_id|>', 'way', '.', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00022618770599365235, 1.6230344772338868e-05, 4.528164863586426e-05, 6.89774751663208e-05, 0.000395322839419047]}}, 30: {'grad': {'score': [0.42756805419921873, 0.49865757245240555, 0.46586022010216344, 0.49939033071133365, 0.5187548123873197], 'topk_tokens': ['en', 'men', 'action', 'men', ' where', ' for', ' use', '      ', 'S', ' when', 'then', 'were', ' senate', ' boat', ' S', ' J', ' cold', ' gold', ' S', ' S'], 'evidence_proportions': [0.32642822265625, 0.44466857910156254, 0.4217742919921875, 0.4755859375, 0.4704170227050781]}, 'weight': {'score': [0.0032608401775360107, 0.00719921828160742, 0.0060673738137269635, 0.0072334070581236325, 0.007962543689287626], 'topk_tokens': ['�', '.\n\n', ' Southern', ' company', ':', ' inaugur', '.', '<|end_header_id|>', '.\n\n', '<|eot_id|>', 'Question', '.', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.004773330688476563, 0.0010945498943328857, 0.001123356819152832, 0.004179716110229492, 0.00497432549794515]}, 'saliency': {'score': [0.00044195055961608886, 0.0002771786281040737, 0.0002761567250276223, 0.00027620057869917825, 0.0004345563741830679], 'topk_tokens': ['�', '<|start_header_id|>', 'L', 'un', 'In', 'polit', '***', 'Min', 'super', ' Southern', '.', ' hidden', ' hallway', ' Hale', '\n\n', 'Question', 'way', '<|start_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.0014684200286865234, 5.262494087219238e-05, 2.658367156982422e-05, 0.00018525868654251099, 0.00042826433976491296]}}, 31: {'grad': {'score': [0.4666514587402344, 0.5375764416691263, 0.4007401588635567, 0.5392810064264122, 0.4379629575289213], 'topk_tokens': [' they', 'ured', ' notified', 'ian', 'ford', ' enraged', ' im', ' and', ' and', 'D', ' I', ' W', ' I', ' wield', ' inverted', ' D', ' I', 'I', 'D', ' I'], 'evidence_proportions': [0.48079681396484375, 0.3755317687988281, 0.4524169921875, 0.4450225830078125, 0.5570780436197917]}, 'weight': {'score': [0.0023572802543640136, 0.006792220483845212, 0.02077045501806797, 0.006688102133066584, 0.0035753470200758715], 'topk_tokens': ['<|start_header_id|>', ' the', '<|eot_id|>', ' directly', 'Question', ' Where', 'Answer', '.', '?\n', '.\n\n', ':', ':', 'assistant', '<|eot_id|>', '<|end_header_id|>', '\n\n', '.', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.003637409210205078, 0.0009893596172332764, 0.0012122094631195068, 0.002839386463165283, 0.0030632615089416504]}, 'saliency': {'score': [0.0006944870948791504, 0.00028169699796115914, 0.0010208036655034774, 0.0002723119793907683, 0.00020983448395362267], 'topk_tokens': [' Wood', ' company', ' the', ' item', ' fall', ' Grow', ' printed', ' Grow', ' moved', ' Do', ' went', '\n\n', 'assistant', '<|end_header_id|>', ' hallway', '<|eot_id|>', '<|begin_of_text|>', '.', 'hall', 'way'], 'evidence_proportions': [0.0025574743747711183, 7.06017017364502e-05, 0.0002493917942047119, 5.988031625747681e-05, 0.0004558861255645752]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:26:50.624 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:26:50.624 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-9_pid-4_0-3-5-6-8.pkl | len: 10 |  size: 9.4 KB
Processing depth (0, 3, 5, 6, 8):   5%|▌         | 5/100 [01:06<20:51, 13.17s/it]Processing depth (0, 3, 5, 6, 8):   5%|▌         | 5/100 [01:06<21:10, 13.37s/it]
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:99 - Selected idx: 10
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:103 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary picked up the apple.', ' John went back to the office.', ' Daniel took the football.', ' Daniel left the apple.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Sandra journeyed to the office.', ' Mary journeyed to the bedroom.', ' Daniel dropped the football.']
2025-01-23 22:26:50.854 | INFO     | __main__:<module>:104 - Real Needle: [' Mary picked up the apple.', ' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Daniel dropped the football.']
2025-01-23 22:26:50.855 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.29s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
Processing depth (0, 2, 3, 8):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-23 22:27:01.277 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:27:01.277 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-23 22:27:01.277 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:27:01.282 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (963, 968) --> . Mary moved to the
2025-01-23 22:27:01.282 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:27:01.290 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1525, 1531) -->  tragedy. Mary journeyed to
2025-01-23 22:27:01.290 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel dropped the football.
2025-01-23 22:27:01.306 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3366, 3370) -->  Daniel dropped the football
2025-01-23 22:27:01.306 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:27:01.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-23 22:27:01.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:27:01.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (34, 39) -->  apple. Sandra moved to
2025-01-23 22:27:01.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:27:01.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2089, 2095) -->  John went back to the office
2025-01-23 22:27:01.318 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:27:01.321 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (785, 789) -->  Daniel took the football
2025-01-23 22:27:01.322 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:27:01.335 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2732, 2736) -->  Daniel left the apple
2025-01-23 22:27:01.335 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John moved to the garden.
2025-01-23 22:27:01.353 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3643, 3648) --> . John moved to the
2025-01-23 22:27:01.353 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra journeyed to the office.
2025-01-23 22:27:01.355 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (431, 437) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:27:01.830 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:27:01.831 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4213])
your chose emoji: ['🐈\u200d⬛', '💁🏽', '👨🏿\u200d🏫', '🚋', '🤷🏻\u200d♂️', '🥀', '🧎🏾\u200d♀️\u200d➡️', '🧎\u200d➡', '🐙', '🏃🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4216, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.73it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.11it/s]
2025-01-23 22:27:04.634 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.17861618995666503, 0.13077110312242887, 0.1924872807094029, 0.1300220120500355, 0.11973555160291267], 'topk_tokens': [' under', ' that', 'user', ' await', ' much', ' ', 'Today', 'ucci', 'Mary', ' We', ' brought', ' picked', ' PA', ' B', ' Think', ' Knowledge', ' up', 'ol', 'Cut', 'ting'], 'evidence_proportions': [0.3849475860595703, 0.11230525970458985, 0.104248046875, 0.115142822265625]}, 'weight': {'score': [0.01773073673248291, 0.007560201116485885, 0.004084707157952445, 0.0075405499692204996, 0.01102823560888117], 'topk_tokens': [' Francis', ' directly', ' curs', 'itol', ' not', ' item', 'pleasant', ' enough', '️', 'polit', ' item', '<|end_header_id|>', 'icians', ' fortunate', ' Adams', '.', ' item', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0027679443359375, 0.045902442932128903, 0.006432175636291504, 0.018167436122894287]}, 'saliency': {'score': [5.713552236557007e-05, 6.13618012963029e-05, 3.349951335362026e-05, 6.161647701286347e-05, 5.225882385716294e-05], 'topk_tokens': [' own', ' item', ' have', '<|start_header_id|>', ' o', ' on', 'of', ' question', ' question', ' what', ' looked', '<|eot_id|>', 'itol', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'b'], 'evidence_proportions': [1.3577938079833985e-05, 8.485913276672364e-05, 6.512304147084555e-05, 6.494671106338501e-05]}}, 25: {'grad': {'score': [0.17326555252075196, 0.21844659765486021, 0.1655301775251116, 0.21910886528457021, 0.20737600326538086], 'topk_tokens': ['remember', '\n', 'ably', 'man', ' back', ',', ' man', '\n', ' had', ' answer', ' related', ' not', ' not', ' *\n\n', ' *', ' hung', ' papers', ',', ' upper', 'system'], 'evidence_proportions': [0.10979919433593749, 0.22346725463867187, 0.17739582061767578, 0.18365097045898438]}, 'weight': {'score': [0.023174105584621428, 0.00755347789351809, 0.003628488097872053, 0.007511411584704234, 0.009906863624399359], 'topk_tokens': [' enough', ' curs', 'cery', 'pleasant', '<|eot_id|>', '️', '<|eot_id|>', ' Francis', ' fortunate', ' item', 'polit', 'icians', 'assistant', ' Adams', '.', ' item', '<|end_header_id|>', 'athroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0021540164947509766, 0.07075631022453308, 0.005773484706878662, 0.016072392463684082]}, 'saliency': {'score': [0.00014392733573913573, 7.926213221034243e-05, 2.9272692544119698e-05, 7.937179967435155e-05, 6.96570584268281e-05], 'topk_tokens': [' measure', ' PA', ' could', ' item', ' Charles', ' over', '\n\n', '️', ' Senator', ' Pa', ' item', ' Francis', '.', '<|eot_id|>', 'athroom', '<|eot_id|>', '<|begin_of_text|>', 'b', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [1.4340877532958984e-05, 0.0004383981227874756, 5.049010117848714e-05, 7.797777652740479e-05]}}, 26: {'grad': {'score': [0.15602483749389648, 0.2460192403485698, 0.20982512065342496, 0.2467562464159631, 0.21366606336651425], 'topk_tokens': ['\n', '\n', '\n', '\n', '\n', ' *', ' John', '\n', '\n', '\n', '!', '\n', '\n', '\n', '\n', '***', '\n', '\n', '\n', '\n'], 'evidence_proportions': [0.13751640319824218, 0.12668228149414062, 0.22449048360188803, 0.11314010620117188]}, 'weight': {'score': [0.01615041196346283, 0.007566315851808726, 0.0050059199333190914, 0.0075465926926916995, 0.010761083978595156], 'topk_tokens': ['<|start_header_id|>', ' Francis', ' curs', 'pleasant', ' enough', 'polit', ' item', '️', 'icians', ' fortunate', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', ' Adams', '.', '<|end_header_id|>', ' item', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005352628231048584, 0.038173580169677736, 0.00836181640625, 0.01380157470703125]}, 'saliency': {'score': [0.0001583963632583618, 7.99302951886939e-05, 7.37079552241734e-05, 7.960548397441004e-05, 8.483050447521788e-05], 'topk_tokens': ['inen', ' reached', 'Bridge', '.', ' mob', 'assistant', 'to', '<|start_header_id|>', '<|start_header_id|>', ' *', 'ien', ' item', ' shore', '<|eot_id|>', ' far', ' own', 'b', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00013587474822998046, 0.0003446221351623535, 0.00010546545187632243, 3.316253423690796e-05]}}, 27: {'grad': {'score': [0.2508838653564453, 0.27814671120109774, 0.31235253470284596, 0.2779900308584714, 0.27807896064989496], 'topk_tokens': [' the', ' an', "'s", ' Hor', ' and', ' an', ' an', ' been', ' well', ' a', ' an', ' compelled', ' moved', ' an', ' The', ' Sandra', ' vigorous', ' ', ' a', ' a'], 'evidence_proportions': [0.290252685546875, 0.2774139404296875, 0.225921630859375, 0.20595359802246094]}, 'weight': {'score': [0.02688029408454895, 0.00755838560650878, 0.0037533351353236607, 0.007497520092673326, 0.01088720921314124], 'topk_tokens': [' item', 'rate', ' not', 'asca', ' enough', 'pleasant', ' Francis', 'itol', '️', ' curs', ' fortunate', 'polit', '<|end_header_id|>', 'icians', ' Adams', '.', ' item', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002867412567138672, 0.08262875080108642, 0.00591890017191569, 0.018652915954589844]}, 'saliency': {'score': [0.0002924710512161255, 0.00015073995577768765, 8.19861888885498e-05, 0.00015063703819353746, 0.00025134827151443017], 'topk_tokens': ['RE', '***', ' item', '<|end_header_id|>', '️', ' Adams', ' reached', ' fortunate', 'assistant', '.', 'polit', '      ', '<|eot_id|>', ' item', 'icians', '<|end_header_id|>', 'athroom', '<|start_header_id|>', '<|end_header_id|>', 'b'], 'evidence_proportions': [4.37617301940918e-05, 0.0006364405155181885, 0.00017520785331726074, 0.000349290668964386]}}, 28: {'grad': {'score': [0.4028143882751465, 0.3909030675435654, 0.48055474417550226, 0.3900917156818136, 0.3036258003928445], 'topk_tokens': [' a', ' a', ' A', ' a', ' a', ' A', ' a', ' a', ' A', ' a', ' a', ' a', ' a', 'A', ' a', ' a', ' a', ' a', ' a', ' a'], 'evidence_proportions': [0.5767662048339843, 0.40867919921875, 0.37247880299886066, 0.22354698181152344]}, 'weight': {'score': [0.045557327568531036, 0.007460801379730399, 0.0035771659442356656, 0.007310355985946765, 0.007608410083886349], 'topk_tokens': ['️', ' item', 'asca', '2', '<|eot_id|>', '<|start_header_id|>', ' Francis', ' fortunate', '\n\n', ' curs', 'polit', 'icians', 'assistant', ' Adams', ' item', '.', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0019125282764434813, 0.16307191848754882, 0.0073239803314208984, 0.010570108890533447]}, 'saliency': {'score': [0.00021695643663406373, 0.00011745435265255155, 3.944805690220424e-05, 0.00011763223745707737, 4.0350538311582625e-05], 'topk_tokens': [' over', '2', '<|eot_id|>', ' Francis', ' curs', 'ian', 'cery', 'polit', 'icians', '<|start_header_id|>', ' thought', ' Adams', '<|eot_id|>', ' item', '.', 'assistant', '<|begin_of_text|>', 'athroom', 'b', '<|end_header_id|>'], 'evidence_proportions': [2.7507543563842773e-05, 0.0006218671798706055, 0.00012788673241933185, 8.12336802482605e-05]}}, 29: {'grad': {'score': [0.2748438835144043, 0.21793577566997377, 0.27341883523123606, 0.2171955523482884, 0.24772439219734885], 'topk_tokens': [',', ' way', ',', ' minds', ' of', ' and', ',', ' also', ',', ' PA', ' and', '.', 'ot', ' of', ' bringing', ',', ' out', ' over', ' being', ' between'], 'evidence_proportions': [0.44785079956054685, 0.27611351013183594, 0.2704900105794271, 0.06352901458740234]}, 'weight': {'score': [0.03908437192440033, 0.007438253180125633, 0.00377090403011867, 0.007316992628663184, 0.006894362695289381], 'topk_tokens': ['\n\n', ' Francis', ' Tribune', '️', 'pleasant', 'polit', ' curs', ' fortunate', '<|start_header_id|>', '<|eot_id|>', 'icians', '<|eot_id|>', 'assistant', ' Adams', '.', ' item', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.005983912944793701, 0.13411540985107423, 0.008917808532714844, 0.006920993328094482]}, 'saliency': {'score': [0.00016715526580810547, 0.0001253335395619358, 8.21896961757115e-05, 0.00012549542372285733, 0.00016870507688233346], 'topk_tokens': ['\n', '.', ' John', ',', '<|eot_id|>', ',', ' party', '.', '.', '.', ' *\n\n', ' We', ' B', ' item', '<|eot_id|>', '<|end_header_id|>', 'assistant', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00016045570373535156, 0.00040628314018249513, 6.261964639027913e-05, 3.342330455780029e-05]}}, 30: {'grad': {'score': [0.3875303268432617, 0.3239922677316973, 0.4162317548479353, 0.32291100283592716, 0.26247609745372424], 'topk_tokens': [' business', ' a', ' M', ' a', ' the', ' considerable', 'Cut', ' a', ' a', ' Pa', ' to', ' Republicans', ' a', ' P', ' web', ' Jul', ' em', 'E', ' B', ' B'], 'evidence_proportions': [0.5701141357421875, 0.363214111328125, 0.26467132568359375, 0.37398433685302734]}, 'weight': {'score': [0.016729353368282317, 0.007294073050569538, 0.0034319571086338587, 0.007281207982464153, 0.005009096680265485], 'topk_tokens': [' state', '\n\n', ' item', 'Min', ' B', ' tele', ' Published', 'E', ' Adams', '<|start_header_id|>', '<|start_header_id|>', '.', 'assistant', ' item', '<|eot_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0018722951412200929, 0.04790287017822266, 0.01135118802388509, 0.004401028156280518]}, 'saliency': {'score': [0.0002166837453842163, 0.00017078114104451898, 8.67511544908796e-05, 0.00017126732163634387, 7.555566050789572e-05], 'topk_tokens': [' STR', 'If', ',', ' A', ' We', ' considerable', ' own', '<|eot_id|>', ' New', ' W', ' Min', '<|end_header_id|>', 'assistant', ' vigorous', 'E', 'athroom', '<|eot_id|>', ' B', '<|begin_of_text|>', ' B'], 'evidence_proportions': [2.3120641708374022e-05, 0.0005581021308898926, 0.00021744767824808758, 3.071874380111694e-05]}}, 31: {'grad': {'score': [0.42938990592956544, 0.5305576360655464, 0.46242513656616213, 0.5316169948940007, 0.5398439349550189], 'topk_tokens': [' the', ' July', ' his', ' traveled', ' received', 'le', 'just', ' their', ' first', ' he', ' as', ' locations', ' between', ' the', '\n', '�', '.', ',', ' day', ' One'], 'evidence_proportions': [0.2796445846557617, 0.46239013671875, 0.6051025390625, 0.3117523193359375]}, 'weight': {'score': [0.00925402045249939, 0.006680052465912966, 0.003132734979901995, 0.006697518640457223, 0.004891597863399621], 'topk_tokens': [' apple', ' bedroom', ' before', ' item', 'Answer', ' \n', 'If', '.\n\n', ' Adams', '.', ' item', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.002196860313415527, 0.024979686737060545, 0.004825194676717122, 0.005061626434326172]}, 'saliency': {'score': [0.00013728588819503785, 0.00011617740513703855, 5.793656621660505e-05, 0.00011656583573089945, 0.0001117392922892715], 'topk_tokens': [' is', ' Minnesota', ' item', ' they', ' item', ' will', '.\n\n', ' four', ' one', ' item', 'assistant', 'Penn', 'If', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', 'athroom', 'b'], 'evidence_proportions': [8.041858673095703e-05, 0.0003321409225463867, 6.639957427978516e-05, 7.113069295883179e-05]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:27:04.641 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:27:04.641 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-10_pid-0_0-2-3-8.pkl | len: 10 |  size: 8.89 KB
Processing depth (0, 2, 3, 8):   1%|          | 1/100 [00:13<22:36, 13.70s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
Processing depth (2, 5, 7, 9):   1%|          | 1/100 [00:23<22:36, 13.70s/it]2025-01-23 22:27:14.584 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:27:14.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (907, 912) --> . Mary picked up the
2025-01-23 22:27:14.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:27:14.600 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2143, 2148) -->  Mary moved to the bathroom
2025-01-23 22:27:14.600 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:27:14.615 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2839, 2845) --> . Mary journeyed to the
2025-01-23 22:27:14.615 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel dropped the football.
2025-01-23 22:27:14.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3602, 3606) -->  dropped the football.
2025-01-23 22:27:14.632 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:27:14.637 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (908, 913) -->  Mary picked up the apple
2025-01-23 22:27:14.637 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:27:14.637 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (32, 37) -->  moved to the kitchen.
2025-01-23 22:27:14.637 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:27:14.647 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2048, 2054) --> . John went back to the
2025-01-23 22:27:14.648 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:27:14.651 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (676, 680) -->  Daniel took the football
2025-01-23 22:27:14.651 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:27:14.663 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2533, 2537) -->  Daniel left the apple
2025-01-23 22:27:14.663 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John moved to the garden.
2025-01-23 22:27:14.681 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3578, 3583) --> . John moved to the
2025-01-23 22:27:14.681 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra journeyed to the office.
2025-01-23 22:27:14.683 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (411, 417) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:27:15.200 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:27:15.200 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4206])
your chose emoji: ['🇵🇪', '🎱', '👏🏻', '🕣', '⛹🏾', '✴', '🙅🏾', '👱\u200d♂', '👩🏾\u200d🦯\u200d➡️', '\U0001faf6🏼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4209, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 223696.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.29it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.68it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.99it/s]
2025-01-23 22:27:18.247 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.6241965293884277, 0.47904346526787833, 0.5647009713309151, 0.4776228889571857, 0.42074410668734846], 'topk_tokens': [' discover', ' moved', ' he', ' would', '      ', ' printed', ' more', ' the', ' election', 'ANK', ' or', ' time', ' conditions', ' make', 'ences', ' cable', ' wonderful', ' held', ' remin', 'EF'], 'evidence_proportions': [0.8017333984374999, 0.711767578125, 0.5218493143717448, 0.4463324546813965]}, 'weight': {'score': [0.00046706199645996094, 0.007507257604293116, 0.000797820942742484, 0.007597684709567776, 0.0014996472103842374], 'topk_tokens': ['b', 'tele', '\n\n', 'system', 'Question', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'user', '\n\n', '\n\n', '<|start_header_id|>', 'ing', 'athroom', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00013247728347778322, 0.0001664102077484131, 0.0008237858613332113, 0.0007260218262672424]}, 'saliency': {'score': [2.32696533203125e-05, 0.00012710001554192316, 0.00013226355825151715, 0.0001275564149761062, 0.00010223327011897646], 'topk_tokens': [' kitchen', '<|eot_id|>', '\n', '<|end_header_id|>', '***', 'system', '<|eot_id|>', '<|start_header_id|>', 'user', '\n\n', 'athroom', ' upper', ' Project', '<|eot_id|>', 'Question', '\n\n', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.067752838134766e-06, 2.7704238891601563e-05, 2.577404181162516e-05, 3.5472214221954346e-05]}}, 25: {'grad': {'score': [0.6135116577148437, 0.6891108408878297, 0.7808855329241071, 0.6887015653563395, 0.8361300764412716], 'topk_tokens': [' Sandra', ' do', ' did', ' local', ' ne', ' Do', ' quite', ' of', 'andra', 'The', ' so', 'Sh', 'ex', 'A', 'out', ' res', 'istributed', 'de', ' shore', ' past'], 'evidence_proportions': [0.626123046875, 0.791864013671875, 0.49195353190104163, 0.5571441650390625]}, 'weight': {'score': [0.000968906283378601, 0.007464954478634766, 0.0005240508488246373, 0.007554711963215526, 0.002000061088594897], 'topk_tokens': ['<|end_header_id|>', 'Civil', 'b', 'Question', '\n\n', ' bedroom', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'user', '<|start_header_id|>', 'tele', 'ing', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00014136433601379393, 0.0002822458744049072, 0.0025126983722050986, 0.0005459710955619812]}, 'saliency': {'score': [0.00019805580377578734, 0.0002819962726972765, 3.0007532664707728e-05, 0.00028452357536448155, 0.00010792808286074935], 'topk_tokens': [' Mary', 'system', 'proc', ' item', ' write', '<|start_header_id|>', ' senate', 'tele', ' Min', '<|eot_id|>', 'user', 'assistant', '<|eot_id|>', ' Min', ' bedroom', 'Civil', 'athroom', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [1.519918441772461e-05, 4.1049718856811526e-05, 0.0005941838026046753, 2.8692185878753662e-05]}}, 26: {'grad': {'score': [0.49275598526000974, 0.5313799589143057, 0.4542422158377511, 0.532215852145111, 0.6446788393217942], 'topk_tokens': [' *', '�', 'national', ' IN', ' hand', ' press', 'machine', ' of', ' EVENTS', ' machine', ' force', ' Press', 'description', 'ible', '800', ' at', ' Ramsey', 'remember', ' bathroom', ' press'], 'evidence_proportions': [0.360107421875, 0.4864322662353515, 0.59161376953125, 0.5181846618652344]}, 'weight': {'score': [0.0013627588748931884, 0.007276629517950165, 0.0013450486319405693, 0.00735507986553598, 0.0038084315842595593], 'topk_tokens': ['.\n\n', '<|end_header_id|>', 'Question', '\n\n', 'Bridge', 'tele', 'user', '<|start_header_id|>', 'b', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'ing', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [6.97016716003418e-05, 0.0003399848937988281, 0.002530803283055623, 0.002505481243133545]}, 'saliency': {'score': [0.00010695904493331909, 0.0002372551627101341, 0.0001665081296648298, 0.00023847857833654765, 0.00018550455570220947], 'topk_tokens': [' Southern', '\n', ' bedroom', 'b', 'ian', '\n\n', ' block', 'Bridge', '\n\n', 'Question', ' carrier', 'ing', 'tele', 'a', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', 'ing'], 'evidence_proportions': [1.9729137420654298e-06, 0.000201338529586792, 0.00012048582235972086, 9.992718696594238e-05]}}, 27: {'grad': {'score': [0.6581474304199219, 0.7834929151075077, 0.7373585292271205, 0.7844851185739413, 0.7490907537526098], 'topk_tokens': [' that', '.\n\n', ' and', ' and', ' requiring', ' of', ',', ' Think', ' producing', '\n', '\n', ',', ',', '\n\n', ' of', ' use', ',', ' take', ',', ','], 'evidence_proportions': [0.6712188720703125, 0.46213378906249997, 0.7185312906901041, 0.7962493896484375]}, 'weight': {'score': [0.0012342721223831178, 0.007380802293207844, 0.0011807918548583985, 0.007462634495605226, 0.004152717775311963], 'topk_tokens': [' morning', ' Hale', 'question', 'tele', '\n\n', '\n\n', '\n\n', 'Question', '<|eot_id|>', '<|eot_id|>', 'ing', '\n\n', 'b', 'user', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00010747313499450683, 0.0005759000778198242, 0.0024170527855555215, 0.001691564917564392]}, 'saliency': {'score': [0.0001688241958618164, 0.0002522641688011751, 0.00010741608483450753, 0.00025388633596478143, 0.00023506530399980215], 'topk_tokens': ['system', ' *\n\n', '<|eot_id|>', '\n\n', ' bedroom', ' Hale', 'ing', 'question', '<|end_header_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'b', 'assistant', 'athroom', 'user', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.370378494262695e-06, 0.0004308164119720459, 0.00011124710241953532, 0.0001320168375968933]}}, 28: {'grad': {'score': [0.7097503662109375, 0.7529327037301021, 0.7595528738839286, 0.7530848319908146, 0.7025495726486732], 'topk_tokens': [' m', ' Min', ' Foster', 'user', '\n\n', ' compos', ',', 'man', ' H', 'd', '\n', '600', ' half', ' of', ' mess', '\n\n', ' Min', ' Min', ' Min', ' Min'], 'evidence_proportions': [0.47943115234375, 0.770892333984375, 0.7385101318359375, 0.878082275390625]}, 'weight': {'score': [0.0010752886533737182, 0.007029839495520434, 0.0005831624780382429, 0.007112825704585146, 0.0018366020301292683], 'topk_tokens': ['<|start_header_id|>', 'Just', '.\n\n', '\n', ',', 'Answer', '<|start_header_id|>', 'Question', '.\n\n', '\n\n', 'ing', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'b', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.307861328125e-05, 9.334087371826172e-05, 0.001325736443201701, 0.0031548142433166504]}, 'saliency': {'score': [8.112341165542603e-05, 0.00019128947395076376, 3.309675625392369e-05, 0.00019315275425054617, 8.897020899016282e-05], 'topk_tokens': ['<|eot_id|>', '\n', '.\n\n', ' bedroom', 'a', ':', '<|start_header_id|>', '.\n\n', '\n', ',', 'assistant', ' \n', 'ing', 'Question', '<|start_header_id|>', 'athroom', 'b', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [2.6464462280273437e-06, 3.317594528198242e-05, 5.467236042022705e-05, 0.00027883052825927734]}}, 29: {'grad': {'score': [0.746826171875, 0.6300691807176586, 0.714292471749442, 0.628797405318222, 0.6773142403569715], 'topk_tokens': [' facts', 'antically', 'fortunate', 'assistant', ' *', ' every', ' up', ' *', ' up', ' *', ' *', ' *', 'S', ' up', ' up', ' the', ' *', 'nes', ' upper', ' STR'], 'evidence_proportions': [0.701708984375, 0.71005859375, 0.813720703125, 0.74884033203125]}, 'weight': {'score': [0.0018260940909385682, 0.007155849577327555, 0.001103412253516061, 0.007232505912440984, 0.0028827842967263585], 'topk_tokens': ['\n\n', '\n', ',', ' *\n\n', ' *', '.', '.\n\n', 'Answer', 'Question', '<|start_header_id|>', ' *', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0001237034797668457, 0.0001088559627532959, 0.0019188622633616128, 0.005961477756500244]}, 'saliency': {'score': [7.457435131072999e-05, 0.00021720183210039852, 7.477998733520507e-05, 0.0002190885230446872, 0.00021531612708650786], 'topk_tokens': [' *', '\n\n', '<|start_header_id|>', 'press', ' news', '\n', ' Min', 'Question', ' *\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '186', '.', 'assistant', 'athroom', '\n\n', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [6.3478946685791016e-06, 4.00543212890625e-06, 8.27858845392863e-05, 0.00023575127124786377]}}, 30: {'grad': {'score': [0.376189136505127, 0.3412090146533099, 0.3440267290387835, 0.3410168571086474, 0.3605912307213093], 'topk_tokens': ['ab', ' bathroom', '<|eot_id|>', ' by', '9', ' back', ' Bor', 'EF', ' brought', ' border', ' Brown', 'Bridge', ' bend', 'ed', ' balance', 'b', ' Bench', 'RI', ' B', ' B'], 'evidence_proportions': [0.3486328125, 0.528521728515625, 0.28024450937906903, 0.3641357421875]}, 'weight': {'score': [0.0029280737042427065, 0.006818728312145477, 0.002207058668136597, 0.006876316547508588, 0.006766943582173051], 'topk_tokens': [' two', '<|start_header_id|>', 'Civil', ',', '.', ' morning', 'columns', 'question', '<|start_header_id|>', '.\n\n', 'Question', '<|eot_id|>', '<|eot_id|>', '\n\n', 'athroom', '<|start_header_id|>', 'assistant', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00012793540954589844, 0.00028973221778869627, 0.0045509934425354, 0.0072917938232421875]}, 'saliency': {'score': [0.00014000684022903443, 0.0003133782456226489, 0.00013232316289629254, 0.00031573846613379354, 0.00039642072957137535], 'topk_tokens': ['Answer', ' two', '.', 'states', 'Bridge', '<|start_header_id|>', 'system', '<|begin_of_text|>', '<|start_header_id|>', ' morning', '<|eot_id|>', 'question', 'Question', '<|eot_id|>', '\n\n', 'athroom', '<|start_header_id|>', 'b', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [3.546476364135742e-06, 0.00013496875762939454, 0.00015543897946675617, 0.000293731689453125]}}, 31: {'grad': {'score': [0.31084022521972654, 0.3087738319880947, 0.3026269912719727, 0.30881567396219956, 0.3426418633296572], 'topk_tokens': [' anything', 'ot', ' journey', ' passed', ' others', ' effort', ' majority', ' employ', '<|end_header_id|>', ' representatives', ' instance', 'assistant', '<|start_header_id|>', 'columns', 'user', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.19205627441406248, 0.326318359375, 0.4397468566894531, 0.246612548828125]}, 'weight': {'score': [0.0011674731969833374, 0.0063596033489202826, 0.0012006776673453195, 0.006428068683993432, 0.0036389909941574624], 'topk_tokens': [' return', ' or', ' \n', 'Just', 'Bridge', ' Where', '.\n\n', ':', 'ing', 'Answer', 'Question', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', '\n\n', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00044708251953124996, 0.00036365389823913576, 0.0013752530018488567, 0.00276106595993042]}, 'saliency': {'score': [6.71282410621643e-05, 0.00018704399894891994, 4.907165254865374e-05, 0.0001887838514481362, 0.00021722655871818806], 'topk_tokens': [' You', ' or', ' return', 'being', '\n\n', '.', ' \n', ' Where', ' genu', ' morning', 'ing', '<|eot_id|>', 'Question', 'assistant', '<|begin_of_text|>', 'Bridge', '<|eot_id|>', 'athroom', '<|end_header_id|>', 'b'], 'evidence_proportions': [5.716085433959961e-06, 0.00014065504074096678, 4.501640796661377e-05, 8.515268564224243e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:27:18.254 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:27:18.254 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-10_pid-1_2-5-7-9.pkl | len: 10 |  size: 9.48 KB
Processing depth (2, 5, 7, 9):   2%|▏         | 2/100 [00:27<22:17, 13.65s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.00it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Processing depth (1, 3, 4, 9):   2%|▏         | 2/100 [00:36<22:17, 13.65s/it]2025-01-23 22:27:27.901 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:27:27.903 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (414, 419) --> . Mary picked up the
2025-01-23 22:27:27.903 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:27:27.910 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1446, 1451) --> . Mary moved to the
2025-01-23 22:27:27.910 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:27:27.919 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1806, 1812) --> . Mary journeyed to the
2025-01-23 22:27:27.920 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel dropped the football.
2025-01-23 22:27:27.937 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3613, 3617) -->  dropped the football.
2025-01-23 22:27:27.937 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:27:27.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (415, 420) -->  Mary picked up the apple
2025-01-23 22:27:27.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:27:27.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (32, 37) -->  moved to the kitchen.
2025-01-23 22:27:27.939 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:27:27.949 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1995, 2001) --> . John went back to the
2025-01-23 22:27:27.949 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:27:27.953 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (675, 679) -->  Daniel took the football
2025-01-23 22:27:27.953 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:27:27.965 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2509, 2513) -->  Daniel left the apple
2025-01-23 22:27:27.965 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John moved to the garden.
2025-01-23 22:27:27.982 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3589, 3594) --> . John moved to the
2025-01-23 22:27:27.983 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra journeyed to the office.
2025-01-23 22:27:27.984 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (345, 351) -->  Sandra journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:27:28.498 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:27:28.498 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4220])
your chose emoji: ['👨🏼\u200d🍼', '🧙🏽\u200d♀', '🙍🏽\u200d♀', '🧑\u200d🏭', '◀', '🤼\u200d♀️', '👩\u200d🦽\u200d➡️', '🦭', '👷🏼\u200d♂', '💆🏼\u200d♂️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4223, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 220752.84it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 118.24it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.31it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.94it/s]
2025-01-23 22:27:31.575 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.34111175537109373, 0.48582069348193674, 0.39554999215262276, 0.4872731055041879, 0.635456297132704], 'topk_tokens': ['system', ' appear', 'que', ' was', ' were', ' took', 'UG', '-per', ' could', ' Wright', ' not', ' had', ' were', ' come', 'G', 'ION', 'ION', 'Dub', ' Dub', 'could'], 'evidence_proportions': [0.4635139465332031, 0.3279701232910156, 0.2922210693359375, 0.27787208557128906]}, 'weight': {'score': [0.00931902527809143, 0.007464483849617815, 0.04133437616484506, 0.007171168816123952, 0.0008175033662054273], 'topk_tokens': ['\n\n', '\n\n', '<|eot_id|>', '\n\n', 'es', '<|end_header_id|>', 'user', '.', '\n\n', 'athroom', 'system', 'assistant', '<|start_header_id|>', 'b', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0002205967903137207, 0.00026988983154296875, 0.0059285660584767665, 0.03708916902542114]}, 'saliency': {'score': [0.00021622925996780395, 0.00011269794435205263, 0.0002734788826533726, 0.00011085102516950435, 4.326014055146111e-05], 'topk_tokens': [' people', ' idol', '.', '\n', ' Knowledge', '<|start_header_id|>', ' morning', 'user', 'system', ' cash', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'b', ' platform', '<|eot_id|>', 'athroom', '\n\n', '***', '<|begin_of_text|>'], 'evidence_proportions': [6.216764450073242e-06, 2.4205446243286134e-05, 0.0005419601996739706, 0.00023017823696136475]}}, 25: {'grad': {'score': [0.775933837890625, 0.6733275008602445, 0.6255790710449218, 0.6732361064991429, 0.7737224102020264], 'topk_tokens': [' At', ' upper', ' called', ' were', '<|eot_id|>', ' took', ' members', ' During', ' *\n\n', ' would', 'u', 'APER', ' awarded', ' from', '<|eot_id|>', '.', ' met', ' at', ' cap', ' among'], 'evidence_proportions': [0.9457763671875, 0.79466552734375, 0.79168701171875, 0.5165863037109375]}, 'weight': {'score': [0.007782042026519775, 0.007427125018890113, 0.04122844338417053, 0.007141581716841791, 0.0009865276515483856], 'topk_tokens': ['<|end_header_id|>', 'system', ' ty', 'athroom', '\n\n', ' of', '<|start_header_id|>', '<|start_header_id|>', '\n\n', 'b', '.', 'user', '<|eot_id|>', 'assistant', 'es', '<|eot_id|>', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0004932820796966553, 0.000659710168838501, 0.0018599331378936768, 0.034679070115089417]}, 'saliency': {'score': [0.00017493218183517456, 0.00011525563004737834, 0.00033407722200666154, 0.00011313176086447747, 5.565169784757826e-05], 'topk_tokens': ['b', 'es', ' Stephen', ' Min', ' Date', ' money', 'RE', '<|start_header_id|>', 'Min', 'user', ' Min', ' Min', 'athroom', ' Min', '.', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [5.1194429397583006e-05, 7.253289222717286e-05, 0.0001049041748046875, 0.0005626454949378967]}}, 26: {'grad': {'score': [0.5625417709350586, 0.6308452543872469, 0.6481227329799107, 0.6310279218340561, 0.5237187279595269], 'topk_tokens': [' com', '\n', ' Eagle', ' office', ' business', ' of', 'void', 'nes', ' platform', 'there', ' could', ' been', ' laid', ' court', ' com', ' room', ' city', ' conditions', ' looked', 'system'], 'evidence_proportions': [0.599053955078125, 0.66343994140625, 0.5016403198242188, 0.4821310043334961]}, 'weight': {'score': [0.014830806851387024, 0.007421657481860866, 0.05804333261081151, 0.006961018898150742, 0.0026699089341693455], 'topk_tokens': ['\n\n', '\n\n', ' idol', ' cash', 'es', '<|start_header_id|>', '<|end_header_id|>', 'assistant', 'user', '<|start_header_id|>', '<|eot_id|>', '.', 'b', 'athroom', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005615651607513428, 0.0012891888618469238, 0.005617504318555196, 0.06341433525085449]}, 'saliency': {'score': [0.0018856048583984375, 0.00018372845937650746, 0.0024202193532671247, 0.0001567815521628294, 0.0001415415770477719], 'topk_tokens': [' instance', ' of', ' next', 'nes', ' news', '\n\n', ' paper', '<|start_header_id|>', 'user', ' Note', ' cash', '<|start_header_id|>', ' Pioneer', ' headlines', '<|start_header_id|>', 'graph', 'es', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [1.4680624008178712e-05, 4.382133483886719e-05, 0.00034037729104359943, 0.008844330906867981]}}, 27: {'grad': {'score': [0.8393207550048828, 0.8193968136728925, 0.7485042027064732, 0.8198965167816221, 0.883607440524631], 'topk_tokens': ['.', ' take', ' were', ';', ',', ',', '\n', ',', ',', ',', ',', ' be', ',', ',', '.', ',', ',', ' and', ',', ','], 'evidence_proportions': [1.262451171875, 0.83304443359375, 0.7264811197916667, 0.48751258850097656]}, 'weight': {'score': [0.009381918609142304, 0.007466688388731421, 0.060749986342021396, 0.007010061701405758, 0.0019632805552747515], 'topk_tokens': ['Question', '<|start_header_id|>', '\n\n', '\n\n', '<|end_header_id|>', '\n\n', ' idol', 'es', '<|eot_id|>', '.', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'athroom', 'user', 'b', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005162477493286133, 0.0006442308425903321, 0.003151531020800273, 0.04073169827461243]}, 'saliency': {'score': [0.0003788799047470093, 0.000201184059979084, 0.000993657112121582, 0.00019367674862583403, 0.0002802862889236874], 'topk_tokens': ['      ', 'rail', ' ty', '<|end_header_id|>', 'athroom', ' perpet', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'graph', 'assistant', 'Question', 'RE', 'user', '<|end_header_id|>', '<|start_header_id|>', 'b', '<|begin_of_text|>', ' idol', '.'], 'evidence_proportions': [3.7282705307006836e-05, 4.7659873962402345e-05, 0.0004714975754419963, 0.0010809749364852905]}}, 28: {'grad': {'score': [1.0078651428222656, 0.8984910340323822, 1.0284824916294644, 0.8968746273165243, 0.8388352923923068], 'topk_tokens': [' a', ' of', ' a', ' a', ' a', ' A', ' A', ' a', ' a', ' a', ' a', 'com', ' a', ' a', ' a', ' a', '.', '\n\n', ' a', ' a'], 'evidence_proportions': [1.18851318359375, 1.0605743408203125, 0.9683430989583333, 0.77545166015625]}, 'weight': {'score': [0.0032234668731689455, 0.007189613367577963, 0.04978353466306414, 0.006850970297651419, 0.0008769254717561933], 'topk_tokens': ['\n', 'Answer', 'Just', 'user', '.', ' the', '<|eot_id|>', 'es', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', 'b', '\n\n', 'athroom', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00017900466918945314, 0.0004498720169067383, 0.0008670687675476074, 0.014030635356903076]}, 'saliency': {'score': [0.00010164231061935425, 0.00012077639206252391, 0.0009962533201490129, 0.00011351655260615065, 2.7906977468066747e-05], 'topk_tokens': ['<|eot_id|>', ' of', ' afterward', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'user', 'Just', 'es', '\n\n', ' party', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'b', '.', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [3.62396240234375e-06, 1.969337463378906e-05, 2.3186206817626953e-05, 0.0004442855715751648]}}, 29: {'grad': {'score': [0.5844314575195313, 0.7028410990825908, 0.5773194449288505, 0.7044633281436854, 0.8159186045328776], 'topk_tokens': ['Democratic', '<|start_header_id|>', 'announcement', 'message', ' Gen', 'd', ' Times', ' Foster', '\u200d', ' daily', ' S', '�', 'Republicans', ' Charles', ' Stephen', ' STR', 'SP', 'L', ' B', 'S'], 'evidence_proportions': [0.55531005859375, 0.4362060546875, 0.6409912109375, 0.7212753295898438]}, 'weight': {'score': [0.0038907885551452636, 0.0072616892856866185, 0.05021440897669111, 0.006917177007717729, 0.0018679193324512905], 'topk_tokens': ['\n', ',', '      ', ' *\n\n', ' *', '\n\n', '\n\n', '<|start_header_id|>', ',', ' *\n\n', '<|start_header_id|>', 'b', '<|eot_id|>', '<|eot_id|>', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00018250346183776853, 0.00020943284034729003, 0.00371625026067098, 0.013389647006988525]}, 'saliency': {'score': [0.0002708569169044495, 0.00025852352359581127, 0.0026196488312312536, 0.00023863723433635513, 0.0003170275853739844], 'topk_tokens': [' cash', ' money', 'system', '�', 'Just', '\n', 'assistant', 'athroom', 'b', ',', '<|start_header_id|>', ' idol', ' *\n\n', ' perpet', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '.', '<|begin_of_text|>'], 'evidence_proportions': [7.152557373046875e-06, 9.322166442871092e-06, 0.0003251582384109497, 0.0008459538221359253]}}, 30: {'grad': {'score': [0.715936279296875, 0.6788506694182157, 0.6468146187918526, 0.6789417321759771, 0.5519280963473849], 'topk_tokens': [' J', ' to', ' boats', ' to', ' bringing', ' a', ' business', ' business', ' been', ' business', ' bend', 'EF', ' Bor', ' Bench', ' balance', 'RI', 'ab', ' B', ' B', ' o'], 'evidence_proportions': [0.5031982421875, 0.9174560546874999, 0.76361083984375, 0.658447265625]}, 'weight': {'score': [0.008481049537658691, 0.007009078506155373, 0.007750374930245536, 0.006995790407433391, 0.0045783631503582], 'topk_tokens': [' cash', '-text', 'Question', ' write', 'ed', '<|start_header_id|>', ' *\n\n', ',', 'user', ' the', '.', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'assistant', 'b', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00040937662124633793, 0.0005222678184509278, 0.023472468058268227, 0.006031990051269531]}, 'saliency': {'score': [0.0005993098020553589, 0.0003496077059005748, 0.00032396061079842706, 0.0003486248859402772, 0.0006261049873299069], 'topk_tokens': [' write', ' idol', ' *', 'assistant', '      ', 'graph', '<|start_header_id|>', '�', '      ', ' *\n\n', '<|eot_id|>', '-text', '<|start_header_id|>', ' B', '\n\n', '<|eot_id|>', 'athroom', '<|begin_of_text|>', '<|end_header_id|>', 'b'], 'evidence_proportions': [3.4743547439575194e-05, 7.46011734008789e-05, 0.0017549643913904827, 0.00022742152214050293]}}, 31: {'grad': {'score': [0.6509590148925781, 0.9348284147543956, 0.6222327096121651, 0.9388155159245526, 1.1233557065327961], 'topk_tokens': [' const', ' affairs', 'graph', 'ot', 'ot', ' about', 'ot', ' against', 'ot', 'ot', ' previous', '8', '<|start_header_id|>', ' just', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.5081268310546875, 0.49760742187499996, 0.7456156412760416, 0.8792037963867188]}, 'weight': {'score': [0.004649496078491211, 0.0063735025628550825, 0.0037622434752328055, 0.006403702706270163, 0.0017350088391039106], 'topk_tokens': [' it', 'Just', ' and', ' location', ':', ' dropped', ' not', 'Question', ' the', 'Answer', '<|start_header_id|>', '<|eot_id|>', ' write', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00028544068336486814, 0.00044730305671691895, 0.0023007094860076904, 0.018880486488342285]}, 'saliency': {'score': [0.00018199533224105835, 0.0002257778566650889, 0.0001540660858154297, 0.00022659013172944082, 0.0001040717793835534], 'topk_tokens': [' perpet', ' item', 'Just', ' not', ' it', '\n\n', 'system', ' Where', '<|start_header_id|>', ' dropped', ' write', 'Question', ' location', 'assistant', '<|begin_of_text|>', '<|eot_id|>', 'athroom', '<|eot_id|>', '<|end_header_id|>', 'b'], 'evidence_proportions': [3.260374069213867e-05, 1.2403726577758789e-05, 7.09990660349528e-05, 0.0007472187280654907]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:27:31.581 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:27:31.581 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-10_pid-2_1-3-4-9.pkl | len: 10 |  size: 9.28 KB
Processing depth (1, 3, 4, 9):   3%|▎         | 3/100 [00:40<21:49, 13.50s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Processing depth (2, 3, 7, 9):   3%|▎         | 3/100 [00:50<21:49, 13.50s/it]2025-01-23 22:27:41.513 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:27:41.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (928, 933) --> . Mary picked up the
2025-01-23 22:27:41.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:27:41.525 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1481, 1486) -->  Mary moved to the bathroom
2025-01-23 22:27:41.526 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:27:41.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2840, 2846) --> . Mary journeyed to the
2025-01-23 22:27:41.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel dropped the football.
2025-01-23 22:27:41.557 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3626, 3630) -->  dropped the football.
2025-01-23 22:27:41.558 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:27:41.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (929, 934) -->  Mary picked up the apple
2025-01-23 22:27:41.562 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:27:41.563 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (32, 37) -->  moved to the kitchen.
2025-01-23 22:27:41.563 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:27:41.573 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2056, 2062) --> . John went back to the
2025-01-23 22:27:41.573 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:27:41.576 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (672, 676) -->  Daniel took the football
2025-01-23 22:27:41.576 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:27:41.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2531, 2535) -->  left the apple.
2025-01-23 22:27:41.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John moved to the garden.
2025-01-23 22:27:41.606 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3602, 3607) --> . John moved to the
2025-01-23 22:27:41.606 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra journeyed to the office.
2025-01-23 22:27:41.608 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (347, 353) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:27:42.302 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The apple was in the office.<|eot_id|>
2025-01-23 22:27:42.303 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4227])
your chose emoji: ['⛹🏻\u200d♀', '⁉️', '👱🏼\u200d♂️', '🐵', '👨🏽\u200d❤️\u200d💋\u200d👨🏾', '👩🏻\u200d🌾', '👩🏽\u200d✈️', '👳🏽\u200d♂️', '⛸', '🪧']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4230, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 198546.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.56it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 122.25it/s]
2025-01-23 22:27:45.280 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.6788649559020996, 0.6436198493831264, 0.630500738961356, 0.6435609911159127, 0.6678710889212692], 'topk_tokens': ['202', '�', ' John', 'ot', ' John', ' John', '�', 'sur', ' of', ' feature', ' did', 'cap', ' conditions', ' board', ' *\n\n', '️', '.\n\n', 'super', ' *\n\n', '\n\n'], 'evidence_proportions': [0.6576351165771485, 0.660247802734375, 0.6758778889973959, 0.733154296875]}, 'weight': {'score': [0.002512800693511963, 0.007428880335309545, 0.00223666855267116, 0.007495957941352251, 0.0012970596929139729], 'topk_tokens': ['Answer', ' ', '\n', ' \n', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'user', 'b', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001323091983795166, 0.005058372020721435, 0.0012733737627665203, 0.0026771128177642822]}, 'saliency': {'score': [0.0004124999046325684, 0.00014194412857082718, 0.0002831527165004185, 0.0001394642684274091, 9.426776366897776e-05], 'topk_tokens': [' bedroom', '.\n\n', '\n\n\n\n', 'Answer', '***', 'user', 'Question', '<|start_header_id|>', '\n', ' bathroom', '<|eot_id|>', '<|eot_id|>', ' \n', 'athroom', '<|eot_id|>', '<|end_header_id|>', 'andra', 'assistant', '\n\n', '\n\n'], 'evidence_proportions': [9.36269760131836e-05, 0.001340949535369873, 9.184579054514568e-05, 0.0001315101981163025]}}, 25: {'grad': {'score': [1.0563072204589843, 1.1209700924848551, 0.9981785365513393, 1.12230924503532, 0.8604628164556962], 'topk_tokens': [' inverted', 'illage', ' exc', 'out', ' into', 'old', 'ured', ' as', ' in', ' a', ' every', ',', ' arrival', 'e', ' revisit', ' in', ' waving', 'ing', ' a', ' at'], 'evidence_proportions': [1.0773834228515624, 1.2902404785156252, 0.9317423502604166, 0.9243927001953125]}, 'weight': {'score': [0.0030084952712059023, 0.007316639181006321, 0.0014306596347263881, 0.007386620537249628, 0.0011350818072693258], 'topk_tokens': ['\n\n', ' apple', ' Do', '.\n\n', ' Bench', 'user', '<|start_header_id|>', ':', '<|eot_id|>', 'b', '\n\n', '<|eot_id|>', ' \n', '<|eot_id|>', 'Answer', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0020028889179229737, 0.005138015747070313, 0.0018871625264485679, 0.0032856017351150513]}, 'saliency': {'score': [0.0003219828009605408, 0.00026236579491455223, 0.00012081776346479143, 0.0002632668346701982, 7.756261885920657e-05], 'topk_tokens': [' \n', '\n\n\n\n', 'reading', ' person', ' obtained', '<|start_header_id|>', 'RE', ' Team', ' be', 'b', '<|eot_id|>', '.\n\n', ' apple', 'assistant', '<|eot_id|>', 'athroom', 'Answer', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0002360224723815918, 0.00046967864036560056, 0.0002653251091639201, 0.00032979995012283325]}}, 26: {'grad': {'score': [0.6970268249511719, 0.8693159387466756, 0.8119546072823661, 0.8706221492561752, 0.6755275484881823], 'topk_tokens': [' the', ' of', ' At', 'a', ' gang', ' the', ' three', ' the', ' force', "'s", ' the', "'s", ' three', ' state', ' the', ' and', 'their', ' the', ' the', ' the'], 'evidence_proportions': [0.67481689453125, 0.7956909179687499, 0.6314315795898438, 0.6998519897460938]}, 'weight': {'score': [0.004713600873947144, 0.0071687060326831, 0.0029353763375963484, 0.007215956006935257, 0.002459103170829483], 'topk_tokens': [':', ' *\n\n', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', 'Answer', '\n\n', '<|eot_id|>', 'b', ' \n', '\n\n', '\n\n', 'user', '<|start_header_id|>', '<|eot_id|>', '.\n\n', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002158927917480469, 0.011682558059692384, 0.0027690132459004717, 0.0021126270294189453]}, 'saliency': {'score': [0.0023014098405838013, 0.00021657493520290294, 0.0003343548093523298, 0.00020560032593275971, 0.00013876489446132997], 'topk_tokens': [' *\n\n', 'andra', ' \n', 'b', ' ', ' Bench', 'user', ' *\n\n', '<|eot_id|>', '\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '.\n\n', ' bathroom', '<|eot_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00012053847312927247, 0.008723044395446777, 0.0001231531302134196, 0.0002678409218788147]}}, 27: {'grad': {'score': [1.4771339416503906, 1.6897163409149676, 1.4551083156040736, 1.6927014735787214, 1.3576354014722607], 'topk_tokens': [',', '.', ';', '.', ',', '.', ' conditions', '.', '.', '.', '.', ',', ',', ',', ',', ' state', '--', ';', 'ors', '.'], 'evidence_proportions': [1.456501770019531, 1.43349609375, 1.4614117940266929, 1.5810546875]}, 'weight': {'score': [0.00910528302192688, 0.007220045950959478, 0.0025869897433689663, 0.007249854867329855, 0.0021372456339341177], 'topk_tokens': ['RE', ' *\n\n', ' bathroom', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', ':', 'Answer', ' \n', '\n\n', '\n\n', 'user', 'b', '<|end_header_id|>', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0016555547714233398, 0.028311395645141603, 0.003407915433247884, 0.0029558539390563965]}, 'saliency': {'score': [0.007945235073566436, 0.000295374833100231, 0.0004449103559766497, 0.00025747520481041094, 0.0001628259314766413], 'topk_tokens': ['user', '<|start_header_id|>', 'EF', '\n\n', '\n\n\n\n', 'Just', 'If', 'NEW', '<|eot_id|>', 'If', 'Answer', 'If', 'Question', 'assistant', ' \n', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom'], 'evidence_proportions': [0.00014461278915405272, 0.031207680702209473, 0.00016396741072336834, 0.0002898573875427246]}}, 28: {'grad': {'score': [0.8567180633544922, 0.8898826797521424, 0.8141741071428571, 0.8906762348106522, 0.7885083065757269], 'topk_tokens': ['ian', 'antics', ' Min', 'antically', ' pet', '\n', '\n', 'ied', 'nes', ' in', ' Dub', 'Penn', 'Min', ' Pa', ' l', 'antic', 't', ' Min', 'nes', ' of'], 'evidence_proportions': [0.5928314208984375, 1.0698974609375, 1.17816162109375, 0.43793678283691406]}, 'weight': {'score': [0.004172119498252869, 0.006861284720418583, 0.002177728073937552, 0.006913430298159936, 0.0011352330823487873], 'topk_tokens': [' bedroom', '.\n\n', ' *\n\n', ' person', '.', '<|start_header_id|>', '<|eot_id|>', ':', '\n\n', '<|eot_id|>', '.\n\n', 'Answer', 'b', '.\n\n', ' \n', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001192939281463623, 0.005916023254394531, 0.0063515305519104, 0.00244709849357605]}, 'saliency': {'score': [0.0007910892367362976, 0.00018091851374218087, 0.00015810132026672363, 0.00017818682208032665, 6.536139717584924e-05], 'topk_tokens': ['.', ' the', 'nes', '.', 'athroom', '<|start_header_id|>', 'being', ' By', ' bathroom', ' During', ' Stephen', '<|eot_id|>', '<|eot_id|>', '.\n\n', ' \n', '\n\n', '<|end_header_id|>', 'assistant', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [6.663799285888672e-05, 0.001254904270172119, 0.0009967337052027385, 0.000808417797088623]}}, 29: {'grad': {'score': [1.1550247192382812, 0.8441161820792701, 1.1840401785714285, 0.8397771376192926, 0.950660029544106], 'topk_tokens': [' the', ' the', ' to', ' to', ' to', 'ION', '.', '.', ' to', ' and', 'b', ' to', ' up', ' in', ' to', ' up', ' to', ' to', ' to', ' to'], 'evidence_proportions': [0.8436767578125, 1.5146484375, 1.3810882568359375, 0.755584716796875]}, 'weight': {'score': [0.002531197667121887, 0.007203122853669151, 0.003182494640350342, 0.007259209198866062, 0.0012701998783063286], 'topk_tokens': [' the', ' Do', '.\n\n', 'If', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'user', ' in', 'Answer', '\n\n', '\n\n', ' \n', ':', 'b', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015154480934143066, 0.0031618118286132813, 0.003520329793294271, 0.001528918743133545]}, 'saliency': {'score': [0.00048055350780487063, 0.00020275523899294806, 0.00045309833117893766, 0.0001993257842377988, 8.615474157695528e-05], 'topk_tokens': ['A', ' the', 'If', '<|start_header_id|>', ' ', '<|start_header_id|>', '\n', '\n\n', '\n\n', '<|eot_id|>', ' the', ':', ' bedroom', '\n\n', ' in', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.00017260909080505372, 0.000580132007598877, 0.0009543846050898234, 3.026425838470459e-05]}}, 30: {'grad': {'score': [0.5111370086669922, 0.41123527364527923, 0.48887045724051337, 0.41010586858509546, 0.4413669079164915], 'topk_tokens': [' for', ' to', ' for', 'blue', ' a', '0', ' board', ' block', 'a', ' bathroom', ' an', ' a', ' body', ' bill', ' at', ' four', ' B', '202', ' B', 'b'], 'evidence_proportions': [0.56241455078125, 0.5951889038085938, 0.44086201985677087, 0.4473876953125]}, 'weight': {'score': [0.008479124307632447, 0.007015066620305921, 0.007642097132546561, 0.007002796627090363, 0.004471569876127605], 'topk_tokens': ['\n\n\n', '\n\n', 'Question', '<|eot_id|>', 'user', '<|eot_id|>', 'nes', ':', '.\n\n', 'Answer', '<|eot_id|>', ' *\n\n', '<|start_header_id|>', '\n\n', ' \n', '<|end_header_id|>', 'b', 'athroom', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.00297398567199707, 0.022948431968688964, 0.0051025549570719395, 0.0023387670516967773]}, 'saliency': {'score': [0.004200702905654908, 0.00019793912028589992, 0.0006159254482814244, 0.0001752601269476428, 0.00018326692943331562], 'topk_tokens': ['In', '<|eot_id|>', '<|eot_id|>', 'RE', '.\n\n', 'user', '      ', '<|start_header_id|>', ' kitchen', 'E', '<|begin_of_text|>', '<|start_header_id|>', ':', '<|end_header_id|>', ' \n', 'athroom', '\n\n', 'b', 'assistant', ' bathroom'], 'evidence_proportions': [0.0001165926456451416, 0.015939152240753172, 0.00046244263648986816, 0.0002401694655418396]}}, 31: {'grad': {'score': [0.5230906486511231, 0.6575306705267435, 0.5805858067103795, 0.658819741346165, 0.5580207003822809], 'topk_tokens': [' of', 'nes', ' PA', ' type', 'nes', ' was', 'ot', ' ever', ' THE', 'Penn', ' members', '0', '      ', ' Proof', ' per', 'user', ' four', ' the', ' Min', ' hours'], 'evidence_proportions': [0.37822265625, 0.6492122650146485, 0.5031013488769531, 0.576507568359375]}, 'weight': {'score': [0.0041164100170135495, 0.006728700092216474, 0.0026361278125217984, 0.0067755230458196765, 0.0017757759064058714], 'topk_tokens': ['Just', '.\n\n', 'us', '.\n\n', 'If', 'Question', '.\n\n', '<|eot_id|>', '<|start_header_id|>', '?', ':', 'Answer', '<|eot_id|>', '<|end_header_id|>', ' \n', 'assistant', '\n\n', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0029510498046874997, 0.00812668800354004, 0.0030461351076761884, 0.002165675163269043]}, 'saliency': {'score': [0.0005657389760017395, 0.00016646748034384798, 0.00014454637254987445, 0.00016473857228627462, 6.470461434955839e-05], 'topk_tokens': ['If', '?', 'andra', 'If', 'us', ' in', '.\n\n', '<|eot_id|>', '<|start_header_id|>', ' bathroom', 'Answer', '<|begin_of_text|>', ':', '\n\n', '<|eot_id|>', ' \n', '<|end_header_id|>', 'assistant', 'athroom', 'b'], 'evidence_proportions': [7.136464118957519e-05, 0.0019447147846221925, 0.00013431409994761148, 0.0001071244478225708]}}, 'pred_res': 'The apple was in the office.<|eot_id|>', 'score': 0}
2025-01-23 22:27:45.286 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:27:45.286 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-10_pid-3_2-3-7-9.pkl | len: 10 |  size: 8.9 KB
Processing depth (2, 3, 7, 9):   4%|▍         | 4/100 [00:54<21:43, 13.58s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.10it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Processing depth (0, 1, 4, 7):   4%|▍         | 4/100 [01:03<21:43, 13.58s/it]2025-01-23 22:27:54.299 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:27:54.299 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-23 22:27:54.299 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:27:54.302 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (481, 486) --> . Mary moved to the
2025-01-23 22:27:54.302 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:27:54.312 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1837, 1843) --> . Mary journeyed to the
2025-01-23 22:27:54.312 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel dropped the football.
2025-01-23 22:27:54.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2878, 2882) -->  Daniel dropped the football
2025-01-23 22:27:54.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:27:54.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-23 22:27:54.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:27:54.326 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (34, 39) -->  apple. Sandra moved to
2025-01-23 22:27:54.327 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:27:54.337 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2031, 2037) -->  the ground. John went back
2025-01-23 22:27:54.337 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:27:54.341 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (738, 742) -->  Daniel took the football
2025-01-23 22:27:54.341 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:27:54.353 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2577, 2581) -->  Daniel left the apple
2025-01-23 22:27:54.353 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John moved to the garden.
2025-01-23 22:27:54.372 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (3666, 3671) --> . John moved to the
2025-01-23 22:27:54.372 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra journeyed to the office.
2025-01-23 22:27:54.374 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (431, 437) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:27:54.919 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:27:54.919 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4232])
your chose emoji: ['🚴🏽', '👩🏻\u200d❤️\u200d👩🏻', '🥈', '☮️', '🧗🏽\u200d♂', '🙎🏿\u200d♀️', '🚣🏽\u200d♀', '🧑🏿\u200d❤\u200d💋\u200d🧑🏼', '🏳️', '👳🏼\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4235, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 237974.70it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.13it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.82it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.38it/s]
2025-01-23 22:27:58.056 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.14705486297607423, 0.13232377918157837, 0.17342376708984375, 0.13190915687232496, 0.1054230535731596], 'topk_tokens': [' curs', ' where', ' be', '25', 'cap', ' past', ' Published', ' rest', 'Democratic', 'ION', ' Daniel', 'EF', ' Sandra', ' during', '-t', ' editor', 'fect', ' B', 'able', ' Do'], 'evidence_proportions': [0.16644706726074218, 0.11295394897460936, 0.1301565170288086, 0.19078826904296875]}, 'weight': {'score': [0.002428892254829407, 0.007447334388633827, 0.0441843398979732, 0.0071637395680235905, 0.0011923106277690215], 'topk_tokens': ['speech', ' Wood', '\n\n', 'Just', '\n\n', ' ', 'user', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', 'athroom', 'b', '<|end_header_id|>', 'assistant', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0032843589782714845, 0.001123058795928955, 0.0019364356994628906, 0.0037305355072021484]}, 'saliency': {'score': [4.911273717880249e-05, 5.4286566366130374e-05, 0.0002809805529458182, 5.2413166139684795e-05, 1.067028326146743e-05], 'topk_tokens': [' Wood', ' boats', ' West', ' boat', ' Bor', '<|end_header_id|>', ' management', ' location', ' struggle', ' boat', '.', 'user', ' ', 'assistant', ' Project', 'athroom', ' ground', '\n\n', ' Bank', '<|begin_of_text|>'], 'evidence_proportions': [3.690123558044433e-05, 3.1936168670654294e-05, 1.573065916697184e-05, 0.0001359209418296814]}}, 25: {'grad': {'score': [0.14244141578674316, 0.1553004673549107, 0.1469942092895508, 0.15543154392516215, 0.1060107848223518], 'topk_tokens': [' Good', ' an', 'hand', ' old', ' ST', ' past', ' hand', '.', ' Knowledge', ' the', ' *\n\n', ' St', ' the', ' ST', ' this', ' THE', '26', '3', ' hand', ' Wood'], 'evidence_proportions': [0.11285839080810547, 0.19547882080078127, 0.12120246887207031, 0.14498186111450195]}, 'weight': {'score': [0.002806900441646576, 0.007397154680812795, 0.054193202938352315, 0.0070272846798007, 0.0008315486066481646], 'topk_tokens': [' printed', 'istributed', 'doctor', 'user', '<|eot_id|>', ' secured', '.\n\n', 'rail', '<|start_header_id|>', '\n\n', ' Reporter', '<|eot_id|>', '<|eot_id|>', 'b', 'athroom', '\n\n', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0033069610595703124, 0.001577216386795044, 0.0027728676795959473, 0.003769978880882263]}, 'saliency': {'score': [8.037835359573365e-05, 8.735952298503276e-05, 0.00012399298804146902, 8.708618616943725e-05, 9.442427579094381e-06], 'topk_tokens': [' of', ' doctor', ' platform', ' bend', ' printed', 'assistant', '.', ' Reporter', ' Note', ' secured', 'doctor', ' rest', ' Times', '<|eot_id|>', '<|eot_id|>', 'b', 'athroom', '\n\n', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [6.753802299499511e-05, 6.710886955261231e-05, 4.4102470080057784e-05, 0.00016742944717407227]}}, 26: {'grad': {'score': [0.08833997249603272, 0.10595117634274462, 0.11839125497000558, 0.1059312771381944, 0.11031233282650219], 'topk_tokens': [' admission', ' com', ' rates', 'positor', ' account', 'description', 'posit', ' three', ' three', ' *', '8', 'remember', 'com', ' three', 'ig', ' *', ' force', ' compos', ' room', ' bathroom'], 'evidence_proportions': [0.08810043334960938, 0.07710857391357422, 0.09923942883809407, 0.08632946014404297]}, 'weight': {'score': [0.005892369151115418, 0.007450879760165704, 0.07185865555490767, 0.006919037190635809, 0.003479146957397461], 'topk_tokens': [' cap', '\n\n', ' *\n\n', '\n\n', '<|start_header_id|>', ' Gal', 'rail', '.\n\n', 'user', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', 'b', 'assistant', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.012493324279785157, 0.0014616668224334715, 0.004766121506690979, 0.004868924617767334]}, 'saliency': {'score': [9.072273969650269e-05, 7.476440708921538e-05, 0.0021822520664760044, 5.7041623660822236e-05, 4.635733716628131e-05], 'topk_tokens': ['ARR', ' Pioneer', ' Pioneer', 'remark', ' cash', '.\n\n', 'doctor', ' Daniel', ' Pioneer', ' Fletcher', '<|start_header_id|>', ' editor', ' Gal', ' acquaintance', 'rail', 'b', 'athroom', '<|begin_of_text|>', 'assistant', '.'], 'evidence_proportions': [0.0001148819923400879, 2.6535987854003906e-05, 5.379319190979004e-05, 0.00019615143537521362]}}, 27: {'grad': {'score': [0.20734844207763672, 0.2747924660566706, 0.22969646453857423, 0.2754927628348319, 0.2747350300059599], 'topk_tokens': ['ot', 'es', 'ot', 'G', 'ot', ' do', 'ot', 'ot', ' state', ' supreme', 'g', ' press', ' pay', ' bill', ' cap', ' state', 'UL', ' interest', ' cap', ' cap'], 'evidence_proportions': [0.21454467773437502, 0.17674026489257813, 0.2574284871419271, 0.16149330139160156]}, 'weight': {'score': [0.004136130213737488, 0.007449797277889679, 0.07267566408429826, 0.006919502541779331, 0.0020949121783761415], 'topk_tokens': [' Gal', 'From', 'Johnson', '\n\n', 'Question', 'Mary', '.\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', '\n\n', '\n\n', 'Just', 'athroom', '<|end_header_id|>', 'assistant', 'b', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.007471656799316407, 0.0027344048023223875, 0.0035166790088017783, 0.0026480555534362793]}, 'saliency': {'score': [0.0001812577247619629, 9.280581119631932e-05, 0.0009592916284288679, 8.51273322789863e-05, 4.187717157251695e-05], 'topk_tokens': ['\n\n', ' Reporter', ' fr', ' board', ' lyn', 'Just', 'Question', '\n\n', 'human', '\n', '\n\n', 'RE', 'assistant', 'Johnson', ' pur', 'athroom', '<|end_header_id|>', '<|begin_of_text|>', 'b', '.'], 'evidence_proportions': [0.0005545973777770995, 6.853342056274414e-05, 6.695091724395752e-05, 2.6948750019073486e-05]}}, 28: {'grad': {'score': [0.15487070083618165, 0.12698604218934476, 0.14410787650517054, 0.12670925812858144, 0.10844446070053998], 'topk_tokens': [' A', ' a', 'A', ' a', ' mob', ' a', ' of', ' a', 'half', ' a', ' newspaper', 'a', 'un', ' attention', ' ago', ' the', ' a', ' a', ' bathroom', ' trib'], 'evidence_proportions': [0.19770278930664062, 0.18564300537109374, 0.14359315236409503, 0.07978153228759766]}, 'weight': {'score': [0.0021820083260536195, 0.007173473578839263, 0.09946941137313843, 0.006424543311436211, 0.0019893190440009624], 'topk_tokens': ['just', '\n\n\n', ' \n', ' unless', 'speech', '.\n\n', '.', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', 'Just', '.\n\n', '\n\n', 'b', 'athroom', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.003481292724609375, 0.0013604402542114259, 0.0012477089961369834, 0.002986311912536621]}, 'saliency': {'score': [3.8865208625793454e-05, 5.519292993556792e-05, 0.0005779658045087542, 5.089376816909279e-05, 3.181871245889103e-05], 'topk_tokens': [' the', ' Gray', 'made', ' platform', ' strife', '\n\n', ' new', '<|eot_id|>', ' the', '<|start_header_id|>', '.\n\n', '<|eot_id|>', 'Bridge', 'Just', 'assistant', 'b', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', '.'], 'evidence_proportions': [4.543662071228027e-05, 1.8858909606933592e-05, 3.4014383951822914e-05, 6.293505430221558e-05]}}, 29: {'grad': {'score': [0.18917746543884278, 0.1646863700929752, 0.1873425074986049, 0.16437948331878516, 0.1926769593182732], 'topk_tokens': ['4', ' Min', ' Min', 'nes', 'nes', ' Min', 'nes', 'Sh', ' Min', ' the', 'Min', ' Press', ' Wins', ' Min', 'still', 'nes', 'nes', ' Min', 'Min', 'Min'], 'evidence_proportions': [0.2203948974609375, 0.2047760009765625, 0.19448518753051758, 0.1226959228515625]}, 'weight': {'score': [0.003218132257461548, 0.007347148368322019, 0.08998393671853201, 0.006674969595585143, 0.0034226603367749383], 'topk_tokens': ['Just', ' ', ',', 'user', ' ', '\n\n', '\n\n', '<|start_header_id|>', ' ', '<|eot_id|>', '.\n\n', '<|eot_id|>', '.', 'b', 'athroom', '\n\n', 'assistant', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.007164287567138672, 0.001075643301010132, 0.0024022112290064497, 0.0021874308586120605]}, 'saliency': {'score': [5.183964967727661e-05, 7.849261780415685e-05, 0.001828582797731672, 6.396627882450961e-05, 0.0001436440383686739], 'topk_tokens': ['A', '      ', 'a', '<|eot_id|>', '�', 'assistant', '<|eot_id|>', ' *\n\n', '      ', '♀', 'athroom', '.\n\n', '<|start_header_id|>', '<|start_header_id|>', ' ', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0001042783260345459, 2.0557641983032225e-05, 5.268057187398275e-05, 2.413243055343628e-05]}}, 30: {'grad': {'score': [0.2094432830810547, 0.18956759294061762, 0.17970521109444754, 0.18955507369703084, 0.20151450213264016], 'topk_tokens': [' boat', 'burn', ' bathroom', ' boat', ' Bank', '9', ' body', ' bogus', ' by', ' boat', ' boats', ' Bor', ' be', 'ab', ' by', ' border', ' bend', ' block', ' B', ' B'], 'evidence_proportions': [0.25557861328125, 0.211297607421875, 0.1957867940266927, 0.16994094848632812]}, 'weight': {'score': [0.005174276232719421, 0.007162297236454951, 0.018786857809339252, 0.007074474461341018, 0.006219222966362448], 'topk_tokens': [' *\n\n', 'Question', ' Gal', ' *\n\n', 'complete', '<|eot_id|>', '.', '\n\n\n', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', '.\n\n', 'b', '.', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006796836853027344, 0.0037546038627624513, 0.005654652913411459, 0.004200100898742676]}, 'saliency': {'score': [0.00011894255876541138, 0.000112264208416449, 0.00017885736056736537, 0.00011167465642308504, 0.00014018837143393124], 'topk_tokens': [' second', '�', ' board', 'Question', ' company', 'ot', 'speech', '<|eot_id|>', ' Gal', ' remains', ' bill', '\n\n', 'athroom', ' bogus', 'b', ' bogus', '<|start_header_id|>', '<|begin_of_text|>', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [0.0001977384090423584, 0.00011577010154724121, 9.927650292714436e-05, 5.391240119934082e-05]}}, 31: {'grad': {'score': [0.13415946960449218, 0.13889795728951446, 0.15650830950055802, 0.13877317437714937, 0.09456890611087575], 'topk_tokens': [' Col', ' set', '-t', ' O', ' of', ' getting', 'to', ' sold', ' Pa', ' picked', 'posit', ' or', 'of', 'APER', 'of', ' scale', 'posit', ' o', ' rejo', ' Do'], 'evidence_proportions': [0.15743560791015626, 0.1750396728515625, 0.07364654541015625, 0.14473342895507812]}, 'weight': {'score': [0.00233854204416275, 0.006820664524046843, 0.0056332715920039585, 0.0068520523714676995, 0.00534135944703046], 'topk_tokens': ['Just', ' person', 'Answer', '.', ' Where', '.\n\n', ',', '<|eot_id|>', '<|start_header_id|>', ' \n', '.', 'Question', '<|eot_id|>', '.\n\n', '\n\n', '<|end_header_id|>', 'assistant', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0034469127655029296, 0.0013514816761016845, 0.002015411853790283, 0.0026715993881225586]}, 'saliency': {'score': [4.072040319442749e-05, 7.826108859308777e-05, 9.132368224007743e-05, 7.833133331326206e-05, 7.880856009090648e-05], 'topk_tokens': [' dropped', 'Just', ' person', ' they', ' Pioneer', '.', ' line', ',', ' the', ' Where', '.\n\n', 'assistant', '<|eot_id|>', ' Wood', 'b', 'Bridge', '<|begin_of_text|>', '<|end_header_id|>', 'Question', 'athroom'], 'evidence_proportions': [6.107091903686523e-05, 1.8095970153808595e-05, 3.7088990211486816e-05, 4.900991916656494e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:27:58.063 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:27:58.064 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-10_pid-4_0-1-4-7.pkl | len: 10 |  size: 8.99 KB
Processing depth (0, 1, 4, 7):   5%|▌         | 5/100 [01:07<21:02, 13.29s/it]Processing depth (0, 1, 4, 7):   5%|▌         | 5/100 [01:07<21:19, 13.46s/it]
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:99 - Selected idx: 11
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:103 - Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' John moved to the garden.', ' John went back to the office.', ' Daniel took the football.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:104 - Real Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-23 22:27:58.262 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.12it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (0, 1, 3, 7, 9):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:28:07.601 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:28:07.601 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:28:07.601 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the milk.
2025-01-23 22:28:07.604 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (442, 447) --> . Mary picked up the
2025-01-23 22:28:07.604 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:28:07.611 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1498, 1504) --> . Mary journeyed to the
2025-01-23 22:28:07.611 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:28:07.625 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2900, 2904) -->  Mary left the milk
2025-01-23 22:28:07.625 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel dropped the football.
2025-01-23 22:28:07.643 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3636, 3640) -->  dropped the football.
2025-01-23 22:28:07.643 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:28:07.656 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2538, 2543) --> . John moved to the
2025-01-23 22:28:07.656 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the office.
2025-01-23 22:28:07.659 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (598, 604) --> . John went back to the
2025-01-23 22:28:07.659 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel took the football.
2025-01-23 22:28:07.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (328, 332) -->  Daniel took the football
2025-01-23 22:28:07.661 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:28:07.664 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (688, 692) -->  Daniel left the apple
2025-01-23 22:28:07.664 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:28:07.666 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (447, 453) -->  milk. Sandra journeyed to
2025-01-23 22:28:07.666 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:28:07.667 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (73, 78) --> . Daniel picked up the
2025-01-23 22:28:07.667 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-23 22:28:07.677 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2048, 2053) -->  the ground. Sandra moved
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:28:08.215 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:28:08.215 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4239])
your chose emoji: ['⛹️\u200d♂️', '👨🏾\u200d🦼', '🧚🏾\u200d♀️', '👩\u200d🦼', '👨🏾\u200d🦯\u200d➡️', '🤜🏾', '🎃', '\U0001fac4🏿', '🧎🏼\u200d♂️\u200d➡', '😖']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4242, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 203360.19it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.02it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.30it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.19it/s]
2025-01-23 22:28:11.201 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.23640309770902, 0.2515011454235546, 0.2445371355329241, 0.2516460398752211, 0.2562831579110561], 'topk_tokens': [' no', ' Min', ' into', ' no', 'Min', ' expected', ' in', 'MIN', ' *\n\n', ' Min', ' com', 'St', 'in', ' men', ' com', '202', ' ty', 'str', ' bathroom', ' minds'], 'evidence_proportions': [0.353912353515625, 0.18482894897460939, 0.15691165129343668, 0.24500274658203125, 0.2646217346191406]}, 'weight': {'score': [0.002671560893456141, 0.00743516758330191, 0.0010368483407156809, 0.007516034839827565, 0.0006719854397651476], 'topk_tokens': ['<|start_header_id|>', '<|end_header_id|>', '\n\n', ':', '<|eot_id|>', '\n\n', '?\n', 'user', '\n\n', ' the', 'b', 'Answer', '\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00841379165649414, 0.00040497183799743653, 0.00038929780324300134, 0.002910226583480835, 0.0015117377042770386]}, 'saliency': {'score': [0.0005788654088973999, 6.843505463247196e-05, 3.173010689871652e-05, 6.581357355867878e-05, 3.0997854012709395e-05], 'topk_tokens': [' dropped', 'If', 'user', '?\n', ' directly', 'in', ' Team', ' Bench', ' during', ' Pioneer', '\n\n', 'Answer', '<|eot_id|>', 'b', '\n\n', 'assistant', ' bathroom', 'athroom', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.0025983691215515138, 2.2524595260620116e-05, 1.9162893295288086e-05, 0.00011189281940460205, 5.6438148021698e-05]}}, 25: {'grad': {'score': [0.2427535057067871, 0.2630318835454974, 0.2533402579171317, 0.263229322695658, 0.3198049251849835], 'topk_tokens': ['�', ' employ', 'Good', ' $', ' cold', ' compos', 'istributed', '�', 'Sh', '�', ' long', '0', '�', ' Wood', ' old', ' Marshall', '0', '�', '�', ' res'], 'evidence_proportions': [0.4134033203125, 0.13889617919921876, 0.23893102010091144, 0.21613311767578125, 0.19161701202392578]}, 'weight': {'score': [0.0016027949750423431, 0.007217633842241646, 0.0008463382720947266, 0.007303158938528504, 0.0006102942503415621], 'topk_tokens': [' dropped', ' random', '\n\n', ' ', 'user', ' Bench', '\n\n', '?\n', 'b', '<|start_header_id|>', ' discarded', ' the', 'Answer', '<|eot_id|>', '<|eot_id|>', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003687620162963867, 0.0007744252681732177, 0.0005793770154317219, 0.0022621750831604004, 0.0009079724550247192]}, 'saliency': {'score': [5.8298309644063316e-05, 9.20746614212475e-05, 5.2613871438162665e-05, 9.25986298869564e-05, 1.555795852954571e-05], 'topk_tokens': ['str', '\n\n', ' ST', 'D', 'Times', ' Stephen', ' paper', '<|eot_id|>', '<|eot_id|>', ' directly', 'RE', 'paper', ' Bench', 'assistant', ' discarded', 'Answer', 'athroom', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [0.00012191534042358399, 2.4366378784179688e-05, 3.425776958465576e-05, 8.831173181533813e-05, 2.7239322662353516e-05]}}, 26: {'grad': {'score': [0.39777565002441406, 0.3996929940273603, 0.38604275839669366, 0.3998182090651667, 0.5055251121520996], 'topk_tokens': [' Pioneer', ' state', ' message', ' state', ' *', 'que', ' state', '�', ' *', ' leve', ' Ramsey', ' West', ' state', ' *', 'agle', ' *', ' leve', ' *', ' *', ' leve'], 'evidence_proportions': [0.4200592041015625, 0.41755599975585933, 0.4342568715413412, 0.3706531524658203, 0.317596435546875]}, 'weight': {'score': [0.003684699535369873, 0.007128686738542542, 0.0014093126569475446, 0.007196301796092618, 0.0020256244983428563], 'topk_tokens': ['\n', '<|end_header_id|>', ' discarded', '\n\n\n', '\n\n', '<|eot_id|>', '?\n', '\n\n', 'b', '\n\n', 'user', ' the', 'Answer', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.01305394172668457, 0.0005400419235229492, 0.0003503561019897461, 0.002578169107437134, 0.002012014389038086]}, 'saliency': {'score': [0.0015020755430062611, 0.00010212167626785142, 7.6458283833095e-05, 9.4304159158957e-05, 0.000103874466358087], 'topk_tokens': ['<|end_header_id|>', ' bogus', 'eward', ' Pioneer', ' block', '<|eot_id|>', '\n\n', ' ', 'b', 'assistant', 'Answer', '\n\n', '<|eot_id|>', ' the', '<|start_header_id|>', 'user', '<|end_header_id|>', 'athroom', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [0.007113116979598999, 1.7505884170532226e-05, 1.1707345644632976e-05, 4.4405460357666016e-05, 3.72081995010376e-05]}}, 27: {'grad': {'score': [0.36955849329630536, 0.4361273420694469, 0.3735901968819754, 0.4370325422731564, 0.35708212241148335], 'topk_tokens': [' in', '\n', '\n', '\n', ' be', ',', '\n', ' states', ' states', ' be', '\n', ',', '\n', '\n', '\n', ' city', '\n', ' be', ' city', ' be'], 'evidence_proportions': [0.44056396484375, 0.418231201171875, 0.3438390096028646, 0.27865028381347656, 0.34944820404052734]}, 'weight': {'score': [0.004945532729228337, 0.0072918808274761455, 0.0011341733591897147, 0.007356865794186156, 0.0014417228790429921], 'topk_tokens': ['From', '\n\n', '\n', 'just', ' discarded', 'speech', ' bathroom', '\n\n', ' the', 'Answer', '<|eot_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'b', 'user', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.01875333786010742, 0.000549536943435669, 0.00037866830825805664, 0.0029641389846801758, 0.0020124614238739014]}, 'saliency': {'score': [0.0022810610632101693, 0.0001255372176469356, 7.663539477757046e-05, 0.00011357905161942189, 7.8315536181132e-05], 'topk_tokens': ['Bridge', ' bogus', '<|start_header_id|>', '<|eot_id|>', '\n\n', ' discarded', '\n', 'Min', 'Just', 'description', 'assistant', 'just', 'speech', '\n\n', 'b', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'user', ' bathroom'], 'evidence_proportions': [0.010748547315597535, 1.2981891632080079e-05, 8.07642936706543e-06, 0.00010356307029724121, 0.00011877715587615967]}}, 28: {'grad': {'score': [0.3021407922108968, 0.2654084705171131, 0.32335837227957587, 0.2647128400408193, 0.3062319144224509], 'topk_tokens': [' up', ' pur', ' two', ' two', 'u', 'two', ' out', ' out', '9', ' two', '2', ' over', ' o', '2', 'u', ' Out', ' so', '!"', ' out', ' out'], 'evidence_proportions': [0.416650390625, 0.27312431335449217, 0.22896575927734375, 0.288482666015625, 0.318695068359375]}, 'weight': {'score': [0.0016965804000695546, 0.006845621048757795, 0.0005137486117226737, 0.0069281437623281384, 0.0006701545073435857], 'topk_tokens': [' dropped', ' *\n\n', ' not', ':', ' the', '\n\n', '?\n', ' discarded', '\n', '.\n\n', '<|eot_id|>', '<|start_header_id|>', 'b', '<|eot_id|>', 'Answer', '\n\n', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0034961700439453125, 0.0002290546894073486, 0.00041150053342183435, 0.002380162477493286, 0.002525538206100464]}, 'saliency': {'score': [0.0002326605220635732, 6.827535718075232e-05, 2.9916422707693917e-05, 6.765315272207896e-05, 1.285549921867175e-05], 'topk_tokens': [' not', ' Bank', 'just', '<|end_header_id|>', '\n', '<|eot_id|>', ' directly', ' During', '.\n\n', 'During', 'b', '\n\n', ' bathroom', 'Answer', 'Bridge', 'athroom', '<|eot_id|>', ' block', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0009178400039672851, 1.2576580047607422e-05, 8.831421534220377e-06, 9.687244892120361e-05, 0.0001228228211402893]}}, 29: {'grad': {'score': [0.3777199586232503, 0.31583140875471477, 0.3702002116612026, 0.3150214079661487, 0.5643309568747495], 'topk_tokens': [' the', ' B', ' the', '�', '.', ' the', ' up', '�', '�', ' the', ' l', ' up', '�', ' the', '�', ' the', ' the', '�', ' the', 'b'], 'evidence_proportions': [0.5011638641357422, 0.3988128662109375, 0.34033711751302087, 0.37567138671875, 0.2551717758178711]}, 'weight': {'score': [0.0009575411677360535, 0.007055911181502498, 0.0005596135343824114, 0.007145256459527742, 0.0008915158418508676], 'topk_tokens': ['.\n\n', '<|start_header_id|>', ' ', ':', '?\n', 'user', '\n', '\n\n', ' discarded', '\n\n', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', '\n\n', 'Answer', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002134466171264648, 0.0001806318759918213, 0.00012176732222239177, 0.0017511844635009766, 0.0009175390005111694]}, 'saliency': {'score': [8.135785659154256e-05, 9.017241332734435e-05, 2.270000321524484e-05, 9.078754211423958e-05, 6.685119408827562e-05], 'topk_tokens': ['just', "'clock", 'description', 'user', ',', 'If', 'A', '<|end_header_id|>', 'assistant', '"', '<|eot_id|>', '\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'athroom', 'a', 'Answer', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0003180980682373047, 6.902217864990234e-06, 3.049770991007487e-06, 5.112588405609131e-05, 2.619624137878418e-05]}}, 30: {'grad': {'score': [0.28501447041829425, 0.25604912786200495, 0.23338394165039061, 0.25607258310849207, 0.36648065615922976], 'topk_tokens': [' Brown', ' a', ' boat', ' trib', 'Times', ' Bor', ' business', ' no', ' bend', 'ball', ' based', ' business', ' bathroom', 'a', 'a', ' bogus', 'ab', ' B', ' B', 'b'], 'evidence_proportions': [0.410333251953125, 0.2485382080078125, 0.2885793050130208, 0.28176116943359375, 0.17186737060546875]}, 'weight': {'score': [0.003096198042233785, 0.007019980456232182, 0.0024070773805890764, 0.007081090278269826, 0.004407998843070788], 'topk_tokens': ['user', '\n\n', '�', '<|start_header_id|>', ' *\n\n', '.\n\n', ':', '.\n\n', '?\n', '<|eot_id|>', '\n\n\n', 'Answer', '<|eot_id|>', 'b', '<|start_header_id|>', '\n\n', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.008144187927246093, 0.0003233015537261963, 0.0004391024510065714, 0.003393232822418213, 0.003940939903259277]}, 'saliency': {'score': [0.0005407122274239858, 0.00013055562101856036, 4.793064934866769e-05, 0.00012889368352267608, 0.00022706198386656932], 'topk_tokens': ['\n\n', ' bogus', '?\n', 'speech', '      ', ' *\n\n', 'Question', '<|eot_id|>', '<|start_header_id|>', 'Bridge', '.\n\n', 'b', '�', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', '\n\n\n', '<|begin_of_text|>', 'athroom', 'assistant'], 'evidence_proportions': [0.0023528099060058597, 1.5968084335327147e-05, 1.405179500579834e-05, 0.00011734664440155029, 0.00014487653970718384]}}, 31: {'grad': {'score': [0.2801218032836914, 0.3383311946568541, 0.3131199428013393, 0.33887611916268695, 0.29084151830428684], 'topk_tokens': [' Published', "'ve", ' offices', ' method', 'AILY', ' he', ' Press', ' web', '<|end_header_id|>', ' He', '<|eot_id|>', '3', ' OF', ' OF', ' Cl', '<|start_header_id|>', 'b', ' o', 'SP', 'user'], 'evidence_proportions': [0.3188873291015625, 0.3534423828125, 0.238616943359375, 0.19771766662597656, 0.28467559814453125]}, 'weight': {'score': [0.001580095539490382, 0.006630701256607681, 0.00095295820917402, 0.006707185967071701, 0.0012561789689919888], 'topk_tokens': [' not', ' the', ' Where', 'Question', '.\n', ' dropped', '<|start_header_id|>', '.\n\n', '<|eot_id|>', ':', 'Answer', '?\n', ' ', '<|eot_id|>', 'b', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0029363632202148438, 0.0005088508129119873, 0.000516424576441447, 0.002613544464111328, 0.001785874366760254]}, 'saliency': {'score': [0.000142009307940801, 9.294739454099447e-05, 2.4919850485665458e-05, 9.323510147868063e-05, 4.59597660945012e-05], 'topk_tokens': ['<|eot_id|>', ' ', 'cap', '\n\n', ' not', ' bathroom', 'If', ' Where', 'assistant', ' the', ' ', '.\n', '?\n', 'Question', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'b', 'athroom', '\n\n'], 'evidence_proportions': [0.000508648157119751, 1.9729137420654297e-05, 1.043577988942464e-05, 5.45307993888855e-05, 0.00012139976024627686]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:28:11.208 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:28:11.208 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-11_pid-0_0-1-3-7-9.pkl | len: 10 |  size: 9.47 KB
Processing depth (0, 1, 3, 7, 9):   1%|          | 1/100 [00:12<21:13, 12.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Processing depth (2, 4, 5, 6, 9):   1%|          | 1/100 [00:23<21:13, 12.86s/it]2025-01-23 22:28:21.463 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:28:21.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (932, 937) --> . Mary moved to the
2025-01-23 22:28:21.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the milk.
2025-01-23 22:28:21.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1815, 1820) --> . Mary picked up the
2025-01-23 22:28:21.478 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:28:21.488 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2116, 2122) --> . Mary journeyed to the
2025-01-23 22:28:21.489 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:28:21.500 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2405, 2409) -->  Mary left the milk
2025-01-23 22:28:21.500 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel dropped the football.
2025-01-23 22:28:21.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3647, 3651) -->  Daniel dropped the football
2025-01-23 22:28:21.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:28:21.530 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2511, 2516) --> . John moved to the
2025-01-23 22:28:21.530 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the office.
2025-01-23 22:28:21.533 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (558, 564) --> . John went back to the
2025-01-23 22:28:21.533 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel took the football.
2025-01-23 22:28:21.534 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (205, 209) -->  Daniel took the football
2025-01-23 22:28:21.534 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:28:21.538 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (654, 658) -->  Daniel left the apple
2025-01-23 22:28:21.538 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:28:21.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (407, 413) --> . Sandra journeyed to the
2025-01-23 22:28:21.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:28:21.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (33, 38) -->  Daniel picked up the apple
2025-01-23 22:28:21.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-23 22:28:21.553 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2016, 2021) -->  the ground. Sandra moved
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:28:22.026 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:28:22.026 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4205])
your chose emoji: ['🧍🏾\u200d♂️', '🛢', '🥊', '👽', '™', '⛷', '🍫', '🇦🇺', '\U0001fabf', '👷🏽\u200d♀️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4208, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 216480.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 122.77it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.82it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.91it/s]
2025-01-23 22:28:24.842 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.46466700236002606, 0.5146272028353731, 0.5016815185546875, 0.5150254057182925, 0.6046532717618075], 'topk_tokens': ['.\n\n', ' full', '202', ' *\n\n', ' agreement', ' *\n\n', ' const', 'remark', '.\n\n', ':', ' Douglas', 'ex', 'MIN', ' Good', 'ab', ' Queen', 'cont', ' ne', ' attention', 'com'], 'evidence_proportions': [0.3849761962890625, 0.6308837890625, 0.39374542236328125, 0.44075775146484375, 0.4868011474609375]}, 'weight': {'score': [0.004235987861951192, 0.007466892779100077, 0.003581022364752633, 0.007518362333815366, 0.00366277586330067], 'topk_tokens': ['Answer', '<|eot_id|>', '.\n\n', '?\n', ' ', ' return', 'user', 'Bridge', '\n\n', '<|eot_id|>', 'Just', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'b', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015950798988342285, 0.0021744012832641602, 0.0037123262882232666, 0.006027936935424805, 0.009107649326324463]}, 'saliency': {'score': [0.0003158487379550934, 0.00016636298721507475, 0.00015908479690551758, 0.00016555968006710513, 0.0001749883998524059], 'topk_tokens': ['.\n\n', ' cold', ' offices', '4', 'Answer', '\n\n', '\n\n', ' Wood', 'Good', 'rail', 'user', 'Just', 'Bridge', ' ', ' return', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [9.230375289916992e-05, 0.0002451658248901367, 0.00016667445500691733, 0.0006743744015693665, 0.0005488693714141846]}}, 25: {'grad': {'score': [0.4702751239140828, 0.47428905646610625, 0.42652980259486606, 0.4747151611339158, 0.4888026064092463], 'topk_tokens': [' *', '�', ' long', ' Wood', 'During', 'P', ' rest', ' no', ' day', ' web', 'ad', 'edit', ' distant', ' large', ' the', 'vent', 'nes', ' the', ' block', 'ball'], 'evidence_proportions': [0.5768798828125, 0.5234313964843751, 0.39055633544921875, 0.4147179126739502, 0.445709228515625]}, 'weight': {'score': [0.005956092228492101, 0.007307065757055246, 0.00236942001751491, 0.007356533331306735, 0.003024186600338329], 'topk_tokens': ['tele', ' boat', ':', 'Bridge', ' Bench', '<|eot_id|>', ' bogus', '<|start_header_id|>', ' return', '\n\n', 'Answer', 'Just', '<|eot_id|>', 'b', '.\n\n', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0019682168960571287, 0.00564684271812439, 0.002419730027516683, 0.009737223386764526, 0.012850910425186157]}, 'saliency': {'score': [0.000695804754892985, 0.00015055619095214872, 0.00013678499630519322, 0.000147518356842258, 0.00020170144059441307], 'topk_tokens': [' Bor', ' Daniel', ' panic', ' Adams', ' Mary', '\n\n', ' Grow', ' Senator', 'polit', 'Bridge', 'assistant', 'tele', 'Answer', ' Mary', ' Bench', 'tele', 'athroom', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.00026644468307495116, 0.0006788432598114013, 0.00031981368859608966, 0.0015958771109580994, 0.0009176209568977356]}}, 26: {'grad': {'score': [0.41954485575358075, 0.4397148741515417, 0.4034881319318499, 0.4401371485355476, 0.4736273071982644], 'topk_tokens': [' state', ' PA', ' bag', ' first', ' nineteenth', ' bogus', ' which', ' the', ' prof', ' that', ' bogus', ' B', 'en', 'ierce', ' message', ' to', '�', 'RI', 'ab', 'b'], 'evidence_proportions': [0.37826843261718746, 0.44429931640625, 0.4258829752604167, 0.4225587844848633, 0.4276762008666992]}, 'weight': {'score': [0.005699453254540761, 0.007249243812415989, 0.002687668800354004, 0.007296689003741893, 0.004891791126944802], 'topk_tokens': ['Bridge', '<|eot_id|>', 'user', '?\n', 'Just', '<|eot_id|>', '\n\n', ' bogus', '<|eot_id|>', 'Answer', 'inen', '\n\n', '<|end_header_id|>', '\n\n', 'b', '<|start_header_id|>', '.\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006521940231323242, 0.0060796022415161135, 0.003371626138687134, 0.010868489742279053, 0.009856045246124268]}, 'saliency': {'score': [0.0004204288125038147, 0.00015054747263968217, 0.00017894251005990165, 0.00014874680296832839, 0.00019930641759525645], 'topk_tokens': ['half', ' Gal', '\n', ' Mary', '\n', '<|eot_id|>', ' carrier', ' Marshall', 'edit', ' bathroom', ' Senator', 'Bridge', '<|end_header_id|>', '\n\n', '.\n\n', '<|start_header_id|>', 'b', 'assistant', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [3.814697265625e-05, 0.0005067884922027588, 0.00015336275100708008, 0.001103922724723816, 0.0005074366927146912]}}, 27: {'grad': {'score': [0.47731653849283856, 0.5832861548594196, 0.46200692313058034, 0.5849222223222558, 0.5190807255831632], 'topk_tokens': ['engers', ' state', ' business', '9', 'ided', '.', '9', ' be', ',', ' considerable', ' one', 'ors', ',', ' means', ' conditions', 'ers', '9', ' business', ' *\n\n', ' St'], 'evidence_proportions': [0.5929443359375, 0.507373046875, 0.468536376953125, 0.390167236328125, 0.39553070068359375]}, 'weight': {'score': [0.0072068919738133745, 0.0073811239616045935, 0.0025963919503348212, 0.007422494698674112, 0.0039396746592088175], 'topk_tokens': ['Bridge', ' Buchanan', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'user', ' return', ' *\n\n', ' bogus', 'Answer', ':', 'Just', '.\n\n', '\n\n', '\n\n', '<|end_header_id|>', 'b', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0009695053100585938, 0.00542762279510498, 0.0031042893727620444, 0.011768698692321777, 0.018819808959960938]}, 'saliency': {'score': [0.0007669118543465933, 0.0001672461430609453, 0.00035014407975333077, 0.00016223447642920534, 0.0002476491711356423], 'topk_tokens': ['assistant', ' Francis', 'user', 'EF', '\n\n', 'Answer', '\n\n', ' bogus', ' Buchanan', 'far', ' *\n\n', ' return', ' Daniel', ' bathroom', ' bogus', 'Bridge', 'Just', '<|end_header_id|>', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [4.432797431945801e-05, 0.0003047525882720947, 0.00020895401636759442, 0.000586681067943573, 0.003265008330345154]}}, 28: {'grad': {'score': [0.34809335072835285, 0.3307680485366415, 0.3213435581752232, 0.33074733267982026, 0.3057759024880149], 'topk_tokens': [' front', ' as', ' $', 'material', '600', 'Dub', ' three', ' up', ' Dub', ' half', ' *', 'half', ' ', ' Dub', ' from', ' ', ' from', 'nes', '"', '600'], 'evidence_proportions': [0.564044189453125, 0.28636474609375, 0.432037353515625, 0.14495277404785156, 0.23254013061523438]}, 'weight': {'score': [0.007482352356115977, 0.007032029755668495, 0.0018767569746289934, 0.007072913536079592, 0.0022322447462515397], 'topk_tokens': ['\n', 'From', '<|start_header_id|>', ' person', ' return', 'Bridge', '\n\n', ':', 'inen', '?\n', '<|eot_id|>', 'Answer', 'Just', 'b', '.\n\n', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0013753652572631836, 0.003735768795013428, 0.007543007532755533, 0.013180583715438843, 0.014010101556777954]}, 'saliency': {'score': [0.0005245668192704519, 0.00015211500539072566, 0.00012616004262651717, 0.00015017949807899606, 6.701729514382102e-05], 'topk_tokens': ['through', 'inen', '.\n\n', 'During', 'press', ' bedroom', ' milk', '<|end_header_id|>', 'far', 'assistant', ' person', '\n\n', 'Just', ' block', 'b', ' bathroom', '<|begin_of_text|>', 'Bridge', '\n\n', 'athroom'], 'evidence_proportions': [0.00011259317398071289, 0.0005622327327728272, 0.0003598084052403768, 0.000726751983165741, 0.00103740394115448]}}, 29: {'grad': {'score': [0.9750496546427408, 0.8495113822443857, 0.9496876307896205, 0.8479401392372409, 0.6933856877413663], 'topk_tokens': [' speakers', ' their', ' steam', ' A', ' boat', ' Bench', ' steam', ' to', ' to', ' great', ' com', 'ER', ' as', ' the', ' Pioneer', ' steam', ' of', 'ION', ' produced', '.'], 'evidence_proportions': [1.0751953125, 1.1060791015625, 1.2394002278645833, 0.8441886901855469, 0.42041587829589844]}, 'weight': {'score': [0.004530712962150574, 0.007253796882049212, 0.0016446735177721296, 0.007316865894299698, 0.0023493969982320614], 'topk_tokens': [' during', ' return', 'Question', 'user', 'nes', '\n\n', 'Just', 'Answer', '.\n\n', ':', '?\n', ' ', '\n\n', 'inen', 'b', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007377386093139649, 0.003867626190185547, 0.002543052037556966, 0.007568538188934326, 0.010044455528259277]}, 'saliency': {'score': [0.00016979997356732687, 0.00016633594699005664, 7.033688681466239e-05, 0.0001671257350036224, 0.00012899867512963036], 'topk_tokens': ['In', '.', '<|start_header_id|>', 'Question', '.\n\n', '.', 'question', ' Where', '<|start_header_id|>', ' ', '<|eot_id|>', 'nes', ':', ' ', 'inen', 'assistant', '\n\n', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [5.745291709899903e-05, 0.0002427518367767334, 6.0434142748514816e-05, 0.0002025216817855835, 0.00035037100315093994]}}, 30: {'grad': {'score': [0.21195300420125326, 0.2773988818034473, 0.21052325793675014, 0.2783416024343914, 0.35142742503773083], 'topk_tokens': [' work', ' Project', ' EAR', ' West', ' Times', ' STR', 'ien', ' Wood', ' OF', 'Times', ' O', ' Written', ' war', ' o', ' It', 'UG', ' Wing', ' B', ' B', 'b'], 'evidence_proportions': [0.25198822021484374, 0.14217453002929686, 0.20430882771809894, 0.20240020751953125, 0.27015113830566406]}, 'weight': {'score': [0.007453908522923787, 0.007174985717004696, 0.0027644736426217214, 0.007210578335770989, 0.008804825219241056], 'topk_tokens': [':', ':', 'Question', '<|eot_id|>', '<|eot_id|>', 'inen', ' return', 'Just', ' *\n\n', '<|start_header_id|>', 'Answer', ' *\n\n', '?\n', '<|end_header_id|>', '\n\n', '.\n\n', 'b', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015626907348632812, 0.005851936340332031, 0.006710569063822429, 0.012155711650848389, 0.013233602046966553]}, 'saliency': {'score': [0.0002617277204990387, 0.00018008321721970809, 0.00011242372649056571, 0.00018018170217917205, 0.00022682073441418734], 'topk_tokens': ['assistant', ' return', '<|eot_id|>', ' bill', ' but', '\n\n', 'Question', 'user', 'Good', ' bogus', ' bill', '<|end_header_id|>', 'inen', ' bogus', 'Bridge', ' bathroom', '\n\n', '<|begin_of_text|>', 'athroom', 'b'], 'evidence_proportions': [4.984736442565918e-05, 0.0001953423023223877, 0.00030168394247690833, 0.00036103278398513794, 0.0004503205418586731]}}, 31: {'grad': {'score': [0.27434249718983966, 0.365365597685027, 0.30412325177873883, 0.366408749412818, 0.25803223523226654], 'topk_tokens': [' fr', ' of', ' of', ' generally', ' be', ' so', ' for', ' now', ' be', ' governor', 'b', ' I', ' or', ' ne', ' now', 'then', ' producing', ' and', ' formally', ' now'], 'evidence_proportions': [0.2376220703125, 0.2195127487182617, 0.30032094319661456, 0.3159027099609375, 0.30825233459472656]}, 'weight': {'score': [0.004344838360945384, 0.0067908913463694065, 0.0019937200205666678, 0.006845508427124594, 0.00427234714681452], 'topk_tokens': [' write', 'Just', ':', ' ', ' return', 'Question', '<|start_header_id|>', '<|eot_id|>', ' Where', '.\n\n', ':', 'Answer', '?\n', '<|eot_id|>', '<|end_header_id|>', 'b', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002704334259033203, 0.002786099910736084, 0.0023606618245442705, 0.006211996078491211, 0.009452998638153076]}, 'saliency': {'score': [9.44348673025767e-05, 7.793700372084919e-05, 7.774829864501954e-05, 7.784316326572568e-05, 9.450790556994352e-05], 'topk_tokens': ['inen', ' One', ' Where', ' Buchanan', ' return', '<|start_header_id|>', 'Question', '.\n\n', 'Bridge', '?\n', ' bathroom', 'Answer', ':', '<|begin_of_text|>', '<|eot_id|>', 'b', 'assistant', '<|end_header_id|>', '\n\n', 'athroom'], 'evidence_proportions': [0.0001281261444091797, 7.638335227966309e-05, 4.091362158457438e-05, 8.462369441986084e-05, 0.00016497820615768433]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:28:24.854 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:28:24.855 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-11_pid-1_2-4-5-6-9.pkl | len: 10 |  size: 9.35 KB
Processing depth (2, 4, 5, 6, 9):   2%|▏         | 2/100 [00:26<21:45, 13.32s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.02it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Processing depth (1, 3, 4, 7, 8):   2%|▏         | 2/100 [00:37<21:45, 13.32s/it]2025-01-23 22:28:36.044 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:28:36.047 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (436, 441) --> . Mary moved to the
2025-01-23 22:28:36.047 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the milk.
2025-01-23 22:28:36.054 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1419, 1424) -->  Mary picked up the milk
2025-01-23 22:28:36.054 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:28:36.063 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1819, 1825) --> . Mary journeyed to the
2025-01-23 22:28:36.064 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:28:36.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2883, 2887) -->  Mary left the milk
2025-01-23 22:28:36.078 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel dropped the football.
2025-01-23 22:28:36.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3376, 3380) -->  Daniel dropped the football
2025-01-23 22:28:36.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:28:36.107 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2521, 2526) --> . John moved to the
2025-01-23 22:28:36.107 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the office.
2025-01-23 22:28:36.110 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (592, 598) --> . John went back to the
2025-01-23 22:28:36.110 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel took the football.
2025-01-23 22:28:36.112 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (322, 326) -->  Daniel took the football
2025-01-23 22:28:36.112 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:28:36.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (673, 677) -->  Daniel left the apple
2025-01-23 22:28:36.115 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:28:36.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (440, 446) -->  the bathroom. Sandra journeyed
2025-01-23 22:28:36.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:28:36.118 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (36, 41) -->  Daniel picked up the apple
2025-01-23 22:28:36.118 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-23 22:28:36.120 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (439, 444) -->  to the bathroom. Sandra
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:28:36.691 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:28:36.692 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4237])
your chose emoji: ['🏃🏽\u200d♂\u200d➡', '⛩️', '🏞️', '🖕🏾', '🤹🏾\u200d♂', '🔛', '\U0001faf1🏻\u200d\U0001faf2🏿', '⛳', '👼', '👨🏾\u200d❤️\u200d💋\u200d👨🏻']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4240, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 215092.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.33it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.49it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.88it/s]
2025-01-23 22:28:40.161 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3182989756266276, 0.34845203903486144, 0.37128274100167413, 0.3484340048212664, 0.3925760168778269], 'topk_tokens': [' acquaintance', ' Min', ' It', ' feature', ' devil', ' come', ' it', ' Douglas', ' arrival', ' entire', 'com', 'de', 'est', ' competition', ' grat', ' It', 'announcement', ' departing', ' admission', ' com'], 'evidence_proportions': [0.227581787109375, 0.3005889892578125, 0.3141581217447917, 0.33496856689453125, 0.4433746337890625]}, 'weight': {'score': [0.0011925771832466125, 0.007419037818908692, 0.003437913315636771, 0.007488106083168535, 0.0050667751776544675], 'topk_tokens': [' bogus', ' ', ' was', ' bogus', '.\n\n', ':', '?\n', 'Answer', '�', '\n\n', 'Bridge', 'Just', '<|eot_id|>', '<|eot_id|>', 'b', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007424294948577881, 0.0014023780822753906, 0.0011564046144485474, 0.0013408958911895752, 0.0013989508152008057]}, 'saliency': {'score': [6.1492125193278e-05, 0.00012410881946671684, 0.0006019038813454764, 0.00012046853568216911, 0.0003373344477854277], 'topk_tokens': ['fast', '.\n\n', 'blue', ' milk', 'If', ' breaking', 'body', ' bill', 'Good', '<|eot_id|>', ' cold', 'Answer', 'Just', '\n\n', '<|end_header_id|>', 'Bridge', '�', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [3.6436319351196286e-05, 6.0570240020751956e-05, 7.715821266174316e-05, 6.229430437088013e-05, 6.966292858123779e-05]}}, 25: {'grad': {'score': [0.35478973388671875, 0.31718455620531766, 0.3676380225590297, 0.31654633673946414, 0.38856477486459834], 'topk_tokens': [' the', ' the', '�', '�', '�', ' Wood', ' He', 'ball', ' o', ' the', ' West', ' Bank', ' long', ' the', ' old', ' He', 'old', ' the', ' Hill', ' the'], 'evidence_proportions': [0.40341796875, 0.33084716796875, 0.2638092041015625, 0.424560546875, 0.39063262939453125]}, 'weight': {'score': [0.0022636850674947104, 0.0072217975022657865, 0.0014100015163421632, 0.007298910049011022, 0.0040903412982037195], 'topk_tokens': [' bend', 'Good', 'Just', ' dry', 'Answer', ' directly', ' distributed', ' milk', ' Bench', ' bogus', ' bogus', '<|eot_id|>', '<|eot_id|>', 'b', ' discarded', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0008209526538848877, 0.003724598884582519, 0.0011509309212366738, 0.004533827304840088, 0.0016399472951889038]}, 'saliency': {'score': [0.00016627460718154907, 0.0001374831739461647, 7.475869996207102e-05, 0.00013784298312860766, 0.00013060671718497025], 'topk_tokens': ['Saturday', ' directly', ' distributed', ' obtained', ' im', 'ree', 'Good', 'Civil', ' bogus', 'Times', ' bend', ' discarded', ' milk', 'Answer', ' distributed', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [4.169940948486328e-05, 0.0003311097621917725, 8.536378542582194e-05, 0.0003034919500350952, 0.00010009855031967163]}}, 26: {'grad': {'score': [0.5656091372172037, 0.5522889479151312, 0.5510793447494506, 0.5522226124852219, 0.6858627921656558], 'topk_tokens': [' *', ' *', ' *', ' *', ' tie', ' *', '�', 'agle', ' that', ' ran', 'engers', ' states', ' *', ' *', '�', '�', ' messenger', '�', ' mess', 'ierce'], 'evidence_proportions': [0.44747161865234375, 0.5460319519042969, 0.6182683308919271, 0.5886611938476562, 0.635711669921875]}, 'weight': {'score': [0.001508882890144984, 0.007192176692890671, 0.004130268096923828, 0.007250432098804265, 0.008360950178221652], 'topk_tokens': ['\n\n', 'Question', ' Bench', ' bathroom', 'Just', '.\n', '?\n', 'Answer', 'Saturday', '<|eot_id|>', ' bogus', '.\n\n', ' bogus', '<|end_header_id|>', '<|eot_id|>', 'b', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010097980499267577, 0.0025232076644897457, 0.000759388009707133, 0.0018395185470581055, 0.0016584396362304688]}, 'saliency': {'score': [5.3063035011291504e-05, 0.00015488656343154187, 0.0027324991566794257, 0.0001338933378678992, 0.0002085311632407339], 'topk_tokens': ['just', '?\n', 'hom', ' *\n\n', 'assistant', 'still', ' Bench', 'Bridge', ' Square', '.\n', '.\n\n', '<|eot_id|>', ' block', '.\n\n', 'stage', '\n\n', 'b', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [2.6160478591918947e-05, 9.021759033203124e-05, 3.739198048909505e-05, 6.415694952011108e-05, 5.266070365905762e-05]}}, 27: {'grad': {'score': [0.4107962449391683, 0.4476445036114387, 0.5043491908482143, 0.4473813355068817, 0.36847802212363795], 'topk_tokens': [' governor', ',', ' Pa', '800', ' hour', 'body', ' luxury', 'doctor', '\n', ' doctor', ' bill', ' be', ' doctor', ' doctor', ' board', ' dollar', '\n\n', ' business', ' business', ' business'], 'evidence_proportions': [0.501806640625, 0.448126220703125, 0.35371144612630206, 0.4499168395996094, 0.296877384185791]}, 'weight': {'score': [0.002170554051796595, 0.007311117874001557, 0.0054302326270512175, 0.007356371285954721, 0.0060212953310263785], 'topk_tokens': [' Buchanan', 'If', 'If', ' milk', '.\n\n', ' huge', ' bathroom', 'until', 'just', '<|eot_id|>', '<|eot_id|>', 'Just', ' bogus', ' bogus', 'b', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0015877485275268555, 0.0031750679016113283, 0.0013761570056279502, 0.0027478933334350586, 0.0022576749324798584]}, 'saliency': {'score': [0.00015417983134587607, 0.00015162027809979782, 0.0037428038460867747, 0.00012154305873661912, 0.00022846028993004248], 'topk_tokens': ['.\n\n', 'NEW', 'u', ' huge', 'street', ' bedroom', 'Minnesota', 'I', '.\n\n', 'blue', 'u', 'v', 'assistant', 'in', '<|end_header_id|>', 'Just', '\n\n', 'athroom', '<|begin_of_text|>', ' bathroom'], 'evidence_proportions': [0.00010176301002502441, 0.00024251341819763183, 0.0001751085122426351, 5.038827657699585e-05, 0.00018168240785598755]}}, 28: {'grad': {'score': [0.7760709126790365, 0.7196640014648438, 0.7819523402622768, 0.7188187831613156, 0.7475736517655222], 'topk_tokens': [' an', '\n', '\n', ' head', '\n\n', '\n', ' border', ',\n', 'half', 'old', ' Gus', ' Of', ' prize', ' prize', ' Dub', 'hand', ' old', ' old', ' Dub', 'blue'], 'evidence_proportions': [1.00087890625, 0.787872314453125, 0.6917037963867188, 0.75347900390625, 0.6294517517089844]}, 'weight': {'score': [0.0012129632135232289, 0.007030083323424717, 0.0009396050657544818, 0.007114459697894006, 0.006059153691718453], 'topk_tokens': [' dropped', 'just', '.\n\n', '<|eot_id|>', '.\n\n', 'through', ' block', 'Question', '.\n', ':', 'Answer', '<|eot_id|>', 'Just', ' milk', 'b', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0011096239089965821, 0.0015046834945678712, 0.0004012932380040487, 0.0017632246017456055, 0.001644730567932129]}, 'saliency': {'score': [9.3020498752594e-05, 0.00012050294229444468, 0.0002166977950504848, 0.00011985543184205124, 0.0001021086385375575], 'topk_tokens': ['Bridge', 'half', 'being', 'street', 'Sh', 'athroom', ' bathroom', 'just', ' dropped', 'stage', 'through', 'b', 'assistant', ' dropped', 'Just', '<|end_header_id|>', ' block', ' milk', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [3.1280517578125e-05, 9.378790855407715e-05, 9.288390477498373e-06, 0.0001361444592475891, 0.0002517104148864746]}}, 29: {'grad': {'score': [0.5628029505411783, 0.5541030595887382, 0.7194806780133929, 0.5526687103833516, 0.32699355326200785], 'topk_tokens': ['pl', ' material', ' hand', 'hand', ' house', '26', 'S', 'ION', ' OF', ' wonderful', ' A', ' THE', ' press', ' AND', 'str', 'APER', ' STR', 'ATION', 'ER', ' the'], 'evidence_proportions': [0.65462646484375, 0.5011215209960938, 0.6168365478515625, 0.6261444091796875, 0.3807334899902344]}, 'weight': {'score': [0.0010210685431957245, 0.007207312448969427, 0.001293820994240897, 0.007292326095143564, 0.004724717846042232], 'topk_tokens': ['\n', '.\n\n', ' milk', 'If', 'Question', 'If', '<|eot_id|>', 'Just', ' new', ':', ' discarded', '.\n', 'Answer', 'I', 'b', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0012349367141723632, 0.0015803098678588865, 0.0005362679560979207, 0.0010741055011749268, 0.0007288455963134766]}, 'saliency': {'score': [3.918508688608805e-05, 0.000143739770887033, 7.787772587367467e-05, 0.00014489128583357455, 0.00019185715600063928], 'topk_tokens': [' Do', 'to', ' new', 'a', 'a', '<|eot_id|>', 'Answer', 'Just', '�', '"', 'If', 'assistant', 'I', 'If', 'If', 'athroom', '<|end_header_id|>', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.918243408203125e-05, 6.843209266662597e-05, 2.3598472277323406e-05, 4.620850086212158e-05, 3.14861536026001e-05]}}, 30: {'grad': {'score': [0.22817182540893555, 0.2304174243279223, 0.23169141496930803, 0.23041964980068175, 0.35384963688097504], 'topk_tokens': [' four', ' B', '<|eot_id|>', ' Written', ' l', ' Online', 'hom', ' Sh', ' bathroom', '<|eot_id|>', ' EAR', ' be', '<|end_header_id|>', 'Sh', ' o', ' no', ' B', ' o', ' o', 'b'], 'evidence_proportions': [0.224945068359375, 0.2279541015625, 0.25619252522786456, 0.22496795654296875, 0.1936502456665039]}, 'weight': {'score': [0.0021583239237467446, 0.007151647333828908, 0.0020124733448028563, 0.007223331345179753, 0.013363646833520187], 'topk_tokens': ['<|start_header_id|>', ' bogus', 'hom', 'I', '�', '<|eot_id|>', '\n', 'Answer', ' bogus', ':', '.\n', '.\n\n', '.\n\n', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'b', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011853218078613282, 0.002224159240722656, 0.002658267815907796, 0.0029826462268829346, 0.0017180442810058594]}, 'saliency': {'score': [0.0001012459397315979, 0.00021574928794267044, 0.0007903269359043666, 0.00021159666002552294, 0.0002800100728085167], 'topk_tokens': ['.', '<|eot_id|>', '<|eot_id|>', 'Good', '�', 'un', '\n', 'I', 'Answer', '.\n\n', '\n\n', 'street', '<|end_header_id|>', ' bathroom', 'Bridge', ' bogus', ' bogus', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [3.484487533569336e-05, 0.0001290440559387207, 0.00012820462385813397, 0.00014765560626983643, 6.265193223953247e-05]}}, 31: {'grad': {'score': [0.32763131459554035, 0.40293027050090285, 0.3291003635951451, 0.4039805507409005, 0.3621310183876439], 'topk_tokens': [' ', '-t', ' Of', ' so', ' B', 'D', 'ely', ' so', 'd', ' o', ' formally', ' B', ' producing', ' so', ' ', 'D', ' o', 'g', ' o', 'b'], 'evidence_proportions': [0.2629364013671875, 0.32071685791015625, 0.336517333984375, 0.319488525390625, 0.411956787109375]}, 'weight': {'score': [0.0015528301397959392, 0.006578955785283502, 0.0008290427071707589, 0.006655940710714177, 0.010254028988511939], 'topk_tokens': [' Do', '�', 'I', '.\n', 'Just', '�', '�', '.\n', '<|eot_id|>', '?\n', 'Answer', ' left', ':', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'b', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007341265678405761, 0.001896977424621582, 0.002081086238225301, 0.0014911890029907227, 0.0014152824878692627]}, 'saliency': {'score': [4.235655069351196e-05, 7.913816931112758e-05, 8.664982659476145e-05, 7.928642351870844e-05, 0.0003118087586603667], 'topk_tokens': [' return', 'If', '<|start_header_id|>', ' prior', '.\n', ' discarded', 'Question', 'Answer', 'Just', ':', '�', 'body', '<|end_header_id|>', '<|eot_id|>', ' left', 'assistant', 'b', '<|begin_of_text|>', '\n\n', 'athroom'], 'evidence_proportions': [1.6820430755615234e-05, 4.9394369125366214e-05, 6.58184289932251e-05, 2.5913119316101074e-05, 4.673004150390625e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:28:40.169 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:28:40.169 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-11_pid-2_1-3-4-7-8.pkl | len: 10 |  size: 9.26 KB
Processing depth (1, 3, 4, 7, 8):   3%|▎         | 3/100 [00:41<23:00, 14.23s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Processing depth (1, 5, 6, 8, 9):   3%|▎         | 3/100 [00:52<23:00, 14.23s/it]2025-01-23 22:28:50.395 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:28:50.397 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (433, 438) --> . Mary moved to the
2025-01-23 22:28:50.397 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the milk.
2025-01-23 22:28:50.408 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2119, 2124) --> . Mary picked up the
2025-01-23 22:28:50.408 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:28:50.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2426, 2432) -->  the senate. Mary journeyed
2025-01-23 22:28:50.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:28:50.441 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3362, 3366) -->  Mary left the milk
2025-01-23 22:28:50.442 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel dropped the football.
2025-01-23 22:28:50.459 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3659, 3663) -->  Daniel dropped the football
2025-01-23 22:28:50.459 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:28:50.472 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2538, 2543) --> . John moved to the
2025-01-23 22:28:50.472 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the office.
2025-01-23 22:28:50.475 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (567, 573) --> . John went back to the
2025-01-23 22:28:50.475 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel took the football.
2025-01-23 22:28:50.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (319, 323) -->  Daniel took the football
2025-01-23 22:28:50.477 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:28:50.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (663, 667) -->  Daniel left the apple
2025-01-23 22:28:50.480 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:28:50.482 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (437, 443) -->  the bathroom. Sandra journeyed
2025-01-23 22:28:50.482 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:28:50.483 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (67, 72) --> . Daniel picked up the
2025-01-23 22:28:50.483 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-23 22:28:50.485 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (436, 441) -->  to the bathroom. Sandra
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:28:51.026 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:28:51.026 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4233])
your chose emoji: ['👩🏾\u200d❤\u200d💋\u200d👨🏼', '👁', '👕', '👴🏾', '🦹🏼\u200d♀️', '🤽🏽\u200d♀', '⌚', '🧝🏿', '🧘🏼', '👷🏻\u200d♂️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4236, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 239674.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 123.78it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.81it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.83it/s]
2025-01-23 22:28:54.008 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.1968525250752767, 0.20417136173410388, 0.269401604788644, 0.20366683541686734, 0.1453077130847507], 'topk_tokens': ['\n', ' Pioneer', '.', '.\n\n', ' the', ' conventions', ' *\n\n', ' and', ' *\n\n', 'ol', ' Good', ' and', 'ARR', 'Cut', '-per', ' past', 'ting', ' latter', ' EAR', '.'], 'evidence_proportions': [0.18121795654296874, 0.06326065063476563, 0.2037353515625, 0.42241477966308594, 0.14749908447265625]}, 'weight': {'score': [0.056773326049248375, 0.007515645905189406, 0.000905416693006243, 0.007288012124717535, 0.014308072212669585], 'topk_tokens': ['<|eot_id|>', '.', '<|eot_id|>', '❤', ' milk', ' Foster', 'ot', 'ian', 'nes', 'nes', ' a', '<|end_header_id|>', ' now', 'b', ' Mary', 'assistant', ' picked', '.', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003201901912689209, 0.2470473289489746, 0.014217694600423176, 0.007252544164657593, 0.0028514713048934937]}, 'saliency': {'score': [8.172293504079182e-05, 5.439376217560232e-05, 9.492465427943639e-06, 5.4612973388770504e-05, 3.9142866929372154e-05], 'topk_tokens': [' only', '<|end_header_id|>', ' discarded', ' came', '\n\n', ' did', '<|eot_id|>', '\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' and', ' of', ' to', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'assistant', 'b', 'athroom'], 'evidence_proportions': [4.13060188293457e-06, 8.397102355957032e-05, 2.6191274325052895e-05, 0.00031703710556030273, 2.3886561393737793e-05]}}, 25: {'grad': {'score': [0.20743385950724283, 0.24771371756779687, 0.2696821893964495, 0.24776107693563273, 0.1815308862262302], 'topk_tokens': [' in', ' return', ' block', '<|start_header_id|>', ' in', ' for', ' of', ' down', ' was', ' received', ' in', ' by', ' about', ' not', ' but', ':', ' have', 'str', ' STR', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.2181793212890625, 0.07891159057617188, 0.23901621500651044, 0.23716163635253906, 0.2775535583496094]}, 'weight': {'score': [0.07249022275209427, 0.007502540537948537, 0.0010293100561414447, 0.007183378147171364, 0.014125810729132758], 'topk_tokens': ['<|eot_id|>', ' writing', '.', ' milk', '❤', ' Foster', 'b', 'ot', 'nes', ' a', 'nes', 'ian', ' now', ' Mary', ' picked', '.', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.000510251522064209, 0.3266768932342529, 0.012417376041412354, 0.00467449426651001, 0.0026568472385406494]}, 'saliency': {'score': [0.00015664969881375632, 7.74937420661321e-05, 2.7108192443847657e-05, 7.746112326671554e-05, 7.114559412002563e-05], 'topk_tokens': [' impressed', 'nes', '\n\n', '.', ' as', ' Pioneer', ' streets', 'ot', ' picked', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', 'ian', '<|start_header_id|>', 'b', '<|begin_of_text|>', 'athroom', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [2.9551982879638673e-05, 0.0005065739154815674, 5.310773849487305e-05, 0.00017139315605163574, 1.868605613708496e-05]}}, 26: {'grad': {'score': [0.30798085530598956, 0.2996066978677924, 0.27857537950788225, 0.29973480808184083, 0.23603502909342447], 'topk_tokens': [' I', ' Pa', '\n', '.', ' St', ' a', ' prepared', ' J', ' ', ' breaking', ' James', ' their', ' against', '5', ' for', ' four', ' It', ' for', ' H', ' would'], 'evidence_proportions': [0.3297943115234375, 0.20723114013671873, 0.28861490885416663, 0.3847808837890625, 0.3589000701904297]}, 'weight': {'score': [0.03937541072567304, 0.007525184224302528, 0.0018647423812321255, 0.0073896108530970145, 0.012569884459177652], 'topk_tokens': ['b', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', ' Foster', 'nes', 'ot', '<|start_header_id|>', 'ian', 'nes', ' a', ' now', ' Mary', '<|eot_id|>', ' picked', '.', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0007680237293243409, 0.15241460800170897, 0.013003826141357422, 0.0198516845703125, 0.005416750907897949]}, 'saliency': {'score': [0.0003156326711177826, 9.967168622471681e-05, 3.817762647356306e-05, 9.89461005062243e-05, 9.283878737025791e-05], 'topk_tokens': ['\n', 'ate', 'athroom', '<|end_header_id|>', 'ig', 'g', ' "', ' *\n\n', ' of', '<|begin_of_text|>', 'years', ' the', ' where', ' the', '<|eot_id|>', ' lesser', '.', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [1.2761354446411132e-05, 0.0008169829845428466, 8.137524127960205e-05, 0.0006789639592170715, 5.558878183364868e-05]}}, 27: {'grad': {'score': [0.26649155219395954, 0.30426202617353487, 0.30414374215262274, 0.3044800370225274, 0.22339639398786756], 'topk_tokens': ['com', ' work', ' to', ' to', 'old', ' of', '\n', ' did', ' the', ' the', ' and', '\n', '\n', ' the', ' do', ' take', ' upper', ' to', ' a', ' of'], 'evidence_proportions': [0.19467544555664062, 0.09445104598999024, 0.278656005859375, 0.58453369140625, 0.23502349853515625]}, 'weight': {'score': [0.10560959577560425, 0.007521727492608925, 0.00144363386290414, 0.007009068751466426, 0.01239061521159278], 'topk_tokens': ['ian', 'b', ' writing', '<|end_header_id|>', '.', '❤', 'assistant', ' milk', ' Foster', 'ot', 'nes', 'ian', ' a', 'nes', ' now', ' Mary', 'athroom', ' picked', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0011914730072021486, 0.4808977127075195, 0.01446358362833659, 0.006329774856567383, 0.003020942211151123]}, 'saliency': {'score': [0.00030512362718582153, 0.00015996883397286968, 3.599779946463449e-05, 0.00016017359125577066, 0.00018106939064131843], 'topk_tokens': [' the', ' and', '<|begin_of_text|>', ' Where', ' versatile', '<|eot_id|>', ' and', ' out', 'years', '\n', ' *\n\n', ' secured', ' lesser', '<|eot_id|>', 'athroom', '<|end_header_id|>', 'b', 'assistant', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [2.3812055587768555e-05, 0.0005696654319763183, 0.0002233882745107015, 0.0006654113531112671, 8.840113878250122e-05]}}, 28: {'grad': {'score': [0.3508006731669108, 0.341955845934586, 0.5116985866001674, 0.3404827140751483, 0.25956087642245823], 'topk_tokens': ['ANK', ' the', '.', 'hand', ' a', '\n', ' to', '\n', ' a', ' a', '\n', ' a', ' a', ' he', '\n', ' to', 'A', '\n', ' to', 'hand'], 'evidence_proportions': [0.6984619140625, 0.172210693359375, 0.2533562978108724, 0.27520751953125, 0.3612213134765625]}, 'weight': {'score': [0.16638880719741186, 0.007417620522892621, 0.0008652286870138986, 0.0065591156711012144, 0.008490342646837234], 'topk_tokens': ['❤', ' writing', '\n\n', '.', 'assistant', ' milk', ' Foster', 'b', 'ot', 'nes', ' a', 'nes', 'ian', ' now', ' Mary', '<|end_header_id|>', ' picked', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0009242892265319824, 0.7847847461700439, 0.007644375165303548, 0.0028688907623291016, 0.0018610954284667969]}, 'saliency': {'score': [0.00044309844573338825, 0.00010100154098865556, 1.4552899769374302e-05, 9.97603096572697e-05, 5.3303937117258705e-05], 'topk_tokens': ['ot', ' propriet', '<|eot_id|>', 'ot', 'nes', ' milk', '<|start_header_id|>', 'ian', 'nes', ' up', ' a', '<|eot_id|>', ' Foster', ' picked', '\n\n', '<|begin_of_text|>', 'assistant', 'b', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [1.5020370483398438e-05, 0.001980394124984741, 5.746384461720785e-05, 5.929917097091675e-05, 1.8827617168426514e-05]}}, 29: {'grad': {'score': [0.3022865454355876, 0.2587637176369585, 0.2820491245814732, 0.2583185327889276, 0.22624926434622872], 'topk_tokens': [' not', ' back', ' and', ' *\n\n', 'ied', 'of', ' to', ' *\n\n', ' and', ' and', ' by', ' manager', '\n', 'ation', ' the', ' of', ' and', 'ian', '<|eot_id|>', ' await'], 'evidence_proportions': [0.4661590576171875, 0.25997314453125, 0.16393407185872397, 0.4430356025695801, 0.2171173095703125]}, 'weight': {'score': [0.23291547099749246, 0.007421451429919999, 0.0012754074164799281, 0.006177318097587862, 0.006823365887006124], 'topk_tokens': ['<|eot_id|>', 'b', '<|eot_id|>', '.', 'assistant', ' writing', ' milk', ' Foster', '<|end_header_id|>', 'ot', 'ian', 'nes', 'nes', ' now', ' a', 'athroom', ' Mary', '.', ' picked', '<|begin_of_text|>'], 'evidence_proportions': [0.001682901382446289, 1.1018775939941405, 0.006923476854960124, 0.005742073059082031, 0.0019149184226989746]}, 'saliency': {'score': [0.001063746710618337, 0.00014213329080819408, 3.5480942044939315e-05, 0.00013773159344914943, 8.01069868935479e-05], 'topk_tokens': [' *\n\n', ' *', 'nes', '.', '.', '.', ' now', '.', ' Mary', ' a', '<|start_header_id|>', ' picked', '<|end_header_id|>', 'assistant', ' The', '<|eot_id|>', '<|eot_id|>', 'athroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [3.0267238616943357e-05, 0.0038845419883728025, 0.00045605997244517005, 0.00077047199010849, 3.440678119659424e-05]}}, 30: {'grad': {'score': [0.5016829172770182, 0.46883361355328584, 0.6178782871791295, 0.4673959916078048, 0.4809165530734592], 'topk_tokens': [' method', 'd', ' or', 'ing', ' M', ' the', ' Daniel', 's', ' em', 'istributed', '6', ' the', ' com', ' Jul', '5', ' web', ' to', ' office', ' bathroom', ' B'], 'evidence_proportions': [0.852294921875, 0.398809814453125, 0.4118499755859375, 0.3584251403808594, 0.4700164794921875]}, 'weight': {'score': [0.1080072671175003, 0.007298938726005518, 0.0016056648322514125, 0.006767998985733916, 0.004404364360703362], 'topk_tokens': ['\n\n', 'nes', 'ot', 'assistant', '***', 'ian', ' a', '<|start_header_id|>', '<|start_header_id|>', ' now', 'nes', ' Mary', '<|eot_id|>', 'b', '<|eot_id|>', ' picked', '.', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015239953994750975, 0.4700324296951294, 0.004263500372568766, 0.04631495475769043, 0.005887866020202637]}, 'saliency': {'score': [0.0010417886078357697, 0.00016278491423195324, 7.16362680707659e-05, 0.0001584981328024943, 0.00011167261335584853], 'topk_tokens': ['<|start_header_id|>', ' came', ' The', ' picked', '.', ' milk', '***', ' the', ' left', ' second', '-text', 'assistant', 'L', 'athroom', 'b', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [6.238222122192382e-05, 0.0026403844356536867, 7.267793019612631e-05, 0.002466365694999695, 0.0002968907356262207]}}, 31: {'grad': {'score': [0.37726910909016925, 0.36360881686548335, 0.35666373116629463, 0.36358852263184177, 0.27962621053059894], 'topk_tokens': ['\n', '\n', '\n', '\n', '<|eot_id|>', '\n', '\n', ' the', '\n', '\n', '\n', '\n', '\n', ' of', '\n', '\n', '\n', '<|start_header_id|>', ',', '<|end_header_id|>'], 'evidence_proportions': [0.3398590087890625, 0.30552978515625, 0.3447723388671875, 0.5939788818359375, 0.34574127197265625]}, 'weight': {'score': [0.05917438864707947, 0.0067989342160895815, 0.0021565641675676617, 0.00653689735838178, 0.004379771649837494], 'topk_tokens': [' a', 'If', ' where', 'ian', ' came', ' after', 'Answer', ' they', '<|start_header_id|>', ' Mary', '<|eot_id|>', '\n\n', 'assistant', ' picked', '.', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.003966617584228516, 0.26372828483581545, 0.002583742141723633, 0.013034343719482422, 0.003517746925354004]}, 'saliency': {'score': [0.0006919341782728831, 0.0001314425941229542, 5.249636513846261e-05, 0.00012888365708557304, 0.00010474315947956509], 'topk_tokens': [':', 'If', '.\n', ' breaking', ' facts', 'Answer', '\n\n', ' after', '<|eot_id|>', ' Mary', ' came', ' picked', '.', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'athroom', 'b'], 'evidence_proportions': [8.347630500793457e-05, 0.0026451587677001954, 7.055699825286865e-05, 0.00059548020362854, 3.949552774429321e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:28:54.016 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:28:54.016 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-11_pid-3_1-5-6-8-9.pkl | len: 10 |  size: 9.27 KB
Processing depth (1, 5, 6, 8, 9):   4%|▍         | 4/100 [00:55<22:31, 14.08s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Processing depth (0, 2, 5, 6, 9):   4%|▍         | 4/100 [01:04<22:31, 14.08s/it]2025-01-23 22:29:03.205 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:29:03.205 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:29:03.206 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary picked up the milk.
2025-01-23 22:29:03.211 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1049, 1054) -->  cable. Mary picked up
2025-01-23 22:29:03.211 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:29:03.222 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2154, 2160) -->  Mary journeyed to the bedroom
2025-01-23 22:29:03.222 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:29:03.234 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2401, 2405) -->  Mary left the milk
2025-01-23 22:29:03.234 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel dropped the football.
2025-01-23 22:29:03.252 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3616, 3620) -->  dropped the football.
2025-01-23 22:29:03.252 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:29:03.265 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2507, 2512) --> . John moved to the
2025-01-23 22:29:03.265 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John went back to the office.
2025-01-23 22:29:03.268 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (645, 651) -->  Republican. John went back to
2025-01-23 22:29:03.268 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel took the football.
2025-01-23 22:29:03.270 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (340, 344) -->  Daniel took the football
2025-01-23 22:29:03.270 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:29:03.273 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (735, 739) -->  Daniel left the apple
2025-01-23 22:29:03.274 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:29:03.276 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (479, 485) --> . Sandra journeyed to the
2025-01-23 22:29:03.276 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:29:03.277 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (73, 78) --> . Daniel picked up the
2025-01-23 22:29:03.277 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-23 22:29:03.289 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 6 at --> (2065, 2070) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:29:03.763 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - bedroom<|eot_id|>
2025-01-23 22:29:03.763 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4223])
your chose emoji: ['🌬️', '🫂', '🦛', '👩🏻\u200d❤️\u200d👩🏿', '🧑🏾\u200d❤\u200d🧑🏼', '🐊', '🐿', '🤱🏻', '🥝', '\U0001faf8🏼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4226, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 217885.92it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.95it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.43it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.28it/s]
2025-01-23 22:29:06.769 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5231456756591797, 0.4262992111473246, 0.4348003387451172, 0.4256700163754966, 0.49652308802450856], 'topk_tokens': [' tour', ' of', ' getting', ' question', ' dry', 'system', ' and', '202', ' first', ' question', ' but', ' question', ' so', ' first', ' do', ' who', ' where', 'user', ' bathroom', '4'], 'evidence_proportions': [0.857470703125, 0.60633544921875, 0.34519704182942706, 0.3819160461425781, 0.4094047546386719]}, 'weight': {'score': [0.0027876185874144235, 0.007483450315938535, 0.0031342864036560058, 0.007547026197487483, 0.0005806737369106661], 'topk_tokens': [' adher', 'Just', '\n\n', 'user', '\n\n', 'pleasant', ' extortion', '<|end_header_id|>', '\n\n', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'athroom', 'assistant', 'b', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.003589034080505371, 0.001391446590423584, 0.00617063045501709, 0.0009618997573852539, 0.00028226524591445923]}, 'saliency': {'score': [0.00033480549852053326, 0.00010642868418632612, 0.00025799104145595006, 0.00010384031700407083, 2.0379501004372875e-05], 'topk_tokens': [' Collection', 'assistant', '\n\n', ' execution', ' Team', '<|end_header_id|>', 'user', ' adher', ' tour', '***', '\n\n', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'Just', '\n\n'], 'evidence_proportions': [0.0007603526115417481, 0.00019687414169311523, 0.000504752000172933, 4.833191633224487e-05, 6.839632987976074e-06]}}, 25: {'grad': {'score': [0.6853459676106771, 0.47433874633038037, 0.5662100655691964, 0.47235178461113736, 0.6511784522764145], 'topk_tokens': [' laid', ' lin', ' Stewart', 'eward', ' inaugur', ' dollars', 'ERS', ' headlines', 'itol', ' Eagle', '\u200d', ' less', ' *', '�', ' grat', ' Gree', '️', ' waiting', ' Eagle', ' Gus'], 'evidence_proportions': [0.8236709594726563, 0.63260498046875, 0.7593282063802084, 0.9223251342773438, 0.23041343688964844]}, 'weight': {'score': [0.0015291286011536915, 0.007445428137862733, 0.0023258686065673827, 0.007522504157175513, 0.0005710932516282605], 'topk_tokens': ['Good', 'RE', '\n\n', 'user', '<|end_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'b', ' extortion', ' discarded', 'pleasant', '<|start_header_id|>', 'athroom', '<|eot_id|>', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0012215614318847657, 0.0011120080947875977, 0.002341747283935547, 0.0024867430329322815, 0.00025844573974609375]}, 'saliency': {'score': [0.00010916590690612793, 0.00016962686422251436, 0.00019006899424961636, 0.0001698033913222611, 3.9110260625039376e-05], 'topk_tokens': ['constitutional', ' Southern', 'assistant', '<|eot_id|>', 'Mary', 'D', 'action', ' senate', ' STR', ' Red', 'Good', '<|eot_id|>', 'RE', '<|eot_id|>', '<|start_header_id|>', ' prepared', 'athroom', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [8.140802383422851e-05, 5.846619606018066e-05, 0.0001610716183980306, 0.0002316981554031372, 6.847083568572998e-06]}}, 26: {'grad': {'score': [0.31941906611124676, 0.25851833273097347, 0.31067210606166296, 0.25772951591607046, 0.18038543578117125], 'topk_tokens': [' could', ' of', ' could', ' extra', ' by', ' W', 'b', 'action', 'ting', ' old', 'Civil', ' political', ' a', ' back', ' prepared', 'user', ' December', ' bathroom', 'system', ' Joseph'], 'evidence_proportions': [0.47560272216796873, 0.2473846435546875, 0.25723203023274743, 0.3040599822998047, 0.3228721618652344]}, 'weight': {'score': [0.002329374353090922, 0.007352181762753084, 0.003160536289215088, 0.007416317824525515, 0.0019022226333618164], 'topk_tokens': ['\n\n', '\n\n', 'Just', 'user', ' accordingly', '\n\n', '\n\n', '<|end_header_id|>', '<|start_header_id|>', 'assistant', 'pleasant', '<|eot_id|>', ' extortion', 'b', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00246281623840332, 0.0015711545944213866, 0.004152059555053711, 0.0017207860946655273, 0.0009849071502685547]}, 'saliency': {'score': [0.00023595492045084635, 0.00021331403642483815, 0.00016908986227852956, 0.00021355508871149438, 0.0001514957797142767], 'topk_tokens': ['athroom', 'user', 'ucci', '<|eot_id|>', '<|start_header_id|>', 'istributed', ' wield', ' heard', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '<|eot_id|>', 'b', '<|eot_id|>', ' accordingly', '<|start_header_id|>', 'pleasant', ' extortion', '<|begin_of_text|>'], 'evidence_proportions': [0.0005777955055236816, 0.00011487007141113282, 0.0002765854199727376, 0.00010354071855545044, 3.1478703022003174e-05]}}, 27: {'grad': {'score': [0.9234771728515625, 0.9396085870023367, 0.933651624407087, 0.9397515309981256, 1.1041760598459551], 'topk_tokens': [',', ' bogus', '\n', 's', ',', ' enabling', ',', ',', ',', 'es', "'s", ',', '�', ',', ',', ' DAYS', ' bringing', '�', 's', ' Wins'], 'evidence_proportions': [0.9034790039062499, 0.9284423828125, 0.9398600260416666, 0.7342376708984375, 1.10693359375]}, 'weight': {'score': [0.002369751532872518, 0.007429386279733047, 0.0036664707320077077, 0.007490133406705698, 0.001786870341147146], 'topk_tokens': ['\n\n', '<|end_header_id|>', '\n\n', '\n\n', '<|start_header_id|>', 'pleasant', 'Just', 'user', ' extortion', 'RE', '<|eot_id|>', '<|start_header_id|>', 'D', '\n\n', 'b', '<|eot_id|>', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0038491725921630856, 0.001310718059539795, 0.003002564112345378, 0.0026161521673202515, 0.0006486475467681885]}, 'saliency': {'score': [0.0005542847017447153, 0.00016595032533310737, 0.000481365408216204, 0.0001610644235009051, 0.00018359961048249277], 'topk_tokens': [' Atlantic', ' FR', '<|end_header_id|>', 'athroom', 'RE', '<|end_header_id|>', 'Good', '<|start_header_id|>', ' John', 'Mary', ' bathroom', 'If', 'Just', '<|eot_id|>', '\n\n', 'b', 'D', 'user', ' Where', '<|begin_of_text|>'], 'evidence_proportions': [0.0015276491641998291, 0.00014597177505493164, 0.00048581759134928387, 0.00048345327377319336, 2.1502375602722168e-05]}}, 28: {'grad': {'score': [1.3329620361328125, 1.2736800808055786, 1.4003104073660715, 1.2722750344034977, 1.1663345829133065], 'topk_tokens': ['.', ' o', ' *\n\n', ',', ',', ' a', ' a', ' a', ',', ' *\n\n', ' a', ' a', ' a', '\n', ' a', ' a', ' a', ' a', 'A', '\n\n'], 'evidence_proportions': [1.52880859375, 1.39970703125, 1.3465576171875, 0.986968994140625, 1.330322265625]}, 'weight': {'score': [0.0013802610337734222, 0.007202769025377453, 0.0010370067187717982, 0.007288092248926696, 0.000855589585919534], 'topk_tokens': ['\n\n', '.\n\n', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'pleasant', ',', ',', '\n\n', ',', '<|eot_id|>', 'Just', '<|start_header_id|>', 'assistant', 'b', '<|eot_id|>', '\n\n', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008009314537048341, 0.0014065563678741454, 0.0026581287384033203, 0.00047704577445983887, 0.0010579675436019897]}, 'saliency': {'score': [7.010127107302348e-05, 0.0001564043438722328, 6.812385150364466e-05, 0.00015764290662243237, 3.294742876483548e-05], 'topk_tokens': ['constitutional', '<|start_header_id|>', 'Answer', '\n\n', 'ian', '<|eot_id|>', ' discarded', ' heard', '\n\n', ',', 'Just', 'b', ',', '<|eot_id|>', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.905960083007813e-05, 9.045004844665527e-05, 0.00011413792769114177, 1.249462366104126e-05, 2.5019049644470215e-05]}}, 29: {'grad': {'score': [0.7123525937398275, 0.4001435524921986, 0.5069008963448661, 0.39744868231586394, 0.5948063635057018], 'topk_tokens': [' proc', ' m', 'to', 'Sh', '26', ' Cl', ' the', ' upper', 'S', ' to', 'd', ' Jul', ' by', ' the', ' the', ' the', ' STR', '.', ' bathroom', ' up'], 'evidence_proportions': [1.4807373046875, 0.7292228698730469, 0.5064493815104167, 0.32245635986328125, 0.429534912109375]}, 'weight': {'score': [0.0024015121161937714, 0.007307358392877033, 0.0019038472856794085, 0.007380999669429064, 0.0017763275292611892], 'topk_tokens': ['.\n\n', ',', ',', 'Just', 'user', '<|eot_id|>', '<|start_header_id|>', ',', '\n\n', ' discarded', '\n\n', 'b', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0035484790802001952, 0.001862311363220215, 0.00358430544535319, 0.001307554543018341, 0.0009615719318389893]}, 'saliency': {'score': [0.0004253797233104706, 0.00022496357101769004, 0.00017361811229160855, 0.00022424053367676997, 0.0002086614408800679], 'topk_tokens': [' As', ' This', '\n\n', '50', '***', ' bathroom', 'Just', 'v', '\n\n', 'athroom', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '-text', 'If', '<|start_header_id|>', ' discarded', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.001214224100112915, 0.0004515528678894043, 0.00023565689722696942, 4.7907233238220215e-05, 6.866455078125e-05]}}, 30: {'grad': {'score': [0.6276235580444336, 0.7337573692802, 0.5606621878487723, 0.7358225343437369, 0.8913298576108871], 'topk_tokens': [' Min', 'year', ' and', ' take', ' and', ' time', ' what', ' what', ' time', ' sound', ' war', ' tele', ' B', ' world', ' fifteen', ' bathroom', ' fifty', '2', ' B', '100'], 'evidence_proportions': [0.80731201171875, 0.7819290161132812, 0.3992462158203125, 0.5125656127929688, 0.667755126953125]}, 'weight': {'score': [0.004546064883470535, 0.0071176207578977704, 0.003778001240321568, 0.0071604822947591625, 0.002371487598265371], 'topk_tokens': [' This', ',', '.\n\n', '.\n\n', ' One', 'P', '<|start_header_id|>', ',', ',', 'Just', '<|eot_id|>', 'Good', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0068777561187744135, 0.0030111074447631836, 0.00670623779296875, 0.0032298192381858826, 0.001626133918762207]}, 'saliency': {'score': [0.0019169909258683522, 0.00032528528262245335, 0.0002773599965231759, 0.00031652033171666335, 0.0003709225885329708], 'topk_tokens': ['If', 'Answer', 'Just', '<|end_header_id|>', '<|eot_id|>', ' political', 'f', '<|eot_id|>', ' B', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'Good', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', '\n\n', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.008330625295639039, 0.00022433400154113768, 0.0003631661335627238, 0.00020745396614074707, 5.6043267250061035e-05]}}, 31: {'grad': {'score': [0.7683442433675131, 0.9583173211129762, 0.8205825805664062, 0.9605683577784481, 1.0803797937208606], 'topk_tokens': ['mail', ' Associated', 'ian', 'ot', ' about', 'assistant', ' present', ' const', ' affairs', 'ian', 'until', 'user', 'ot', 'ot', 'ian', ' members', 'ian', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.457159423828125, 0.65946044921875, 0.9873504638671875, 0.8242835998535156, 0.9089813232421875]}, 'weight': {'score': [0.002215107282002767, 0.006723598908091742, 0.0016220433371407644, 0.006792415381336144, 0.0014201143095570227], 'topk_tokens': ['If', 'If', '.\n\n', 'Answer', ' two', ' One', '<|eot_id|>', ' Where', '<|start_header_id|>', 'Just', ',', 'assistant', ' extortion', '\n\n', 'pleasant', '<|eot_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.002827739715576172, 0.002117753028869629, 0.002450982729593913, 0.0017375051975250244, 0.001694798469543457]}, 'saliency': {'score': [0.0001887492835521698, 0.0003086073501269173, 0.00012671266283307756, 0.00031082547051402513, 0.00012813364305803851], 'topk_tokens': ['.\n\n', 'If', ' You', ' set', ' One', ' extortion', 'Question', ' two', 'Answer', '<|start_header_id|>', ' Where', '<|eot_id|>', 'Just', 'pleasant', 'assistant', '<|end_header_id|>', 'b', '<|eot_id|>', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0004651963710784912, 6.920695304870605e-05, 0.00018688539663950604, 5.5849552154541016e-05, 0.00012831389904022217]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:29:06.777 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:29:06.778 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-11_pid-4_0-2-5-6-9.pkl | len: 10 |  size: 9.88 KB
Processing depth (0, 2, 5, 6, 9):   5%|▌         | 5/100 [01:08<21:32, 13.60s/it]Processing depth (0, 2, 5, 6, 9):   5%|▌         | 5/100 [01:08<21:44, 13.73s/it]
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:99 - Selected idx: 12
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:103 - Needle: [' John moved to the garden.', ' Sandra moved to the kitchen.', ' John went back to the office.', ' Daniel left the apple.', ' Mary picked up the apple.', ' Mary moved to the bathroom.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Mary journeyed to the bedroom.', ' Daniel journeyed to the kitchen.']
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:104 - Real Needle: [' Mary picked up the apple.', ' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Daniel journeyed to the kitchen.']
2025-01-23 22:29:06.981 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.35it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.17it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Processing depth (4, 6, 8, 9):   0%|          | 0/100 [00:08<?, ?it/s]2025-01-23 22:29:15.566 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:29:15.568 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (346, 351) -->  Daniel picked up the apple
2025-01-23 22:29:15.568 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:29:15.580 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2351, 2356) --> . Mary moved to the
2025-01-23 22:29:15.580 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:29:15.597 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3342, 3348) --> . Mary journeyed to the
2025-01-23 22:29:15.597 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-23 22:29:15.616 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3661, 3667) --> . Daniel journeyed to the
2025-01-23 22:29:15.616 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:29:15.634 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3600, 3605) -->  John moved to the garden
2025-01-23 22:29:15.634 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:29:15.649 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3133, 3138) --> . Sandra moved to the
2025-01-23 22:29:15.649 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:29:15.666 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3394, 3400) --> . John went back to the
2025-01-23 22:29:15.666 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:29:15.674 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1518, 1522) -->  Daniel left the apple
2025-01-23 22:29:15.674 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:29:15.679 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1053, 1059) -->  cable. Sandra journeyed to
2025-01-23 22:29:15.679 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:29:15.681 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (345, 350) --> . Daniel picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:29:16.298 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The People's theater.<|eot_id|>
2025-01-23 22:29:16.298 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4236])
your chose emoji: ['⚔', '👱🏾', '👨🏽\u200d🚒', '🧑🏻\u200d🔧', '👩🏿\u200d❤\u200d💋\u200d👨🏻', '✴️', '👩🏾\u200d❤️\u200d💋\u200d👨🏿', '⬆️', '🎺', '👨🏿\u200d❤️\u200d👨🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4239, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 236298.82it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.91it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.92it/s]
2025-01-23 22:29:19.429 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.6601066589355469, 0.6399120160728503, 0.5310828916488155, 0.6406118299080548, 0.8667621612548828], 'topk_tokens': ['ISC', 'g', '      ', '      ', '      ', '      ', ' compos', ' goods', ' required', '\u200d', 'MIN', ' em', '      ', ' const', 'G', 'ORE', ' em', 'ANK', ' Gov', ' Gov'], 'evidence_proportions': [0.49286041259765623, 0.57357177734375, 0.6833089192708333, 0.848388671875]}, 'weight': {'score': [0.0002989376133138483, 0.007450792412061572, 0.000582960344129993, 0.007539240476963228, 0.0025087828221528425], 'topk_tokens': ['\n', ' ', '\n\n', ' Project', 'system', 'b', '\n\n', 'user', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'assistant', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [8.175969123840331e-05, 0.00010445117950439454, 0.00021275877952575684, 0.0007281700770060221]}, 'saliency': {'score': [5.518983710895885e-05, 0.00016989243792655138, 8.668534217342253e-05, 0.00017111147213255568, 0.00019888288300970325], 'topk_tokens': ['\n', '\n\n', ' Project', ' Knowledge', '<|end_header_id|>', ' *', 'system', '<|eot_id|>', 'user', '\n\n', 'athroom', '***', '<|start_header_id|>', '\n\n', 'assistant', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [3.653764724731445e-06, 5.9962272644042965e-06, 1.2462337811787922e-05, 0.0001818587382634481]}}, 25: {'grad': {'score': [0.7061476273970171, 0.6467747923393636, 0.6683861517137096, 0.6463027057383428, 0.9813543195309846], 'topk_tokens': ['      ', '�', '      ', '      ', ' generally', ' get', 'APER', '�', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      ', '      '], 'evidence_proportions': [0.88585205078125, 0.45836181640624996, 0.6587727864583333, 0.8102569580078125]}, 'weight': {'score': [0.0004182579842480746, 0.007425724883550854, 0.0004967133845052411, 0.007513867174103906, 0.002795198689336362], 'topk_tokens': ['�', '      ', '?', ' \n', 'system', '\n\n', '\n\n', 'user', '<|eot_id|>', 'b', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0001698136329650879, 0.00014283657073974607, 0.00025007128715515137, 0.0010229994853337605]}, 'saliency': {'score': [3.0375339768149637e-05, 0.00011502922382746195, 3.510232894651351e-05, 0.00011606604159875059, 0.00021478890076927516], 'topk_tokens': ['user', '\n\n', '�', ' Date', 'istributed', '      ', '\n\n', '<|eot_id|>', 'system', 'assistant', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>', 'athroom', '\n\n'], 'evidence_proportions': [1.2838840484619141e-05, 2.657771110534668e-05, 1.8268823623657227e-05, 6.026029586791992e-05]}}, 26: {'grad': {'score': [0.6940290277654474, 0.6330858407422152, 0.5435271724577873, 0.633428785941046, 0.743625723797342], 'topk_tokens': ['graph', ' huge', ' in', 'ot', ' Reporter', ' John', ' distributed', ' so', ' throughout', ' heard', ' for', 'ian', "'s", ' em', ' em', 'ot', ' state', ' it', ' bathroom', ' writer'], 'evidence_proportions': [0.7144088745117188, 0.454388427734375, 0.6140187581380208, 0.956756591796875]}, 'weight': {'score': [0.0009610192342237992, 0.007375215916004797, 0.0013820038687798284, 0.007453310015494323, 0.00742254956908848], 'topk_tokens': [' \n', '.', '�', '�', '<|start_header_id|>', '�', '\n\n', 'user', 'b', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '\n\n', 'athroom', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.372762680053711e-05, 9.957551956176757e-05, 0.0003129939238230387, 0.0030829906463623047]}, 'saliency': {'score': [3.883649002421986e-05, 0.0001440098978039453, 0.00012460158717247748, 0.00014470637955280564, 0.0006039638234221417], 'topk_tokens': ['\n\n', '      ', '<|start_header_id|>', ' and', '<|end_header_id|>', 'user', '.', '�', '<|eot_id|>', ' John', '�', 'assistant', '�', 'athroom', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [7.331371307373047e-06, 6.282329559326172e-06, 1.0718901952107748e-05, 0.0001203368107477824]}}, 27: {'grad': {'score': [1.5467655875466086, 1.4903219177577258, 1.2355696155178932, 1.4919118749087243, 1.0313705776048743], 'topk_tokens': [',', ',', ',', ',', ',', ',', ',', '.', ',', ',', 'In', ',', ' *', ' ', ',', 'In', '.', ',', '.', ','], 'evidence_proportions': [1.3044860839843748, 1.92099609375, 2.1859537760416665, 0.7976182301839193]}, 'weight': {'score': [0.0007907829501412132, 0.007427558214997986, 0.0007033232719667496, 0.007512235792413375, 0.008715019601842632], 'topk_tokens': ['�', '�', '<|eot_id|>', '\n\n', '�', '�', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'user', '�', 'b', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [7.99238681793213e-05, 0.00011640191078186036, 0.0002507766087849935, 0.0024851560592651367]}, 'saliency': {'score': [0.0001583234830336137, 0.00022942612378713684, 0.0001132651682822935, 0.00023066005778551898, 0.0013394316901331363], 'topk_tokens': ['<|eot_id|>', '\n\n', '.', '<|eot_id|>', '�', ' Joseph', 'system', '�', '<|end_header_id|>', 'user', '�', '<|start_header_id|>', '�', 'b', '�', '<|start_header_id|>', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.418062210083008e-06, 1.8262863159179686e-05, 3.0502676963806152e-05, 0.0005302826563517252]}}, 28: {'grad': {'score': [0.9013706554066051, 0.8045393396419409, 0.8160198580834174, 0.8039454110660909, 0.7805305149244226], 'topk_tokens': [' a', ' a', 'man', ' a', ' a', ' a', 'A', ' a', 'a', ' A', ' man', ' a', ' a', ' a', ' a', 'man', 'a', '\n\n', ' *', ' man'], 'evidence_proportions': [1.26502685546875, 0.7939056396484375, 0.8175608317057291, 0.7716878255208334]}, 'weight': {'score': [0.0005177205259149724, 0.007255726531024923, 0.0009543520788992605, 0.007337804586478414, 0.003539611463961394], 'topk_tokens': ['user', '\n', ' apple', ' \n', 'Answer', '<|start_header_id|>', '<|end_header_id|>', ' was', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'b', 'athroom', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.7011165618896483e-05, 3.0362606048583983e-05, 0.0002046823501586914, 0.0016541481018066406]}, 'saliency': {'score': [1.0852109302173961e-05, 8.667351277714266e-05, 1.8449560288460026e-05, 8.757724268722351e-05, 0.00010208785533905029], 'topk_tokens': ['�', '<|start_header_id|>', ' Wood', 'ford', 'user', '<|end_header_id|>', '\n', 'system', '\n\n', '<|eot_id|>', '<|eot_id|>', '\n', '<|eot_id|>', '<|start_header_id|>', 'b', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.5079975128173829e-06, 1.3887882232666014e-06, 7.361173629760742e-06, 3.0015905698140464e-05]}}, 29: {'grad': {'score': [0.7942300709811124, 0.7646800277648768, 0.8699815504012569, 0.7637448992047992, 0.8026590761931046], 'topk_tokens': ['ian', ' would', '�', ' night', '�', ' that', ' breaking', ' night', ' small', ' feature', '�', ' gold', ' gold', '\u200d', ' face', ' new', '�', ' double', ' Date', '\n\n'], 'evidence_proportions': [0.7011701583862304, 0.8287841796875, 0.76580810546875, 0.87140687306722]}, 'weight': {'score': [0.0005308511582287875, 0.007332423633774319, 0.0006319630530572706, 0.007417791257392147, 0.0069081002603406496], 'topk_tokens': [' was', '\n\n', '<|eot_id|>', '�', 'user', 'system', '\n', '<|start_header_id|>', '<|start_header_id|>', '\n\n', '�', 'b', '<|eot_id|>', 'assistant', 'athroom', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [2.587437629699707e-05, 3.71396541595459e-05, 0.00036301712195078534, 0.001530925432840983]}, 'saliency': {'score': [5.961141803047874e-05, 0.00018786932674605037, 8.761498235887097e-05, 0.0001892858481669164, 0.0008521504376245582], 'topk_tokens': ['\n\n', '�', '<|end_header_id|>', '<|eot_id|>', ' *\n\n', '�', 'user', '\n\n', '<|start_header_id|>', 'assistant', 'b', '<|start_header_id|>', 'athroom', '<|start_header_id|>', 'system', '\n\n', '�', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [9.119510650634766e-07, 2.4616718292236326e-06, 0.00010293225447336832, 0.00011283159255981445]}}, 30: {'grad': {'score': [1.0661964416503906, 1.213767434831328, 1.1686551493983115, 1.2148770962499627, 0.8220676753831946], 'topk_tokens': [' cash', ' in', ' impressed', ' absent', ' on', ' proof', ' all', ' engaged', ' raid', ' down', ' Gus', ' news', ' all', ' any', 's', ' B', ' all', 'ails', ' B', ' prof'], 'evidence_proportions': [0.32143707275390626, 1.548681640625, 1.3510564168294272, 0.9998982747395833]}, 'weight': {'score': [0.0013773901896043258, 0.007174421014806014, 0.0010197249151045276, 0.007250467421219101, 0.016105911330036495], 'topk_tokens': ['�', '�', '\n\n', '<|eot_id|>', ' *', 'system', '�', '\n', 'user', '<|start_header_id|>', 'athroom', '<|start_header_id|>', 'assistant', 'b', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [7.19606876373291e-05, 5.756020545959473e-05, 0.0007064342498779297, 0.004236062367757162]}, 'saliency': {'score': [0.00011478229002519087, 0.0003434408945586215, 8.871382282626244e-05, 0.0003465290523520851, 0.0011747945909914763], 'topk_tokens': ['�', '�', '�', '�', '�', ' John', 'system', '<|eot_id|>', 'assistant', '�', '<|start_header_id|>', 'user', 'athroom', '<|eot_id|>', '<|begin_of_text|>', 'b', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [1.61588191986084e-05, 6.5982341766357415e-06, 6.0811638832092285e-05, 0.0003410925467809041]}}, 31: {'grad': {'score': [1.1191434860229492, 1.0448653177709954, 1.2454126419559601, 1.0429897595408768, 1.8186895950980808], 'topk_tokens': ['.', 'ian', 'ot', '\n', ' connected', ' based', 'user', '�', '\n', 'ate', '\n', '�', '<|end_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '�', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.6907592773437501, 0.619415283203125, 0.8868001302083333, 2.1249138514200845]}, 'weight': {'score': [0.0009834834120490334, 0.006887174490685451, 0.0004944618671171127, 0.0069655441263903285, 0.003525861903377201], 'topk_tokens': [' John', ' apple', ' P', '\n\n', ',', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'system', ' was', 'user', '<|eot_id|>', 'assistant', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0002039194107055664, 0.00031247735023498535, 0.002159516016642253, 0.0010162591934204102]}, 'saliency': {'score': [5.9641220352866433e-05, 0.0003221256605266653, 3.5171547243672035e-05, 0.00032563025565222583, 0.00022528385338575944], 'topk_tokens': ['      ', '      ', 'Today', 'illustr', 'men', '\n\n', 'Answer', 'user', 'Saturday', ' P', 'system', '<|eot_id|>', 'assistant', '\n\n', 'b', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', 'athroom'], 'evidence_proportions': [2.046823501586914e-05, 1.2046098709106445e-05, 0.00015827020009358722, 3.331899642944336e-05]}}, 'pred_res': "The People's theater.<|eot_id|>", 'score': 0}
2025-01-23 22:29:19.436 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:29:19.436 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-12_pid-0_4-6-8-9.pkl | len: 10 |  size: 9.5 KB
Processing depth (4, 6, 8, 9):   1%|          | 1/100 [00:12<20:24, 12.37s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (1, 5, 7, 8):   1%|          | 1/100 [00:21<20:24, 12.37s/it]2025-01-23 22:29:28.927 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:29:28.929 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (330, 335) -->  Daniel picked up the apple
2025-01-23 22:29:28.929 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:29:28.940 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2159, 2164) --> . Mary moved to the
2025-01-23 22:29:28.940 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:29:28.955 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2941, 2947) --> . Mary journeyed to the
2025-01-23 22:29:28.955 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-23 22:29:28.973 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3358, 3364) --> . Daniel journeyed to the
2025-01-23 22:29:28.973 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:29:28.991 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3602, 3607) --> . John moved to the
2025-01-23 22:29:28.991 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:29:29.007 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3156, 3161) --> . Sandra moved to the
2025-01-23 22:29:29.007 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:29:29.024 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3410, 3416) --> . John went back to the
2025-01-23 22:29:29.024 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:29:29.032 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1571, 1575) -->  Daniel left the apple
2025-01-23 22:29:29.032 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:29:29.037 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1095, 1101) --> . Sandra journeyed to the
2025-01-23 22:29:29.037 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:29:29.039 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (329, 334) --> . Daniel picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:29:29.715 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The apple was in the bathroom.<|eot_id|>
2025-01-23 22:29:29.715 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4222])
your chose emoji: ['🥞', '📟', '↖', '👨🏾\u200d❤️\u200d💋\u200d👨🏻', '🦻🏼', '🧝🏾\u200d♀️', '👨🏾\u200d❤\u200d👨🏼', '🧔🏽\u200d♂', '🥨', '🏄🏾\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4225, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 239674.51it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 121.32it/s]
2025-01-23 22:29:32.851 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.2280394814231179, 0.2714319353272929, 0.3018430894420993, 0.27143478484990385, 0.28460837021852153], 'topk_tokens': [' feature', ' grat', ' was', 'graph', ' than', ' paper', ' whistle', 'graph', ' Cl', ' tie', ' were', ' and', 'de', 'ford', 'com', ' Out', ' of', ' context', ' bathroom', 'a'], 'evidence_proportions': [0.2945343017578125, 0.26775360107421875, 0.18273099263509113, 0.18484052022298178]}, 'weight': {'score': [0.0014547583731738005, 0.007461489389633992, 0.0022656311911921348, 0.00753177215246089, 0.0015544123374498808], 'topk_tokens': ['Answer', '.\n\n', 'Question', ' ', 'b', ' \n', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '\n\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010283052921295165, 0.0011183083057403564, 0.0016480088233947754, 0.0018972605466842651]}, 'saliency': {'score': [7.135217840021306e-05, 8.248518909928362e-05, 0.0001481088899797009, 8.205628006478856e-05, 5.708596645257412e-05], 'topk_tokens': ['The', ' Where', 'In', 'athroom', 'EF', 'assistant', '<|eot_id|>', '<|eot_id|>', 'Question', ' bedroom', 'ER', 'NEW', '\n\n', '<|eot_id|>', 'user', '<|start_header_id|>', ' bathroom', '\n\n', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [3.5303831100463864e-05, 8.201003074645997e-05, 0.00011094411214192709, 5.291899045308431e-05]}}, 25: {'grad': {'score': [0.28602530739524146, 0.3181317752634985, 0.31165830550655244, 0.31834918174853377, 0.4212893217037886], 'topk_tokens': [' well', ' tele', ' large', ' tele', 'Another', 'through', 'ad', 'y', ' rest', ' long', 'out', '�', ' candidates', ' wood', 'out', ' papers', 'tele', ' county', 'New', ' Wood'], 'evidence_proportions': [0.27818756103515624, 0.29523773193359376, 0.3044382731119792, 0.26646677652994794]}, 'weight': {'score': [0.0027801191264932804, 0.007377379603639862, 0.0022210344191520444, 0.007439936274592966, 0.001999968519577613], 'topk_tokens': ['user', ',', ' bedroom', '.\n\n', 'MIN', 'b', '.\n\n', ' apple', ' Do', '<|eot_id|>', '\n\n', 'Answer', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00351606011390686, 0.002328968048095703, 0.0016363461812337239, 0.0036865671475728354]}, 'saliency': {'score': [0.0001292553814974698, 0.00012691824393864918, 9.222857413753386e-05, 0.00012716368083642976, 8.57099508627867e-05], 'topk_tokens': ['paper', ' bathroom', ' *\n\n', '<|start_header_id|>', ' Bor', ' Moore', ' Douglas', 'Minnesota', 'ree', 'tele', '<|eot_id|>', 'Answer', 'athroom', '<|eot_id|>', ' Do', ' bedroom', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.0001598656177520752, 0.00015184283256530762, 8.418162663777669e-05, 0.00012999773025512695]}}, 26: {'grad': {'score': [0.48877421292391693, 0.4623817579280695, 0.4827542458811114, 0.4620912063613263, 0.4783349893031976], 'topk_tokens': [' men', ' *', ' vigorous', ' state', 'ir', 'Penn', ' bathroom', ' generally', ' pen', '�', '�', ' force', 'engers', '�', ' mess', ' tell', '�', 'x', '�', ' patent'], 'evidence_proportions': [0.4936492919921875, 0.48465576171875, 0.409210205078125, 0.5677076975504557]}, 'weight': {'score': [0.0016384991732510653, 0.00717491849639712, 0.002009275459474133, 0.007242496674550003, 0.003932437835595546], 'topk_tokens': [' *\n\n', '<|start_header_id|>', '.\n\n', 'Answer', '.\n\n', 'user', ',', '.\n', '<|eot_id|>', 'b', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|start_header_id|>', '\n\n', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0011284828186035156, 0.0008069992065429688, 0.0008845925331115723, 0.0035103360811869306]}, 'saliency': {'score': [9.545683860778809e-05, 0.0001030879331058299, 0.00016660555716483823, 0.0001026562067714885, 0.00011152487534743089], 'topk_tokens': [' Southern', 'In', '.\n\n', '.\n', '<|eot_id|>', '.\n', '<|start_header_id|>', 'Answer', '<|eot_id|>', ':', 'assistant', '\n\n', '<|start_header_id|>', 'b', '\n\n', 'athroom', ' bathroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.00012958645820617675, 4.8226118087768556e-05, 2.368787924448649e-05, 0.00017814338207244873]}}, 27: {'grad': {'score': [0.2340056679465554, 0.32295443382488903, 0.24779756607547884, 0.32398193520301827, 0.27994556916065705], 'topk_tokens': [' cap', ' cap', ' business', ' bill', 'graph', 'St', ' state', ' bill', ' state', 'asca', ' circ', ' cap', 'state', ' business', ' state', ' St', ' state', 'itol', ' state', ' state'], 'evidence_proportions': [0.3234344482421875, 0.1736572265625, 0.20566940307617188, 0.23810831705729166]}, 'weight': {'score': [0.003163001754067161, 0.007359123229980469, 0.002270406292330834, 0.0074190620836566985, 0.004251025426082122], 'topk_tokens': [' bedroom', '\n\n', '.\n\n', 'RE', 'EF', '.\n\n', 'NEW', '<|eot_id|>', '<|eot_id|>', 'user', '<|start_header_id|>', '<|eot_id|>', 'b', '\n\n', '\n\n', '\n\n', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00267946720123291, 0.0008809804916381837, 0.0021087725957234698, 0.006521860758463542]}, 'saliency': {'score': [0.00032196532596241343, 0.00012180705042280389, 0.00028665604129914313, 0.00011952665713923775, 0.00012246118142054632], 'topk_tokens': ['If', ' bedroom', ' *\n\n', '<|eot_id|>', ' *\n\n', 'If', 'Just', '<|start_header_id|>', '<|eot_id|>', 'Question', 'EF', '\n\n', '\n\n', 'NEW', 'athroom', 'assistant', ' bathroom', '<|begin_of_text|>', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.0005529224872589111, 9.08493995666504e-05, 0.00021002689997355142, 0.00043403605620066327]}}, 28: {'grad': {'score': [0.2731824354691939, 0.2831071011695636, 0.2856675732520319, 0.28314041085576963, 0.266935788668119], 'topk_tokens': [' com', ' *', ' Fletcher', ' under', ' three', 'blue', 'de', ' being', ' fifty', '�', ' four', ' of', ' *', ' Foster', ' Dub', ' George', ' *', ' *', ' *', ' half'], 'evidence_proportions': [0.29007568359375, 0.32834930419921876, 0.26284027099609375, 0.22347450256347656]}, 'weight': {'score': [0.0011491016908125443, 0.007023728049013037, 0.0011118640822748984, 0.007098634463885829, 0.0018556568867120987], 'topk_tokens': ['\n\n', ':', 'If', ':', 'Answer', ' bedroom', '.\n', '<|eot_id|>', '<|start_header_id|>', '.\n\n', 'During', '.\n\n', '<|eot_id|>', 'b', '\n\n', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00023670792579650877, 0.0009083032608032226, 0.0005510101715723673, 0.0027081867059071856]}, 'saliency': {'score': [2.8256665576588023e-05, 8.946145074607353e-05, 3.83055979205716e-05, 9.016431189452012e-05, 4.8064650633396246e-05], 'topk_tokens': [' the', 'just', '<|eot_id|>', ' just', 'MIN', ' Square', ' During', ' bedroom', ' person', '.\n\n', ':', '<|eot_id|>', 'In', 'During', 'athroom', '<|end_header_id|>', 'assistant', '\n\n', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [5.459785461425781e-06, 2.4533271789550783e-05, 1.7488996187845867e-05, 6.112456321716309e-05]}}, 29: {'grad': {'score': [0.42466215653852984, 0.38043275009245564, 0.3933322045110887, 0.38010366811048263, 0.3911883036295573], 'topk_tokens': [' the', ' office', ':', ' to', ' out', ' up', ' the', '.', '.', ' out', ' in', ' of', ' kitchen', '.', ' in', ' the', ' the', ' in', ' dry', ' bathroom'], 'evidence_proportions': [0.2461700439453125, 0.501513671875, 0.4787025451660156, 0.455322265625]}, 'weight': {'score': [0.00110255859114907, 0.007221427104882235, 0.0007304789558533699, 0.007301924348391828, 0.002020615415695386], 'topk_tokens': [' to', 'If', 'Answer', ' Do', '.\n\n', ' ', 'If', ' bedroom', 'b', 'user', '<|eot_id|>', '<|eot_id|>', '\n\n', '<|start_header_id|>', '\n\n', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000274193286895752, 0.000677192211151123, 0.0005820691585540771, 0.002667824427286784]}, 'saliency': {'score': [3.5937536846507676e-05, 0.00011135546413399059, 3.0167641178254158e-05, 0.00011235642696044132, 7.08229266680204e-05], 'topk_tokens': ['<|eot_id|>', 'A', ' of', ',', '<|end_header_id|>', ' ', ' to', '<|start_header_id|>', 'If', '<|start_header_id|>', '<|start_header_id|>', ' ', '<|end_header_id|>', 'assistant', '<|eot_id|>', ' bedroom', 'b', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [1.0877847671508789e-05, 1.4078617095947265e-05, 1.4061729113260906e-05, 9.691218535105388e-05]}}, 30: {'grad': {'score': [0.2506294250488281, 0.24949608797152367, 0.2057097957980248, 0.2498154651627216, 0.2765370874832838], 'topk_tokens': [' no', ' o', ' he', ' NEW', 'ball', 'ab', 'LY', ' for', ' OF', ' bend', ' STR', ' OF', ' Wing', ' no', ' EAR', 'UG', 'ien', ' B', ' B', 'b'], 'evidence_proportions': [0.26902008056640625, 0.2544158935546875, 0.23076375325520831, 0.25201416015625]}, 'weight': {'score': [0.0032198429107666016, 0.007066693277754022, 0.0021044438885104272, 0.007123850621747376, 0.007667090648259872], 'topk_tokens': [' *\n\n', 'NEW', '\n\n', ' *\n\n', '.\n', '.\n\n', '<|eot_id|>', '<|start_header_id|>', '\n\n', ' *\n\n', '<|eot_id|>', '<|eot_id|>', '.\n\n', '<|start_header_id|>', 'b', 'athroom', '\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0015149116516113281, 0.001449573040008545, 0.0018040041128794353, 0.00753168265024821]}, 'saliency': {'score': [0.0001538639718836004, 0.00015805381289600622, 9.392153832220262e-05, 0.0001585524411352346, 0.0002222427955040565], 'topk_tokens': [' Articles', ' kitchen', '<|start_header_id|>', ':', ' *\n\n', 'Today', ' bedroom', '.\n', ' *\n\n', 'NEW', ' B', 'user', '\n\n', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>', 'b'], 'evidence_proportions': [9.946227073669433e-05, 7.69972801208496e-05, 0.00010105470816294353, 0.00031606356302897137]}}, 31: {'grad': {'score': [0.32415632768110797, 0.34780143421782544, 0.31214843257781, 0.3481910400171179, 0.20845882097880045], 'topk_tokens': [' made', ' and', ' especial', 'for', ' of', ' or', ' generally', ' and', ' compelled', ' so', ' governor', ' for', ' formally', ' now', ' now', ' as', ' to', ' of', ' season', 'then'], 'evidence_proportions': [0.313018798828125, 0.3543212890625, 0.3340301513671875, 0.2984263102213542]}, 'weight': {'score': [0.002044577490199696, 0.006706520170855099, 0.00188734358356845, 0.006766912599709438, 0.0031139224003522824], 'topk_tokens': ['Just', '<|eot_id|>', 'Question', 'If', 'If', ' Where', '<|start_header_id|>', ':', ':', 'Answer', ' \n', '.\n\n', '.\n\n', '<|eot_id|>', 'assistant', 'b', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006263971328735352, 0.0015074133872985838, 0.0014289021492004395, 0.004289706548055013]}, 'saliency': {'score': [3.604455427689986e-05, 9.887248806699494e-05, 9.669315430425829e-05, 9.921998899940791e-05, 0.00010807315508524577], 'topk_tokens': ['Answer', 'user', ':', '      ', ' bathroom', ' bedroom', 'If', ' \n', '<|eot_id|>', '\n', ' Where', '.\n\n', '<|begin_of_text|>', 'assistant', '.\n\n', '<|eot_id|>', '\n\n', 'athroom', 'b', '<|end_header_id|>'], 'evidence_proportions': [1.410841941833496e-05, 3.448724746704102e-05, 3.4665067990620926e-05, 5.700190862019857e-05]}}, 'pred_res': 'The apple was in the bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:29:32.859 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:29:32.859 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-12_pid-1_1-5-7-8.pkl | len: 10 |  size: 9.13 KB
Processing depth (1, 5, 7, 8):   2%|▏         | 2/100 [00:25<21:12, 12.99s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Processing depth (1, 5, 8, 9):   2%|▏         | 2/100 [00:35<21:12, 12.99s/it]2025-01-23 22:29:42.650 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:29:42.652 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (391, 396) -->  Daniel picked up the apple
2025-01-23 22:29:42.652 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:29:42.663 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2129, 2134) -->  Mary moved to the bathroom
2025-01-23 22:29:42.663 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:29:42.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3274, 3280) -->  Mary journeyed to the bedroom
2025-01-23 22:29:42.680 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-23 22:29:42.701 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3605, 3611) -->  Daniel journeyed to the kitchen
2025-01-23 22:29:42.701 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:29:42.719 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3582, 3587) --> . John moved to the
2025-01-23 22:29:42.719 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:29:42.735 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3125, 3130) --> . Sandra moved to the
2025-01-23 22:29:42.735 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:29:42.752 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3367, 3373) --> . John went back to the
2025-01-23 22:29:42.752 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:29:42.759 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1504, 1508) -->  left the apple.
2025-01-23 22:29:42.760 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:29:42.765 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1081, 1087) --> . Sandra journeyed to the
2025-01-23 22:29:42.765 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:29:42.767 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (390, 395) --> . Daniel picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:29:43.283 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:29:43.283 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4209])
your chose emoji: ['🤷\u200d♂️', '🧛🏼\u200d♂️', '⚛', '🌺', '\U0001faf1🏿\u200d\U0001faf2🏻', '🛳️', '🤳🏽', '👈🏻', '🗂️', '👨🏿\u200d💼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4212, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 198546.93it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.33it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.60it/s]
2025-01-23 22:29:46.152 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.47194914384321734, 0.349156683100368, 0.3853356146043347, 0.34823747631677443, 0.1662062718318059], 'topk_tokens': ['.', ' Think', ' of', ' this', ' in', '***', 'otyping', '\n\n\n\n\n\n\n', ',', ' he', ' method', ' ago', 'ANK', ' Paul', 'str', ' century', ' of', ' method', ' Published', ' in'], 'evidence_proportions': [0.873919677734375, 0.469140625, 0.2734667460123698, 0.3377965291341146]}, 'weight': {'score': [0.0005599707365036011, 0.007486077115746306, 0.00032554422655413226, 0.007576086940201294, 0.0002392126963688777], 'topk_tokens': [' Papers', '\n\n', ' under', '\n', 'b', 'system', '<|start_header_id|>', 'assistant', '\n\n', 'user', '\n\n', '\n\n', 'athroom', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0012594223022460937, 0.0005232810974121094, 0.00012140969435373943, 0.0004462301731109619]}, 'saliency': {'score': [2.50122763893821e-05, 0.00010777730406292936, 1.2790003130512854e-05, 0.00010892311722419961, 1.2964468735914964e-05], 'topk_tokens': ['athroom', ':', ' Knowledge', '<|end_header_id|>', '\n', ' Papers', '<|start_header_id|>', '<|eot_id|>', '\n\n', '\n\n', '\n\n', 'assistant', '<|eot_id|>', '<|eot_id|>', 'user', '***', '<|start_header_id|>', 'system', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [4.39763069152832e-05, 3.2454729080200195e-05, 1.0152657826741537e-05, 1.786649227142334e-05]}}, 25: {'grad': {'score': [0.9980357776988636, 0.9306710180733618, 0.8929034048511136, 0.9305961854929647, 1.149541766826923], 'topk_tokens': ['otyping', ' per', ' web', 'SP', 'there', 'ERS', ' the', ' make', ' newspaper', ' stere', '-per', ' fifty', 'cap', 'APER', ' Think', 'AP', ' machine', 'ing', 'machine', ' forms'], 'evidence_proportions': [0.99169921875, 0.9268554687499999, 0.9657389322916667, 1.0949300130208333]}, 'weight': {'score': [0.0007771389050917192, 0.007453137087120063, 0.0005070557517390098, 0.00754022544523534, 0.00020871254114004283], 'topk_tokens': ['b', ' printed', 'system', ' Papers', ' these', '<|start_header_id|>', '\n\n', ' under', '\n\n', '<|eot_id|>', 'assistant', 'user', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0024496078491210937, 0.00042136907577514646, 9.404122829437256e-05, 0.0003629873196283976]}, 'saliency': {'score': [7.893957874991678e-05, 0.0001306571314024676, 5.2224243840863625e-05, 0.00013151531983063926, 7.1039566626915565e-06], 'topk_tokens': ['assistant', ' ', ' prepared', '<|start_header_id|>', ' conditions', ' under', ' these', 'athroom', '\n\n', ' would', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'user', 'system', '<|eot_id|>', '<|end_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00031791925430297847, 8.07642936706543e-06, 3.059705098470052e-06, 1.4722347259521484e-05]}}, 26: {'grad': {'score': [1.2409473765980115, 1.2583554839363351, 1.2406264274351058, 1.2585797155095453, 1.1072076650766225], 'topk_tokens': [' remains', ' but', ' it', ' news', ' about', ' a', ' only', ' report', ' and', ' and', ' return', ' both', ' the', ' where', ' represented', ' as', ' on', ' but', ' em', ' em'], 'evidence_proportions': [1.1717041015625, 1.10296630859375, 1.1930948893229167, 1.46148681640625]}, 'weight': {'score': [0.0008033270185643977, 0.007407901407080611, 0.0003911506745123094, 0.007495138701924799, 0.00032683886014498196], 'topk_tokens': ['\n\n', '\n\n', 'b', ' Papers', '\n\n', ' these', '<|start_header_id|>', ' under', 'assistant', '<|start_header_id|>', 'user', 'athroom', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016602993011474608, 0.0007309079170227051, 0.00022349754969278973, 0.00072936216990153]}, 'saliency': {'score': [6.459653377532959e-05, 0.0001280102578097033, 3.801526561860115e-05, 0.0001290164964936851, 9.652284475473256e-06], 'topk_tokens': ['d', 'system', ' to', '<|eot_id|>', 'athroom', '\n\n', ' Joseph', 'b', '<|eot_id|>', ' printed', '<|start_header_id|>', 'user', ' these', '<|eot_id|>', ' under', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00018132328987121584, 3.0368566513061523e-05, 8.652607599894205e-06, 5.1791469256083175e-05]}}, 27: {'grad': {'score': [0.6640524430708452, 0.5437285449543343, 0.6673522456999748, 0.5421706078344309, 0.5581977844238282], 'topk_tokens': [' work', ' of', ' and', ' getting', ' were', ' out', '.', 'RE', ' requiring', ' with', ' and', '.', ',', ' take', ' days', ',', ' as', ',', ',', ','], 'evidence_proportions': [0.9952621459960938, 0.676287841796875, 0.4829610188802083, 0.5589396158854166]}, 'weight': {'score': [0.000541267069903287, 0.007467565599896177, 0.00035757307083375995, 0.00755719972734527, 0.00017969470757704515], 'topk_tokens': [' Joseph', ' these', ' printed', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'b', ' under', 'user', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|eot_id|>', 'assistant', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009473204612731934, 0.00045353174209594727, 0.00020651022593180338, 0.0006107588609059652]}, 'saliency': {'score': [0.0001163902607831088, 0.00012768293462927185, 5.6349462078463646e-05, 0.00012827436922260935, 2.2693322255061223e-05], 'topk_tokens': ['<|start_header_id|>', 'system', 'b', ' printed', '<|start_header_id|>', '\n\n', ' under', 'user', '***', ' these', '<|eot_id|>', '<|eot_id|>', 'assistant', ' Joseph', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00021995902061462403, 0.00010895729064941406, 4.629790782928467e-05, 0.00010636945565541586]}}, 28: {'grad': {'score': [1.0645689530806108, 0.7867675520415999, 0.9512412163519091, 0.784072116981134, 0.634851573063777], 'topk_tokens': ['.', '.', '.', ' If', '\n\n', '202', ' STR', ' to', ' A', '.', '.', '.', '.', 'A', ' picked', ' Daniel', ' getting', ' that', ' of', '\n\n'], 'evidence_proportions': [1.588360595703125, 1.1499114990234376, 0.6949615478515625, 0.9265645345052084]}, 'weight': {'score': [0.0004760948094454679, 0.007215988036008076, 0.0004259328688344648, 0.007302251407291259, 0.0004153430461883545], 'topk_tokens': ['\n\n', ' printed', '\n\n', ' under', 'system', 'user', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'b', '<|eot_id|>', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006712913513183595, 0.0004541456699371338, 0.00015766918659210205, 0.0006501475969950359]}, 'saliency': {'score': [2.8940764340487394e-05, 0.00016064066634445336, 2.423313356214954e-05, 0.00016235406652727102, 1.3960783298198993e-05], 'topk_tokens': [':', '\n', 'system', '<|end_header_id|>', ' these', ' under', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'user', '<|start_header_id|>', 'athroom', '<|eot_id|>', 'b', 'assistant', '\n\n', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [9.295344352722168e-05, 1.7887353897094728e-05, 5.21540641784668e-06, 8.533398310343426e-06]}}, 29: {'grad': {'score': [0.81854248046875, 1.075247798097326, 0.9745818107358871, 1.0773560362786276, 0.8638916015625], 'topk_tokens': [' to', ' and', '\n\n', ' not', ' these', ' that', ' or', ' of', ' it', ' and', ' not', ' under', ' be', ' as', ' Date', ' in', ' each', ' at', ' be', ' would'], 'evidence_proportions': [0.8906005859375, 0.5996337890625, 1.0415445963541667, 0.7179158528645834]}, 'weight': {'score': [0.000952812758359042, 0.007369455335713067, 0.0005114876454876315, 0.007454514997915225, 0.0004110611402071439], 'topk_tokens': [' under', ' printed', '\n\n', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', 'user', 'system', 'assistant', '<|start_header_id|>', '<|start_header_id|>', 'athroom', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0016899824142456053, 0.0017993390560150145, 8.301436901092529e-05, 0.0005028645197550456]}, 'saliency': {'score': [0.00015555728565562856, 0.00022170969337253262, 4.271730299918882e-05, 0.00022339378018939858, 2.076579974247859e-05], 'topk_tokens': ['***', '\n\n', 'assistant', '<|eot_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '<|eot_id|>', ' printed', '\n\n', '<|eot_id|>', '<|start_header_id|>', 'user', ' under', ' these', '\n\n', 'system', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00017220377922058105, 0.0004785418510437012, 6.298224131266276e-06, 2.1790464719136558e-05]}}, 30: {'grad': {'score': [1.5235091122713955, 1.0625807426141085, 1.2700333133820565, 1.058596262251938, 0.9562943678635817], 'topk_tokens': [' made', ' presidents', ' Online', ' or', ' were', ' formally', ' The', ' St', ' four', ' the', '202', ' not', ' could', ' B', '�', ' head', ' need', ' low', ' of', ' A'], 'evidence_proportions': [2.53583984375, 1.6071998596191408, 1.0716552734375, 1.06201171875]}, 'weight': {'score': [0.002158961512825706, 0.0071829540437442965, 0.001001888705838111, 0.007255601521781156, 0.00047530577732966497], 'topk_tokens': [' Papers', '\n\n', ' under', ' printed', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'assistant', 'system', '<|start_header_id|>', '<|start_header_id|>', 'user', 'b', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0038119316101074222, 0.0047610759735107425, 0.00011519591013590494, 0.0006568233172098796]}, 'saliency': {'score': [0.0005461600693789395, 0.00023938046108510307, 0.00010803630275111044, 0.0002387366807354732, 4.8575493005605844e-05], 'topk_tokens': [' under', '<|end_header_id|>', ' these', '<|start_header_id|>', '<|eot_id|>', ' printed', ' Papers', '<|start_header_id|>', 'assistant', '\n\n', 'athroom', '<|eot_id|>', '<|eot_id|>', 'user', 'b', '\n\n', 'system', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004626154899597168, 0.0018811166286468507, 4.897514979044596e-06, 4.457930723826091e-05]}}, 31: {'grad': {'score': [1.9118874289772727, 2.368142790038135, 2.129514632686492, 2.3723349253642327, 2.5191180889423075], 'topk_tokens': [' law', '\n', ' *\n\n', ' kids', ' thought', '\n', '\n', '\n', ',', 'user', ' over', '<|end_header_id|>', ' be', '<|eot_id|>', 'd', ' to', '<|start_header_id|>', ' Papers', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.8457275390625, 1.3871826171875, 2.653238932291667, 2.4962565104166665]}, 'weight': {'score': [0.0032935210249640727, 0.006842098797601626, 0.0018173523487583284, 0.006898322854084474, 0.00028071311803964466], 'topk_tokens': [' picked', ' ', ' conditions', '\n', '\n\n\n', '\n\n', 'system', '<|start_header_id|>', '\n\n', '\n\n', 'user', '<|start_header_id|>', '<|eot_id|>', 'assistant', '\n\n', '<|eot_id|>', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010795021057128906, 0.003094702959060669, 7.168451944986978e-05, 0.00042978922526041663]}, 'saliency': {'score': [0.00017715313217856667, 0.0002346939185292859, 6.760704901910597e-05, 0.00023624371181000996, 1.143033687884991e-05], 'topk_tokens': ['PA', 'UL', ' conditions', '\n\n', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'system', '\n\n\n', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', 'assistant', '<|eot_id|>', 'b', '<|begin_of_text|>', '\n\n', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [0.0004512429237365723, 0.0002867698669433594, 3.919005393981934e-06, 3.06318203608195e-05]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:29:46.162 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:29:46.163 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-12_pid-2_1-5-8-9.pkl | len: 10 |  size: 9.67 KB
Processing depth (1, 5, 8, 9):   3%|▎         | 3/100 [00:39<21:13, 13.13s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Processing depth (1, 4, 6, 9):   3%|▎         | 3/100 [00:48<21:13, 13.13s/it]2025-01-23 22:29:55.481 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:29:55.482 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (208, 213) -->  picked up the apple.
2025-01-23 22:29:55.482 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:29:55.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1829, 1834) --> . Mary moved to the
2025-01-23 22:29:55.492 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:29:55.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2406, 2412) -->  the senate. Mary journeyed
2025-01-23 22:29:55.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-23 22:29:55.522 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3597, 3603) -->  Daniel journeyed to the kitchen
2025-01-23 22:29:55.523 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:29:55.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3574, 3579) --> . John moved to the
2025-01-23 22:29:55.540 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:29:55.556 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3122, 3127) --> . Sandra moved to the
2025-01-23 22:29:55.556 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:29:55.577 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3363, 3369) --> . John went back to the
2025-01-23 22:29:55.577 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:29:55.584 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1500, 1504) -->  Daniel left the apple
2025-01-23 22:29:55.584 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:29:55.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (964, 970) --> . Sandra journeyed to the
2025-01-23 22:29:55.589 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:29:55.590 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (207, 212) -->  Daniel picked up the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:29:56.064 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom<|eot_id|>
2025-01-23 22:29:56.065 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4201])
your chose emoji: ['🏋️\u200d♀️', '🇬🇶', '🇸🇲', '🏋🏻\u200d♂', '😿', '🧯', '🌴', '🏌️\u200d♀', '👵🏽', '🧎🏽\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4204, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 250406.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.28it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.71it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.49it/s]
2025-01-23 22:29:58.928 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.4694055210460316, 0.4475251187833574, 0.5320358276367188, 0.4467780214997551, 0.44788226746676263], 'topk_tokens': [' method', ' a', ' producing', ' past', 'ORE', ' CONNECT', ' up', ' Pa', 'Today', ' tour', 'UL', 'LES', ' MO', ' would', ' M', ' Jul', 'ol', ' made', 'ioneer', ' wonderful'], 'evidence_proportions': [0.97889404296875, 0.30666351318359375, 0.35507361094156903, 0.29478200276692706]}, 'weight': {'score': [0.009863302111625671, 0.0076093705464952, 0.007816361804162302, 0.007595879044827999, 0.005992311134673001], 'topk_tokens': [' that', ' else', ' item', ' after', ' they', ' not', ':', '?', ' Do', ' before', 'Question', ' the', ' Where', ':', 'Answer', ' was', 'assistant', 'athroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0016593992710113525, 0.0076636791229248045, 0.01372234026590983, 0.014673868815104168]}, 'saliency': {'score': [0.00010168958793986927, 0.00010314059189452544, 0.00011331227517897083, 0.00010307231916631167, 7.162177771852727e-05], 'topk_tokens': [' moved', '.\n\n', ' persons', '.', ' during', ' the', '.\n', ' have', ' and', ' continued', '.\n\n', ' well', ' were', ',', ' and', ' facts', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [4.291534423828125e-06, 0.00012250542640686036, 0.00012723604838053385, 0.00013996164004007974]}}, 25: {'grad': {'score': [0.19026934016834607, 0.304908433037639, 0.2223683326475082, 0.3061324280160101, 0.3808667366964775], 'topk_tokens': [' wield', ' was', 'The', ' the', '-text', ' *\n\n', ' had', 'ides', ' ST', ' the', ' the', ' a', ' STR', 'str', '\n\n\n\n\n\n\n', ' a', ' ST', '\n\n\n\n', ' wonderful', 'UG'], 'evidence_proportions': [0.44142017364501956, 0.122296142578125, 0.16863441467285156, 0.059256235758463546]}, 'weight': {'score': [0.011144142259250988, 0.007608426626697254, 0.007667835681669174, 0.007589243917802936, 0.006211354544288234], 'topk_tokens': [' before', 'Question', ' will', ' Where', ' made', ' the', ' item', '.', '?', ' they', ' Do', ':', '<|eot_id|>', ' was', 'Answer', '<|end_header_id|>', 'athroom', 'assistant', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0020452380180358888, 0.00874483585357666, 0.015664021174112953, 0.016206105550130207]}, 'saliency': {'score': [5.019930276003751e-05, 5.380937618714758e-05, 4.895464066536196e-05, 5.3864764868566945e-05, 6.029532666791949e-05], 'topk_tokens': [' One', ' fully', ' of', ' Red', 'men', ' message', ' as', 'ors', 'nes', ' The', '\n', ' *\n\n', '.', ',', 'complete', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [3.265738487243652e-05, 6.176233291625975e-05, 5.239248275756836e-05, 5.2988529205322266e-05]}}, 26: {'grad': {'score': [0.14078809998252176, 0.2710384818965429, 0.26205029026154547, 0.2717959240412775, 0.34775705504835697], 'topk_tokens': [' *\n\n', '\n', ' pur', ' next', '\n', ' apple', '.', '\n', '\n', '\n', '\n', '\n', ' This', '\n', '\n', '\n', '\n', '\n', '\n', ' Team'], 'evidence_proportions': [0.21713409423828126, 0.26242361068725584, 0.09174887339274088, 0.02484273910522461]}, 'weight': {'score': [0.008875375444238836, 0.007609248047891285, 0.007683280975587906, 0.007601984780370629, 0.006759200179786013], 'topk_tokens': [':', '?', ' before', ' the', ' item', ' they', ' Do', 'Question', ' Where', ':', 'Answer', 'athroom', '<|eot_id|>', ' was', 'b', 'assistant', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.0021782159805297852, 0.007825660705566406, 0.012055714925130207, 0.012150764465332031]}, 'saliency': {'score': [0.00012911856174468994, 6.48860926859499e-05, 7.649583201254568e-05, 6.445896278028459e-05, 6.154650136044151e-05], 'topk_tokens': [' the', '\n', '\n', ' closely', 'ages', ',', '<|eot_id|>', '\n', ' have', ' persons', ' in', ' facts', ' moved', ' were', '4', '<|start_header_id|>', 'ed', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [2.1016597747802736e-05, 7.242560386657716e-05, 0.00033819178740183514, 5.737443765004476e-05]}}, 27: {'grad': {'score': [0.12083539095791904, 0.1558634778865738, 0.12432442941973286, 0.15628466035222122, 0.2639152125308388], 'topk_tokens': ['EF', '�', ' Written', ' Project', 'PA', ' OF', ' ', '�', ' OF', '.', '***', 'AILY', '\n\n', '�', '\n', ' prepared', ' was', '\n\n', ' This', '.'], 'evidence_proportions': [0.18913116455078124, 0.2344696044921875, 0.05025482177734375, 0.039807637532552086]}, 'weight': {'score': [0.010545742782679472, 0.00760902414766524, 0.008268364975529333, 0.007588535741104387, 0.006159700322569462], 'topk_tokens': ['ian', ' item', '?', ' not', ' they', ' before', ':', '<|end_header_id|>', ' the', ' Do', 'Question', ':', '.', 'Answer', ' Where', ' was', 'assistant', 'athroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.002021747827529907, 0.008054542541503906, 0.014748175938924154, 0.015522638956705729]}, 'saliency': {'score': [6.268105723641135e-05, 7.83745784287675e-05, 7.667656867734848e-05, 7.847043382952052e-05, 7.702384078711795e-05], 'topk_tokens': [' *', ' got', '\n', '<|end_header_id|>', ' apple', ' was', '<|eot_id|>', ' *', '3', ' war', '.\n\n', ',', ' \n', ' the', ' bedroom', '<|start_header_id|>', ',', ',', '\n', '<|end_header_id|>'], 'evidence_proportions': [4.673004150390625e-05, 7.439851760864258e-05, 6.865958372751872e-05, 6.0230493545532227e-05]}}, 28: {'grad': {'score': [0.41439940712668677, 0.4636079114919385, 0.49530576890514744, 0.46363199087431045, 0.6649048788505688], 'topk_tokens': ['.', '.', '<|start_header_id|>', ',', ',', '***', 'str', '.\n\n', ',', ',', '\n', '.', ' he', ',', ',', ',', ',', ',', '\n', '\n'], 'evidence_proportions': [0.9403884887695313, 0.42793121337890627, 0.2872549692789714, 0.09194310506184895]}, 'weight': {'score': [0.011820875785567543, 0.007605346467356586, 0.008047083693165933, 0.0075797055377009055, 0.0059545510693600305], 'topk_tokens': [' item', ' of', ' Buchanan', ' the', ' Where', ' Do', 'over', ' made', 'ian', ':', '?', 'ors', 'athroom', 'Answer', ' was', '.', 'b', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.001648956537246704, 0.008140015602111816, 0.019045114517211914, 0.01614061991373698]}, 'saliency': {'score': [0.00010412254116751932, 6.64435546813297e-05, 5.153879042594663e-05, 6.635516874763197e-05, 4.4191615623340274e-05], 'topk_tokens': [' When', '?', ' in', ' arrival', ' an', ' expected', ' to', ' courts', ' before', 'ors', ' some', ' \n', ' of', '\n', '.', '<|start_header_id|>', 'b', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [1.0788440704345703e-05, 5.905032157897949e-05, 0.0002517600854237874, 7.18235969543457e-05]}}, 29: {'grad': {'score': [0.13959849964488635, 0.4748320792767119, 0.3774208561066658, 0.4773362678265003, 0.5472710927327474], 'topk_tokens': [' boat', ' be', ' was', ' century', ' with', ' looked', 'ball', ' themselves', ' heard', ' of', ' very', ' very', 'ern', ' very', ' paper', ' heading', 'ly', ' out', ' paper', ' into'], 'evidence_proportions': [0.16515960693359377, 0.11431427001953125, 0.16045633951822916, 0.11850992838541667]}, 'weight': {'score': [0.01733736964789304, 0.0076063534512506225, 0.008139374756043958, 0.007550799122950279, 0.006386888131760715], 'topk_tokens': ['ors', 'Question', ' the', ' Where', ' they', ' the', ' some', 'ate', ' made', '<|end_header_id|>', ':', ' Do', ' paper', 'Answer', ' was', 'athroom', 'assistant', '.', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0028135716915130616, 0.008641219139099121, 0.030831336975097656, 0.023193359375]}, 'saliency': {'score': [0.00030089643868533045, 0.0001344940528542966, 0.00013514007291486188, 0.0001336073077061009, 0.00023761757633142304], 'topk_tokens': [' Paul', ' genu', ' of', '.', ' heading', '-known', ' Min', ' vigorous', ' doctor', ' bringing', 'remember', 'assistant', 'ed', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [5.168914794921875e-05, 7.123947143554688e-05, 0.0006557802359263102, 0.0003450661897659302]}}, 30: {'grad': {'score': [0.15160629966042258, 0.22767915008864326, 0.19485842797064012, 0.2283274384757979, 0.2377185821533203], 'topk_tokens': [' offices', ' time', 'room', 'reading', ' do', '<|eot_id|>', 'D', 'A', ' prepared', ',', ' newspaper', ' apple', 'LY', ' a', ' PA', ' By', ' Good', ' EAR', ' PA', ' B'], 'evidence_proportions': [0.18714370727539062, 0.19154052734375, 0.13165918986002603, 0.10866038004557291]}, 'weight': {'score': [0.01774936778978868, 0.007541071449882297, 0.008481413125991821, 0.007479945670205753, 0.00608451899729277], 'topk_tokens': [' they', '?', ' item', ' before', ' Where', ' the', ' Do', '<|start_header_id|>', ':', 'Answer', ' was', '<|start_header_id|>', '.', '<|eot_id|>', 'assistant', 'b', '<|eot_id|>', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.002301090955734253, 0.007691574096679688, 0.03830273946126302, 0.018451054890950523]}, 'saliency': {'score': [0.00016552209854125977, 0.00012111280301318182, 0.00010456000604937154, 0.00012100105456805063, 0.00010516716722856488], 'topk_tokens': ['Just', ' not', 'ed', ' from', ' bedroom', '<|start_header_id|>', '?', ' the', 'nes', ' item', '<|eot_id|>', ' before', '<|start_header_id|>', 'assistant', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', 'b', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00012739896774291993, 0.00012338757514953613, 0.0003208667039871216, 7.705887158711752e-05]}}, 31: {'grad': {'score': [0.1945022236217152, 0.2475192490132166, 0.21232691118794103, 0.2480630546097467, 0.2760003407796224], 'topk_tokens': [' revisit', ' OF', 'In', ' WITH', ' impressions', ' PA', ' in', ' PA', ' in', ' in', ' OF', 'user', ' attract', ' OF', '<|start_header_id|>', ' apple', ' in', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [0.1616119384765625, 0.17174835205078126, 0.22028160095214844, 0.21509297688802081]}, 'weight': {'score': [0.02462033521045338, 0.006870676742974743, 0.005523433608393516, 0.006786666155378378, 0.0032859036796971373], 'topk_tokens': [' the', ' apple', ' bedroom', '\n\n', ' \n', '?', ' the', ' Do', ':', 'Answer', '<|start_header_id|>', ' was', '<|eot_id|>', 'assistant', '.', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0020496249198913574, 0.003409004211425781, 0.07504685719807944, 0.010678847630818685]}, 'saliency': {'score': [0.00019414181059057063, 8.44106971366875e-05, 0.00010829875546116983, 8.36507322356891e-05, 6.227691968282063e-05], 'topk_tokens': [' the', ' made', ' apple', ',', ' Times', ' the', ' was', '.', 'ed', ' man', ' bedroom', ' \n', 'b', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [3.316402435302734e-05, 9.818077087402344e-05, 0.0005396207173665364, 6.277859210968018e-05]}}, 'pred_res': 'The bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:29:58.935 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:29:58.935 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-12_pid-3_1-4-6-9.pkl | len: 10 |  size: 8.77 KB
Processing depth (1, 4, 6, 9):   4%|▍         | 4/100 [00:51<20:47, 12.99s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.14it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Processing depth (2, 4, 5, 6):   4%|▍         | 4/100 [01:01<20:47, 12.99s/it]2025-01-23 22:30:08.674 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary picked up the apple.
2025-01-23 22:30:08.677 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (391, 396) -->  Daniel picked up the apple
2025-01-23 22:30:08.677 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:30:08.686 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1836, 1841) -->  Hon. Mary moved to
2025-01-23 22:30:08.686 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:30:08.697 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2089, 2095) --> . Mary journeyed to the
2025-01-23 22:30:08.697 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-23 22:30:08.709 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2358, 2364) --> . Daniel journeyed to the
2025-01-23 22:30:08.710 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:30:08.728 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3603, 3608) -->  John moved to the garden
2025-01-23 22:30:08.728 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:30:08.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3141, 3146) --> . Sandra moved to the
2025-01-23 22:30:08.743 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John went back to the office.
2025-01-23 22:30:08.760 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3395, 3401) --> . John went back to the
2025-01-23 22:30:08.760 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:30:08.768 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1572, 1576) -->  Daniel left the apple
2025-01-23 22:30:08.768 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra journeyed to the office.
2025-01-23 22:30:08.774 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1210, 1216) --> . Sandra journeyed to the
2025-01-23 22:30:08.774 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel picked up the apple.
2025-01-23 22:30:08.776 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (390, 395) --> . Daniel picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:30:09.311 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The kitchen.<|eot_id|>
2025-01-23 22:30:09.311 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4225])
your chose emoji: ['🧑🏻\u200d✈', '🚶\u200d♀', '🧎🏽\u200d➡️', '🤷🏾\u200d♀', '🚴', '👨🏾\u200d🦳', '🇲🇬', '💁🏿\u200d♂', '👠', '🧑🏻\u200d❤️\u200d💋\u200d🧑🏼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4228, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 228261.44it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.44it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.47it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.39it/s]
2025-01-23 22:30:12.280 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.4169346202503551, 0.3621449393829086, 0.3054695129394531, 0.362277050817798, 0.4273209277494454], 'topk_tokens': ['Dub', ' family', ' Good', 'system', ',', ' a', 'ball', 'd', ':', ' both', 'ION', ',', ' Good', 'str', 'EF', ' balance', 'u', 'RI', ' bend', 'user'], 'evidence_proportions': [0.3754058837890625, 0.4952392578125, 0.41533406575520837, 0.38788859049479163]}, 'weight': {'score': [0.0009434521198272705, 0.00745457506676045, 0.2647444413554284, 0.005578469402061965, 0.0014231683295450094], 'topk_tokens': ['\n\n', "'clock", '***', '\n\n', '\n\n', 'b', 'user', '<|eot_id|>', '\n\n', 'system', '<|end_header_id|>', '<|start_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0005454421043395996, 0.00032673478126525883, 0.002342437704404195, 0.0003900726636250814]}, 'saliency': {'score': [7.474422454833984e-05, 8.042273001503922e-05, 0.00035984381552665466, 7.837790928914875e-05, 4.8989866986686803e-05], 'topk_tokens': [':', '\n\n', 'assistant', ' Knowledge', 'b', '<|eot_id|>', '***', '\n\n', '<|eot_id|>', 'user', '<|start_header_id|>', '.', '\n\n', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', 'athroom', 'system'], 'evidence_proportions': [4.824399948120117e-05, 1.4108419418334962e-05, 0.00020785133043924967, 1.4250477155049642e-05]}}, 25: {'grad': {'score': [0.38950781388716266, 0.40025612534276694, 0.38294195359753025, 0.4004413234687851, 0.36682197194040556], 'topk_tokens': [' as', ' Papers', ' *\n\n', '      ', '\n', ' cap', ' wood', ' Bor', ' Good', ' under', '-per', ' composing', ' Bench', ' up', 'que', ' Think', '\n', ' web', '\n', 'ad'], 'evidence_proportions': [0.533154296875, 0.47395019531250004, 0.3096580505371094, 0.2792835235595703]}, 'weight': {'score': [0.0009316978129473599, 0.007495727421770484, 0.32715903751311765, 0.005156768389090806, 0.0018673311045140396], 'topk_tokens': ['\n\n', '�', "'clock", 'b', '\n\n', '<|eot_id|>', 'system', '\n\n', 'user', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0010041475296020507, 0.00040310025215148924, 0.0016569296518961587, 0.0005865891774495442]}, 'saliency': {'score': [6.979297507892956e-05, 0.00012781822749095473, 0.001200440429872082, 0.00012015960887520613, 6.683042019973567e-05], 'topk_tokens': [' prepared', '      ', 'Today', '�', '\n\n', '\n\n\n', '\n\n', 'user', '<|eot_id|>', '<|end_header_id|>', 'system', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', '.', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [9.284019470214845e-05, 2.3645162582397464e-05, 0.00012848774592081705, 3.034869829813639e-05]}}, 26: {'grad': {'score': [0.5571965304287997, 0.6166779113018862, 0.586579107469128, 0.6172148342475205, 0.5674147076076932], 'topk_tokens': [' com', '600', ' a', ' old', 'user', 'RI', 'itor', 'com', '\n', ' six', ' six', ' proof', 'an', 'hand', '600', 'system', ' em', 'ad', ' border', ' em'], 'evidence_proportions': [0.46126556396484375, 0.351812744140625, 0.6973978678385417, 0.6680908203125]}, 'weight': {'score': [0.00205203348940069, 0.007428277520041705, 0.2690274071308874, 0.005514194729799282, 0.003741818813630092], 'topk_tokens': ['\n\n', 'rate', '\n\n', '\n\n', 'b', "'clock", '\n\n', 'assistant', 'user', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0006634175777435302, 0.0003287732601165771, 0.005388776461283365, 0.0013085206349690757]}, 'saliency': {'score': [0.00011201067404313521, 0.00017760826544369182, 0.0013158907813410605, 0.00016950201131626518, 0.00010735900313765915], 'topk_tokens': [' president', '<|start_header_id|>', ' Newspaper', 'system', '\n\n', '<|eot_id|>', 'rate', 'b', '<|start_header_id|>', 'user', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', "'clock", '.', 'athroom', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [2.943277359008789e-05, 6.091594696044922e-06, 0.00035837292671203613, 2.2729237874348957e-05]}}, 27: {'grad': {'score': [0.5124151056463068, 0.6375016976665977, 0.567321285124748, 0.6386819366637818, 0.5637690461712119], 'topk_tokens': [' different', ' on', '\n', ' proceed', '\n', ' one', ' the', 'action', ',', 'there', '--', ' day', ' every', ' time', ' the', ' time', 'when', ' Bench', ' out', 'old'], 'evidence_proportions': [0.6256469726562499, 0.38935546875, 0.5225626627604166, 0.5104573567708334]}, 'weight': {'score': [0.0011248805306174538, 0.007498903554232479, 0.42806670934923235, 0.004409712303184463, 0.003138930709273727], 'topk_tokens': ['\n\n', "'clock", '\n\n', 'system', '\n\n', 'b', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', 'user', '<|eot_id|>', '<|end_header_id|>', 'assistant', '<|start_header_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0005184471607208252, 0.00034344792366027834, 0.0025323530038197832, 0.0008739630381266277]}, 'saliency': {'score': [0.0001749938184564764, 0.00017127122592565128, 0.0015057508022554458, 0.00016134289924256103, 0.0003623763720194499], 'topk_tokens': [' bedroom', '\n\n', ' bogus', '<|start_header_id|>', '�', '<|eot_id|>', '***', 'system', '\n\n', 'user', '<|eot_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', 'athroom', '<|end_header_id|>', 'assistant', '.', '<|start_header_id|>'], 'evidence_proportions': [5.831718444824219e-05, 2.5933980941772462e-05, 0.0004956424236297607, 7.579227288564047e-05]}}, 28: {'grad': {'score': [1.1639896739612927, 0.9865002014102412, 1.1017830141129032, 0.984708935400683, 0.9338000262225116], 'topk_tokens': [' the', ' a', ' for', ' the', ' a', ' the', ' a', ' the', ' O', ' the', ' of', 'a', '.', ' A', ' Fletcher', ' a', 'A', ' a', 'hand', 'com'], 'evidence_proportions': [1.3901123046875, 1.1855010986328125, 0.9202423095703125, 1.2013753255208335]}, 'weight': {'score': [0.0007102272727272727, 0.007344966579859507, 0.42333430147940115, 0.004291144994918458, 0.0016256876197862037], 'topk_tokens': ['\n\n', '\n', '\n\n', '\n\n', 'system', '<|start_header_id|>', 'user', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'b', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [3.0493736267089842e-05, 0.00019751787185668946, 0.001602540413538615, 0.0008116165796915691]}, 'saliency': {'score': [1.857903870669278e-05, 0.00014179031467708418, 0.004107927122423726, 0.00011299041216958782, 3.3591632489804865e-05], 'topk_tokens': ['\n\n', '\n\n', '\n\n', ' Gree', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'user', 'system', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', 'b', '\n\n', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', '.'], 'evidence_proportions': [1.4960765838623048e-06, 3.1054019927978512e-06, 5.282461643218994e-05, 1.146396001180013e-05]}}, 29: {'grad': {'score': [0.6608146320689808, 0.5792886522883159, 0.5303065392278856, 0.579222753833154, 0.6924060303487896], 'topk_tokens': ['APER', 'E', ' M', ' L', 'ISC', ' may', ' the', ' Mary', 'SP', ' Min', 'PA', 'MIN', 'an', ' STR', 'd', '.', '26', ' the', 'SP', '.'], 'evidence_proportions': [0.32861328125, 0.8698455810546875, 0.6966451009114584, 0.7276261647542317]}, 'weight': {'score': [0.002236176620830189, 0.007368529418108028, 0.3896615880150949, 0.004556990961828632, 0.002856767103995806], 'topk_tokens': ['\n', '\n\n', '\n\n', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'user', 'system', 'assistant', 'b', '<|start_header_id|>', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00018063783645629885, 0.0005368709564208985, 0.004811098178227743, 0.002790292104085286]}, 'saliency': {'score': [0.00036169724030928177, 0.0002637721436025968, 0.005492621852505592, 0.00022443113926641956, 0.00023562304767561548], 'topk_tokens': ['<|start_header_id|>', '\n', '\n\n', 'b', ' the', '\n\n', ' S', 'assistant', '\n', '\n', '<|start_header_id|>', '<|start_header_id|>', ' bogus', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'athroom', 'system', '<|begin_of_text|>', '.'], 'evidence_proportions': [5.859136581420898e-06, 5.940794944763184e-05, 0.0009863326946894326, 0.0002855012814203898]}}, 30: {'grad': {'score': [0.7584596113725142, 0.7216763383618141, 0.8071524097073463, 0.7208478377108088, 0.7988011160014589], 'topk_tokens': [' method', ' news', '.', ' with', ' more', ' that', ' STR', 'there', ' time', ' to', ' and', ' a', ' B', ',', 'pend', '      ', ',', ',', ' NEW', ' at'], 'evidence_proportions': [0.6337554931640625, 0.961199951171875, 0.7800750732421875, 0.67181396484375]}, 'weight': {'score': [0.00297809053551067, 0.007152938346537984, 0.2715214413981284, 0.005211961833302846, 0.0064332010569395845], 'topk_tokens': ['�', ' the', 'assistant', '\n', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'system', '<|start_header_id|>', 'user', 'athroom', '<|start_header_id|>', '<|eot_id|>', '\n\n', 'b', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00023289918899536133, 0.0008039772510528564, 0.0067146023114522295, 0.003340999285380046]}, 'saliency': {'score': [0.00023658167232166636, 0.00019331534402823833, 0.0009891112004556963, 0.00018717845043022475, 0.00038217210475309396], 'topk_tokens': ['\n', ' the', ':', 'assistant', '<|end_header_id|>', '<|eot_id|>', '\n\n', ' Joseph', '�', '<|eot_id|>', 'athroom', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '.', '<|begin_of_text|>', '\n\n', '<|start_header_id|>', 'b'], 'evidence_proportions': [1.9299983978271485e-05, 0.0001017153263092041, 0.0006726036469141643, 9.401639302571615e-05]}}, 31: {'grad': {'score': [0.8415652188387784, 0.7597349919343513, 0.7226806148405998, 0.7595789250356708, 0.7379595674114463], 'topk_tokens': [' at', ' advantage', ' met', '.', ' heard', ' tele', '\n', '\n', 'nes', '\n', ' tie', ' Tribune', ' return', 'nes', ' Gal', 'nes', 'nes', 'nes', 'nes', '<|start_header_id|>'], 'evidence_proportions': [0.48572387695312497, 0.68681640625, 1.101348876953125, 1.0072733561197917]}, 'weight': {'score': [0.002618461847305298, 0.006878678259583386, 0.26992357161737257, 0.004947979592991447, 0.002439597138652095], 'topk_tokens': ['Today', ' dropped', ' the', '<|start_header_id|>', '\n', '\n\n', '\n\n\n', 'system', ' at', 'user', 'assistant', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.000740659236907959, 0.0035655677318573, 0.004371389746665955, 0.0016411145528157551]}, 'saliency': {'score': [0.0001228397542780096, 0.0002497251629265503, 0.0031010989219911636, 0.00022922190363535623, 0.00020177643976093811], 'topk_tokens': [' item', ' at', '.', 'system', '<|eot_id|>', 'Today', 'Just', 'user', ' dropped', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.', 'b'], 'evidence_proportions': [2.8538703918457032e-05, 0.00011083483695983887, 0.00018681585788726807, 0.00014745195706685385]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-23 22:30:12.286 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:30:12.287 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-12_pid-4_2-4-5-6.pkl | len: 10 |  size: 9.34 KB
Processing depth (2, 4, 5, 6):   5%|▌         | 5/100 [01:05<20:46, 13.12s/it]Processing depth (2, 4, 5, 6):   5%|▌         | 5/100 [01:05<20:43, 13.09s/it]
2025-01-23 22:30:12.495 | INFO     | __main__:<module>:99 - Selected idx: 13
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:103 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Mary got the football.', ' Sandra journeyed to the office.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:104 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the football.', ' Mary journeyed to the bedroom.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-23 22:30:12.496 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.07it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Processing depth (1, 3, 6, 7, 8):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-23 22:30:22.653 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:30:22.655 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (462, 467) --> . Mary moved to the
2025-01-23 22:30:22.656 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football.
2025-01-23 22:30:22.664 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1544, 1548) -->  Mary got the football
2025-01-23 22:30:22.664 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:30:22.677 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2454, 2460) --> . Mary journeyed to the
2025-01-23 22:30:22.677 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the football.
2025-01-23 22:30:22.691 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2934, 2938) -->  Mary left the football
2025-01-23 22:30:22.691 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-23 22:30:22.709 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3378, 3384) --> . Daniel journeyed to the
2025-01-23 22:30:22.710 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:30:22.727 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3203, 3208) -->  Daniel picked up the apple
2025-01-23 22:30:22.727 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:30:22.740 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2569, 2574) --> . Sandra moved to the
2025-01-23 22:30:22.740 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:30:22.751 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2082, 2087) --> . John moved to the
2025-01-23 22:30:22.751 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:30:22.755 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (771, 777) --> . Sandra journeyed to the
2025-01-23 22:30:22.755 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:30:22.764 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1932, 1936) -->  Daniel left the apple
2025-01-23 22:30:22.764 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:30:22.773 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1754, 1760) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:30:23.309 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bedroom.<|eot_id|>
2025-01-23 22:30:23.309 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4250])
your chose emoji: ['🕵🏽', '👩🏾\u200d✈', '👩🏻\u200d🤝\u200d👨🏾', '\U0001faf6🏻', '👨🏽\u200d❤\u200d💋\u200d👨🏿', '👨🏿\u200d🦽\u200d➡', '🛰️', '💆🏼\u200d♂️', '🛡', '🏃🏿\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4253, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 211034.16it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.65it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.09it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.16it/s]
2025-01-23 22:30:26.492 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3375634765625, 0.24174784021940396, 0.28466599987399194, 0.24086009805646147, 0.17438796771469936], 'topk_tokens': [' IN', ' of', ',', ' no', ' the', 'ollar', ' Daily', 'EF', ' book', ' business', ' const', '600', ' bend', ' office', ' Sandra', ' to', ' make', ' B', 'RI', ' bathroom'], 'evidence_proportions': [0.42318115234375, 0.4206085205078125, 0.28656514485677087, 0.2790374755859375, 0.3008677164713542]}, 'weight': {'score': [0.0036133384704589845, 0.007444492934760895, 0.002961936496919201, 0.007500422911215658, 0.003969793358156758], 'topk_tokens': ['<|start_header_id|>', '\n\n', 'Answer', '<|start_header_id|>', ' directly', 'b', ' goods', ' some', ' paper', 'made', 's', ' learned', ' wait', '<|eot_id|>', '<|eot_id|>', ' where', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003249824047088623, 0.00474095344543457, 0.004240766167640686, 0.0021438002586364746, 0.005954156319300334]}, 'saliency': {'score': [0.00023467063903808593, 8.974522667550043e-05, 5.070528676432948e-05, 8.917031669889373e-05, 3.163981181319042e-05], 'topk_tokens': ['system', ' location', '?\n', ' front', '<|end_header_id|>', '<|eot_id|>', '***', '<|eot_id|>', '<|eot_id|>', ' platform', ' Project', ' Charleston', 'user', 'b', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', 'assistant', 'athroom'], 'evidence_proportions': [1.1467933654785155e-05, 0.000819869339466095, 0.00011634329954783121, 2.8371810913085938e-05, 0.00028640031814575195]}}, 25: {'grad': {'score': [0.4190786743164063, 0.35405290084756347, 0.35087314728767643, 0.353689052390007, 0.4114793346774194], 'topk_tokens': ['      ', ' his', '      ', ' a', ' I', ' o', '<|eot_id|>', ' doctor', ' old', ' a', ' *\n\n', "'s", ' *\n\n', ' get', ' *\n\n', ' S', ' S', 'hand', ' *\n\n', ' *\n\n'], 'evidence_proportions': [0.24383087158203123, 0.5302734375, 0.41825358072916663, 0.41626739501953125, 0.49368794759114587]}, 'weight': {'score': [0.004232107400894165, 0.007444235287972851, 0.0025401144258437617, 0.007499591719686687, 0.00267549580143344], 'topk_tokens': ['Answer', "'clock", ' then', ' goods', 's', '<|eot_id|>', '-known', 'made', ' directly', '<|eot_id|>', ' wait', ' some', ' learned', ' paper', ' where', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005839645862579345, 0.003455221652984619, 0.0037629803021748858, 0.009787961840629578, 0.004555374383926392]}, 'saliency': {'score': [0.0003519392013549805, 0.0001328656204218251, 4.905173855443155e-05, 0.0001321797473731142, 2.1677824758714245e-05], 'topk_tokens': [' paper', 'description', ' veto', ' Marshall', ' tele', ' Paul', 'b', ' Mary', ' senator', 'de', ' predicted', ' typ', ' senate', ' enterprise', ' item', 'assistant', '\n\n', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.00011605620384216308, 0.0002259090542793274, 0.00037651260693868, 0.0009192153811454773, 0.0002297709385553996]}}, 26: {'grad': {'score': [0.18243492126464844, 0.16618308061267414, 0.20277952378795994, 0.16581596463584947, 0.14154915912176974], 'topk_tokens': ['      ', ' could', ' second', '600', ' prepared', ' office', 'RI', ' as', '185', ' a', 'user', ' business', ' book', ' Press', ' doctor', ' him', ' so', ' platform', ' bathroom', 'system'], 'evidence_proportions': [0.1377777099609375, 0.2778606414794922, 0.2016932169596354, 0.13664007186889648, 0.16730372111002606]}, 'weight': {'score': [0.005674160718917846, 0.0074023666758551866, 0.005778893347709409, 0.007424652313714599, 0.005103161258082236], 'topk_tokens': ['?\n', '\n\n', 'b', ' directly', ' incorporated', 's', ' some', 'made', ' wait', '<|eot_id|>', ' paper', ' learned', '<|start_header_id|>', '<|eot_id|>', 'columns', ' where', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00018457770347595214, 0.003941774368286133, 0.007679382960001627, 0.005854964256286621, 0.009277979532877604]}, 'saliency': {'score': [0.00015929222106933594, 0.0001407796260807729, 0.00040761886104460687, 0.0001386984177990019, 0.0002022718870511619], 'topk_tokens': ['s', ' where', '�', 'years', ' boat', ' Pioneer', 'made', ' some', ' the', ' laid', '<|start_header_id|>', '<|eot_id|>', 'papers', "'clock", 'street', ' the', 'athroom', ' incorporated', 'columns', '<|begin_of_text|>'], 'evidence_proportions': [3.439188003540039e-06, 7.265061140060425e-05, 0.0002472947041193644, 3.650784492492676e-05, 0.00034078458944956463]}}, 27: {'grad': {'score': [0.28625244140625, 0.35810904791011494, 0.2817001035136561, 0.35910144544141986, 0.44819095570554013], 'topk_tokens': [' take', ' manifest', ' be', ' rates', 'nes', ' presidents', ' ranks', ' work', ' parade', ' affairs', 'g', ' discover', ' Pioneer', 'nes', ' take', ' great', ' candidates', 'es', ' Pioneer', ' Think'], 'evidence_proportions': [0.321929931640625, 0.199981689453125, 0.2748998006184896, 0.2792015075683594, 0.33008829752604163]}, 'weight': {'score': [0.0037290000915527343, 0.007462556912090087, 0.002984696818936256, 0.007517870846662687, 0.0036231574191842027], 'topk_tokens': [' the', '\n\n', '<|start_header_id|>', ' day', 'gro', 'b', ' paper', ' then', "'clock", ' goods', ' some', 's', ' wait', ' learned', 'made', ' where', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0007244110107421874, 0.002997756004333496, 0.005535880724589029, 0.0035009384155273438, 0.005065480868021647]}, 'saliency': {'score': [0.00020939111709594727, 0.00015632528964979297, 0.00011574068377094884, 0.0001563089630107866, 0.00014382216238206433], 'topk_tokens': [' item', ' Brown', '<|start_header_id|>', ' incorporated', '�', ' learned', ' late', 'description', '<|eot_id|>', ' landing', '<|eot_id|>', ' facts', ' Paul', ' laid', ' where', 'b', '<|begin_of_text|>', 'assistant', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [3.602504730224609e-05, 0.0001476183533668518, 0.00040805836518605554, 5.821138620376587e-05, 0.0002971639235814413]}}, 28: {'grad': {'score': [0.5938316345214844, 0.5732874938692173, 0.5825853655415196, 0.5730964437052555, 0.5389258887178154], 'topk_tokens': [' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' A', 'a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a'], 'evidence_proportions': [0.57646484375, 0.558868408203125, 0.5626977284749349, 0.57952880859375, 0.6722819010416666]}, 'weight': {'score': [0.002830154895782471, 0.0072873064189581714, 0.0013340057865265877, 0.00735782848416779, 0.001628931491605697], 'topk_tokens': [' wait', ',', ' generally', 'made', ' goods', ' learned', 'Question', 's', '?\n', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'b', ' where', ' paper', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [8.941888809204102e-05, 0.0028013885021209717, 0.002042671044667562, 0.007766842842102051, 0.0026296377182006836]}, 'saliency': {'score': [6.608724594116211e-05, 0.00011707470609304795, 2.9877308876283707e-05, 0.00011802247969741222, 3.292804123252951e-05], 'topk_tokens': ['b', ':', ' about', 'Answer', '.\n\n', 'just', 'Question', ' Press', '?\n', '<|eot_id|>', ' where', '<|start_header_id|>', ' paper', '<|eot_id|>', 'assistant', '\n\n', ' generally', '<|begin_of_text|>', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [1.341104507446289e-06, 0.000100746750831604, 4.8259894053141274e-05, 0.00018100440502166748, 3.815193970998128e-05]}}, 29: {'grad': {'score': [0.4781430053710938, 0.47550217022818453, 0.4109008542952999, 0.4759635998005807, 0.6561431884765625], 'topk_tokens': [' and', ' and', '�', 'nes', ' Gree', '<|end_header_id|>', ' containing', ' and', ' introduced', ' about', 'nes', 'APER', ' seen', ' different', ' Jul', '<|start_header_id|>', '<|eot_id|>', ' STR', ' happened', 'SP'], 'evidence_proportions': [0.34773101806640627, 0.45700836181640625, 0.47827911376953125, 0.469879150390625, 0.6062825520833333]}, 'weight': {'score': [0.003223944902420044, 0.007279583518151365, 0.0020031698288456084, 0.0073427142757786605, 0.001980191917829616], 'topk_tokens': ['P', ' *\n\n', '.\n', 's', 'b', 'Answer', ' learned', ' wait', ' where', 'made', ' paper', '?\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003829061985015869, 0.002218484878540039, 0.004274288813273111, 0.0048735737800598145, 0.004111687342325847]}, 'saliency': {'score': [8.383870124816894e-05, 0.00016563819656085048, 6.315112113952637e-05, 0.00016688243928681618, 0.00013086648397548224], 'topk_tokens': ['<|start_header_id|>', 'street', ' facts', 'If', 'P', '50', ' *\n\n', '***', '�', '<|eot_id|>', 'columns', '<|eot_id|>', 'athroom', 'v', ' paper', 'P', 'assistant', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.0045042037963868e-05, 0.0001071542501449585, 7.914503415425618e-05, 0.00010636448860168457, 0.00011113286018371582]}}, 30: {'grad': {'score': [0.26432411193847655, 0.33691284265793997, 0.3435316701089182, 0.3372963391118371, 0.40404549465384537], 'topk_tokens': [' a', ' but', 'ION', ' Buchanan', 'body', 'RE', 'PA', ' back', ' EAR', 'ION', ' a', ' PA', 'EF', ' Bor', ' business', ' bend', ' bathroom', 'RI', 'b', ' B'], 'evidence_proportions': [0.20520629882812502, 0.3379058837890625, 0.18580786387125653, 0.3659019470214844, 0.27533213297526044]}, 'weight': {'score': [0.007578301429748535, 0.007205298051760109, 0.003275389632871074, 0.007232103407141536, 0.0049872193285214], 'topk_tokens': [' location', '\n', ' board', '.\n', 'Min', '<|start_header_id|>', '?\n', 'years', '�', 'Answer', 'columns', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'b', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010586023330688477, 0.007128000259399414, 0.006485303243001302, 0.012449145317077637, 0.01115735371907552]}, 'saliency': {'score': [0.00018618345260620116, 0.00022123128617199453, 8.022016094576927e-05, 0.00022248159370622325, 0.000565899956610895], 'topk_tokens': ['�', ' bill', 'years', '\u200d', ' generally', 'Answer', '<|eot_id|>', 'athroom', ' board', 'columns', '<|start_header_id|>', ' war', '-text', '<|eot_id|>', 'assistant', '<|start_header_id|>', '<|begin_of_text|>', '�', '<|end_header_id|>', 'b'], 'evidence_proportions': [2.4986267089843753e-05, 0.00030072033405303955, 0.0001513014237085978, 0.0002913549542427063, 0.00020892421404520672]}}, 31: {'grad': {'score': [0.26065673828125, 0.22533319340070612, 0.22805736910912297, 0.225102662528899, 0.22274554160333448], 'topk_tokens': [' facts', 'street', ' seen', 'ot', ' and', 'graph', 'ing', ' fastest', ' started', 'user', '�', ' about', 'ot', ' it', ' happened', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [0.2079498291015625, 0.24908065795898438, 0.2553240458170573, 0.23253250122070312, 0.3363787333170573]}, 'weight': {'score': [0.003616840839385986, 0.006671618271681272, 0.001928619800075408, 0.006724847337306043, 0.002771716925405687], 'topk_tokens': [' left', ' moved', ',', '.\n', ' item', ',', ' Where', ' write', '?\n', '<|eot_id|>', 'Question', '<|start_header_id|>', 'Answer', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006726861000061035, 0.0029827356338500977, 0.002106348673502604, 0.008404016494750977, 0.004812081654866537]}, 'saliency': {'score': [9.961247444152832e-05, 0.00014851653460976435, 5.895091641333795e-05, 0.00014946939038014678, 7.839581017853111e-05], 'topk_tokens': [',', 'assistant', 'Just', ':', ' location', ' write', ' dropped', 'Question', '\n\n', ' Where', ' moved', ' item', 'Answer', ' Pioneer', '<|start_header_id|>', '<|begin_of_text|>', 'athroom', '<|end_header_id|>', '<|eot_id|>', 'b'], 'evidence_proportions': [1.3846158981323243e-05, 0.00011197477579116821, 4.749496777852376e-05, 0.00022008270025253296, 0.00013464689254760742]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-23 22:30:26.500 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:30:26.500 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-13_pid-0_1-3-6-7-8.pkl | len: 10 |  size: 9.77 KB
Processing depth (1, 3, 6, 7, 8):   1%|          | 1/100 [00:13<22:58, 13.92s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.14it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Processing depth (0, 2, 4, 6, 9):   1%|          | 1/100 [00:22<22:58, 13.92s/it]2025-01-23 22:30:35.463 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:30:35.463 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:30:35.463 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football.
2025-01-23 22:30:35.469 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1028, 1032) -->  Mary got the football
2025-01-23 22:30:35.469 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:30:35.478 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1861, 1867) -->  Hon. Mary journeyed to
2025-01-23 22:30:35.479 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the football.
2025-01-23 22:30:35.493 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2448, 2452) -->  Mary left the football
2025-01-23 22:30:35.493 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-23 22:30:35.512 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3681, 3687) --> . Daniel journeyed to the
2025-01-23 22:30:35.512 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:30:35.528 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3214, 3219) --> . Daniel picked up the
2025-01-23 22:30:35.528 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:30:35.541 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2563, 2568) --> . Sandra moved to the
2025-01-23 22:30:35.541 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:30:35.551 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2115, 2120) --> . John moved to the
2025-01-23 22:30:35.551 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:30:35.555 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (771, 777) --> . Sandra journeyed to the
2025-01-23 22:30:35.555 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:30:35.565 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1968, 1972) -->  Daniel left the apple
2025-01-23 22:30:35.565 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:30:35.574 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1747, 1753) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:30:36.031 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary<|eot_id|>
2025-01-23 22:30:36.031 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4248])
your chose emoji: ['🚣🏻\u200d♀', '👩🏾\u200d❤️\u200d💋\u200d👩🏿', '🍅', '🧑🏽\u200d💻', '🦸🏾\u200d♀', '🧑🏼\u200d❤️\u200d🧑🏽', '👫🏻', '🚶🏾\u200d♂', '⛸️', '👨🏽\u200d💼']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4251, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 223696.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.94it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.79it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.28it/s]
2025-01-23 22:30:39.195 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5529215240478516, 0.4151971709781522, 0.5130278987269248, 0.413653461469939, 0.6794561029790522], 'topk_tokens': [' out', 'ian', '-four', 'ION', 'ian', 'E', ' John', 'ab', ' John', 'ex', ' to', '.', ' slow', 'ian', ' John', 'ian', ' Hor', 'ian', 'ION', ' P'], 'evidence_proportions': [0.4207649230957031, 0.6114044189453125, 0.7343721389770508, 0.5683803558349609, 0.4323069254557292]}, 'weight': {'score': [0.02585988998413086, 0.00735861204618904, 0.0038327382456871772, 0.007274409457241963, 0.0007847953628707718], 'topk_tokens': ['Mary', 'NEW', ' Where', '\n\n', 'user', ' football', '\n\n', ':', '<|start_header_id|>', '<|eot_id|>', '\n\n', '\n\n', 'Answer', 'b', '<|eot_id|>', 'assistant', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.10197257995605469, 0.008695423603057861, 0.0029352108637491865, 0.017062902450561523, 0.002664963404337565]}, 'saliency': {'score': [0.0006040048599243164, 0.0001048493303991267, 0.0001790648506533715, 0.00010132619109852806, 2.477096987294627e-05], 'topk_tokens': ['If', 'RE', '\n', 'Question', 'NEW', ' PA', '<|eot_id|>', ' bathroom', ' return', '<|eot_id|>', 'Mary', '<|end_header_id|>', ' football', ' Where', '\n\n', ' ', 'Answer', '<|begin_of_text|>', 'athroom', 'b'], 'evidence_proportions': [0.0021580159664154053, 0.00042444467544555664, 0.00011836489041646321, 0.00042719393968582153, 3.221631050109863e-05]}}, 25: {'grad': {'score': [1.5174530029296875, 1.220406693140217, 1.6205752588087512, 1.2156793073761023, 0.9950231195806147], 'topk_tokens': ['\n', ' the', ' the', ' inverted', ' to', ' to', ' the', ' the', ' set', ' into', ' the', ' late', ' of', ' the', 'old', ' of', ' the', ' old', ' the', ' at'], 'evidence_proportions': [1.56600341796875, 1.396240234375, 1.3709538777669272, 1.7205123901367188, 1.5689290364583333]}, 'weight': {'score': [0.009831598997116088, 0.00708476214430187, 0.0029799717087899487, 0.007098725811091026, 0.000961287008537041], 'topk_tokens': [' prior', '\n\n\n', '\n\n', ' discarded', ' return', 'Mary', ' Bench', 'b', '<|start_header_id|>', ' bathroom', '?\n', '<|eot_id|>', '<|eot_id|>', ':', 'Answer', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.03462028503417969, 0.004812717437744141, 0.002057686448097229, 0.00578916072845459, 0.0029891530672709146]}, 'saliency': {'score': [0.0003909587860107422, 0.0001536307055034292, 0.0001328885555267334, 0.0001523696339030942, 4.204780190855592e-05], 'topk_tokens': [' discarded', ' bathroom', '<|eot_id|>', '\n\n', ' Bench', 'ENCES', 'RE', ' Do', 'MIN', '<|eot_id|>', 'athroom', ' return', '?\n', 'Mary', 'Answer', 'assistant', ':', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0009404242038726807, 0.0003900974988937378, 4.963576793670654e-05, 0.0003458932042121887, 0.0003050118684768677]}}, 26: {'grad': {'score': [0.4035073852539062, 0.549061698258498, 0.5715135143649194, 0.5497632123290143, 0.5489999750158289], 'topk_tokens': [' idea', '�', 'human', 'ian', 'ley', '�', '�', ' joke', 'rich', 'machine', ' question', 'proc', 'most', ' steam', ' Press', ' medicine', ' trib', 'hue', 'Johnson', ' Eagle'], 'evidence_proportions': [0.38734283447265627, 0.36318206787109375, 0.490753173828125, 0.4051666259765625, 0.35550944010416663]}, 'weight': {'score': [0.02787860631942749, 0.006907115793598312, 0.003561962996759722, 0.006806856550107553, 0.0011533351389916389], 'topk_tokens': [' dropped', ' football', '\n\n', ' discarded', '.\n\n', ' football', '\n\n', '<|eot_id|>', 'b', '<|start_header_id|>', 'Answer', '<|eot_id|>', '?\n', '\n\n', ':', 'assistant', ' bathroom', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.108563232421875, 0.005022495985031128, 0.0021298627058664956, 0.023110151290893555, 0.004806538422902425]}, 'saliency': {'score': [0.0003057312965393066, 9.720812991377158e-05, 6.490945816040039e-05, 9.620412029725576e-05, 2.6458894813453758e-05], 'topk_tokens': ['Question', ' football', 'UL', ' bathroom', 'user', '?\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', ' PA', 'NEW', '\n\n', 'assistant', '\n\n', ':', 'RE', 'b', 'athroom', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.000806713104248047, 0.00017227232456207275, 2.1159648895263672e-05, 0.0005909502506256104, 7.164478302001953e-05]}}, 27: {'grad': {'score': [0.5157232666015625, 0.5709213152741635, 0.569548576108871, 0.5712604108953646, 0.4376718500158289], 'topk_tokens': [' convention', ' conventions', ' second', ' sound', ' fire', ',', ' an', ' the', ' expedition', 'an', '.', ' measure', ' to', ' conventions', ' one', '\n', 'Republicans', ' other', 'ers', ' work'], 'evidence_proportions': [0.70859375, 0.3623008728027344, 0.6212259928385417, 0.40821075439453125, 0.42345174153645837]}, 'weight': {'score': [0.037313861846923826, 0.007219643908033818, 0.0024495845840823267, 0.007075547696012422, 0.001339952696810712], 'topk_tokens': [' football', 'Question', '\n\n', '\n\n', 'If', 'Mary', '<|start_header_id|>', '<|eot_id|>', '?\n', 'Answer', 'RE', 'b', 'NEW', 'assistant', '\n\n', '<|end_header_id|>', ' bathroom', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.16396675109863282, 0.006089210510253906, 0.0016196171442667644, 0.012941718101501465, 0.004528562227884929]}, 'saliency': {'score': [0.001891707181930542, 0.0002082752160705193, 0.00011989282023522162, 0.0001988959681859886, 5.459785461425781e-05], 'topk_tokens': [' FR', 'If', 'Question', ' PA', '<|eot_id|>', 'UL', '"The', '\n\n', '\n\n', '<|start_header_id|>', 'Answer', 'b', 'athroom', ' bathroom', 'Mary', 'RE', 'NEW', ':', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [0.00815211534500122, 0.0006460323929786682, 6.39955202738444e-05, 0.0005000308156013489, 0.0002606461445490519]}}, 28: {'grad': {'score': [0.41753692626953126, 0.5469218065418431, 0.5131976681370889, 0.5479420855161828, 0.5324272113841969], 'topk_tokens': [' Date', ' account', 'nes', 'E', ' afternoon', ' instance', '.', ' result', ' season', ' instead', ' that', ' spring', ' half', ' summer', 'S', ' summer', ' platform', ' from', ' the', 'nes'], 'evidence_proportions': [0.42570190429687504, 0.38137245178222656, 0.40369415283203125, 0.4537696838378906, 0.424530029296875]}, 'weight': {'score': [0.008454471826553345, 0.006670395252817352, 0.005409532977688697, 0.006669080548689959, 0.000859192111989954], 'topk_tokens': [' left', ' bathroom', ' or', ' discarded', ' dropped', ' football', ',', ' was', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'b', 'Answer', '?\n', 'assistant', '\n\n', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.016614961624145507, 0.004538744688034058, 0.002100035548210144, 0.017147421836853027, 0.0048236846923828125]}, 'saliency': {'score': [0.00017571806907653808, 0.00012513371922778568, 0.00015269364080121441, 0.00012462860210859733, 2.3127257168947996e-05], 'topk_tokens': ['4', ' prepared', 'As', '.', ' was', ' FR', ' about', 'If', ' in', '<|start_header_id|>', ' was', 'nes', 'b', '<|begin_of_text|>', ':', 'assistant', 'athroom', '<|end_header_id|>', '?\n', '\n\n'], 'evidence_proportions': [0.000434565544128418, 8.375942707061768e-05, 3.096958001454671e-05, 0.0001361072063446045, 0.00019247333208719888]}}, 29: {'grad': {'score': [0.5553068542480468, 0.540076421586465, 0.46946839363344256, 0.5405074318486829, 0.8011810805771377], 'topk_tokens': [' for', ' for', '�', ':', ' whole', "'s", ' be', ' from', ' feature', ' H', 'pend', '.', '�', ' wholly', ' a', ' fact', '�', ' extra', 'assistant', '�'], 'evidence_proportions': [0.6598876953125, 0.27318763732910156, 0.7581539154052734, 0.42462158203125, 0.5405120849609375]}, 'weight': {'score': [0.0049061524868011476, 0.006885942025230621, 0.003546989733173001, 0.006922414554316324, 0.0009484212477128584], 'topk_tokens': [' the', ',', ' the', '\n\n', '<|eot_id|>', ' in', '<|eot_id|>', 'Answer', 'If', '\n\n', '<|start_header_id|>', ' was', 'b', '?\n', 'assistant', ':', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.006281709671020507, 0.0016595572233200073, 0.0017784088850021362, 0.0161018967628479, 0.0015881657600402832]}, 'saliency': {'score': [5.9973001480102536e-05, 9.427289911170086e-05, 3.408808862009356e-05, 9.492205919895468e-05, 5.343687403333056e-05], 'topk_tokens': [',', '<|eot_id|>', '.', '<|start_header_id|>', 'NEW', 'If', 'athroom', '<|eot_id|>', ' ', '\n\n', '?\n', '<|start_header_id|>', ' was', '\n\n', 'assistant', '\n\n', 'b', ':', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00014580488204956054, 1.95428729057312e-05, 2.302229404449463e-05, 8.911639451980591e-05, 3.292163213094076e-05]}}, 30: {'grad': {'score': [0.8260128021240234, 0.6504982800462391, 0.798842030186807, 0.6483560823570134, 0.5971583429273668], 'topk_tokens': [' more', '      ', '202', ' fact', ' company', '50', ' coach', ' a', '202', ' Online', ' at', ' time', ' affairs', 'vent', ' B', ' part', ' B', ' concerned', ' an', 'b'], 'evidence_proportions': [0.700482177734375, 0.5778217315673828, 1.039983113606771, 0.7955894470214844, 0.9023942947387695]}, 'weight': {'score': [0.012504990100860596, 0.006890396319173752, 0.004197647494654502, 0.006876835024541552, 0.002823995364891304], 'topk_tokens': [',', '.\n\n', 'Question', 'ANK', '\n\n', 'nes', 'Mary', '<|eot_id|>', ' bathroom', 'b', 'Answer', '<|eot_id|>', '?\n', 'assistant', '<|start_header_id|>', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03595256805419922, 0.00651317834854126, 0.004331082105636597, 0.010554075241088867, 0.006434400876363119]}, 'saliency': {'score': [0.0007231235504150391, 0.00017025262657206807, 0.00019262683007024948, 0.00016679246604655725, 7.921326291430128e-05], 'topk_tokens': [' about', ',', 'ern', 'Mary', 'Question', 'ANK', '\n\n', 'nes', ' ', 'Answer', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', ' bathroom', 'assistant', '<|begin_of_text|>', 'b', '<|end_header_id|>', ':', 'athroom'], 'evidence_proportions': [0.0029574871063232423, 0.00020182877779006958, 0.00016038119792938232, 5.876272916793823e-05, 0.00021433333555857342]}}, 31: {'grad': {'score': [0.5640921020507812, 0.8728693475501169, 0.6663427795133283, 0.8762356776446637, 0.49895050237467004], 'topk_tokens': ["'clock", ' company', 'Answer', 'user', ' city', ' that', ' location', ' happened', ' to', ' morning', ' be', ' affairs', ' goods', ' part', ' time', '1', ' location', ' concerned', ' city', ' time'], 'evidence_proportions': [0.6968048095703125, 0.31406593322753906, 0.27592722574869794, 0.6045131683349609, 0.8813997904459635]}, 'weight': {'score': [0.0039436864852905274, 0.006509748129978148, 0.002149260813190091, 0.006557263421500824, 0.0009965480683924077], 'topk_tokens': ['Question', ' Where', ' football', ':', ' bathroom', ' in', ',', ' was', 'Answer', '<|eot_id|>', '<|start_header_id|>', 'b', '?\n', '<|eot_id|>', 'assistant', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00941176414489746, 0.003840923309326172, 0.0011252959569295247, 0.0039757490158081055, 0.0022524793942769366]}, 'saliency': {'score': [0.00015598416328430176, 0.00010852753709439978, 4.752701328646752e-05, 0.00010869549909280224, 3.5522403297843515e-05], 'topk_tokens': ['\n\n', '<|eot_id|>', 'Question', '<|eot_id|>', ' Where', ' was', ' in', 'RE', ' bathroom', '\n\n', '<|eot_id|>', 'Answer', '<|start_header_id|>', ':', '?\n', 'athroom', 'b', '<|begin_of_text|>', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [0.0006195366382598877, 5.7674944400787354e-05, 1.5487273534139e-05, 7.867813110351562e-05, 2.7264157931009926e-05]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-23 22:30:39.202 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:30:39.203 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-13_pid-1_0-2-4-6-9.pkl | len: 10 |  size: 9.43 KB
Processing depth (0, 2, 4, 6, 9):   2%|▏         | 2/100 [00:26<21:33, 13.20s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.31s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Processing depth (2, 5, 6, 7, 8):   2%|▏         | 2/100 [00:36<21:33, 13.20s/it]2025-01-23 22:30:49.405 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:30:49.410 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (942, 947) --> . Mary moved to the
2025-01-23 22:30:49.410 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football.
2025-01-23 22:30:49.420 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2135, 2139) -->  got the football.
2025-01-23 22:30:49.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:30:49.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2408, 2414) -->  the senate. Mary journeyed
2025-01-23 22:30:49.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the football.
2025-01-23 22:30:49.447 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2898, 2902) -->  Mary left the football
2025-01-23 22:30:49.448 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-23 22:30:49.465 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3364, 3370) --> . Daniel journeyed to the
2025-01-23 22:30:49.465 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:30:49.481 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3225, 3230) --> . Daniel picked up the
2025-01-23 22:30:49.481 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:30:49.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2550, 2555) --> . Sandra moved to the
2025-01-23 22:30:49.495 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:30:49.505 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2091, 2096) --> . John moved to the
2025-01-23 22:30:49.506 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:30:49.509 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (706, 712) --> . Sandra journeyed to the
2025-01-23 22:30:49.509 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:30:49.519 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1949, 1953) -->  Daniel left the apple
2025-01-23 22:30:49.519 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:30:49.527 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1726, 1732) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:30:50.004 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:30:50.004 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4214])
your chose emoji: ['🔽', '💔', '🦦', '☣', '🧑🏾\u200d❤️\u200d🧑🏼', '⛎', '👨🏾\u200d🦯\u200d➡', '🎉', '🛒', '🧛🏻\u200d♀️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4217, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.52it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.45it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.08it/s]
2025-01-23 22:30:53.064 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5850590515136719, 0.6580488075145986, 0.6611214914629536, 0.6584644505565656, 0.7519414466724061], 'topk_tokens': [' arranged', 'ot', ' We', ' attention', ' he', ' been', ' agreement', 'nes', ' ', '202', '!', ' *\n\n', 'the', 'agle', ' ', ' attract', ' it', ' *\n\n', '202', ' *\n\n'], 'evidence_proportions': [0.266156005859375, 0.7293548583984375, 0.6746590932210287, 0.41718292236328125, 0.7769317626953125]}, 'weight': {'score': [0.002045485973358154, 0.007442628599496597, 0.0026643728056261616, 0.007510654193167215, 0.0014971259393190082], 'topk_tokens': ['\n', '\n\n', '?\n', '<|start_header_id|>', 'Just', '<|eot_id|>', ' ', 'NEW', 'Answer', '<|eot_id|>', '<|eot_id|>', 'user', '\n\n', '\n\n', 'b', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00032674074172973634, 0.0024798810482025146, 0.0025265018145243325, 0.003331780433654785, 0.0018496314684549966]}, 'saliency': {'score': [0.0001441073417663574, 0.00019012197572980865, 0.00023147752208094443, 0.0001900903352376676, 0.00010088870399876644], 'topk_tokens': ['Just', 'In', '<|eot_id|>', '.\n\n', ' Hale', '\n', 'Good', 'If', ' bedroom', 'athroom', '<|end_header_id|>', 'If', ' bathroom', '\n\n', 'user', '\n\n', 'Answer', 'assistant', ' ', '<|begin_of_text|>'], 'evidence_proportions': [1.7964839935302733e-05, 8.06078314781189e-05, 0.0003099590539932251, 0.00017458200454711914, 0.00010539094607035318]}}, 25: {'grad': {'score': [0.5708673095703125, 0.6857837498795797, 0.6303087049914945, 0.6868874839337162, 0.5185011144269976], 'topk_tokens': [' Paul', ' farther', 'RI', 'ollar', ' not', ' paper', 'material', 'national', ' STR', ' elect', ' the', 'year', ' football', ' fully', ' work', ' heard', ' John', ',', 'nes', ' EAR'], 'evidence_proportions': [0.331585693359375, 0.7613487243652344, 0.5194346110026041, 0.7007980346679688, 0.60809326171875]}, 'weight': {'score': [0.002135562896728516, 0.007233320493564691, 0.00270694782657008, 0.0072976707681616, 0.0013941725095113118], 'topk_tokens': ['<|eot_id|>', '\n\n\n\n\n\n\n', ' ', 's', '\n\n', 'Question', 'ree', 'Just', ' Douglas', '<|eot_id|>', 'user', 'Answer', 'b', ' discarded', '\n\n', 'assistant', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0004394888877868652, 0.0013586357235908508, 0.002990464369455973, 0.004172682762145996, 0.0018539279699325562]}, 'saliency': {'score': [0.00010317683219909668, 0.00017556141755409592, 9.108647223441832e-05, 0.00017662566603734154, 9.639221325255276e-05], 'topk_tokens': [' Mary', ' boat', ' ', 'E', ' Bor', ' veto', 'Answer', 'Minnesota', 'Civil', ' Douglas', ' Moore', 'athroom', 'P', 'ree', '\n\n', 's', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [4.5579671859741214e-05, 7.803738117218018e-05, 0.00022852420806884766, 0.00012630224227905273, 2.716978391011556e-05]}}, 26: {'grad': {'score': [0.8132200622558594, 0.7589073407302288, 0.7733656360257056, 0.7584733043946603, 1.083082165634423], 'topk_tokens': ['material', ' *', ' *', ' *', ' team', ' versatile', ' week', ' Pioneer', ' state', '�', ' *', ' state', '\u200d', ' message', '️', '50', '\u200d', '♀', '❤', '8'], 'evidence_proportions': [0.6816345214843751, 0.8364171981811523, 0.8781331380208333, 0.9754638671875, 0.7343343098958333]}, 'weight': {'score': [0.002255721092224121, 0.007169160849680006, 0.0028381866793478687, 0.00723094796653093, 0.0024085750705317445], 'topk_tokens': ['\n\n\n', 'ree', '\n', ' ', ' discarded', '\n\n\n\n', '<|eot_id|>', '.\n\n', 'Question', 'Just', '.\n', 'user', '\n\n', '<|eot_id|>', 'b', '\n\n', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00010211467742919922, 0.004768580198287964, 0.0023882786432902017, 0.0037595033645629883, 0.001240074634552002]}, 'saliency': {'score': [0.00013636946678161622, 0.00016945222415204917, 0.0002740006293019941, 0.00016887209158166045, 8.842035343772487e-05], 'topk_tokens': ['Minnesota', ' ', '4', ' Bor', 'doctor', '\n\n', '<|eot_id|>', ' Becker', ' senate', ' Moore', 'Just', ' bedroom', '\n\n', ' Gal', 'ree', 'assistant', 'b', ' bathroom', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [5.877017974853515e-06, 8.301436901092529e-05, 0.0002582669258117676, 0.00019332021474838257, 0.0001208186149597168]}}, 27: {'grad': {'score': [1.019193115234375, 1.0654898108196733, 1.0317953786542338, 1.0660189972620577, 0.8817138671875], 'topk_tokens': ['      ', ',', ' *', ' *', ' *', ' sent', ' *', 'ors', ',', ' *', ',', ',', '.', ' *', ';', ';', ',', ' means', ' *', '.'], 'evidence_proportions': [1.1292663574218749, 1.1507072448730469, 0.9933420817057291, 0.7136917114257812, 1.0693079630533853]}, 'weight': {'score': [0.0030160284042358397, 0.0072276124689628195, 0.003741237425035046, 0.0072788903415847706, 0.0024978586456231903], 'topk_tokens': [' Bor', ' Hale', 'RE', 'user', 'If', '<|eot_id|>', ' Buchanan', 's', 'ree', '\n\n', 'Just', '\n\n', 'Minnesota', 'NEW', '<|end_header_id|>', 'b', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00029153823852539067, 0.002711355686187744, 0.004826347033182779, 0.004215717315673828, 0.0028794407844543457]}, 'saliency': {'score': [0.0003024637699127197, 0.0002100423229286279, 0.0006065705130177159, 0.0002065328516314964, 0.00013328121419538532], 'topk_tokens': ['user', ' Daniel', ' laid', '\n\n', '\n\n', ' bedroom', '"The', 'If', ' Buchanan', 'b', 'If', 'NEW', 'assistant', ' Hale', 'athroom', 'ree', 'Minnesota', '<|end_header_id|>', '<|begin_of_text|>', ' bathroom'], 'evidence_proportions': [1.5348196029663086e-05, 0.00016710907220840454, 0.0005426555871963501, 0.00048057734966278076, 0.0002730290095011393]}}, 28: {'grad': {'score': [0.63610595703125, 0.6740533052136441, 0.6129960090883316, 0.6747361843014703, 0.7517866837350946], 'topk_tokens': [' from', ' ', ' front', 'material', ' from', ' out', ' from', ' *', ' Dub', '600', ' out', ' so', ' that', ' *', '600', '!"', ' *', ' Gus', ' *', ' *'], 'evidence_proportions': [0.6829605102539062, 0.8899917602539062, 0.6534322102864584, 0.44130516052246094, 0.54034423828125]}, 'weight': {'score': [0.0019315743446350097, 0.007005614637339146, 0.0019621637559706167, 0.00707367471535886, 0.0022267364619071024], 'topk_tokens': ['E', '\n\n', '\n', 'In', '.\n\n', ' second', ' discarded', '<|eot_id|>', '?\n', 'Answer', 'Question', '.\n', '\n\n', 'Just', 'b', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00050581693649292, 0.0024284720420837402, 0.0015649795532226562, 0.004486680030822754, 0.0014516313870747883]}, 'saliency': {'score': [0.00023484230041503906, 0.00014854843565036482, 0.0001535848263771303, 0.000147992445568258, 6.893852300811232e-05], 'topk_tokens': [' second', 'Answer', 'ree', ' football', '.\n', '.', ' *\n\n', ' bathroom', ',', '\n', 'As', 'Just', '\n\n', 'b', ' bedroom', 'athroom', '<|end_header_id|>', 'assistant', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [1.8674135208129883e-05, 0.0003278106451034546, 8.239348729451497e-05, 0.0009220018982887268, 4.734595616658529e-05]}}, 29: {'grad': {'score': [0.898291015625, 0.6838864350675095, 0.8607177734375, 0.6812808388157895, 0.5357169101112768], 'topk_tokens': [' in', ' team', ' summer', ' the', ' Pioneer', ' to', ' earth', 'UL', 'ed', 'ER', '.', ' in', 'g', ' to', ' com', ' spring', ' journey', ' com', 'b', 'ION'], 'evidence_proportions': [0.8497314453125, 1.07568359375, 0.8897501627604166, 0.9197998046875, 0.814697265625]}, 'weight': {'score': [0.0018102848529815674, 0.007154777034753189, 0.001499135648050616, 0.007229022934184112, 0.0021580112607855547], 'topk_tokens': ['\n\n\n\n\n\n\n', 'Just', ' discarded', ' OF', '\n\n\n\n', 'A', 'user', '<|eot_id|>', '\n\n', '\n\n', '?\n', 'Question', ',', 'b', '\n\n', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00016536116600036622, 0.003916382789611816, 0.001952787240346273, 0.003049790859222412, 0.0008081495761871338]}, 'saliency': {'score': [5.317091941833496e-05, 0.00020136558560078987, 4.6793491609634895e-05, 0.00020340754704474258, 0.00010646278398078785], 'topk_tokens': ['\n\n', ':', ',', ' of', '\n\n', ' ', '<|start_header_id|>', 'A', '<|end_header_id|>', '?\n', ',', ' OF', 'athroom', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'assistant', 'b', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [7.88569450378418e-06, 3.4652650356292725e-05, 9.937584400177002e-05, 8.101761341094971e-05, 3.848473230997722e-05]}}, 30: {'grad': {'score': [0.4108421325683594, 0.4478094559131936, 0.39210780974357357, 0.44844654654402255, 0.48561330427203264], 'topk_tokens': [' ', ' a', ' STR', ' Team', 'LES', ' Bor', 'ab', 'UL', 'Sh', ' l', ' OF', ' OF', ' Sh', 'LY', ' o', 'UG', ' o', ' B', ' B', 'b'], 'evidence_proportions': [0.4161376953125, 0.3636617660522461, 0.4552815755208333, 0.42437744140625, 0.38441975911458337]}, 'weight': {'score': [0.0035076665878295897, 0.0071116871027576715, 0.004070659798960532, 0.00715599673008065, 0.006351912230776067], 'topk_tokens': ['ANK', '\n\n', '<|eot_id|>', '.\n\n', 'user', 'Answer', '\n\n\n\n', 'ree', '<|eot_id|>', '\n\n', '.\n', ' Bor', 'Question', '<|end_header_id|>', '\n\n', 'assistant', '\n\n', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0003212571144104004, 0.002407968044281006, 0.004874030749003093, 0.006814479827880859, 0.003325233856836955]}, 'saliency': {'score': [0.000172346830368042, 0.00019540864058176374, 0.00014643707583027502, 0.00019591204451414518, 0.00018228302922165183], 'topk_tokens': ['In', '<|start_header_id|>', 'hours', '<|end_header_id|>', '\n\n\n\n', 'Question', '\n\n', 'user', ' kitchen', '\n\n', ' bedroom', 'In', 'Bridge', 'ree', ' Bor', '\n\n', ' bathroom', 'athroom', '<|begin_of_text|>', 'b'], 'evidence_proportions': [1.3631582260131837e-05, 0.0001713186502456665, 0.00020928680896759033, 0.0004046782851219177, 0.00011346737543741862]}}, 31: {'grad': {'score': [0.40340667724609375, 0.42126268002838213, 0.41748206846175656, 0.4213981280043789, 0.3129043077167712], 'topk_tokens': [' of', 'ot', ' J', ' generally', ' be', ' of', ' of', '�', ',', ' for', ' or', ' for', ' for', '�', 'user', ' producing', ' five', ',', ' so', ' and'], 'evidence_proportions': [0.5109619140625, 0.43707275390625, 0.3582763671875, 0.2807750701904297, 0.4182179768880208]}, 'weight': {'score': [0.002560889720916748, 0.006741830175850724, 0.0025567623876756236, 0.006798129289719211, 0.004364897284591407], 'topk_tokens': ['.\n\n', 'Just', ' write', '<|start_header_id|>', '.\n\n', ':', 'If', '.', ':', 'Answer', 'Question', '.\n', '?\n', '<|eot_id|>', 'assistant', '<|end_header_id|>', '\n\n', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001320791244506836, 0.0017380714416503906, 0.002260605494181315, 0.0054416656494140625, 0.0025226175785064697]}, 'saliency': {'score': [6.763935089111328e-05, 0.00010904165871312388, 9.605192369030368e-05, 0.00010938718610587988, 0.0002526290584028813], 'topk_tokens': [' Hale', ' bathroom', 'fax', 'Bridge', 'peak', 'Question', '\n\n', '<|eot_id|>', '�', 'If', '.\n', 'assistant', '.', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [3.604888916015625e-05, 4.602968692779541e-05, 6.744762261708577e-05, 0.0001529306173324585, 5.170206228892009e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:30:53.072 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:30:53.072 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-13_pid-2_2-5-6-7-8.pkl | len: 10 |  size: 9.04 KB
Processing depth (2, 5, 6, 7, 8):   3%|▎         | 3/100 [00:40<21:50, 13.51s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Processing depth (0, 2, 3, 4, 8):   3%|▎         | 3/100 [00:50<21:50, 13.51s/it]2025-01-23 22:31:03.328 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:31:03.329 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:31:03.329 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football.
2025-01-23 22:31:03.334 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (950, 954) -->  got the football.
2025-01-23 22:31:03.334 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:31:03.341 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1484, 1490) -->  Mary journeyed to the bedroom
2025-01-23 22:31:03.342 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the football.
2025-01-23 22:31:03.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1838, 1842) -->  Mary left the football
2025-01-23 22:31:03.351 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-23 22:31:03.369 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3305, 3311) --> . Daniel journeyed to the
2025-01-23 22:31:03.369 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:31:03.385 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3158, 3163) --> . Daniel picked up the
2025-01-23 22:31:03.385 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:31:03.397 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2531, 2536) -->  Sandra moved to the kitchen
2025-01-23 22:31:03.397 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:31:03.408 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2088, 2093) --> . John moved to the
2025-01-23 22:31:03.408 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:31:03.412 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (771, 777) --> . Sandra journeyed to the
2025-01-23 22:31:03.412 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:31:03.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1933, 1937) -->  left the apple.
2025-01-23 22:31:03.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:31:03.429 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1656, 1662) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:31:03.945 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:31:03.945 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4221])
your chose emoji: ['🍳', '🎛️', '🧑🏽\u200d⚕️', '🧑🏿\u200d❤️\u200d💋\u200d🧑🏾', '🪦', '🧑🏻\u200d🦱', '🐩', '🚶\u200d➡', '☠', '👁']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4224, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 209715.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.74it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.75it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.29it/s]
2025-01-23 22:31:06.774 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3734920501708984, 0.40189443212566955, 0.3648956052718624, 0.4023399751154338, 0.6514384597539902], 'topk_tokens': [' galaxy', 'ess', ' Gal', ' John', 'P', 'E', ' absent', 'burn', 'com', 'RI', 'P', ' bathroom', 'ION', ' John', '202', 'ien', '.', ' P', ' bend', 'athroom'], 'evidence_proportions': [0.6256450653076171, 0.33634185791015625, 0.3030192057291667, 0.36560821533203125, 0.2638600667317708]}, 'weight': {'score': [0.006634067296981811, 0.007387536718989863, 0.0005543174282197029, 0.007442878977522512, 0.002042097970843315], 'topk_tokens': ['\n\n', '<|start_header_id|>', ' Where', '<|eot_id|>', 'user', '<|start_header_id|>', ' bathroom', ':', 'b', '\n\n', 'Answer', '\n\n', '<|eot_id|>', ' *\n\n', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'assistant', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.031066322326660158, 0.0008727312088012695, 0.0006172209978103638, 0.00016782432794570923, 0.0004424204428990682]}, 'saliency': {'score': [0.000524129867553711, 0.00011529487727040595, 4.308742861593923e-05, 0.00011337970360226915, 5.283951759338379e-05], 'topk_tokens': [' return', '<|end_header_id|>', 'In', ' but', '4', '<|eot_id|>', '\n\n', 'user', ' ', '<|eot_id|>', 'Question', 'Answer', ':', ' bathroom', '<|begin_of_text|>', 'b', '\n\n', ' Where', ' *\n\n', 'athroom'], 'evidence_proportions': [0.0025037169456481934, 1.9825994968414307e-05, 4.7037998835245766e-05, 1.870095729827881e-05, 2.4721026420593262e-05]}}, 25: {'grad': {'score': [0.8499346923828125, 0.8602237845912124, 0.9186248779296875, 0.8598511342581312, 0.7126388549804688], 'topk_tokens': [' new', ' from', ' the', ' late', ' Gray', 'en', ' at', 'out', ' at', 'eward', ' out', ' old', ' at', ' Gray', ' at', ' Gray', ' at', ' at', ' at', ' at'], 'evidence_proportions': [0.5926513671875, 0.6074066162109375, 0.99755859375, 0.8199958801269531, 1.098358154296875]}, 'weight': {'score': [0.0018500196933746338, 0.007152956091996395, 0.0003069937229156494, 0.007235681198380044, 0.0015522642061114311], 'topk_tokens': [':', ' bathroom', ' Where', ' return', '      ', '.\n\n', '\n\n', '<|start_header_id|>', '?\n', ' *\n\n', '<|eot_id|>', ':', 'b', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.007439136505126953, 0.0003172457218170166, 0.0006640652815500895, 0.00011980533599853516, 0.0005537023146947224]}, 'saliency': {'score': [0.00011830806732177735, 0.00013080864410960314, 1.930421398532006e-05, 0.0001317129511522011, 7.436051964759827e-05], 'topk_tokens': [' obtained', ' Do', ' ', '.\n\n', 'b', ' provided', '<|eot_id|>', '      ', ' answer', ' *\n\n', ':', '?\n', 'Mary', ' return', 'athroom', 'Answer', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.0004306316375732422, 7.048249244689941e-06, 5.599856376647949e-05, 1.0214745998382568e-05, 6.65833552678426e-05]}}, 26: {'grad': {'score': [0.787681884765625, 0.9460767688173236, 0.8943497442430065, 0.9474115600512719, 0.8019853830337524], 'topk_tokens': [' Adams', ' store', ' on', ' one', ' St', ' supreme', ' take', ' Press', ' week', ' effect', ' Virginia', ' Becker', ' Fourth', ' Jackson', ' Wright', ' instead', 'RI', 'Johnson', 'b', ' Team'], 'evidence_proportions': [0.58006591796875, 0.91021728515625, 0.6685384114583334, 0.8163681030273438, 0.9790242513020834]}, 'weight': {'score': [0.0036353909969329833, 0.006780006668784402, 0.0005212820345355619, 0.006845418342358777, 0.003719109110534191], 'topk_tokens': ['.\n\n', ':', ' bathroom', '\n\n', '<|eot_id|>', '.\n\n', '<|start_header_id|>', '?\n', 'Answer', '\n\n', ' *\n\n', '<|eot_id|>', ':', 'b', '<|eot_id|>', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.016744470596313475, 0.00027489662170410156, 0.00041585663954416907, 0.00013372302055358887, 0.0005054672559102377]}, 'saliency': {'score': [0.0001512312889099121, 0.00014930087226358327, 4.949396656405541e-05, 0.00015003161930305715, 0.00018656346946954727], 'topk_tokens': [' *\n\n', ' *\n\n', ' *\n\n', '<|eot_id|>', '.', '?\n', ' bathroom', 'As', '.\n\n', '\n\n', '<|eot_id|>', '.\n\n', ':', '\n\n', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0006831705570220947, 1.601874828338623e-05, 1.214444637298584e-05, 4.649162292480469e-06, 3.489851951599121e-05]}}, 27: {'grad': {'score': [1.110213623046875, 1.1030133565266926, 1.1076796747023059, 1.1029354624464507, 0.9514086246490479], 'topk_tokens': [',', ' is', ' hour', ' If', ',', ' hours', ',', ' bill', ',', ' city', ',', ' company', ' hear', ',', ' city', ' fire', ' conditions', ',', ' time', '�'], 'evidence_proportions': [1.14111328125, 0.867462158203125, 1.1844482421875, 1.28778076171875, 1.0536855061848958]}, 'weight': {'score': [0.0035108935832977293, 0.007220264862884174, 0.000566895930997787, 0.007291999200426914, 0.003140595741569996], 'topk_tokens': [' Where', '?\n', ':', ' bathroom', ' *\n\n', '\n\n', 'Question', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'Answer', 'b', '<|eot_id|>', ':', 'assistant', ' *\n\n', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.015112113952636718, 0.0003935694694519043, 0.0007050732771555583, 0.00022915750741958618, 0.000915070374806722]}, 'saliency': {'score': [0.0002843832969665527, 0.0001656082035465674, 2.9478342302383915e-05, 0.00016590826313463603, 0.00011898484081029892], 'topk_tokens': ['<|eot_id|>', ' bathroom', ' *\n\n', ' *\n\n', '<|eot_id|>', '\n\n', ':', 'Question', ' *\n\n', '\n\n', ' *\n\n', 'Answer', 'assistant', ' Where', 'Mary', 'b', 'athroom', '<|end_header_id|>', ' *\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0012643456459045408, 1.8261373043060303e-05, 4.989902178446452e-05, 2.0809471607208252e-05, 5.536278088887533e-05]}}, 28: {'grad': {'score': [0.5735507202148438, 0.5479051705562707, 0.5599089591733871, 0.5476620668641894, 0.4361549913883209], 'topk_tokens': [' Min', ' Min', ' Min', ' Min', ' Min', ' Min', ' Dub', ' Min', 'graph', ' Min', ' elect', '\n\n', ' Min', ' Min', ' Min', ' Min', ' Min', 'Min', ' under', 'Min'], 'evidence_proportions': [0.8198974609374999, 0.3329639434814453, 0.5684763590494791, 0.2719268798828125, 0.7348098754882812]}, 'weight': {'score': [0.0021249175071716307, 0.006877268354098002, 0.0004942667099737352, 0.0069532476780281875, 0.0021330355666577816], 'topk_tokens': ['.\n\n', 'Just', '.\n\n', 'Question', ' Where', '\n\n', ':', '.\n\n', 'Answer', '.', '?\n', '<|eot_id|>', '<|eot_id|>', 'b', ':', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.009182333946228027, 0.00030288100242614746, 0.0004997948805491129, 0.0001252591609954834, 0.00041665633519490564]}, 'saliency': {'score': [0.0001362597942352295, 0.00015807936363147968, 1.8379380626063194e-05, 0.00015924927454992357, 6.496720016002655e-05], 'topk_tokens': ['.\n\n', ' *\n\n', ' *\n\n', '?\n', ' based', ' bathroom', '<|eot_id|>', '<|eot_id|>', 'In', 'Bridge', '\n\n', 'Answer', 'As', 'Just', 'assistant', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0006141841411590577, 9.573996067047119e-06, 2.596775690714518e-05, 6.556510925292969e-06, 1.920759677886963e-05]}}, 29: {'grad': {'score': [0.7395635986328125, 0.6401007970174154, 0.769303967875819, 0.6385432470325316, 0.5883461236953735], 'topk_tokens': [' George', ' to', ' Wins', ' took', 'called', ' the', ' ', ' in', '.', ' efforts', '.', ' *', ' Red', '.', ' o', ' H', ' o', '!', ' Francis', ' o'], 'evidence_proportions': [0.9879150390625, 0.6816520690917969, 0.6744181315104167, 0.6041030883789062, 0.7266642252604166]}, 'weight': {'score': [0.0010397756099700928, 0.00707074503103892, 0.00031292919189699237, 0.007157181337790389, 0.0017390395514667034], 'topk_tokens': ['.\n\n', ' Where', 'If', '<|start_header_id|>', 'Question', ' ', '\n\n', '<|eot_id|>', '\n\n', ':', '?\n', '<|eot_id|>', ':', 'Answer', 'b', '<|end_header_id|>', 'assistant', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.004570579528808594, 9.728223085403442e-05, 0.00015784303347269693, 0.00011392682790756226, 0.00022493302822113037]}, 'saliency': {'score': [8.179187774658203e-05, 0.0001317662624360034, 2.0737609555644375e-05, 0.00013289180174898965, 8.642300963401794e-05], 'topk_tokens': [' *\n\n', 'Question', ':', '<|end_header_id|>', ':', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'If', '<|start_header_id|>', '\n\n', '<|eot_id|>', 'athroom', ' ', '<|eot_id|>', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0003820478916168213, 4.380941390991211e-06, 4.0332476298014326e-06, 1.8402934074401855e-06, 1.424551010131836e-05]}}, 30: {'grad': {'score': [0.4246281433105469, 0.41280259508075134, 0.46499590719899825, 0.4123434704690886, 0.27887242659926414], 'topk_tokens': [' Bench', ' a', ' be', 'ball', ' bend', 'body', ' block', 'burn', ' based', ' Bor', ' a', ' body', 'athroom', ' border', ' o', ' web', ' bodies', ' B', ' B', 'b'], 'evidence_proportions': [0.5001396179199219, 0.3969268798828125, 0.5441309611002604, 0.4527397155761719, 0.24192555745442706]}, 'weight': {'score': [0.0027533388137817383, 0.00686199940515287, 0.0010188664159467144, 0.00693010248515519, 0.008882461115717888], 'topk_tokens': ['Mary', '.\n\n', ' Where', '.', '.\n\n', 'Question', '<|start_header_id|>', '?\n', ' *\n\n', ':', '<|eot_id|>', 'Answer', '<|end_header_id|>', 'assistant', 'b', '<|eot_id|>', ':', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.010967254638671875, 0.0002383887767791748, 0.0009585221608479818, 0.00028821080923080444, 0.0010232776403427124]}, 'saliency': {'score': [0.00017078280448913576, 0.00023100966813437867, 2.209121181118873e-05, 0.00023292476979876206, 0.00020488351583480835], 'topk_tokens': [' *', '\n\n\n', ' return', 'Answer', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' bathroom', ' Where', ' *\n\n', '?\n', '.', 'Mary', 'Question', ' *\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'b'], 'evidence_proportions': [0.0007111430168151855, 1.1682510375976562e-05, 9.260574976603189e-05, 1.3522803783416748e-05, 9.566545486450195e-06]}}, 31: {'grad': {'score': [0.31682567596435546, 0.3894343340035641, 0.38119740639963456, 0.3899311097249417, 0.3220645487308502], 'topk_tokens': [' location', ' person', ' had', ' had', ' item', ' location', ' second', ' item', ' up', ' item', ' location', ' the', ' item', ' time', '<|start_header_id|>', ' apple', ' it', 'assistant', '<|end_header_id|>', 'user'], 'evidence_proportions': [0.5012090682983398, 0.2122650146484375, 0.22102864583333331, 0.21849822998046875, 0.39422861735026044]}, 'weight': {'score': [0.001069730520248413, 0.006592746033813014, 0.0007204278822868101, 0.006669549596801601, 0.00272928923368454], 'topk_tokens': ['.\n\n', '.', 'If', 'Question', '.\n\n', '.\n\n', '<|start_header_id|>', ' Where', ':', '<|eot_id|>', 'Answer', '?\n', 'assistant', ':', '<|end_header_id|>', 'b', '<|eot_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0030858993530273435, 0.00047150254249572754, 0.0005875428517659504, 0.00020020455121994019, 0.0008502801259358724]}, 'saliency': {'score': [4.69505786895752e-05, 7.125199066870141e-05, 2.0046387949297504e-05, 7.17786003097234e-05, 4.6654604375362396e-05], 'topk_tokens': ['If', 'RE', ' bathroom', ',', ' Where', 'If', '<|start_header_id|>', '.\n\n', '.\n\n', ':', 'Answer', 'Question', '\n\n', '<|eot_id|>', '?\n', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', 'b', 'assistant'], 'evidence_proportions': [0.00019019246101379395, 7.733702659606934e-06, 1.6207496325174965e-05, 3.4347176551818848e-06, 1.3480583826700847e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:31:06.782 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:31:06.782 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-13_pid-3_0-2-3-4-8.pkl | len: 10 |  size: 9.34 KB
Processing depth (0, 2, 3, 4, 8):   4%|▍         | 4/100 [00:54<21:44, 13.59s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Processing depth (0, 5, 6, 7, 8):   4%|▍         | 4/100 [01:03<21:44, 13.59s/it]2025-01-23 22:31:15.913 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:31:15.913 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:31:15.913 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the football.
2025-01-23 22:31:15.923 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2106, 2110) -->  Mary got the football
2025-01-23 22:31:15.923 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:31:15.936 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2399, 2405) --> . Mary journeyed to the
2025-01-23 22:31:15.936 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the football.
2025-01-23 22:31:15.954 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2870, 2874) -->  Mary left the football
2025-01-23 22:31:15.954 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-23 22:31:15.971 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3368, 3374) --> . Daniel journeyed to the
2025-01-23 22:31:15.972 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:31:15.987 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3193, 3198) --> . Daniel picked up the
2025-01-23 22:31:15.987 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:31:16.000 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2537, 2542) --> . Sandra moved to the
2025-01-23 22:31:16.000 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:31:16.010 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2066, 2071) --> . John moved to the
2025-01-23 22:31:16.010 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:31:16.014 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (651, 657) -->  Republican. Sandra journeyed to
2025-01-23 22:31:16.014 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:31:16.023 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1911, 1915) -->  Daniel left the apple
2025-01-23 22:31:16.023 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:31:16.031 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1637, 1643) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:31:16.647 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Mary got the football.<|eot_id|>
2025-01-23 22:31:16.647 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4228])
your chose emoji: ['🤦🏻\u200d♀', '🧑🏿\u200d🦰', '💁🏿\u200d♀', '🙂\u200d↕️', '👨🏾\u200d💻', '🏖', '🌲', '👩🏿\u200d❤️\u200d💋\u200d👩🏻', '🐐', '🐍']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4231, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 219310.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 108.34it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.01it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.66it/s]
2025-01-23 22:31:19.818 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5059398651123047, 0.41364315197952983, 0.5106573412495274, 0.4123701321721791, 0.28675621999821194], 'topk_tokens': ['APER', ' election', ' a', ' election', ' step', ' tragedy', ' the', ' compos', ' days', ' to', ' prepared', ' temper', ' business', ' If', 'icing', ' a', ' business', ':', ' rejo', ' business'], 'evidence_proportions': [0.6518981933593749, 0.5246734619140625, 0.34012365341186523, 0.3180885314941406, 0.6628692944844564]}, 'weight': {'score': [0.009565219879150391, 0.007430560987234059, 0.01376324507497972, 0.007370757471301599, 0.007253880651903824], 'topk_tokens': ['\n\n', 'Bridge', ' ', '<|end_header_id|>', 'user', '<|start_header_id|>', '<|eot_id|>', '.', '\n\n', 'athroom', '\n\n', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '.', '❤', 'assistant', '<|end_header_id|>', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.007152438163757324, 0.005575329065322876, 0.02107385794321696, 0.0074055492877960205, 0.004166940848032633]}, 'saliency': {'score': [0.0002573060989379883, 0.00012210584847647033, 0.00020076478681256695, 0.00012071221174594171, 9.03459501938081e-05], 'topk_tokens': ['<|eot_id|>', '<|eot_id|>', ' Wood', '\n\n', '<|end_header_id|>', '❤', 'user', 'During', 'Bridge', '<|end_header_id|>', '<|start_header_id|>', 'Today', '\n\n', '<|begin_of_text|>', 'NEW', 'Question', '<|start_header_id|>', 'b', '<|eot_id|>', 'assistant'], 'evidence_proportions': [0.00034703016281127934, 0.0003600791096687317, 0.00021454195181528726, 0.00013475120067596436, 0.00023848811785380045]}}, 25: {'grad': {'score': [0.8562057495117188, 0.7111081632985147, 0.7690789007371471, 0.7098088740731427, 0.5170380968443105], 'topk_tokens': [' for', ' for', ' veto', 'ot', ' Written', 'MIN', ' Published', ' Pioneer', ' EAR', ' employed', ' rejo', 'far', ' fr', ' fore', ' employ', ' for', ' prepared', 'ched', ' enterprise', ' temper'], 'evidence_proportions': [1.2515502929687499, 0.9306640625, 0.699462890625, 0.8560905456542969, 0.6339327494303385]}, 'weight': {'score': [0.008116899728775025, 0.007344032455296473, 0.015560046319038637, 0.007278399374670611, 0.0075874530093770634], 'topk_tokens': ['ely', ' the', ' distant', ' the', 'user', '\n\n', 'tele', '\n\n', '<|start_header_id|>', '.', '\n\n', '<|eot_id|>', '<|eot_id|>', 'assistant', '.', '❤', 'athroom', '<|end_header_id|>', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.002235674858093262, 0.0029380321502685547, 0.022876332203547158, 0.0045726001262664795, 0.004073932766914368]}, 'saliency': {'score': [0.00024003148078918457, 0.00019264633958149792, 0.00026289589943424347, 0.00019184098272266502, 0.00010512622309402681], 'topk_tokens': ['Mary', ' tele', ' actions', ' obtained', ' Ear', 'nes', ' distant', '❤', '\n\n', 'tickets', '<|start_header_id|>', 'RE', 'remember', 'athroom', '<|eot_id|>', 'b', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [4.70280647277832e-05, 0.0001427978277206421, 0.0004201531410217285, 0.0003178119659423828, 0.00023371477921803793]}}, 26: {'grad': {'score': [0.31095783233642577, 0.2784464075840892, 0.27037885112147175, 0.2783116312084084, 0.30602566625030947], 'topk_tokens': [':', 'two', ' per', ' office', ' those', ' location', ' perpet', 'ian', ' P', 'ting', 'ollar', ' prize', ' city', ' to', ' front', ' steam', ' cap', '�', ' Date', ' circ'], 'evidence_proportions': [0.40257568359375, 0.32933616638183594, 0.3064130147298177, 0.27701687812805176, 0.24952952067057294]}, 'weight': {'score': [0.012983589172363282, 0.007468898649673264, 0.021846646262753393, 0.007329119622350453, 0.013222037066876049], 'topk_tokens': [' the', 'Bridge', 'user', 'tele', ' circ', '<|eot_id|>', 'athroom', ' the', '\n\n', '\n\n', '<|eot_id|>', '.', '<|eot_id|>', 'assistant', '.', '<|start_header_id|>', '<|end_header_id|>', '❤', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.004941630363464356, 0.002108365297317505, 0.03809102376302083, 0.008227825164794922, 0.004998445510864258]}, 'saliency': {'score': [0.0016829919815063476, 0.0001864859931605895, 0.0023256607594028594, 0.00016164118658282798, 0.0014462848784218372], 'topk_tokens': ['�', ' versatile', ' Gal', '�', 'athroom', '<|eot_id|>', 'Saturday', 'Bridge', 'tele', '\n\n', ' the', 'ely', ' the', 'assistant', '.', '<|end_header_id|>', '<|begin_of_text|>', '.', '<|start_header_id|>', '❤'], 'evidence_proportions': [0.0003181934356689453, 3.155320882797241e-05, 0.006557067235310873, 0.00012204796075820923, 8.783737818400066e-05]}}, 27: {'grad': {'score': [0.35201034545898435, 0.3761679122589407, 0.31421396809239543, 0.37677258565754235, 0.5948505065810512], 'topk_tokens': [' Pioneer', ' crack', ' Pioneer', '<|start_header_id|>', ' printers', ' celebrity', ' Daily', 'AILY', ' Reporter', ' court', '\u200d', 'ANK', ' Democratic', ' St', ' newspaper', ' printer', '\u200d', 'itor', 'APER', ' doctor'], 'evidence_proportions': [0.2696197509765625, 0.47093963623046875, 0.4245719909667969, 0.4264259338378906, 0.21921094258626303]}, 'weight': {'score': [0.010565001964569092, 0.007406343835602131, 0.01558611758293644, 0.007326693670478409, 0.008898260307983614], 'topk_tokens': ['tele', '\n\n', 'Saturday', 'Bridge', 'just', 'Just', '\n\n', '\n\n', '.', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'user', 'athroom', 'assistant', '.', '❤', '<|end_header_id|>', 'b', '<|begin_of_text|>'], 'evidence_proportions': [0.0033109188079833984, 0.0020711272954940796, 0.023049871126810707, 0.011792898178100586, 0.00896918773651123]}, 'saliency': {'score': [0.0003052759170532227, 0.00024078322538837027, 0.0005041322400493007, 0.0002384416357485834, 0.00033817744590866736], 'topk_tokens': ['assistant', '<|eot_id|>', ' among', 'illustr', '�', 'Just', ' *\n\n', '<|end_header_id|>', '❤', 'During', ' Alexander', '<|start_header_id|>', 'just', '\n\n', 'athroom', '<|start_header_id|>', '<|begin_of_text|>', 'user', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.0002568542957305908, 0.00010617822408676147, 0.00048685570557912194, 0.00025356560945510864, 0.0003312528133392334]}}, 28: {'grad': {'score': [0.34374420166015623, 0.25725835819555437, 0.22422276966033444, 0.2569857730836925, 0.2429778559107176], 'topk_tokens': [' Min', ' Min', ' part', '\n\n', ' Gal', ' Min', 'Min', ' Mr', ' Min', ' got', '2', '186', ' Min', ' Min', ' Min', 'Min', '\n\n', ' thought', ' thought', ' thought'], 'evidence_proportions': [0.3424896240234375, 0.5433349609375, 0.2598775227864583, 0.1909160614013672, 0.39748128255208337]}, 'weight': {'score': [0.008974314928054809, 0.0069422864992831, 0.021393662498843287, 0.0068228150342039, 0.0075296759605407715], 'topk_tokens': ['.\n\n', 'Bridge', 'tele', '.', ',', '.\n\n', '<|eot_id|>', '<|eot_id|>', '?\n', '.\n\n', '\n\n', 'athroom', 'Just', '<|start_header_id|>', '❤', 'assistant', '.', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.002193784713745117, 0.003908485174179077, 0.023086845874786377, 0.00485152006149292, 0.006637975573539734]}, 'saliency': {'score': [0.00016066670417785644, 0.00016995378347526232, 0.0001314647736087922, 0.00017029518138862655, 0.00010728500258754677], 'topk_tokens': ['tele', '\n', ',', '\n\n', 'just', '\n\n', 'Today', '<|start_header_id|>', ' During', '?\n', '❤', '.\n\n', 'athroom', 'Bridge', 'During', '<|begin_of_text|>', 'assistant', 'Just', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.00014977455139160155, 3.556162118911743e-05, 0.0003829151391983032, 6.182491779327393e-05, 9.67929760615031e-05]}}, 29: {'grad': {'score': [0.2699695587158203, 0.28906459172717, 0.2732528717287125, 0.2892963376302205, 0.49526481561257807], 'topk_tokens': ['.', ' wonderful', 'UL', '202', ' ', 'E', ' FR', ' Date', '<|end_header_id|>', '�', '�', ' about', '�', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.273193359375, 0.16217708587646484, 0.2648185094197591, 0.3125133514404297, 0.3159332275390625]}, 'weight': {'score': [0.005887261629104614, 0.007292944949390589, 0.014646183098516157, 0.007246763320740111, 0.005197801640335942], 'topk_tokens': [' ', 'user', ' the', '\n\n', ',', '\n\n', 'athroom', '<|start_header_id|>', '\n\n', '?\n', '.\n\n', '<|start_header_id|>', '❤', '<|eot_id|>', '<|eot_id|>', 'assistant', '.', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0053958892822265625, 0.0014736279845237732, 0.012144436438878376, 0.005053341388702393, 0.003537933031717936]}, 'saliency': {'score': [0.00026755690574645994, 0.00016094567053877583, 0.0005852878093719482, 0.00015715647600368112, 0.00028988635036307324], 'topk_tokens': ['the', 'Saturday', '?\n', '<|end_header_id|>', 'b', 'a', 'P', '<|eot_id|>', 'assistant', 'During', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', '❤', '.', 'A', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0006150722503662109, 3.9868056774139404e-05, 0.0003729561964670817, 0.00015022605657577515, 0.00010257462660471599]}}, 30: {'grad': {'score': [0.5395308303833007, 0.5856269722823357, 0.5971852117969144, 0.5858171754254552, 0.43400565671249175], 'topk_tokens': [' balance', ' be', ' Bench', ' be', 'a', '3', ' lyn', ' a', ' by', 'ed', 'A', 'ab', ' prof', ' a', ' lowered', 'ball', ' A', ' bend', ' B', ' B'], 'evidence_proportions': [0.55947265625, 0.455780029296875, 0.5951563517252605, 0.47200894355773926, 0.5681355794270833]}, 'weight': {'score': [0.008085837364196777, 0.007065679919378776, 0.008675217628479004, 0.007047620133725469, 0.005961005536603256], 'topk_tokens': ['Question', '.\n', '.\n\n', ',', '<|eot_id|>', '.', '<|end_header_id|>', '.\n\n', '?\n', '.', '.\n\n', 'athroom', 'assistant', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'b', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.005682468414306641, 0.0013571679592132568, 0.005736807982126872, 0.00722271203994751, 0.017498870690663658]}, 'saliency': {'score': [0.0014650297164916992, 0.00028814079913087724, 0.00014304057244331607, 0.0002821709510095105, 0.0003293097858697596], 'topk_tokens': ['.\n\n', '<|end_header_id|>', 'fortunate', ' *\n\n', 'user', 'P', ' *\n\n', 'athroom', '�', '<|start_header_id|>', '<|start_header_id|>', 'Question', '?\n', '<|begin_of_text|>', '<|eot_id|>', 'b', '<|eot_id|>', ' bathroom', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [0.005949485301971436, 7.333606481552124e-05, 9.0216596921285e-05, 0.00028768181800842285, 0.0008154908816019695]}}, 31: {'grad': {'score': [0.31697845458984375, 0.32478288160489543, 0.29574843375913556, 0.32504519982252295, 0.5166316771171462], 'topk_tokens': [' their', ' football', 'ER', ' ST', ' football', ' office', '�', 'E', '<|end_header_id|>', ' entire', 'SP', '<|start_header_id|>', 'user', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>'], 'evidence_proportions': [0.327874755859375, 0.35320281982421875, 0.26554107666015625, 0.4439048767089844, 0.2505683898925781]}, 'weight': {'score': [0.005258994102478027, 0.006567935857950895, 0.0023098309193888018, 0.006607390899144247, 0.0064189895777635175], 'topk_tokens': ['Answer', ':', ' Do', ' was', 'Bridge', '❤', ' ', '?\n', '.\n', '\n\n', '<|start_header_id|>', '<|eot_id|>', '.\n\n', '<|eot_id|>', 'Question', 'assistant', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.001996278762817383, 0.000599294900894165, 0.012994209925333658, 0.004017472267150879, 0.0041768550872802734]}, 'saliency': {'score': [0.00013322830200195311, 0.0002653346969511627, 4.4168003143802765e-05, 0.0002677679490186497, 0.0002331414692838427], 'topk_tokens': ['?\n', ' dropped', ':', ' discarded', '❤', 'question', ' Pioneer', ' the', '<|eot_id|>', '<|eot_id|>', '.\n', 'assistant', '<|begin_of_text|>', '<|start_header_id|>', 'Question', 'Bridge', '.\n\n', 'athroom', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.00011017322540283204, 1.109391450881958e-05, 0.0003020664056142171, 0.00012402981519699097, 7.115801175435384e-05]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-23 22:31:19.827 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:31:19.827 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-13_pid-4_0-5-6-7-8.pkl | len: 10 |  size: 9.83 KB
Processing depth (0, 5, 6, 7, 8):   5%|▌         | 5/100 [01:07<21:12, 13.39s/it]Processing depth (0, 5, 6, 7, 8):   5%|▌         | 5/100 [01:07<21:21, 13.49s/it]
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:99 - Selected idx: 14
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:103 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary took the apple.', ' John moved to the garden.', ' Sandra journeyed to the office.', ' John went back to the office.', ' Mary moved to the bathroom.', ' Daniel took the football.', ' Mary journeyed to the bedroom.', ' Daniel left the apple.']
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:104 - Real Needle: [' Mary took the apple.', ' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Daniel left the apple.']
2025-01-23 22:31:20.038 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Processing depth (0, 1, 6, 7):   0%|          | 0/100 [00:09<?, ?it/s]2025-01-23 22:31:29.469 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the apple.
2025-01-23 22:31:29.490 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:31:29.492 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (467, 472) --> . Mary moved to the
2025-01-23 22:31:29.492 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:31:29.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2326, 2332) --> . Mary journeyed to the
2025-01-23 22:31:29.504 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:31:29.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2809, 2813) -->  Daniel left the apple
2025-01-23 22:31:29.518 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:31:29.536 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3645, 3650) --> . Daniel picked up the
2025-01-23 22:31:29.537 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:31:29.551 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2838, 2843) --> . Sandra moved to the
2025-01-23 22:31:29.551 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:31:29.553 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (545, 550) --> . John moved to the
2025-01-23 22:31:29.554 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:31:29.566 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2383, 2389) --> . Sandra journeyed to the
2025-01-23 22:31:29.566 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the office.
2025-01-23 22:31:29.579 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2692, 2698) --> . John went back to the
2025-01-23 22:31:29.579 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel took the football.
2025-01-23 22:31:29.588 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1796, 1800) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:31:30.064 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the office<|eot_id|>
2025-01-23 22:31:30.064 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4190])
your chose emoji: ['🥖', '🔨', '🆑', '⚧', '📥', '🦹🏽\u200d♀️', '🇸🇨', '🍌', '🧑🏽\u200d🏭', '🖐🏽']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4193, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.82it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.21it/s]
2025-01-23 22:31:33.113 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.16002451578776042, 0.17329867085721365, 0.1935915793142011, 0.1731949891991175, 0.12370323648258132], 'topk_tokens': ['ting', ' Min', ' of', ' IN', '185', ' border', ' took', 'ION', 'user', ' business', ' John', ' em', ' but', 'ol', 'EF', ' but', ' MO', ' bathroom', ' B', 'ION'], 'evidence_proportions': [0.18860931396484376, 0.1480738321940104, 0.14221954345703125]}, 'weight': {'score': [0.005525030692418416, 0.007573327215309789, 0.03959585774329401, 0.0073413585636280575, 0.005831282965990962], 'topk_tokens': ['estead', ' person', 'rate', 'As', 'ian', ' after', 'ated', 'b', '<|end_header_id|>', ' of', 'ian', "'clock", 'made', ' write', 'years', ' closely', ' journey', '.', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005661308765411377, 0.0063186486562093105, 0.010533228516578674]}, 'saliency': {'score': [4.6318769454956056e-05, 6.224346166574782e-05, 0.000100131957761703, 6.201783518978623e-05, 2.2604757425736408e-05], 'topk_tokens': [' Knowledge', ' location', 'system', 'b', ' platform', ' assistance', '.\n', '\n', ' Buchanan', '\n\n', '<|eot_id|>', ' value', ' Project', ' result', ' late', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'athroom', ' block'], 'evidence_proportions': [3.6102533340454104e-05, 4.706283410390218e-05, 5.797296762466431e-05]}}, 25: {'grad': {'score': [0.1370519002278646, 0.21877535004164306, 0.2528174615675403, 0.2188164752622613, 0.19696241495560626], 'topk_tokens': [' prof', ' the', ' this', '\n', ' *\n\n', 'system', ' attract', 'g', '600', ' of', ' *\n\n', ' web', 'super', 'hand', ' *\n\n', ' *\n\n', ' newspaper', ' trials', ' trib', '\n\n\n\n\n\n\n'], 'evidence_proportions': [0.1351104736328125, 0.14278793334960938, 0.1308746337890625]}, 'weight': {'score': [0.004804599285125733, 0.007549652983913154, 0.057110663383237774, 0.0071890994471643585, 0.005322326202781833], 'topk_tokens': ['As', 'b', 'graph', ' not', ' of', 'estead', ' write', 'made', 'ian', 'assistant', "'clock", 'ian', 'years', ' closely', ' journey', '\n\n', '<|end_header_id|>', '.', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006838440895080566, 0.0049364566802978516, 0.009757757186889648]}, 'saliency': {'score': [3.753900527954101e-05, 0.00010404248042007691, 0.00019250569805022208, 0.00010362174069993548, 2.8551841268734056e-05], 'topk_tokens': ['de', 'man', 'at', "'clock", ' composing', ' senate', '<|start_header_id|>', ' typ', ' messages', '<|eot_id|>', 'b', ' Gen', '.', '<|eot_id|>', ' tele', '<|begin_of_text|>', 'assistant', '\n\n', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [1.170039176940918e-05, 6.918112436930338e-05, 2.2374093532562256e-05]}}, 26: {'grad': {'score': [0.17699534098307293, 0.22625493696245974, 0.22009138907155684, 0.22647918676335407, 0.23859098006267937], 'topk_tokens': [' *', '.', ' $', ' *', ' *', '\n', ' for', ':', '      ', 'user', '600', ' office', 'com', '\n', ' a', ' first', ' prepared', ' material', ' *', 'system'], 'evidence_proportions': [0.186627197265625, 0.1879119873046875, 0.14858055114746094]}, 'weight': {'score': [0.005105469624201457, 0.007604774586090517, 0.029520865409604965, 0.00744998576499078, 0.007222008948423424], 'topk_tokens': [' after', ' write', 'b', '<|eot_id|>', '<|start_header_id|>', 'estead', 'port', "'clock", 'ian', '<|eot_id|>', 'rate', 'ian', 'years', 'made', ' closely', ' journey', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.001101130247116089, 0.007023493448893229, 0.007233858108520508]}, 'saliency': {'score': [5.4780642191569014e-05, 0.00013189441743227965, 0.00027468896681262603, 0.00013110591383879518, 0.000166293309659374], 'topk_tokens': [' tie', '<|eot_id|>', 'event', 'rich', '.', 'rail', '<|start_header_id|>', '<|begin_of_text|>', ' composing', 'eward', ' reached', 'body', ' from', ' great', 'cuts', ' veto', ' came', ' hung', '<|start_header_id|>', ' laid'], 'evidence_proportions': [2.1445751190185544e-05, 6.099045276641846e-05, 8.713454008102417e-05]}}, 27: {'grad': {'score': [0.2045305649439494, 0.3562443753912771, 0.3344375395005749, 0.35695614753242055, 0.5553016273342833], 'topk_tokens': ['�', '�', 'his', ' Grow', '�', 'ideas', ' Gree', ' Republican', ' work', 's', ' the', 'human', ' Falls', '\n', ' take', 'rich', 'old', ' Wins', ' impressed', 'nes'], 'evidence_proportions': [0.1669586181640625, 0.2598978678385417, 0.16844454407691956]}, 'weight': {'score': [0.005612035592397054, 0.007589511703029044, 0.09205737806135608, 0.006965243143721434, 0.0053389929995244865], 'topk_tokens': ['assistant', 'b', ' not', 'graph', 'ated', 'rate', 'ian', ' of', 'As', 'estead', "'clock", 'ian', 'made', ' closely', '<|end_header_id|>', 'years', ' journey', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0018189787864685057, 0.006003657976786295, 0.009765923023223877]}, 'saliency': {'score': [9.59932804107666e-05, 0.0001819183796310152, 0.0005682245377571352, 0.00017934142896460428, 0.0003233308694800552], 'topk_tokens': [' composing', ' incorporated', 'complete', '<|eot_id|>', ' write', '      ', ' closely', 'made', ' *\n\n', '<|begin_of_text|>', ' journey', '\n\n', '<|end_header_id|>', 'human', '�', '<|start_header_id|>', 'assistant', '.', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [5.941390991210937e-05, 8.381903171539307e-05, 0.00015997886657714844]}}, 28: {'grad': {'score': [0.5232177734375, 0.5661009577672237, 0.6030224215599799, 0.5659800709544361, 0.5478652642697704], 'topk_tokens': [' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' PA', ' A', ' a', ' a', ' A', 'A', ' a', ' a', ' a', ' a'], 'evidence_proportions': [0.483251953125, 0.5876668294270834, 0.47650146484375]}, 'weight': {'score': [0.002751257022221883, 0.007441912577597474, 0.12729103238351883, 0.0065629728909196525, 0.0033187878375150717], 'topk_tokens': [' some', '<|start_header_id|>', "'clock", 'made', 'estead', 'ian', 'c', 'ian', 'As', ' time', 'years', ' closely', 'assistant', ' journey', 'b', '\n\n', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.000859600305557251, 0.0029343763987223306, 0.004841148853302002]}, 'saliency': {'score': [2.3096799850463867e-05, 0.00012501634989665, 0.0004934620472692675, 0.00012263076432446327, 3.301854036292251e-05], 'topk_tokens': [' first', ' Hor', '<|start_header_id|>', 'ace', ' before', ' far', ' closely', '<|eot_id|>', '<|eot_id|>', ' versatile', ' journey', ' through', ' some', 'assistant', '.', 'b', '\n\n', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [1.2630224227905274e-05, 3.542999426523845e-05, 1.7680227756500244e-05]}}, 29: {'grad': {'score': [0.31256205240885415, 0.3507335106967714, 0.3630916226294733, 0.35077919924377043, 0.5109446000079719], 'topk_tokens': ['9', ' made', ' between', ' incorporated', 'made', ' and', ' that', ' dispatch', ' up', ' every', ' After', '5', ' an', 'str', 'latest', '�', '�', 'g', 'APER', 'SP'], 'evidence_proportions': [0.413739013671875, 0.24276987711588544, 0.29077911376953125]}, 'weight': {'score': [0.0032399574915568034, 0.007384552467759231, 0.12917534958931706, 0.006489123052247955, 0.0037842751765737727], 'topk_tokens': [' *\n\n', ' not', '<|start_header_id|>', "'clock", 'made', 'ian', 'ian', 'estead', 'years', ' closely', '<|eot_id|>', '<|eot_id|>', 'b', 'assistant', ' journey', '\n\n', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0012937545776367187, 0.005745251973470052, 0.0019147694110870361]}, 'saliency': {'score': [0.0001533051331837972, 0.0001668646601143129, 0.00085395093887083, 0.00016177754129649417, 0.0002574500988940803], 'topk_tokens': [' versatile', '.', ' *\n\n', 'ian', '\n', '�', ' then', '<|start_header_id|>', '\n', ' some', ' *\n\n', 'human', '<|eot_id|>', '<|eot_id|>', '\n\n', 'assistant', '.', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [2.1755695343017578e-05, 0.0003580947717030843, 1.0557472705841064e-05]}}, 30: {'grad': {'score': [0.6827189127604166, 0.5313004526237927, 0.5742643417850617, 0.5304315938183793, 0.35932396869270167], 'topk_tokens': [' bag', ' a', ' em', ' but', ' trials', ' A', 'ulations', 'ab', ' business', ' web', ' trib', ' bathroom', ' B', 'In', 'ences', ' B', ' and', ' remin', ' of', ' few'], 'evidence_proportions': [0.58673095703125, 0.6996256510416667, 0.77734375]}, 'weight': {'score': [0.002847866217295329, 0.007350023452973496, 0.036361402080905054, 0.007149439807222181, 0.003357955387660435], 'topk_tokens': [' and', '.\n\n', 'Just', ' first', ' then', 'constitutional', ' and', ' location', 'rich', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'assistant', '<|eot_id|>', 'b', '.', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009510159492492677, 0.00549006462097168, 0.001255631446838379]}, 'saliency': {'score': [7.500847180684408e-05, 0.00023187948188718054, 0.0005551970774127591, 0.00023003000508224012, 0.00021261949928439393], 'topk_tokens': [' be', ' tele', '<|start_header_id|>', '<|eot_id|>', ' late', ' result', ' Gov', 'assistant', '<|end_header_id|>', ' first', '.', '-text', ' block', '<|eot_id|>', '<|begin_of_text|>', '<|eot_id|>', ' B', 'b', '<|end_header_id|>', '<|start_header_id|>'], 'evidence_proportions': [3.724098205566406e-05, 0.00013902783393859863, 2.6188790798187256e-05]}}, 31: {'grad': {'score': [0.4769831339518229, 0.6216642605840463, 0.5845012049521169, 0.6224653870909365, 0.8357954998405612], 'topk_tokens': ['<|end_header_id|>', ' morning', ' people', 'latest', ' only', '<|eot_id|>', '♀', 'ian', '\n', '<|eot_id|>', ',', '<|start_header_id|>', ' morning', '\n', ',', '\n', ' and', 'rail', ',', ' acquaintance'], 'evidence_proportions': [0.33342742919921875, 0.5577596028645833, 0.5352630615234375]}, 'weight': {'score': [0.0027849753697713215, 0.00685715078312487, 0.015515963877401045, 0.006807153055919116, 0.0025577393113350383], 'topk_tokens': ['Answer', ' location', ' first', ' and', ' apple', '.\n\n', ' \n', ' Where', ' heading', 'Just', '<|eot_id|>', '<|start_header_id|>', 'assistant', '.', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [0.0016615629196166993, 0.004474163055419922, 0.0016554594039916992]}, 'saliency': {'score': [5.04453976949056e-05, 0.00018446430681202708, 0.0001782880675408148, 0.00018499523207225483, 0.00010299135227592623], 'topk_tokens': ['<|eot_id|>', ' Hor', ' be', ' apple', ' minds', ' value', ' item', ' will', 'Just', '<|eot_id|>', ' Where', ' bedroom', '\n\n', 'assistant', '<|start_header_id|>', ' heading', '<|end_header_id|>', '<|begin_of_text|>', 'b', 'athroom'], 'evidence_proportions': [6.644129753112793e-05, 6.0752034187316895e-05, 1.4990568161010742e-05]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-23 22:31:33.119 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:31:33.119 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-14_pid-0_0-1-6-7.pkl | len: 10 |  size: 8.63 KB
Processing depth (0, 1, 6, 7):   1%|          | 1/100 [00:12<21:26, 13.00s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Processing depth (1, 3, 4, 8):   1%|          | 1/100 [00:22<21:26, 13.00s/it]2025-01-23 22:31:42.898 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the apple.
2025-01-23 22:31:42.900 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (463, 467) -->  Mary took the apple
2025-01-23 22:31:42.900 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:31:42.908 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1541, 1546) --> . Mary moved to the
2025-01-23 22:31:42.908 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:31:42.918 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1885, 1891) --> . Mary journeyed to the
2025-01-23 22:31:42.918 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:31:42.934 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3358, 3362) -->  Daniel left the apple
2025-01-23 22:31:42.934 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:31:42.952 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3689, 3694) -->  cold. Daniel picked up
2025-01-23 22:31:42.952 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:31:42.970 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2943, 2948) --> . Sandra moved to the
2025-01-23 22:31:42.970 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:31:42.973 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (539, 544) --> . John moved to the
2025-01-23 22:31:42.973 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:31:42.986 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2484, 2490) --> . Sandra journeyed to the
2025-01-23 22:31:42.986 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the office.
2025-01-23 22:31:43.000 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2801, 2807) --> . John went back to the
2025-01-23 22:31:43.000 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel took the football.
2025-01-23 22:31:43.009 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1829, 1833) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:31:43.521 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The garden.<|eot_id|>
2025-01-23 22:31:43.521 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4212])
your chose emoji: ['🏳️\u200d⚧️', '👩🏽\u200d🎤', '🐬', '🔜', '⏯️', '👨\u200d👩\u200d👦\u200d👦', '🤘🏻', '🇹🇿', '🙋🏼\u200d♂', '🇨🇮']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4215, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 209715.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.11it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.40it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.08it/s]
2025-01-23 22:31:46.570 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.35843859220805924, 0.35646470546156767, 0.3702759281281502, 0.35635290432090805, 0.4619347478302432], 'topk_tokens': [' of', 'ian', 'announcement', ' bathroom', ' P', ' exc', ' the', ' THE', 'ian', ' out', ' Hor', ' said', 'ian', '202', 'ian', ' considerable', 'ian', 'ex', 'ly', ' slow'], 'evidence_proportions': [0.3658294677734375, 0.40902099609375003, 0.4247283935546875, 0.188385009765625]}, 'weight': {'score': [0.011927639183245207, 0.007461626136571629, 0.2803442314747841, 0.005410193960492064, 0.0017024731971848179], 'topk_tokens': [' bedroom', ' apple', 'user', '\n\n', ' Where', ' \n', '<|start_header_id|>', ' bathroom', '\n\n', ':', '\n\n', '<|eot_id|>', 'b', 'Answer', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.012633860111236572, 0.009632623195648194, 0.01962823669115702, 0.0025392919778823853]}, 'saliency': {'score': [0.0004374541734394274, 0.00010011507947249769, 0.00039298688211748676, 9.63963595043425e-05, 3.764369118381554e-05], 'topk_tokens': [' Mary', ' bathroom', '\n', ':', ' bedroom', 'user', '<|eot_id|>', 'assistant', '.', 'Question', ' ', '<|begin_of_text|>', ' before', '\n\n', '<|end_header_id|>', '\n\n', 'Answer', ' Where', 'athroom', 'b'], 'evidence_proportions': [0.00035437196493148804, 0.0002866387367248535, 0.0008876969416936238, 3.369152545928955e-05]}}, 25: {'grad': {'score': [0.7103476273386102, 0.5289252918103129, 0.5914681957614037, 0.527632169505986, 0.4787144190828565], 'topk_tokens': [' or', ' the', ' the', ' inverted', ' to', 'at', ' in', ' in', ' to', ' to', ' res', ' the', ' in', ' the', ' to', ' of', ' his', ' of', ' the', ' into'], 'evidence_proportions': [0.44212913513183594, 0.8667541503906251, 0.6919047037760416, 0.8107223510742188]}, 'weight': {'score': [0.009232847314131888, 0.0073325296313737215, 0.3220247568622712, 0.004981613645748216, 0.0021825739195649053], 'topk_tokens': [' before', ' bedroom', ' Mary', '\n\n', ' bathroom', 'b', ' Bench', '?', ' \n', '<|eot_id|>', ':', '<|start_header_id|>', '<|eot_id|>', 'Answer', 'assistant', '\n\n', 'athroom', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00621071457862854, 0.01248013973236084, 0.013507723808288574, 0.0017835497856140137]}, 'saliency': {'score': [0.0003517223031897294, 9.85538620117296e-05, 0.0014755235564324163, 8.715019792783446e-05, 5.517123450695629e-05], 'topk_tokens': [' Mary', ' write', 'MIN', ' bathroom', ' Daniel', ' ', '<|start_header_id|>', 'athroom', ' \n', '<|eot_id|>', ' return', ' Bench', '<|eot_id|>', 'Answer', ':', 'assistant', '<|begin_of_text|>', '.', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [0.0002717897295951843, 0.0005882382392883301, 0.0004053364197413127, 5.558878183364868e-05]}}, 26: {'grad': {'score': [0.4187509637129934, 0.480392771004782, 0.5737048733618951, 0.47997945027620426, 0.4272391762531979], 'topk_tokens': [' execution', 'hue', 'ab', ' looked', ' of', ' Press', ' When', ' Paul', ' when', ' Eagle', 'most', 'agle', ' Paul', ' Wood', ' Press', ' trib', 'Johnson', 'b', ' Press', ' John'], 'evidence_proportions': [0.5215835571289062, 0.4509124755859375, 0.4390462239583333, 0.24527359008789062]}, 'weight': {'score': [0.007176981160515233, 0.007189816285989746, 0.27664496629468854, 0.005184326302103636, 0.0035566746349066074], 'topk_tokens': [' Bench', ' before', '\n\n', '<|eot_id|>', 'b', ' apple', ' bathroom', ' bedroom', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'Answer', ' \n', '?', ':', '<|end_header_id|>', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.004292339086532593, 0.007224369049072266, 0.010322233041127522, 0.005284510552883148]}, 'saliency': {'score': [0.0005067132021251478, 0.00011878704402511796, 0.0010121714684271043, 0.00011036795299021708, 0.00010592039202300595], 'topk_tokens': ['.\n\n', ' Sandra', ' Mary', 'UL', '<|eot_id|>', '\n\n', '\n\n', ' Mary', ' ', 'RE', 'assistant', ' bathroom', '\n\n', 'Answer', '.', '<|end_header_id|>', 'b', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0004985257983207703, 0.0006511509418487549, 0.0007058084011077881, 3.571063280105591e-05]}}, 27: {'grad': {'score': [0.3844857466848273, 0.37650379619847274, 0.33584468595443234, 0.376770008769499, 0.41419273698833625], 'topk_tokens': [' report', ' work', ' book', ' step', 'an', ' state', ' West', 'ides', ' minds', ' ', ' expedition', ' business', ' other', ' business', ' Jackson', ' nineteenth', 'ers', 'ad', ' prev', ' medicine'], 'evidence_proportions': [0.46268463134765625, 0.4167221069335938, 0.38158671061197913, 0.2703399658203125]}, 'weight': {'score': [0.009794887743498149, 0.007361738763811467, 0.32068771316159156, 0.005018561083872636, 0.003637133349835033], 'topk_tokens': [' Where', 'Question', ' before', 'NEW', ' bedroom', '<|start_header_id|>', ' \n', '<|eot_id|>', ' bathroom', ' apple', '\n\n', 'b', 'Answer', 'assistant', '?', '<|end_header_id|>', ':', 'athroom', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.00733029842376709, 0.01160576343536377, 0.01320509115854899, 0.004880577325820923]}, 'saliency': {'score': [0.000527339546304, 0.00015669581021843996, 0.0012502228060076313, 0.00014686590196991882, 0.00010753265568907832], 'topk_tokens': ['"The', 'PA', 'Question', ' \n', '\n\n', '\n\n', 'Answer', ' before', ' Where', 'RE', 'assistant', ' bathroom', ':', '<|begin_of_text|>', '?', 'NEW', '.', 'athroom', '<|end_header_id|>', 'b'], 'evidence_proportions': [0.0006853565573692322, 0.0006692171096801757, 0.0005893210570017496, 9.900331497192383e-05]}}, 28: {'grad': {'score': [0.25115655597887543, 0.39973499302620846, 0.30392699087819747, 0.4011258785893508, 0.30688197176221393], 'topk_tokens': [' containing', 'body', 'nes', ' from', ' spring', ' instead', ' speakers', ' summer', ' instance', ' balance', ' about', ' season', ' face', '.', ' requiring', 'ball', ' instead', 'E', ' inside', ' the'], 'evidence_proportions': [0.1467299461364746, 0.238812255859375, 0.3304589589436849, 0.2520599365234375]}, 'weight': {'score': [0.008576038636659322, 0.0070759833212566264, 0.3342676854902698, 0.004633859955534643, 0.002034865634542116], 'topk_tokens': [' bathroom', 'Question', '<|eot_id|>', ' the', '<|start_header_id|>', '<|eot_id|>', 'b', ' bedroom', ' before', 'Answer', 'assistant', ' apple', ' \n', '\n\n', ':', '?', '<|end_header_id|>', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0013983100652694702, 0.008158254623413085, 0.011437416076660156, 0.011983931064605713]}, 'saliency': {'score': [0.00022087442247491134, 8.722658259164396e-05, 0.002230697101162326, 7.066310239153034e-05, 3.632105572122923e-05], 'topk_tokens': [' bathroom', 'Question', '.\n\n', ' the', '.\n\n', ' was', ' Mary', ' before', 'Answer', 'b', ' \n', ' apple', '?', ':', 'athroom', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n', '.'], 'evidence_proportions': [9.609013795852661e-05, 0.00014213323593139648, 0.00043491025765736896, 0.00012303143739700317]}}, 29: {'grad': {'score': [0.521192048725329, 0.3178457456975645, 0.42470575148059475, 0.31612275891802033, 0.3907013879695409], 'topk_tokens': [' come', ' prize', '\n', 'pend', ' river', 'rate', ' Bench', 'super', ' Pioneer', ' com', ' journey', ' journey', 'stage', ' an', 'assistant', ' H', ' l', ' m', ' a', ' res'], 'evidence_proportions': [0.3780231475830078, 0.419537353515625, 0.5899340311686198, 0.6883163452148438]}, 'weight': {'score': [0.004944318219235069, 0.00718557305748918, 0.39541879488575843, 0.0043061862544280715, 0.003008777826604709], 'topk_tokens': [' the', '      ', '<|eot_id|>', '\n\n', '<|eot_id|>', '<|start_header_id|>', ' bedroom', 'b', ' apple', 'Answer', ' before', ' \n', 'assistant', '?', '\n\n', '<|end_header_id|>', ':', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0007849633693695068, 0.004094940423965454, 0.005585233370463054, 0.009204022586345673]}, 'saliency': {'score': [5.222935425607782e-05, 0.000108094274785595, 0.0033245759625588696, 8.440892092463205e-05, 0.00012828514609538333], 'topk_tokens': ['<|eot_id|>', '\n\n', '\n\n', '<|eot_id|>', 'Question', ' apple', '<|eot_id|>', 'b', ' \n', 'athroom', '?', ' ', ' before', '\n\n', 'assistant', '\n\n', ':', '<|end_header_id|>', '<|begin_of_text|>', '.'], 'evidence_proportions': [1.5810132026672363e-05, 5.592703819274902e-05, 6.670256455739339e-05, 6.23166561126709e-05]}}, 30: {'grad': {'score': [0.5849510995965255, 0.450376669452708, 0.4672152611517137, 0.4496374352353246, 0.35379130403760456], 'topk_tokens': ['name', ' fight', 'ree', ' to', ' and', '      ', '      ', ' in', '202', '-four', ' B', '202', ' ', ' part', ' B', '      ', ' an', ' concerned', ' to', 'b'], 'evidence_proportions': [0.5397415161132812, 0.641107177734375, 0.4682769775390625, 0.7349767684936523]}, 'weight': {'score': [0.01386452976026033, 0.007208223670968523, 0.2895963143917822, 0.005076050651030523, 0.00924426233264762], 'topk_tokens': ['�', ' Where', ' bedroom', 'Question', ' apple', '<|start_header_id|>', 'b', '<|eot_id|>', 'Answer', '<|start_header_id|>', 'assistant', ' \n', '<|eot_id|>', '\n\n', '?', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.004203349351882935, 0.01725006103515625, 0.023601531982421875, 0.004688292741775513]}, 'saliency': {'score': [0.00042454976784555534, 0.00015827845848327853, 0.0006972157186077487, 0.00015305247770494918, 0.0002389978355085346], 'topk_tokens': [' bedroom', ' Where', ' apple', ' ', ' Mary', '<|start_header_id|>', '<|eot_id|>', 'ANK', '<|eot_id|>', ':', 'b', ' bathroom', 'Answer', 'assistant', '.', ' \n', '?', '<|begin_of_text|>', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [0.00015860050916671753, 0.00043358802795410154, 0.0008201847473780314, 8.574873208999634e-05]}}, 31: {'grad': {'score': [0.3987083184091668, 0.5830586534095121, 0.4426340518459197, 0.5849448044331563, 0.20007061622512173], 'topk_tokens': [' expected', ' concerned', ' that', 'user', ' location', ' dollars', ' question', ' to', ' night', ' location', 'Answer', ' paper', ' location', ' location', ' location', ' part', ' location', ' city', '1', ' location'], 'evidence_proportions': [0.38346171379089355, 0.46618309020996096, 0.33387287457784015, 0.4268646240234375]}, 'weight': {'score': [0.003924438827916195, 0.006946308955037579, 0.2901122800765499, 0.004852496332624236, 0.0027665722538048115], 'topk_tokens': [':', '.\n\n', ' Where', 'Question', ' before', ' bedroom', '<|start_header_id|>', 'Answer', '<|eot_id|>', 'b', '?', '<|eot_id|>', ' \n', ':', 'assistant', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0019229650497436523, 0.0057852745056152345, 0.005302786827087402, 0.0015323460102081299]}, 'saliency': {'score': [5.655382808886076e-05, 9.687516047295586e-05, 0.0009726949276462678, 9.054039277377821e-05, 4.665532582242724e-05], 'topk_tokens': [':', ':', ' bathroom', ' before', ' Where', '.\n\n', '\n\n', ' apple', '<|eot_id|>', '<|eot_id|>', 'Question', ' bedroom', ' \n', 'Answer', '<|begin_of_text|>', 'athroom', 'b', '.', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [7.355213165283203e-05, 7.132291793823242e-05, 5.207459131876628e-05, 2.781301736831665e-05]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-23 22:31:46.577 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:31:46.577 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-14_pid-1_1-3-4-8.pkl | len: 10 |  size: 8.95 KB
Processing depth (1, 3, 4, 8):   2%|▏         | 2/100 [00:26<21:40, 13.27s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.35it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.19it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Processing depth (4, 5, 6, 7):   2%|▏         | 2/100 [00:35<21:40, 13.27s/it]2025-01-23 22:31:55.411 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the apple.
2025-01-23 22:31:55.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1797, 1801) -->  Mary took the apple
2025-01-23 22:31:55.421 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:31:55.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2090, 2095) --> . Mary moved to the
2025-01-23 22:31:55.432 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:31:55.444 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2397, 2403) -->  the senate. Mary journeyed
2025-01-23 22:31:55.444 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:31:55.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2856, 2860) -->  Daniel left the apple
2025-01-23 22:31:55.458 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:31:55.476 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3655, 3660) --> . Daniel picked up the
2025-01-23 22:31:55.476 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:31:55.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2905, 2910) --> . Sandra moved to the
2025-01-23 22:31:55.491 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:31:55.493 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (534, 539) --> . John moved to the
2025-01-23 22:31:55.494 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:31:55.506 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2455, 2461) --> . Sandra journeyed to the
2025-01-23 22:31:55.506 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the office.
2025-01-23 22:31:55.520 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2739, 2745) -->  John went back to the office
2025-01-23 22:31:55.520 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel took the football.
2025-01-23 22:31:55.529 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1770, 1774) -->  took the football.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:31:56.245 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The apple was in Mary's hand.<|eot_id|>
2025-01-23 22:31:56.245 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4203])
your chose emoji: ['🧚🏼', '👉🏽', '🧮', '💂🏾\u200d♂️', '👩🏻\u200d💼', '👦🏽', '👉🏼', '🏂🏿', '💚', '🚶\u200d♂\u200d➡']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4206, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 231409.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.72it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.14it/s]
2025-01-23 22:31:59.133 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.24424061022306742, 0.3132885666727963, 0.2887389890609249, 0.31378735109977246, 0.34894032632150956], 'topk_tokens': ['.', '.', '202', ' out', '.', 'were', ' Do', ' ', '.', ' the', '.', ' the', ',', 'ed', 'remark', ' competition', 'the', ',', ' out', '�'], 'evidence_proportions': [0.16265869140625, 0.26478729248046873, 0.31179046630859375, 0.19881439208984375]}, 'weight': {'score': [0.01864777270116304, 0.00749588058043592, 0.00845785871628792, 0.00743772194893803, 0.0015763313539566533], 'topk_tokens': ['ree', ' Wright', ' office', ' Fourth', ' bedroom', 'Bridge', ' Bench', '<|eot_id|>', 'assistant', ' Foster', ':', ' bedroom', '\n\n', '<|eot_id|>', 'b', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01777029037475586, 0.02901477813720703, 0.01237591604391734, 0.01597428321838379]}, 'saliency': {'score': [0.0011972361489346153, 9.679709283499506e-05, 0.00040558076673938385, 8.946296483765896e-05, 2.6927359642521027e-05], 'topk_tokens': [' United', 'ree', ' ', '.', ' Mary', '<|eot_id|>', ' Fourth', ' Mary', ' journey', ' Foster', ' Senator', '<|eot_id|>', 'Bridge', 'b', '<|start_header_id|>', ' office', ' Bench', ' bedroom', 'athroom', ' bathroom'], 'evidence_proportions': [0.001407906413078308, 0.001107358932495117, 0.0013647923866907756, 0.0008475780487060547]}}, 25: {'grad': {'score': [0.49194637097810445, 0.6203197092880706, 0.4146614074707031, 0.622440619005161, 0.41699034936966434], 'topk_tokens': [' ', ' the', ' the', ' old', ' the', ' the', ' in', ' the', ' the', ' old', ' old', ' the', 'old', ' a', ' l', ' old', ' old', ' inverted', ' York', 'ivery'], 'evidence_proportions': [0.5518598556518555, 0.4358123779296875, 0.44407145182291663, 0.5740127563476562]}, 'weight': {'score': [0.026809460238406534, 0.00725642806056336, 0.006828278303146362, 0.007170231003561654, 0.0028043571979768814], 'topk_tokens': [' Paul', ' Paul', ' Mary', 'b', 'ree', 'Answer', ' Senator', ' Mary', '<|eot_id|>', '.', 'assistant', '<|eot_id|>', ':', ' Bench', ' bathroom', '<|start_header_id|>', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.04790997505187988, 0.02807893753051758, 0.004801412423451741, 0.03713417053222656]}, 'saliency': {'score': [0.0026112826246964304, 0.0001229943758070724, 0.00015184283256530762, 0.00011140347617078676, 8.416944934475807e-05], 'topk_tokens': ['<|start_header_id|>', 'assistant', ' Foster', ':', 'ree', 'Answer', ' Senator', ' St', ' There', ' Daniel', ' Paul', ' Paul', ' Mary', ' bathroom', '.', ' Mary', ' Bench', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.006050124764442444, 0.002602177858352661, 0.00043696165084838867, 0.002445302903652191]}}, 26: {'grad': {'score': [0.26636565359015213, 0.33408184840573735, 0.2598237068422379, 0.3349453253282505, 0.42955518537952053], 'topk_tokens': ['hue', ' God', ' he', ' Eagle', ' messenger', 'ree', ' and', ' galaxy', ' Press', 'b', 'RI', ' Gutenberg', ' considerable', ' Marshall', ' bend', ' when', ' Press', ' favor', ' and', ' discover'], 'evidence_proportions': [0.2216653823852539, 0.258245849609375, 0.29430643717447913, 0.27930450439453125]}, 'weight': {'score': [0.014209132445485969, 0.0072139244787023, 0.0042822476356260235, 0.007203812118443993, 0.001744979812252906], 'topk_tokens': [' corner', 'Answer', ' apple', '<|eot_id|>', ' bedroom', ' \n', ' the', 'Bridge', ' bedroom', '<|eot_id|>', 'assistant', ' Bench', 'b', '\n\n', ' bathroom', '<|end_header_id|>', 'athroom', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.020208120346069336, 0.011085200309753417, 0.0045954982439676915, 0.026535511016845703]}, 'saliency': {'score': [0.0005908843718077007, 0.00012538585983680873, 0.00015413857275439847, 0.0001230432694066133, 4.974103743030179e-05], 'topk_tokens': [' formally', ' Mary', '<|eot_id|>', '<|eot_id|>', ' Bench', ' corner', ' fall', 'assistant', 'Bridge', 'ree', ' Mary', '<|start_header_id|>', ' Fourth', 'athroom', ' bathroom', ':', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', 'b'], 'evidence_proportions': [0.0015867650508880615, 0.0005338370800018311, 9.792546431223552e-05, 0.00040575116872787476]}}, 27: {'grad': {'score': [0.3616181423789577, 0.41962671211884806, 0.41650030113035635, 0.41991523030116307, 0.39815355116321194], 'topk_tokens': [' completed', ' work', ' platform', ' concerned', ',\n', ' excessive', ' for', ' four', ' execution', ' duration', ' December', 'ides', ' started', ' conventions', ' other', 'ers', ' convention', 'event', ' step', ' conventions'], 'evidence_proportions': [0.24695825576782227, 0.5532089233398437, 0.28988901774088544, 0.34438323974609375]}, 'weight': {'score': [0.019485779498752794, 0.0073784450888350304, 0.00598721734939083, 0.007333471245267738, 0.0023954529916086504], 'topk_tokens': [' bend', ' \n', 'ree', 'Answer', ' Fourth', ' apple', ' Bench', '.', ' bedroom', 'assistant', ' bedroom', 'b', '\n\n', '.\n\n', '<|end_header_id|>', ':', '<|start_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.025281429290771484, 0.023584580421447756, 0.005126630266507466, 0.0301053524017334]}, 'saliency': {'score': [0.0015444159507751465, 0.0001243826000111136, 0.0004914537552864321, 0.00011515260976134181, 5.699501883599066e-05], 'topk_tokens': [' St', ' apple', ' kitchen', '<|end_header_id|>', 'assistant', 'NEW', ' Daniel', '.', 'Bridge', ' the', 'ree', ' bathroom', ' the', ' bedroom', '<|start_header_id|>', ' bedroom', 'b', '<|begin_of_text|>', '.\n\n', 'athroom'], 'evidence_proportions': [0.0008604153990745544, 0.0031287550926208493, 0.0001242359479268392, 0.0023782625794410706]}}, 28: {'grad': {'score': [0.2950937371504934, 0.32663563190048445, 0.2920922463940036, 0.32703749449695957, 0.30126430142310356], 'topk_tokens': ['.', '      ', 'nes', ' and', ' the', 'half', ' ', 'ern', 'nes', 'nes', ',', 'S', ' the', 'E', '<|end_header_id|>', '      ', 'nes', 'ot', '.', 'nes'], 'evidence_proportions': [0.28023529052734375, 0.275335693359375, 0.33979034423828125, 0.2676048278808594]}, 'weight': {'score': [0.009512484073638916, 0.007028503073321826, 0.006583606043169575, 0.007020465577876373, 0.0015853412689701204], 'topk_tokens': [' Fourth', 'Bridge', 'Answer', ' the', ' bedroom', ' \n', '?', '<|eot_id|>', ' apple', ' before', 'assistant', ' bedroom', ' bathroom', 'b', '<|end_header_id|>', '\n\n', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0035881996154785156, 0.019610559940338133, 0.002862592538197835, 0.012789011001586914]}, 'saliency': {'score': [0.00044144454755281145, 0.00011377925114352761, 0.0001090505430775304, 0.00011231653442511316, 2.819347766137892e-05], 'topk_tokens': ['.', 'b', ' on', ' the', ' the', ' of', ' St', ' bathroom', ' bedroom', ' Fourth', ' apple', ' bedroom', 'assistant', '<|end_header_id|>', 'athroom', 'Bridge', '\n\n', '<|begin_of_text|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.00014506280422210693, 0.0013898491859436035, 2.7760863304138184e-05, 0.0001728460192680359]}}, 29: {'grad': {'score': [0.20973526804070725, 0.36100465753675925, 0.27463026969663556, 0.3623404911323966, 0.47901818060105844], 'topk_tokens': [' Pioneer', ' was', ' an', ' writer', ' value', '.', ' temper', ' were', ' work', ' idea', ' *', 'antics', ' not', 'itated', ' extra', 'ely', ' facts', 'assistant', ' enough', ':'], 'evidence_proportions': [0.1365966796875, 0.20013885498046877, 0.29268773396809894, 0.170440673828125]}, 'weight': {'score': [0.010621613577792519, 0.007158661478199508, 0.0032569602612526186, 0.007171933048900443, 0.0011887396535565776], 'topk_tokens': [' Where', ' to', ' the', ' of', 'Answer', ' bedroom', '<|eot_id|>', '.\n\n', '?', ' \n', 'nes', 'assistant', ' before', 'b', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007323741912841797, 0.021825075149536133, 0.0021054844061533613, 0.012689352035522461]}, 'saliency': {'score': [0.0003081277797096654, 0.00010517792252773225, 0.0001327068574966923, 0.00010404475499392703, 4.090753293806507e-05], 'topk_tokens': [' to', ' Where', ' the', ' Sandra', '.', '<|eot_id|>', ' kitchen', '<|eot_id|>', ' which', ' the', ':', 'athroom', '\n\n', 'assistant', 'nes', '<|end_header_id|>', ' before', 'b', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [0.00020553171634674072, 0.0007472217082977294, 9.157260258992514e-05, 0.00018668919801712036]}}, 30: {'grad': {'score': [0.3145286158511513, 0.43364272614178406, 0.3932976876535723, 0.43448821803029625, 0.3367882274812268], 'topk_tokens': [' concerned', '202', ' Paul', ' said', 'b', ' have', '-four', ' method', ' fact', '202', '      ', ' day', ' be', ' for', 'vent', '      ', '7', ' an', ' part', ' at'], 'evidence_proportions': [0.20713043212890625, 0.25592651367187497, 0.3406639099121094, 0.4559764862060547]}, 'weight': {'score': [0.024577663133018894, 0.007223856205380195, 0.006720523680410077, 0.007148274149713434, 0.003809080969902777], 'topk_tokens': [' bedroom', '<|eot_id|>', 'Question', ' before', '.\n\n', ' Mary', '?', ' Fourth', 'Answer', ' bedroom', 'assistant', ' \n', 'b', ' bathroom', '<|end_header_id|>', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.03917205333709717, 0.026249456405639648, 0.01005604366461436, 0.029675960540771484]}, 'saliency': {'score': [0.0020672129957299483, 0.00016541217147765476, 0.0002646984592560799, 0.00015597711599595968, 5.0400053301165184e-05], 'topk_tokens': [' bedroom', ' the', ' Fourth', '.\n\n', ' apple', 'assistant', ' St', ' Senator', ' Bench', ' Mary', 'Bridge', ' bedroom', '<|begin_of_text|>', ' Mary', 'b', '<|end_header_id|>', 'athroom', ' bathroom', ':', '<|start_header_id|>'], 'evidence_proportions': [0.004815496504306793, 0.0020277678966522214, 0.000674709677696228, 0.0014569908380508423]}}, 31: {'grad': {'score': [0.32077147929291977, 0.37068397058966723, 0.2866139421539922, 0.3715392420564969, 0.2241397250083185], 'topk_tokens': [' of', ' the', ' the', "'clock", 'hours', ' the', ' the', ' the', ' the', ' the', 'user', ' location', ' the', ' hours', ' the', ' the', ' the', ' happened', ' have', 'If'], 'evidence_proportions': [0.34096986055374146, 0.28811289668083195, 0.30855971574783325, 0.35971397161483765]}, 'weight': {'score': [0.006029370583985981, 0.006602959807464411, 0.0033058524131774902, 0.006630175525575331, 0.0014821156378715269], 'topk_tokens': [' Where', '<|eot_id|>', '.\n\n', ' apple', '?', ' before', 'Answer', ' the', ' bathroom', ' \n', ' bedroom', 'assistant', '<|eot_id|>', 'b', ':', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.008812189102172852, 0.007816052436828614, 0.0026368995507558184, 0.0061019062995910645]}, 'saliency': {'score': [0.00022405542825397693, 6.118872394688743e-05, 8.379163280610115e-05, 6.027554840394462e-05, 1.1721926350747385e-05], 'topk_tokens': [' the', 'Bridge', ' People', '.\n\n', '<|eot_id|>', ' before', 'Answer', ' Mary', ' Mary', ' apple', ' the', ' \n', ' the', '<|begin_of_text|>', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', 'athroom'], 'evidence_proportions': [0.00039079785346984863, 0.00037673115730285645, 7.228553295135498e-05, 9.412318468093872e-05]}}, 'pred_res': "The apple was in Mary's hand.<|eot_id|>", 'score': 0}
2025-01-23 22:31:59.141 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:31:59.141 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-14_pid-2_4-5-6-7.pkl | len: 10 |  size: 9.14 KB
Processing depth (4, 5, 6, 7):   3%|▎         | 3/100 [00:39<20:55, 12.95s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (1, 3, 4, 9):   3%|▎         | 3/100 [00:48<20:55, 12.95s/it]2025-01-23 22:32:09.018 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the apple.
2025-01-23 22:32:09.020 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (405, 409) -->  Mary took the apple
2025-01-23 22:32:09.020 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:32:09.027 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1440, 1445) --> . Mary moved to the
2025-01-23 22:32:09.027 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:32:09.037 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1835, 1841) --> . Mary journeyed to the
2025-01-23 22:32:09.037 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:32:09.055 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3668, 3672) -->  left the apple.
2025-01-23 22:32:09.055 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:32:09.077 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3699, 3704) -->  cold. Daniel picked up
2025-01-23 22:32:09.077 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:32:09.092 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2925, 2930) --> . Sandra moved to the
2025-01-23 22:32:09.092 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:32:09.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (444, 449) --> . John moved to the
2025-01-23 22:32:09.094 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:32:09.107 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2476, 2482) --> . Sandra journeyed to the
2025-01-23 22:32:09.107 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the office.
2025-01-23 22:32:09.121 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2763, 2769) --> . John went back to the
2025-01-23 22:32:09.121 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel took the football.
2025-01-23 22:32:09.130 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1805, 1809) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:32:09.645 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The garden.<|eot_id|>
2025-01-23 22:32:09.646 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4219])
your chose emoji: ['👩🏿\u200d❤️\u200d💋\u200d👩🏻', '🙆🏼\u200d♀', '🎱', '🥷🏾', '🧙🏾\u200d♂️', '🧔🏾\u200d♀', '🇹🇳', '🇬🇳', '🏇🏻', '👨\u200d✈']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4222, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 223696.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.04it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.85it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.69it/s]
2025-01-23 22:32:12.528 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.21241087662546257, 0.16793041147930482, 0.18615242742723034, 0.16759244136133677, 0.1650244150406275], 'topk_tokens': ['.', ' and', '600', ':', ' means', ',', 'RI', ' to', 'ION', ' discover', '.', ' under', ' to', ' things', ' IN', '.', ' these', 'EF', ' to', ' to'], 'evidence_proportions': [0.20493316650390625, 0.22284584045410158, 0.274163564046224, 0.11421585083007812]}, 'weight': {'score': [0.0010502103127931293, 0.007504225567809104, 0.004233588134088824, 0.007557920690122295, 0.004436476872517512], 'topk_tokens': ['<|start_header_id|>', '<|eot_id|>', 'b', ' devil', ' *\n\n', 'ian', ' as', '<|eot_id|>', 'le', 'es', 'assistant', 'ot', 'pleasant', ' of', '."', 'athroom', ' proprietor', ' headlines', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00015905499458312988, 0.0005830883979797363, 0.0002964884042739868, 0.003655850887298584]}, 'saliency': {'score': [2.5568824065359014e-05, 8.154254934337115e-05, 6.940576337998914e-05, 8.18876454951322e-05, 6.343309695904072e-05], 'topk_tokens': ['b', ' happened', ' of', 'user', 'ian', ' time', '\n\n', ' genu', 'le', ' as', ' of', '<|begin_of_text|>', ' headlines', 'system', '<|start_header_id|>', '<|eot_id|>', ' proprietor', 'assistant', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [6.318092346191406e-06, 1.850724220275879e-05, 9.313225746154785e-06, 7.802993059158325e-05]}}, 25: {'grad': {'score': [0.33922115125154195, 0.3466196787521095, 0.31923011041456656, 0.3468568907945291, 0.3256186032906557], 'topk_tokens': [' Good', ' full', ' as', ' an', ' soon', ' summer', 'UL', ' man', 'MIN', ' by', ' afterward', ' Think', 'b', 'hand', ' than', ' was', ' lesser', ' fifty', '\n\n\n\n\n\n\n', ' web'], 'evidence_proportions': [0.4309253692626953, 0.2975196838378906, 0.27831268310546875, 0.3910064697265625]}, 'weight': {'score': [0.001296146919852809, 0.007503237559410251, 0.004046387249423611, 0.00755719179760804, 0.003939806268765376], 'topk_tokens': ['<|eot_id|>', 'athroom', ' of', 'es', 'assistant', ' *\n\n', ' devil', 'pleasant', ' of', ' as', 'le', ' speakers', 'ot', ' out', ' proprietor', '."', ' headlines', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002559646964073181, 0.0009296178817749023, 0.0006876538197199504, 0.0037072300910949707]}, 'saliency': {'score': [2.509198690715589e-05, 0.00010591434788670037, 7.39722482619747e-05, 0.00010651977213094241, 5.872433002178486e-05], 'topk_tokens': ['user', 'called', 'men', ' news', ' Newspaper', ' time', '\n\n\n', 'b', "'clock", ' man', '<|start_header_id|>', ' speakers', ' out', '<|eot_id|>', '<|eot_id|>', 'athroom', '<|begin_of_text|>', 'assistant', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [4.798173904418945e-06, 3.3807754516601565e-05, 1.66247288386027e-05, 4.719197750091553e-05]}}, 26: {'grad': {'score': [0.3719283656070107, 0.32071168611310547, 0.3354967794110698, 0.32036857614124037, 0.32216905936216694], 'topk_tokens': ['s', 'posit', ' and', ' the', 'ION', ' Good', ' Joseph', ' had', ' em', ' so', ' em', ' com', ' a', '240', 'ION', '600', ' a', 'Good', 'user', 'system'], 'evidence_proportions': [0.4208240509033203, 0.33181076049804686, 0.39215850830078125, 0.34283447265625]}, 'weight': {'score': [0.002813979199058131, 0.007524007872234075, 0.005516567537861485, 0.007560374409903273, 0.006023330566210625], 'topk_tokens': ['b', '\n\n', 'assistant', 'athroom', ' *\n\n', ' of', ' devil', ' on', ' proprietor', 'pleasant', ' as', '<|eot_id|>', 'le', 'ot', '<|start_header_id|>', '<|eot_id|>', ' headlines', '."', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0003204047679901123, 0.003982639312744141, 0.0015783905982971191, 0.005700111389160156]}, 'saliency': {'score': [5.190466579638029e-05, 0.00017192488962414703, 0.00012376135395419214, 0.0001728293608270464, 0.00023314738884950295], 'topk_tokens': [' Reporter', ' bend', 'ollar', ' laid', 'event', ' Gal', 'ared', '\n\n', '�', '<|end_header_id|>', '<|begin_of_text|>', ' hung', 'estead', ' Pioneer', ' had', ' tie', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'cuts'], 'evidence_proportions': [8.016824722290039e-06, 8.467435836791992e-05, 3.5087267557779946e-05, 8.005648851394653e-05]}}, 27: {'grad': {'score': [0.43392823871813324, 0.48917850438366, 0.49368027717836444, 0.48939667314948193, 0.5156544294112768], 'topk_tokens': [' and', ' Republicans', 'ails', 'a', ' lesser', 'his', ' impressed', ' Williams', 'nes', ' take', ' Grow', ' Grow', '\n', 'es', 'ared', ' enraged', 'nes', 'ern', 'cuts', "'s"], 'evidence_proportions': [0.4160308837890625, 0.6211639404296875, 0.394012451171875, 0.27765464782714844]}, 'weight': {'score': [0.0013664490298220986, 0.007524706299753338, 0.003808802173983666, 0.007580363038973749, 0.004695024245824569], 'topk_tokens': ['athroom', 'ian', ' out', ' of', 'assistant', 'ian', ' devil', ' on', 'es', 'pleasant', ' as', ' *\n\n', 'ot', ' of', 'le', ' proprietor', ' headlines', '<|end_header_id|>', '."', '<|begin_of_text|>'], 'evidence_proportions': [0.0005063563585281372, 0.0015997171401977539, 0.0006132523218790691, 0.003064751625061035]}, 'saliency': {'score': [9.95162286256489e-05, 0.0002557600083818711, 0.00017889276627571353, 0.0002570427304146274, 0.0003828234397448026], 'topk_tokens': ['<|end_header_id|>', ' on', ' *\n\n', 'athroom', ' as', 'es', ' enter', 'pleasant', 'ot', '<|start_header_id|>', ' of', ' headlines', ' proprietor', 'le', '."', '\n\n', 'cuts', '<|begin_of_text|>', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [3.7163496017456055e-05, 0.0002108633518218994, 3.6234656969706215e-05, 0.00011760741472244263]}}, 28: {'grad': {'score': [0.6282553421823602, 0.6860008366240081, 0.6896618258568549, 0.6862366165205022, 0.6333399063501602], 'topk_tokens': [' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' a', ' A', ' a', ' A', ' a', ' A', ' A', ' a', ' a'], 'evidence_proportions': [0.4910869598388672, 0.7405517578125, 0.6486968994140625, 0.594390869140625]}, 'weight': {'score': [0.0010623037815093994, 0.007338375686074478, 0.0030088684251231533, 0.007399128344577888, 0.0026330286875749244], 'topk_tokens': ['ographical', '<|start_header_id|>', 'es', ' as', 'pleasant', ' devil', 'le', 'assistant', '<|eot_id|>', 'ot', ' *\n\n', ' of', 'b', '."', ' proprietor', ' headlines', '\n\n', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00012554973363876343, 0.0017745018005371094, 0.0005097190539042155, 0.0019376873970031738]}, 'saliency': {'score': [2.6415837438482987e-05, 0.00018974260547498795, 4.863739013671875e-05, 0.000191534904196987, 3.9635178370353504e-05], 'topk_tokens': [' of', 'pleasant', "'s", 'ian', 'le', '0', ' out', 'ot', 'years', ' headlines', ' proprietor', '."', '<|eot_id|>', 'ographical', '<|start_header_id|>', '<|eot_id|>', 'athroom', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [4.105269908905029e-06, 7.443428039550781e-05, 4.28160031636556e-06, 2.1904706954956055e-05]}}, 29: {'grad': {'score': [0.35865663227282074, 0.3552155162643223, 0.36142552283502394, 0.3551537014494006, 0.36281690842066056], 'topk_tokens': [',', ' James', '�', '6', 'A', ',', 'MIN', 'ISC', '1', 'E', ' looked', '25', ' STR', ',', 'PA', 'str', '5', ' an', 'SP', ' the'], 'evidence_proportions': [0.6846990585327148, 0.228076171875, 0.40085601806640625, 0.13254070281982422]}, 'weight': {'score': [0.001295575970097592, 0.0073530395016293, 0.0023996022439772085, 0.007417432685254976, 0.002439270798976605], 'topk_tokens': [' devil', 'ian', ' as', 'assistant', '<|start_header_id|>', ' of', 'b', '<|eot_id|>', ' *\n\n', 'le', 'pleasant', ' proprietor', 'ot', ' headlines', 'athroom', '<|eot_id|>', '\n\n', '."', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000208929181098938, 0.0016605377197265625, 0.0006350477536519368, 0.0029168128967285156]}, 'saliency': {'score': [4.754568401135897e-05, 0.00017884917193710493, 8.340035715410786e-05, 0.0001801563817954132, 0.0001433808834124834], 'topk_tokens': ['le', ' *\n\n', 'athroom', ' headlines', 'b', ' *\n\n', 'ot', '<|start_header_id|>', ' as', '<|eot_id|>', 'pleasant', '***', ' as', ' Newspaper', '."', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [7.651746273040771e-06, 7.44163990020752e-05, 4.670520623524984e-05, 5.511194467544556e-05]}}, 30: {'grad': {'score': [0.7980940969366777, 0.669978313003316, 0.7111089152674521, 0.6690892313722224, 0.5874960972712591], 'topk_tokens': ['fect', ' prepared', 'ing', 'D', 'b', ' Republicans', ' M', ' these', ' If', ' under', 'istributed', ' By', ' em', ' PA', ' A', ' web', ' a', ' be', ' B', ' of'], 'evidence_proportions': [1.03033447265625, 0.798907470703125, 0.6686299641927083, 0.759033203125]}, 'weight': {'score': [0.005719278988085295, 0.007183616390616788, 0.004697595873186665, 0.007208757581098196, 0.005182221913949037], 'topk_tokens': [' journey', 'Question', ' celebrity', ' location', '<|start_header_id|>', 'Good', '."', ' bedroom', ' \n', ' location', ' new', '<|eot_id|>', 'assistant', '<|start_header_id|>', '\n\n', 'b', '<|eot_id|>', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0002643167972564697, 0.014641475677490235, 0.0038021008173624677, 0.0028972625732421875]}, 'saliency': {'score': [0.0002172557931197317, 0.00020566042680302145, 0.0001342911874094317, 0.00020613792787233683, 0.0002443702557148078], 'topk_tokens': [' W', ' directly', ' bedroom', 'super', ' celebrity', ' journey', ' location', '�', 'assistant', '<|eot_id|>', '<|start_header_id|>', '-text', ' new', 'athroom', '<|start_header_id|>', '\n\n', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'b'], 'evidence_proportions': [2.431124448776245e-05, 0.0004939019680023193, 0.00019351641337076822, 0.00010000169277191162]}}, 31: {'grad': {'score': [0.41350259278949936, 0.47084066372017114, 0.4626045842324534, 0.47116298917841704, 0.4462773249699519], 'topk_tokens': [',', ' paper', ' and', '<|end_header_id|>', ' had', ' provided', ' and', ' perpet', '\n', '\n', ' and', ',', '.', ' in', '<|eot_id|>', ' moved', 'Another', '<|start_header_id|>', ',', ','], 'evidence_proportions': [0.3084852695465088, 0.513946533203125, 0.39119211832682294, 0.42643070220947266]}, 'weight': {'score': [0.00262895383332905, 0.006413727674027971, 0.003917409527686334, 0.006449513044476166, 0.0027686082399808443], 'topk_tokens': [' return', ' journey', ' location', '.\n\n', 'If', 'Answer', 'Question', ' \n', '.\n\n', '<|eot_id|>', ' location', ' new', '<|start_header_id|>', 'assistant', '<|eot_id|>', '\n\n', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0006932318210601807, 0.004179668426513672, 0.0019989609718322754, 0.0035712718963623047]}, 'saliency': {'score': [7.254355832150108e-05, 0.0002010668879829273, 0.00019884974725784793, 0.00020166867959990826, 0.00010722149641085893], 'topk_tokens': [' dropped', ' location', ' and', 'Answer', ' be', '.\n\n', 'Question', ' news', ' bedroom', '<|eot_id|>', ' journey', ' new', '<|eot_id|>', ' location', 'assistant', '<|begin_of_text|>', '\n\n', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [4.1522085666656494e-05, 7.963776588439941e-05, 5.729496479034424e-05, 0.00011757016181945801]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-23 22:32:12.535 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:32:12.535 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-14_pid-3_1-3-4-9.pkl | len: 10 |  size: 9.08 KB
Processing depth (1, 3, 4, 9):   4%|▍         | 4/100 [00:52<20:59, 13.12s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Processing depth (0, 2, 4, 9):   4%|▍         | 4/100 [01:01<20:59, 13.12s/it]2025-01-23 22:32:21.858 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary took the apple.
2025-01-23 22:32:21.879 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary moved to the bathroom.
2025-01-23 22:32:21.884 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (953, 958) --> . Mary moved to the
2025-01-23 22:32:21.884 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:32:21.894 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1847, 1853) -->  Mary journeyed to the bedroom
2025-01-23 22:32:21.894 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel left the apple.
2025-01-23 22:32:21.912 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3663, 3667) -->  Daniel left the apple
2025-01-23 22:32:21.912 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel picked up the apple.
2025-01-23 22:32:21.933 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (3695, 3700) -->  cold. Daniel picked up
2025-01-23 22:32:21.934 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-23 22:32:21.949 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (3077, 3082) -->  Sandra moved to the kitchen
2025-01-23 22:32:21.949 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:32:21.952 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (509, 514) --> . John moved to the
2025-01-23 22:32:21.952 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:32:21.965 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2484, 2490) --> . Sandra journeyed to the
2025-01-23 22:32:21.965 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  John went back to the office.
2025-01-23 22:32:21.979 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2822, 2828) --> . John went back to the
2025-01-23 22:32:21.979 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Daniel took the football.
2025-01-23 22:32:21.988 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1836, 1840) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:32:22.493 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the garden<|eot_id|>
2025-01-23 22:32:22.493 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4225])
your chose emoji: ['👶🏻', '👨🏾\u200d🔬', '⚜', '🏝', '🤹🏼\u200d♀️', '🧎🏼\u200d♂️\u200d➡️', '👰🏼\u200d♀', '🧎🏽\u200d♀\u200d➡', '🧏🏽\u200d♂️', '🧖🏼\u200d♂️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4228, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 159783.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 97.29it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 99.30it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 98.89it/s]
2025-01-23 22:32:25.562 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3698150634765625, 0.44961920298826275, 0.2857017517089844, 0.4511205188855208, 0.543139514468965], 'topk_tokens': [' Of', ' out', ' departing', ' of', ' P', 'ex', ' THE', 'out', ' seeing', ' said', ' talk', 'RI', ' out', ' ', ' notified', 'announcement', ' account', ' Out', 'announcement', ' considerable'], 'evidence_proportions': [0.343328857421875, 0.29430389404296875, 0.5161895751953125]}, 'weight': {'score': [0.018336105346679687, 0.007380800869742315, 0.0038297945453274634, 0.007367828997170053, 0.003102099611645653], 'topk_tokens': ['\n\n', 'Mary', ' Where', ' bedroom', ' apple', 'user', 'Answer', '\n\n', ':', ' bathroom', '<|eot_id|>', '\n\n', 'b', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.025512647628784177, 0.020945628484090168, 0.00545114278793335]}, 'saliency': {'score': [0.0013527512550354005, 0.0001106453581925485, 0.00020251254881581952, 0.00010550918617868812, 0.00012752520186560496], 'topk_tokens': [' before', ':', ' bathroom', ' return', ' Do', ' Grow', 'assistant', '<|start_header_id|>', 'Question', ' ', '<|begin_of_text|>', 'Mary', ' Mary', '\n\n', 'Answer', 'Just', '<|end_header_id|>', 'b', ' Where', 'athroom'], 'evidence_proportions': [0.002636963129043579, 0.0010815163453420005, 0.0001543387770652771]}}, 25: {'grad': {'score': [0.8467702229817708, 0.6000432760623005, 0.7495020589520854, 0.5980504194211298, 0.44628833589099703], 'topk_tokens': ['eward', ' charges', ' small', 'stage', ' from', 'asc', ' hidden', ' got', ' from', ' to', ' than', ' of', ' laid', ' was', ' of', ' the', ' of', ' his', ' from', ' res'], 'evidence_proportions': [0.6946960449218751, 0.8896255493164062, 0.9725799560546875]}, 'weight': {'score': [0.01114954153696696, 0.007180423249359275, 0.0021944420952950757, 0.007203146501741359, 0.0025291581239019123], 'topk_tokens': [' return', ' Mary', '\n\n', ' bedroom', ' Do', ' bathroom', '?', ' \n', 'Mary', 'Answer', ':', 'b', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.01615149974822998, 0.011589765548706055, 0.004236757755279541]}, 'saliency': {'score': [0.0015100498994191488, 0.00010264679690859965, 8.710738151304184e-05, 9.771391200654418e-05, 6.557220504397438e-05], 'topk_tokens': ['Question', ' return', 'athroom', '<|eot_id|>', 'Answer', ' Mary', ' Grow', ' \n', '<|start_header_id|>', '<|eot_id|>', ':', 'b', '\n\n', 'assistant', ' Mary', 'Mary', ' Do', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0028693974018096924, 0.0011615703503290813, 0.00033358484506607056]}}, 26: {'grad': {'score': [0.357784907023112, 0.5117921711931617, 0.48984601420740925, 0.5125072452795101, 0.3682538668314616], 'topk_tokens': ['graph', ' out', ' out', ' into', 'graph', ' Press', ' out', ' newspaper', ' Press', 'rich', 'graph', ' steam', ' secret', ' Press', 'Johnson', 'hue', 'b', 'most', ' trib', 'ab'], 'evidence_proportions': [0.3737442016601562, 0.3638976414998372, 0.32866668701171875]}, 'weight': {'score': [0.009893715381622314, 0.006970070168559238, 0.0023452341556549072, 0.006993866256125973, 0.003751539048694429], 'topk_tokens': [' before', ' the', 'Question', '\n\n', ' bedroom', ' apple', '<|eot_id|>', 'Answer', 'b', '<|eot_id|>', ' bathroom', ' \n', '\n\n', '?', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.013554799556732177, 0.010231534639994303, 0.004810631275177002]}, 'saliency': {'score': [0.0004895369211832683, 0.00012740771953182167, 8.711795653066327e-05, 0.00012640749108329565, 0.00011089302244640531], 'topk_tokens': ['�', ' ', '\n\n', ' apple', '?', 'Mary', ' Mary', '\n\n', ' Jackson', ' \n', 'Answer', 'assistant', '\n\n', ' bathroom', '<|start_header_id|>', 'b', ':', '<|begin_of_text|>', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [0.0009210646152496338, 0.000401914119720459, 8.156150579452515e-05]}}, 27: {'grad': {'score': [0.6954020182291667, 0.790580756729689, 0.588241207984186, 0.7924220305433233, 0.4758120945521763], 'topk_tokens': [' first', ' hours', ' days', ' at', 'ollar', ' paying', ' which', ' first', '000', ' summer', ',', ' book', ' arrival', ' second', ' time', ' governor', ' two', ' hours', ' hear', ' report'], 'evidence_proportions': [0.580908203125, 0.819091796875, 0.652984619140625]}, 'weight': {'score': [0.013733275731404622, 0.0072582728928175305, 0.001987140024861982, 0.007274121787204861, 0.0038107559084892273], 'topk_tokens': ['<|eot_id|>', ' Where', ' Mary', ' apple', 'Just', ' \n', 'Question', 'Answer', 'Mary', '<|eot_id|>', '?', ' bathroom', '\n\n', 'b', '<|start_header_id|>', 'assistant', ':', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02098245620727539, 0.012904644012451172, 0.005914747714996338]}, 'saliency': {'score': [0.0015984634558359782, 0.00014583018582839388, 0.0001486434090522028, 0.00014059902634248956, 0.00015359194505782354], 'topk_tokens': [' \n', ' Mary', '<|eot_id|>', 'Answer', '\n\n', 'Question', '?', 'NEW', ' Mary', 'assistant', ' Where', 'Just', 'b', ' bathroom', '<|begin_of_text|>', ':', '\n\n', 'Mary', '<|end_header_id|>', 'athroom'], 'evidence_proportions': [0.00320625901222229, 0.0011974722146987915, 0.0001902058720588684]}}, 28: {'grad': {'score': [0.37707265218098956, 0.49289035706894513, 0.42443749212449594, 0.4938131940815222, 0.4072595550900414], 'topk_tokens': [',', ' from', ' absolutely', ' wholly', ' border', ' what', ' three', ' instead', ' both', ' Dub', 'Dub', '600', ' pleasure', ' as', ' Dub', ' which', ' prev', ' half', ' balance', 'ball'], 'evidence_proportions': [0.48485107421875, 0.3290252685546875, 0.3144207000732422]}, 'weight': {'score': [0.006575258572896322, 0.006873612823414149, 0.0018038865058652817, 0.006912263428292624, 0.0020454685602869305], 'topk_tokens': ['Question', ' bathroom', 'Just', ' the', ' bedroom', '<|eot_id|>', ' before', '<|eot_id|>', 'Answer', ' \n', 'b', ' apple', '<|start_header_id|>', 'assistant', '?', ':', '\n\n', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.011300086975097656, 0.004844963550567627, 0.0032646656036376953]}, 'saliency': {'score': [0.00012380480766296386, 0.00015132155977788688, 5.3850873824088806e-05, 0.00015214277989895135, 4.533252545765468e-05], 'topk_tokens': ['<|eot_id|>', '.', ' was', ' before', 'Answer', '.', 'nes', ' bedroom', ' apple', 'Just', ' \n', 'assistant', '<|start_header_id|>', ':', 'b', '?', 'athroom', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [0.0002262592315673828, 8.574128150939941e-05, 5.2832067012786865e-05]}}, 29: {'grad': {'score': [0.5646652221679688, 0.4151988972327342, 0.4534149169921875, 0.41437950663336265, 0.7098417282104492], 'topk_tokens': [' *', ',', ' *', ' ', 'half', ' *', 'doctor', 'ION', ' to', ' m', 'pend', ' of', '***', ' *', ' com', ' res', 'assistant', ' o', ' H', '\u200d'], 'evidence_proportions': [0.6462646484375, 0.6078046162923176, 0.39795684814453125]}, 'weight': {'score': [0.0030001163482666015, 0.007081034957178381, 0.0007665762978215371, 0.007142479719869375, 0.002396637485140846], 'topk_tokens': ['\n\n', ' the', 'user', '<|eot_id|>', ' bedroom', '\n\n', '<|eot_id|>', ' apple', 'Answer', ' \n', ' before', 'b', '<|start_header_id|>', '?', 'assistant', '\n\n', ':', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.004159998893737793, 0.0016603072484334311, 0.003559976816177368]}, 'saliency': {'score': [4.230737686157226e-05, 9.436934989019735e-05, 1.6182661056518555e-05, 9.513566192971753e-05, 0.00020278458084378923], 'topk_tokens': ['<|eot_id|>', 'ern', ',', '�', '<|eot_id|>', '\n\n', ' before', ' \n', '<|start_header_id|>', '�', '\n\n', ' ', '?', 'athroom', '\n\n', '<|begin_of_text|>', ':', 'assistant', 'b', '<|end_header_id|>'], 'evidence_proportions': [4.454255104064941e-05, 1.89592440923055e-05, 7.453560829162598e-05]}}, 30: {'grad': {'score': [0.32841339111328127, 0.3802415528383138, 0.4030897694249307, 0.38025808265937794, 0.3253583794548398], 'topk_tokens': [' message', ' time', ' B', '      ', ' fight', ' and', ' time', ' with', '-four', '.', '      ', '      ', 'time', ' part', ' ground', ' concerned', '      ', ' room', ' time', 'b'], 'evidence_proportions': [0.3645675659179688, 0.2175877888997396, 0.4494590759277344]}, 'weight': {'score': [0.012792698542277018, 0.007021122646422012, 0.0025415728169102822, 0.007033626808611643, 0.007158065126055763], 'topk_tokens': [':', ' bedroom', ' bathroom', ' ', 'Mary', ' apple', 'Question', '<|eot_id|>', 'b', 'Answer', ' \n', '<|eot_id|>', 'assistant', '?', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.016924953460693358, 0.013567209243774414, 0.006465613842010498]}, 'saliency': {'score': [0.00030109286308288574, 0.00023730118937839736, 8.617677996235509e-05, 0.0002381926244779502, 0.00012269154900596257], 'topk_tokens': [' ', ' Where', ' before', 'Mary', ' Grow', '\n\n', 'nes', 'ANK', ' ', ' \n', 'Question', ' bathroom', 'assistant', '?', '<|begin_of_text|>', '<|start_header_id|>', ':', '<|end_header_id|>', 'b', 'athroom'], 'evidence_proportions': [0.0004216909408569336, 0.0002918640772501628, 0.0001641884446144104]}}, 31: {'grad': {'score': [0.553040329615275, 0.5625994961106112, 0.5692890228763703, 0.5625841953377014, 0.2898407436552502], 'topk_tokens': ['hours', ' location', ' earlier', 'ing', ' Paul', ' concerned', ' time', ' city', ' goods', ' location', ' room', ' to', ' him', ' night', ' expected', 'time', 'Answer', ' paper', ' part', ' ground'], 'evidence_proportions': [0.5041115283966064, 0.4792073567708333, 0.7249507904052734]}, 'weight': {'score': [0.0056061983108520504, 0.006631185112973938, 0.0015526490826760569, 0.006672507307849849, 0.0019326813164211455], 'topk_tokens': ['.\n\n', ':', 'Question', ' before', ' Where', ' apple', ' bedroom', '?', 'Answer', ' \n', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', ':', 'assistant', '\n\n', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.00867910385131836, 0.004635989665985107, 0.00322037935256958]}, 'saliency': {'score': [0.00010252992312113444, 6.331332810328835e-05, 5.176663398742676e-05, 6.325825842187371e-05, 3.634854441597348e-05], 'topk_tokens': ['?', ' Where', '<|eot_id|>', 'RE', ':', ' bathroom', ' \n', ' apple', ' bedroom', '<|start_header_id|>', '\n\n', 'Question', ':', '<|eot_id|>', 'Answer', 'b', '<|begin_of_text|>', 'assistant', 'athroom', '<|end_header_id|>'], 'evidence_proportions': [9.627938270568849e-05, 0.00014933447043100992, 4.013627767562866e-05]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-23 22:32:25.575 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:32:25.575 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-14_pid-4_0-2-4-9.pkl | len: 10 |  size: 8.57 KB
Processing depth (0, 2, 4, 9):   5%|▌         | 5/100 [01:05<20:43, 13.09s/it]Processing depth (0, 2, 4, 9):   5%|▌         | 5/100 [01:05<20:47, 13.13s/it]
2025-01-23 22:32:25.789 | INFO     | __main__:<module>:99 - Selected idx: 15
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:101 - Answer: bathroom
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:103 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Mary moved to the bathroom.', ' Daniel picked up the apple.', ' Daniel took the football.', ' Mary got the milk.', ' Sandra moved to the kitchen.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:104 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-23 22:32:25.790 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.23it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.01it/s][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
Processing depth (0, 1, 4, 7, 9):   0%|          | 0/100 [00:08<?, ?it/s]2025-01-23 22:32:34.722 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:32:34.722 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:32:34.722 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the milk.
2025-01-23 22:32:34.724 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (436, 440) -->  Mary got the milk
2025-01-23 22:32:34.724 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:32:34.734 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1833, 1839) --> . Mary journeyed to the
2025-01-23 22:32:34.734 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:32:34.748 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2838, 2842) -->  Mary left the milk
2025-01-23 22:32:34.748 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:32:34.766 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3660, 3664) -->  Daniel left the apple
2025-01-23 22:32:34.766 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:32:34.768 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (522, 527) --> . John moved to the
2025-01-23 22:32:34.768 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the office.
2025-01-23 22:32:34.771 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (483, 489) --> . Sandra journeyed to the
2025-01-23 22:32:34.771 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel picked up the apple.
2025-01-23 22:32:34.790 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3742, 3747) --> . Daniel picked up the
2025-01-23 22:32:34.790 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:32:34.798 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1780, 1784) -->  Daniel took the football
2025-01-23 22:32:34.798 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:32:34.812 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1975, 1980) --> . Sandra moved to the
2025-01-23 22:32:34.812 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:32:34.824 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2301, 2307) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:32:35.359 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:32:35.359 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4230])
your chose emoji: ['🧘\u200d♀', '🏌🏾\u200d♀️', '🎿', '🛸', '🙆🏿\u200d♂️', '👩🏼\u200d❤\u200d💋\u200d👨🏼', '🐸', '🏃🏽', '🖱', '👨\u200d👨\u200d👦\u200d👦']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4233, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 223696.21it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.16it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.92it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.51it/s]
2025-01-23 22:32:38.484 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.07221575923587965, 0.09436550090815929, 0.07604848184893208, 0.09462328306879544, 0.10482806126276652], 'topk_tokens': [' advantage', ' of', 'ATION', ' completed', ' convention', 'the', ' context', ' in', ' scramble', ' conventions', ' the', ' the', ' the', ' the', ',', ' conventions', ' the', 'announcement', ' announcement', 'announcement'], 'evidence_proportions': [0.06005370616912842, 0.05000472068786621, 0.08658663431803386, 0.0947260856628418, 0.06556272506713867]}, 'weight': {'score': [0.07593772722327191, 0.007426468272201282, 0.005928875938538582, 0.0070605114419713035, 0.0006639719009399414], 'topk_tokens': ['Mary', '\n\n', '?\n', ' ', '.', ' bedroom', 'Answer', '<|eot_id|>', '\n\n', ' milk', 'assistant', ':', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [0.3208168029785156, 0.021393299102783203, 0.003195325533548991, 0.006081819534301758, 0.003352820873260498]}, 'saliency': {'score': [0.0003082169138866922, 1.925985469002547e-05, 7.236869104446904e-05, 1.7275555506366438e-05, 5.1053365071614585e-06], 'topk_tokens': ['.\n\n', '.', ' milk', ' office', ' milk', '\n\n', '\n\n', '<|eot_id|>', '\n\n', '<|eot_id|>', ':', ' location', ' ', '<|begin_of_text|>', ' bedroom', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ' bathroom', 'b'], 'evidence_proportions': [0.001098191738128662, 0.0002495124936103821, 4.127621650695801e-05, 6.347149610519409e-05, 2.4609267711639404e-05]}}, 25: {'grad': {'score': [0.10258056806481403, 0.12091699244704, 0.08874520947856288, 0.12125656244771336, 0.07297679583231607], 'topk_tokens': [' not', ' the', ' the', ' composing', ' compos', ' the', ' the', '\n', ' of', ' the', ' or', ' for', ' the', ' of', 'of', 'ible', 'If', ' of', 'If', 'iring'], 'evidence_proportions': [0.09172515869140625, 0.07947182655334473, 0.10384623209635417, 0.09831047058105469, 0.1416301727294922]}, 'weight': {'score': [0.019936056240745213, 0.007223574455652343, 0.003665629894502701, 0.007180001639270075, 0.0005057235558827718], 'topk_tokens': ['\n\n', ' Bench', '.\n\n', ' discarded', ' prior', ' to', '<|eot_id|>', 'Answer', 'Mary', '?\n', 'b', '<|eot_id|>', 'assistant', '<|start_header_id|>', ':', ' bathroom', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.06844673156738282, 0.01789987087249756, 0.002743542194366455, 0.004056870937347412, 0.003001853823661804]}, 'saliency': {'score': [8.501306824062182e-05, 1.5597189399274056e-05, 2.264015136226531e-05, 1.516289960884483e-05, 4.51962153116862e-06], 'topk_tokens': [' place', ' ST', 'MIN', '.\n\n', ' to', ' bathroom', '<|eot_id|>', ' PA', ' prior', 'Answer', ' Bench', 'athroom', '<|eot_id|>', 'Mary', '<|start_header_id|>', '<|end_header_id|>', 'b', 'assistant', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.00023788809776306155, 8.787959814071655e-05, 2.18351682027181e-05, 1.8961727619171143e-05, 5.187094211578369e-05]}}, 26: {'grad': {'score': [0.08416957440583603, 0.08335594395331787, 0.07979854460685484, 0.083377854907932, 0.07393018881479899], 'topk_tokens': ['\n', '\n', ' fr', 'city', ' du', '\n', ',\n', ' Moore', '\n', '\n', '\n', ',\n', '\n', '\n', '\n', '\n', '\n', ',\n', '\n', '\n'], 'evidence_proportions': [0.06903610229492188, 0.08905792236328125, 0.09154764811197916, 0.09357607364654541, 0.07772445678710938]}, 'weight': {'score': [0.040752392748127815, 0.007064434757138042, 0.003667810270863195, 0.0069042223439487035, 0.0007120879491170247], 'topk_tokens': [' kitchen', ' prior', '\n\n', ' discarded', ' the', ' the', '.\n\n', '<|eot_id|>', '?\n', 'Answer', '<|eot_id|>', 'assistant', 'b', '\n\n', ' bathroom', '<|start_header_id|>', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.16804332733154295, 0.00668489933013916, 0.0029635528723398847, 0.008025169372558594, 0.005116701126098633]}, 'saliency': {'score': [0.0006167007529217264, 2.9314713305737597e-05, 3.442360508826471e-05, 2.6044013483668663e-05, 4.51048215230306e-06], 'topk_tokens': [' Gray', ' prior', '<|eot_id|>', '***', 'Bridge', '.\n\n', 'Mary', ' kitchen', '?\n', '185', '\n\n', 'Answer', 'assistant', 'athroom', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', ' bathroom', ':', 'b'], 'evidence_proportions': [0.0026216983795166017, 7.477402687072754e-05, 2.8774142265319824e-05, 7.030367851257324e-05, 8.066743612289429e-05]}}, 27: {'grad': {'score': [0.16010893946108612, 0.15030426126322846, 0.16658575304092899, 0.15012952236788044, 0.08927621205647786], 'topk_tokens': [' location', ',\n', ' work', '.', ' location', '.', ' actions', ' ground', 'ers', ' *\n\n', '.', '.', ' hour', '1', ' other', 'Republicans', '!"', '!"', ' execution', ' convention'], 'evidence_proportions': [0.1773529052734375, 0.15503692626953125, 0.1517643928527832, 0.16043472290039062, 0.15581703186035156]}, 'weight': {'score': [0.07618242631787839, 0.007292763944103621, 0.0042956740625443, 0.006935847828222194, 0.0006653996308644613], 'topk_tokens': [' PA', 'RE', '<|eot_id|>', ' milk', ' to', 'Answer', '?\n', 'Mary', ' bedroom', 'NEW', 'assistant', '\n\n', '.\n\n', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.33079051971435547, 0.011259913444519043, 0.002228155732154846, 0.004594206809997559, 0.005364447832107544]}, 'saliency': {'score': [0.000567081181899361, 4.457593133143716e-05, 0.0001015941942891767, 4.127725056695949e-05, 3.514687220255534e-06], 'topk_tokens': ['?\n', '\n\n', ' the', ' Minnesota', ' the', ' PA', ' to', ' prior', 'Mary', 'RE', 'assistant', 'athroom', '<|start_header_id|>', ' bathroom', ':', '<|begin_of_text|>', 'NEW', '<|end_header_id|>', 'b', '.\n\n'], 'evidence_proportions': [0.002158170938491821, 0.00030212849378585815, 4.402796427408854e-05, 4.60892915725708e-05, 0.0001487433910369873]}}, 28: {'grad': {'score': [0.13760630980781888, 0.12141423143170108, 0.1319253598490069, 0.12124714294555912, 0.0850332768758138], 'topk_tokens': [' it', ' the', ' the', 'half', '.', ' the', 'ivery', "'clock", ' Peter', ' L', "'clock", "'clock", ' the', 'the', ' and', ' out', ' bend', ' in', 'ien', ' it'], 'evidence_proportions': [0.18224334716796875, 0.12515294551849365, 0.13191668192545572, 0.13067388534545898, 0.10973024368286133]}, 'weight': {'score': [0.015636994786884472, 0.006851876229787025, 0.005575815516133462, 0.006812991366257477, 0.0004638628164927165], 'topk_tokens': [' the', '.\n\n', ' to', 'Answer', ' discarded', ' the', '<|eot_id|>', '?\n', ' the', '<|eot_id|>', ' the', ' bathroom', 'assistant', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.05251798629760742, 0.003231048583984375, 0.004447266459465027, 0.008159279823303223, 0.006204009056091309]}, 'saliency': {'score': [0.0001816049866054369, 2.916411054689158e-05, 3.1875987206735916e-05, 2.8305003505541564e-05, 1.8703937530517578e-06], 'topk_tokens': ['Just', ' was', ' the', ' to', ' the', '.\n\n', ' kitchen', ' milk', 'Bridge', 'Answer', ' garden', '\n\n', ' bathroom', '<|end_header_id|>', 'b', 'assistant', 'athroom', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.000755375623703003, 2.7865171432495117e-05, 1.385807991027832e-05, 2.9519200325012207e-05, 2.183765172958374e-05]}}, 29: {'grad': {'score': [0.08081602013629416, 0.1146768902932502, 0.090507568851594, 0.11504253972572241, 0.07937697251637776], 'topk_tokens': [' where', ' temper', ' have', 'itated', ' employed', ' with', ' available', 'ely', ' which', ' affairs', ' very', ' give', ' wholly', ',', '\u200d', ' impressed', ' generally', ' who', ' enough', ' its'], 'evidence_proportions': [0.11539764404296876, 0.1091461181640625, 0.06054115295410156, 0.0588836669921875, 0.061603546142578125]}, 'weight': {'score': [0.003162001786024674, 0.0070313931975893, 0.0029519998258160005, 0.007082950315797235, 0.0004052384694417318], 'topk_tokens': [' the', ' Where', '\n\n', ' where', '.', ' to', ' was', 'Answer', '<|eot_id|>', '?\n', ' the', '.\n\n', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00677947998046875, 0.0017622709274291992, 0.0024710545937220254, 0.0017105937004089355, 0.0025277137756347656]}, 'saliency': {'score': [2.400771431300951e-05, 2.8937400274459222e-05, 1.6431654653241558e-05, 2.905729998524436e-05, 2.875328063964844e-06], 'topk_tokens': [' the', 'us', '"The', ' PA', ':', ' where', ' the', ' to', ' the', '      ', ' the', 'NEW', 'assistant', ' was', '<|end_header_id|>', ':', 'b', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [2.384781837463379e-05, 1.5132129192352295e-05, 3.499289353688558e-05, 2.065300941467285e-05, 1.9960105419158936e-05]}}, 30: {'grad': {'score': [0.08750944552214249, 0.08761287394522944, 0.09663010028100782, 0.08754655301613681, 0.05973496238390605], 'topk_tokens': [' to', ' Online', ' It', ' Out', 'ch', ' Clean', ' up', ' up', ' Fourth', ' Gray', ' Collection', ' Knowledge', ' B', ' In', 'Civil', ' Bank', ' B', ' The', 'at', ' Note'], 'evidence_proportions': [0.07394638061523437, 0.05784296989440918, 0.09518718719482422, 0.13106727600097656, 0.0790553092956543]}, 'weight': {'score': [0.022782220788623974, 0.007111209407079144, 0.007047216738423993, 0.007025435420707471, 0.0015058815479278564], 'topk_tokens': [':', ' the', '\n\n', '<|eot_id|>', ' garden', ' the', 'NEW', 'Mary', 'Answer', '.\n\n', '?\n', 'assistant', ' bathroom', '\n\n', 'b', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0744089126586914, 0.011421442031860352, 0.00776079793771108, 0.007849931716918945, 0.0070740580558776855]}, 'saliency': {'score': [0.0008300244808197021, 7.829763577842216e-05, 8.887437082106067e-05, 7.408189128876645e-05, 1.1301438013712564e-05], 'topk_tokens': ['<|eot_id|>', '.\n\n', ' ST', 'Mary', ' the', ':', 'E', ' the', 'Answer', ' garden', 'assistant', '?\n', '<|end_header_id|>', ' the', '<|begin_of_text|>', ':', ' bathroom', 'athroom', 'b', '<|start_header_id|>'], 'evidence_proportions': [0.0033774912357330323, 0.00021327286958694458, 4.2065978050231934e-05, 0.00018887221813201904, 8.553266525268555e-05]}}, 31: {'grad': {'score': [0.08041357864504275, 0.07812020529704879, 0.06355375724454079, 0.07821563776955989, 0.06741324782371522], 'topk_tokens': [' m', 'L', ' Bench', ' to', ' throughout', '\n', ':', 'RE', ' to', ' l', ' B', ' of', '\n', 'g', ' in', 'b', ' by', '\n', ' in', ' to'], 'evidence_proportions': [0.06790783405303956, 0.0801195278763771, 0.07277551293373108, 0.1120796799659729, 0.07613080739974976]}, 'weight': {'score': [0.0064986685047978944, 0.00652923173947225, 0.0020640780848841514, 0.0065625226506207885, 0.0008987140655517578], 'topk_tokens': [' the', 'Question', ',', ':', ' bathroom', ' Where', '.\n\n', '<|eot_id|>', ' the', 'Answer', '?\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', 'b', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.02052783966064453, 0.0028013139963150024, 0.0015301605065663655, 0.003016352653503418, 0.003594636917114258]}, 'saliency': {'score': [6.234775418820588e-05, 2.1129550873008204e-05, 3.2434540410195626e-05, 2.081883698167e-05, 3.8492679595947265e-06], 'topk_tokens': [' The', ' the', ' location', ' the', ' was', 'Bridge', 'Answer', ' bathroom', ' the', ' the', '<|eot_id|>', ' the', '?\n', '<|begin_of_text|>', 'b', 'assistant', 'athroom', '<|end_header_id|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.0002050161361694336, 2.4400651454925537e-05, 1.0261933008829752e-05, 2.9809772968292236e-05, 3.262609243392944e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:32:38.496 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:32:38.497 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-15_pid-0_0-1-4-7-9.pkl | len: 10 |  size: 9.33 KB
Processing depth (0, 1, 4, 7, 9):   1%|          | 1/100 [00:12<20:47, 12.60s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.10it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Processing depth (3, 5, 7, 8, 9):   1%|          | 1/100 [00:22<20:47, 12.60s/it]2025-01-23 22:32:48.049 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:32:48.057 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1437, 1442) --> . Mary moved to the
2025-01-23 22:32:48.057 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the milk.
2025-01-23 22:32:48.067 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2105, 2109) -->  Mary got the milk
2025-01-23 22:32:48.067 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:32:48.082 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2857, 2863) --> . Mary journeyed to the
2025-01-23 22:32:48.082 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:32:48.098 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3353, 3357) -->  Mary left the milk
2025-01-23 22:32:48.098 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:32:48.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3647, 3651) -->  Daniel left the apple
2025-01-23 22:32:48.117 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:32:48.119 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (510, 515) --> . John moved to the
2025-01-23 22:32:48.120 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the office.
2025-01-23 22:32:48.122 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (471, 477) --> . Sandra journeyed to the
2025-01-23 22:32:48.122 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel picked up the apple.
2025-01-23 22:32:48.140 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3729, 3734) --> . Daniel picked up the
2025-01-23 22:32:48.140 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:32:48.149 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1779, 1783) -->  Daniel took the football
2025-01-23 22:32:48.149 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:32:48.159 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1959, 1964) --> . Sandra moved to the
2025-01-23 22:32:48.159 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:32:48.170 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2275, 2281) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:32:48.648 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:32:48.648 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4217])
your chose emoji: ['🤵🏾', '👨🏽\u200d💼', '🎓', '🐌', '🦸🏻\u200d♂', '🪶', '🦂', '👨🏾\u200d✈', '🕰', '👩🏾\u200d❤\u200d👨🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4220, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 220752.84it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.28it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.38it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.93it/s]
2025-01-23 22:32:51.520 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.5317515497622283, 0.5249566823950311, 0.6519664641349546, 0.5239740647321931, 0.562254874936996], 'topk_tokens': [' extraordinary', '\n\n\n\n\n\n\n', 'Cut', ' of', ' but', '.', '.', ' time', '.', ' Gov', ' EVENTS', '.', 'system', 'ences', ' interest', ' rebellion', ' res', ',', 'ION', 'ION'], 'evidence_proportions': [0.8091064453125, 0.6348342895507812, 0.44164021809895837, 0.5722732543945312, 0.1766204833984375]}, 'weight': {'score': [0.0008671231891797936, 0.007495804872558015, 0.000919606416456161, 0.007581335796911557, 0.0023767645320584697], 'topk_tokens': ['\n\n', 'b', 'rate', 'system', '\n\n', ' combined', '\n\n', '<|start_header_id|>', 'athroom', 'user', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', 'assistant', ' offices', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011627793312072754, 0.001999959349632263, 0.0005165735880533854, 0.000438787043094635, 0.0003188773989677429]}, 'saliency': {'score': [6.247862525608228e-05, 0.0001230556394251602, 8.152088811320644e-05, 0.0001236991460541951, 0.0001764931986408849], 'topk_tokens': ['b', '4', '<|start_header_id|>', ' Hor', '\n\n', 'Bridge', '\n\n', '<|eot_id|>', ' Project', ' offices', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', 'user', 'system', '\n\n', '<|end_header_id|>', '***', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [8.031129837036133e-05, 0.0001503676176071167, 4.635751247406006e-05, 2.9325485229492188e-05, 9.63360071182251e-06]}}, 25: {'grad': {'score': [0.6585811117421025, 0.6964655727006813, 0.9124643264278289, 0.69506744049629, 0.40624812341505484], 'topk_tokens': [' Democrats', ' their', 'cont', ' waiting', ' news', ' Written', ' newspaper', 'APER', ' news', ' to', 'New', 'ERS', ' was', ' whistle', ' NEW', 'NEW', ' newspapers', ' Republicans', ' wholly', ' await'], 'evidence_proportions': [1.24140625, 0.8167266845703125, 0.38930320739746094, 0.6122589111328125, 0.22214317321777344]}, 'weight': {'score': [0.0014567478843357253, 0.007466287748508544, 0.0012826410032087756, 0.0075454794110098086, 0.0031832418134135586], 'topk_tokens': ['b', ' wait', '<|end_header_id|>', '<|start_header_id|>', ' discarded', '<|eot_id|>', 'athroom', ' combined', '\n\n', 'rate', '\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', ' offices', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010582447052001954, 0.004890918731689453, 0.0008005897204081218, 0.0006356239318847656, 0.0003260672092437744]}, 'saliency': {'score': [0.00018041678096937096, 0.00015901665410724298, 0.00013068606776575888, 0.00015910931979585483, 0.0002164283106403966], 'topk_tokens': ['<|eot_id|>', 'PA', '�', ' Stephen', ' Daniel', '\n\n', ' Min', ' offices', '<|start_header_id|>', ' met', 'user', '<|eot_id|>', ' Min', 'system', 'MIN', ' discarded', 'assistant', '<|begin_of_text|>', '\n\n', '<|end_header_id|>'], 'evidence_proportions': [8.58306884765625e-05, 0.0006670281291007996, 0.00012834370136260986, 5.883723497390747e-05, 1.1727213859558105e-05]}}, 26: {'grad': {'score': [0.36357962566873303, 0.4216761005998223, 0.3669781838693926, 0.4224038620837537, 0.4118875688122165], 'topk_tokens': ['antic', 'ine', 'hom', '.', 'ioneer', ' Bench', ' offices', '3', 'room', 'political', ' at', ' St', ' Daily', 'user', ' MO', ' gang', 'ol', 'system', ' bathroom', '�'], 'evidence_proportions': [0.29429473876953127, 0.31561851501464844, 0.40343475341796875, 0.5779523849487305, 0.22399139404296875]}, 'weight': {'score': [0.001736283302307129, 0.007438756617324612, 0.0014290213584899902, 0.0075149588927133, 0.007491580901607391], 'topk_tokens': ['b', 'Bridge', '\n\n', '\n\n', ' combined', 'assistant', 'athroom', '<|start_header_id|>', '\n\n', 'rate', '<|end_header_id|>', '<|start_header_id|>', 'user', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', ' offices', '<|begin_of_text|>'], 'evidence_proportions': [0.0008079290390014648, 0.0034183859825134277, 0.0012411673863728843, 0.0020092129707336426, 0.0016843676567077637]}, 'saliency': {'score': [6.010610124339228e-05, 0.00014966695935805262, 4.321048336644327e-05, 0.00015095357733509944, 0.0004323514238480599], 'topk_tokens': ['\n\n', '<|start_header_id|>', 'user', '<|eot_id|>', ' been', ' bogus', '<|eot_id|>', 'athroom', ' wait', 'had', '<|end_header_id|>', 'Bridge', '\n\n', '<|eot_id|>', ' combined', '<|start_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'rate', ' offices'], 'evidence_proportions': [1.9729137420654297e-05, 0.00014065951108932495, 5.2387515703837075e-05, 4.252046346664429e-05, 5.918741226196289e-05]}}, 27: {'grad': {'score': [0.9194893215013586, 0.9525152558964011, 0.7878472112840221, 0.9539229145315594, 1.1271300931130686], 'topk_tokens': ['ex', ' take', '.', '.', ',', ',', 'ye', '�', ' were', 'a', 'in', ',', '.', ' be', ' be', 'ages', ',', ',', ',', 'was'], 'evidence_proportions': [0.8614990234375, 0.836456298828125, 0.974151611328125, 1.091552734375, 0.820953369140625]}, 'weight': {'score': [0.0016408381254776664, 0.007454003880939212, 0.0015371345704601658, 0.007530126243157279, 0.0063091732801929595], 'topk_tokens': [' *\n\n', '<|end_header_id|>', 'Just', 'rate', ' combined', '\n\n', '\n\n', '\n\n', '<|eot_id|>', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'b', 'athroom', 'user', ' offices', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0009804725646972655, 0.003948003053665161, 0.0012943545977274575, 0.0015310347080230713, 0.0007886588573455811]}, 'saliency': {'score': [0.00017888520074927288, 0.00023638055222859316, 0.0003037068151658581, 0.00023619698980251758, 0.0008197474864221388], 'topk_tokens': ['NEW', '***', '<|start_header_id|>', '\n\n', '\n\n', '\n\n', '<|start_header_id|>', 'system', 'athroom', 'b', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', 'RE', '<|end_header_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', 'user'], 'evidence_proportions': [7.465481758117676e-05, 0.000593043863773346, 0.00011755526065826416, 0.00014463067054748535, 2.1263957023620605e-05]}}, 28: {'grad': {'score': [1.3148087211277173, 1.1419968736115225, 1.3594498172883065, 1.139424690763022, 1.057404056672127], 'topk_tokens': [' A', '.', ' It', '.', '.', 'ex', ' Daniel', ' a', ' persons', ' and', ' gang', ' of', ' bag', 'PA', ' afternoon', '.', ' com', ' com', 'com', ' Daniel'], 'evidence_proportions': [1.524951171875, 1.58880615234375, 1.1931966145833333, 0.979278564453125, 1.29608154296875]}, 'weight': {'score': [0.0009460215983183488, 0.007110867794091103, 0.0006331551459527784, 0.007193105085160489, 0.002654920662603071], 'topk_tokens': ['.\n\n', ',', '\n\n', '\n', '\n\n', 'Just', 'user', '<|eot_id|>', '<|start_header_id|>', ' offices', '<|eot_id|>', '<|start_header_id|>', 'assistant', '\n\n', '<|eot_id|>', 'b', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005437612533569336, 0.0009399205446243286, 0.0014503200848897297, 0.0009066462516784668, 0.0007378756999969482]}, 'saliency': {'score': [3.1948089599609375e-05, 0.00017581874442891486, 3.677414309593939e-05, 0.0001776476949095669, 7.729857198653682e-05], 'topk_tokens': ['\n', 'user', ' December', '\n\n\n\n', ' *\n\n', 'b', '\n\n', ',', '<|eot_id|>', '\n\n', '<|start_header_id|>', ' offices', 'Just', '<|eot_id|>', 'assistant', '<|end_header_id|>', 'athroom', '\n\n', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.35140323638916e-05, 2.7798116207122803e-05, 4.582603772481282e-05, 3.3371150493621826e-05, 2.4400651454925537e-05]}}, 29: {'grad': {'score': [0.8355649865191915, 0.8804038386774289, 0.8895665445635396, 0.8805832073085307, 0.8915060720136089], 'topk_tokens': ['E', ' WITH', 'ERS', '<|start_header_id|>', 'PA', 'P', 'SP', 'RE', '�', ' S', '�', ' L', ' res', ' B', 'APER', 'ISC', ' STR', 'UG', 'SP', 'MIN'], 'evidence_proportions': [1.0525390625, 0.8490142822265625, 0.719573974609375, 0.931671142578125, 0.6287784576416016]}, 'weight': {'score': [0.0015243006789165995, 0.0073135592926170025, 0.0012015671499313848, 0.007391001612477387, 0.004563566177122055], 'topk_tokens': ['<|start_header_id|>', '\n', '\n\n', '\n', '<|eot_id|>', 'system', 'user', ' offices', 'b', '\n\n', '<|start_header_id|>', '\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', 'athroom', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0004625082015991211, 0.0016179680824279785, 0.0032468636830647784, 0.0006196349859237671, 0.0010786950588226318]}, 'saliency': {'score': [7.512128871420155e-05, 0.00028144949561611734, 6.796948371394988e-05, 0.00028417715503227466, 0.0005420340645697809], 'topk_tokens': ['<|end_header_id|>', 'Just', '\n\n', '4', 'system', '\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', ' offices', 'b', '***', '<|start_header_id|>', '<|eot_id|>', "'clock", 'athroom', '<|eot_id|>', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [2.4110078811645508e-05, 0.00012173503637313843, 0.00012707213560740155, 3.337860107421875e-05, 5.608797073364258e-05]}}, 30: {'grad': {'score': [1.428920890973962, 1.2351705976007108, 1.3662543758269279, 1.2331255054565444, 1.1625622164818548], 'topk_tokens': [' the', ' of', ' minutes', ' o', ' The', ' like', 'nes', ' the', ' had', 'burn', ' the', ' Col', ' had', ' to', ' Republicans', "'s", ' B', ' to', ' Sh', 'Sh'], 'evidence_proportions': [1.4314208984375, 1.7246721982955933, 1.2007649739583333, 1.3189697265625, 1.5822296142578125]}, 'weight': {'score': [0.0023654751155687413, 0.00707384014581617, 0.0034731809170015396, 0.007126627671449618, 0.012234235002148536], 'topk_tokens': [' *\n\n', '<|start_header_id|>', 'NEW', '\n\n', ' *\n\n', ' *\n\n', '\n\n', '�', 'system', 'user', '<|start_header_id|>', 'assistant', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '\n\n', 'b', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0005676150321960449, 0.0042432695627212524, 0.002244760592778524, 0.0022079795598983765, 0.003073573112487793]}, 'saliency': {'score': [0.00015420499055281928, 0.0003339588783363595, 0.00021041400970951203, 0.00033587059949489377, 0.0014430553682388799], 'topk_tokens': ['<|eot_id|>', '-text', '***', '�', '<|start_header_id|>', 'user', '<|start_header_id|>', '\n\n', '\n\n', ' discarded', '<|eot_id|>', 'NEW', '<|start_header_id|>', 'system', '<|eot_id|>', '�', '<|end_header_id|>', 'b', '<|begin_of_text|>', 'athroom'], 'evidence_proportions': [2.8115510940551757e-05, 0.0005161464214324951, 4.667043685913086e-05, 7.40736722946167e-05, 0.00019130855798721313]}}, 31: {'grad': {'score': [1.0214186129362688, 1.036096364965936, 0.8823648268176664, 1.037321344797736, 1.1725510627992692], 'topk_tokens': [' throughout', ' based', 'ot', ' it', ' previous', '\n', 'ot', ' and', ' traveled', 'his', 'ot', '<|eot_id|>', 'at', ' until', ' first', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ' governor'], 'evidence_proportions': [0.80142822265625, 0.8133158683776855, 1.2286300659179688, 1.0790939331054688, 1.136016845703125]}, 'weight': {'score': [0.0015681774719901707, 0.006610870813306474, 0.0012332777823171309, 0.0066787266296317125, 0.0032863987068976123], 'topk_tokens': ['?\n', 'user', '.\n\n', 'If', ' discarded', ',', ' Where', 'Question', ' wait', 'Just', ':', '<|start_header_id|>', '<|eot_id|>', 'assistant', '<|eot_id|>', '\n\n', '<|end_header_id|>', 'b', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0010200023651123046, 0.002133890986442566, 0.0015868842601776123, 0.0008794963359832764, 0.00234830379486084]}, 'saliency': {'score': [9.717630303424338e-05, 0.0002508967691123203, 6.518633134903447e-05, 0.00025312734863475373, 0.0003869370106727846], 'topk_tokens': [' presidents', 'If', 'PA', '.\n', 'Question', ' combined', ' offices', 'assistant', '<|eot_id|>', 'system', ' Where', '<|eot_id|>', 'Just', ' wait', '\n\n', ':', '<|begin_of_text|>', '<|end_header_id|>', 'athroom', 'b'], 'evidence_proportions': [0.00011240243911743165, 0.00023332983255386353, 6.673733393351236e-05, 4.174560308456421e-05, 4.3079257011413574e-05]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:32:51.529 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:32:51.529 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-15_pid-1_3-5-7-8-9.pkl | len: 10 |  size: 9.87 KB
Processing depth (3, 5, 7, 8, 9):   2%|▏         | 2/100 [00:25<20:59, 12.86s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Processing depth (0, 1, 3, 6, 7):   2%|▏         | 2/100 [00:36<20:59, 12.86s/it]2025-01-23 22:33:02.122 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:33:02.122 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:33:02.123 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the milk.
2025-01-23 22:33:02.125 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (416, 420) -->  Mary got the milk
2025-01-23 22:33:02.125 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:33:02.132 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (1478, 1484) --> . Mary journeyed to the
2025-01-23 22:33:02.133 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:33:02.144 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2357, 2361) -->  left the milk.
2025-01-23 22:33:02.144 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:33:02.158 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (2839, 2843) -->  Daniel left the apple
2025-01-23 22:33:02.158 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:33:02.161 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (499, 504) --> . John moved to the
2025-01-23 22:33:02.161 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the office.
2025-01-23 22:33:02.163 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (449, 455) --> . Sandra journeyed to the
2025-01-23 22:33:02.163 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel picked up the apple.
2025-01-23 22:33:02.185 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3698, 3703) -->  cold. Daniel picked up
2025-01-23 22:33:02.185 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:33:02.194 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1774, 1778) -->  Daniel took the football
2025-01-23 22:33:02.194 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:33:02.204 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1962, 1967) --> . Sandra moved to the
2025-01-23 22:33:02.204 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:33:02.215 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2225, 2231) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:33:02.751 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:33:02.751 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4228])
your chose emoji: ['🎇', '🧑🏽\u200d🍼', '⚒️', '👨🏿\u200d🦼', '🇬🇫', '🧑🏽\u200d🦽\u200d➡️', '👯', '🏛️', '🤵🏼\u200d♂️', '🧑🏾\u200d🚒']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4231, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 204600.20it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.51it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.85it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.57it/s]
2025-01-23 22:33:05.914 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.08091628033181895, 0.09662423869786435, 0.08565370498165008, 0.09679215097644268, 0.13715348831594806], 'topk_tokens': ['asc', ' of', ' the', ' competitors', ' context', '�', ' the', ' the', ' the', '.', ' in', ' conventions', ' press', ',', ' the', 'the', ' conventions', ' announcement', 'announcement', 'announcement'], 'evidence_proportions': [0.0584228515625, 0.07408714294433594, 0.09865125020345052, 0.08208608627319336, 0.0880899429321289]}, 'weight': {'score': [0.08155054631440536, 0.007429701756202591, 0.005977557551476264, 0.007032344094126425, 0.0008565644695334238], 'topk_tokens': [' ', ' the', '?\n', 'Mary', 'Answer', '.', '\n\n', '<|eot_id|>', ' milk', ' bedroom', 'assistant', ':', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [0.33353424072265625, 0.021862268447875977, 0.011530578136444092, 0.011197030544281006, 0.001642674207687378]}, 'saliency': {'score': [0.0011790148589922035, 2.669146285781734e-05, 9.425609342513545e-05, 1.9844936245732653e-05, 8.830877199564895e-06], 'topk_tokens': [' office', 'A', ' milk', ' location', '\n\n', ' During', '\n\n', ':', ' location', '<|eot_id|>', ' office', '\n\n', ' ', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', ' bedroom', 'athroom', 'b', ' bathroom'], 'evidence_proportions': [0.004805046319961548, 0.00022583454847335815, 0.00017859041690826416, 0.00026123225688934326, 1.8075108528137207e-05]}}, 25: {'grad': {'score': [0.1268910843393077, 0.18267985636729828, 0.12447567139902423, 0.18341901640815544, 0.08521178323928624], 'topk_tokens': [' the', ' the', ' of', ' the', 'ible', ' under', ' his', ' composing', ' the', ' the', 'If', ' under', 'com', 'iring', 'of', ' considerable', ' of', ' composing', ' compos', 'posit'], 'evidence_proportions': [0.11363863945007324, 0.09659051895141602, 0.11021359761555989, 0.1538848876953125, 0.17177963256835938]}, 'weight': {'score': [0.02340883405312248, 0.007210502047483534, 0.004475811796803628, 0.007141604216897319, 0.0007929932581235285], 'topk_tokens': ['.\n\n', ' discarded', ' Bench', ' prior', ' to', '<|eot_id|>', ' the', 'Answer', '?\n', '<|eot_id|>', 'Mary', 'b', ':', 'assistant', '<|start_header_id|>', ' bathroom', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.07364768981933593, 0.021139860153198242, 0.008046622077624003, 0.006911769509315491, 0.0024196207523345947]}, 'saliency': {'score': [0.0002007950907168181, 2.521275769618915e-05, 4.267980975489462e-05, 2.4116307546968575e-05, 9.04643372313617e-06], 'topk_tokens': ['RE', ' PA', 'MIN', ' ST', 'E', '.\n\n', 'athroom', ' Mary', ' prior', ' to', ' Bench', ':', ' bathroom', 'b', '<|start_header_id|>', 'assistant', 'Mary', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0005070626735687256, 0.000277884304523468, 8.872648080190022e-05, 7.060915231704712e-05, 3.916025161743164e-05]}}, 26: {'grad': {'score': [0.1064462247102157, 0.10834051468952191, 0.11351074710969002, 0.10831257393407513, 0.10111974036856873], 'topk_tokens': ['irie', 'ers', '�', '\n', '\n', '\n', ' du', '\n', '\n', '\n', '\n', '\n', '\n', ',\n', '\n', ' Moore', ',\n', '\n', '\n', '\n'], 'evidence_proportions': [0.08524169921875, 0.13043880462646484, 0.12113444010416666, 0.08438301086425781, 0.10899019241333008]}, 'weight': {'score': [0.045246774735658066, 0.007069090922316657, 0.003696430114007765, 0.00688390197267594, 0.0010771694248669769], 'topk_tokens': [' kitchen', '\n\n', ' discarded', ' the', '.\n\n', '<|eot_id|>', ' bedroom', '<|eot_id|>', '?\n', 'Answer', 'assistant', ' the', 'b', '\n\n', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.18416576385498049, 0.0076177120208740234, 0.006254444519678752, 0.0103321373462677, 0.0026302337646484375]}, 'saliency': {'score': [0.0007702902607295824, 3.4716817623220475e-05, 4.0781113409226944e-05, 3.062149026846526e-05, 8.859454768977753e-06], 'topk_tokens': [' the', ' Mary', ' the', 'Bridge', '.\n\n', '***', ' kitchen', '?\n', 'Mary', 'Answer', ' the', '\n\n', 'assistant', 'athroom', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.0032873094081878663, 0.00011440366506576538, 4.006425539652507e-05, 0.00011014193296432495, 3.5390257835388184e-05]}}, 27: {'grad': {'score': [0.22484675697658374, 0.2005964733133697, 0.22912085440851027, 0.2002512465146617, 0.11310888969734924], 'topk_tokens': [',', '.', ' ground', ' *\n\n', ' duration', ' actions', 'ers', ' expected', 'Republicans', '.', '.', '.', '1', ' location', '.', ' other', '!"', '!"', ' execution', ' convention'], 'evidence_proportions': [0.2336334228515625, 0.2111959457397461, 0.19407033920288086, 0.28873252868652344, 0.2097930908203125]}, 'weight': {'score': [0.08696781034054964, 0.007292754034206, 0.004305486717531758, 0.0068762060312783, 0.0010858259788931234], 'topk_tokens': ['<|eot_id|>', ' to', ' milk', 'RE', '?\n', 'Answer', ' the', 'Mary', 'NEW', ' bedroom', 'assistant', '\n\n', '.\n\n', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.3740821838378906, 0.012920856475830078, 0.0054087042808532715, 0.008140742778778076, 0.0032875239849090576]}, 'saliency': {'score': [0.0007878736309383227, 4.604815225233792e-05, 0.00012190303494853358, 4.1400441605136644e-05, 8.75086000520889e-06], 'topk_tokens': [' Mary', '?\n', ' the', ' the', ' the', ' prior', ' to', 'RE', ' bedroom', 'Mary', 'assistant', 'athroom', ':', 'NEW', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', 'b', '.\n\n'], 'evidence_proportions': [0.0029018878936767577, 0.00041907280683517456, 0.00013788541158040365, 0.00019191205501556396, 8.510053157806396e-05]}}, 28: {'grad': {'score': [0.17482648725095, 0.1628183406792868, 0.17781846561739523, 0.16264089484633742, 0.1209663887546487], 'topk_tokens': ['\n', ' the', ' the', '.', "'clock", 'eward', '.', "'clock", '.', ',', ' Peter', 'ivery', 'the', ' L', ' and', ' bend', ' it', ' in', ' out', 'ien'], 'evidence_proportions': [0.2377044677734375, 0.13137531280517578, 0.1853710412979126, 0.1693572998046875, 0.1293325424194336]}, 'weight': {'score': [0.02077156046162481, 0.006860090412166039, 0.006660961335705173, 0.006784966924072368, 0.0006519731593458619], 'topk_tokens': ['.\n\n', ' to', ' the', ' the', '?\n', 'Answer', '<|eot_id|>', ' discarded', '<|eot_id|>', ' the', 'assistant', ' bathroom', ' the', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.060120582580566406, 0.004083156585693359, 0.012115180492401123, 0.018234074115753174, 0.0037957429885864258]}, 'saliency': {'score': [0.0003082933633223824, 2.9701546725582157e-05, 2.978405644816737e-05, 2.8166911919949755e-05, 3.5097337748906386e-06], 'topk_tokens': [' the', ' to', '.\n\n', ' was', ' bedroom', ' the', 'Bridge', 'Answer', ' kitchen', ' the', ' milk', ' garden', '\n\n', 'b', ' bathroom', 'assistant', 'athroom', ':', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0011457562446594237, 4.6119093894958496e-05, 5.348026752471924e-05, 0.0001986473798751831, 1.5504658222198486e-05]}}, 29: {'grad': {'score': [0.09699323902959409, 0.14497369357356787, 0.12064392335953252, 0.14541845616182414, 0.12337927622337864], 'topk_tokens': [' these', ' evidently', ' producing', ' take', ' employed', 'ely', 'ifying', ' impressed', ' who', 'itated', ' generally', ' with', ' available', ' give', ' very', ',', ' which', ' wholly', ' its', ' enough'], 'evidence_proportions': [0.13471412658691406, 0.13202667236328125, 0.0821218490600586, 0.07751607894897461, 0.05659294128417969]}, 'weight': {'score': [0.004677442104920097, 0.007020788704410735, 0.003387733813255064, 0.007060655037523999, 0.0007370698125395056], 'topk_tokens': [' Where', '<|eot_id|>', ' where', ' to', '\n\n', '.', ' the', ' was', '?\n', '.\n\n', 'Answer', ' the', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.007597827911376953, 0.002142608165740967, 0.004183207949002584, 0.006861329078674316, 0.0021192580461502075]}, 'saliency': {'score': [6.127098332280698e-05, 3.95295862834861e-05, 3.9703422977078346e-05, 3.940858052111939e-05, 4.379716638016374e-06], 'topk_tokens': [' where', ' Where', ' the', ' a', ' place', '.', ':', ' to', 'NEW', ' the', ' was', ' the', 'assistant', 'athroom', '<|end_header_id|>', ':', '\n\n', 'b', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [5.0061941146850585e-05, 3.28943133354187e-05, 6.795426209767659e-05, 0.00011910498142242432, 3.5800039768218994e-05]}}, 30: {'grad': {'score': [0.08846684642460036, 0.07709123236832678, 0.0763288467161117, 0.07703425243821531, 0.08557770676808814], 'topk_tokens': [' B', 'ch', ' Out', ' Collection', ' B', ' Dub', ' In', ' exc', ' Fourth', ' The', ' Bench', 'Bridge', ' The', ' Online', ' Clean', 'Civil', ' Gray', ' The', ' Note', ' Bank'], 'evidence_proportions': [0.057836842536926274, 0.11668020486831665, 0.09550778071085612, 0.07761430740356445, 0.0988321304321289]}, 'weight': {'score': [0.03072438032730766, 0.00709532925040692, 0.008053426781008321, 0.006958108949181792, 0.0028944505404119624], 'topk_tokens': [' Where', ':', ' garden', ' the', ' the', 'NEW', 'Mary', ' the', '.\n\n', 'Answer', '?\n', 'assistant', '\n\n', 'b', ' bathroom', '<|end_header_id|>', ':', '<|start_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.09262313842773437, 0.013476371765136719, 0.014326552549997965, 0.0215623676776886, 0.004357695579528809]}, 'saliency': {'score': [0.001525785612023395, 0.00010552834010636996, 7.076994065315492e-05, 9.796587736491869e-05, 2.5025785785831817e-05], 'topk_tokens': ['.\n\n', 'A', ' Mary', ' the', ' the', '<|end_header_id|>', 'E', ' garden', ' bedroom', 'Answer', 'assistant', '?\n', 'Mary', '<|begin_of_text|>', ':', ' the', ' bathroom', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.00595163106918335, 0.0005087926983833313, 0.0002279927333196004, 0.0004277154803276062, 5.523115396499634e-05]}}, 31: {'grad': {'score': [0.07777124254599861, 0.07570570111415595, 0.06201216482347058, 0.07579595540481404, 0.06822749074191263], 'topk_tokens': ['L', '\n', '<|eot_id|>', ' to', ' Bench', 'sur', ' m', ':', ' of', ' B', 'RE', ' by', ' l', '\n', ' in', 'd', ' in', 'g', ' to', 'b'], 'evidence_proportions': [0.08635475039482117, 0.07115130871534348, 0.059555704394976296, 0.0855875164270401, 0.09316882491111755]}, 'weight': {'score': [0.00805968823640243, 0.006538492093213595, 0.0030629029197077598, 0.006555910276858636, 0.0010529087014394265], 'topk_tokens': [' was', ' the', ',', ':', ' Where', ' bathroom', '.\n\n', '<|eot_id|>', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', ':', '<|start_header_id|>', 'b', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.021435546875, 0.00409284234046936, 0.004419227441151937, 0.005117297172546387, 0.0037097930908203125]}, 'saliency': {'score': [9.818051172339398e-05, 2.3071956364039523e-05, 4.340660187505907e-05, 2.2507467308711116e-05, 5.068844311857877e-06], 'topk_tokens': [' Where', 'Answer', 'Bridge', ' location', 'Mary', '<|begin_of_text|>', ' the', ' bathroom', ' the', ' was', '<|eot_id|>', ' the', ' the', '?\n', 'b', 'assistant', 'athroom', '<|end_header_id|>', ':', '<|start_header_id|>'], 'evidence_proportions': [0.000289374589920044, 3.342330455780029e-05, 5.174179871877034e-05, 6.807595491409302e-05, 2.370774745941162e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:33:05.921 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:33:05.921 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-15_pid-2_0-1-3-6-7.pkl | len: 10 |  size: 9.34 KB
Processing depth (0, 1, 3, 6, 7):   3%|▎         | 3/100 [00:40<21:55, 13.56s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Processing depth (1, 2, 5, 6, 8):   3%|▎         | 3/100 [00:51<21:55, 13.56s/it]2025-01-23 22:33:17.420 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:33:17.423 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (462, 467) --> . Mary moved to the
2025-01-23 22:33:17.423 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the milk.
2025-01-23 22:33:17.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1034, 1038) -->  Mary got the milk
2025-01-23 22:33:17.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:33:17.439 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2149, 2155) -->  Mary journeyed to the bedroom
2025-01-23 22:33:17.440 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:33:17.452 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2435, 2439) -->  left the milk.
2025-01-23 22:33:17.452 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:33:17.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3366, 3370) -->  left the apple.
2025-01-23 22:33:17.468 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:33:17.471 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (547, 552) --> . John moved to the
2025-01-23 22:33:17.471 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the office.
2025-01-23 22:33:17.474 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (500, 506) --> . Sandra journeyed to the
2025-01-23 22:33:17.474 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel picked up the apple.
2025-01-23 22:33:17.492 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3743, 3748) --> . Daniel picked up the
2025-01-23 22:33:17.492 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:33:17.501 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1824, 1828) -->  Daniel took the football
2025-01-23 22:33:17.501 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:33:17.511 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1974, 1979) --> . Sandra moved to the
2025-01-23 22:33:17.511 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:33:17.523 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2313, 2319) -->  John went back to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:33:18.010 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bathroom<|eot_id|>
2025-01-23 22:33:18.011 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4223])
your chose emoji: ['🇸🇿', '💆🏻\u200d♀', '👰🏻', '🚶\u200d♂\u200d➡', '🙇🏿\u200d♀', '👨🏻\u200d🦽', '👩🏿\u200d🎨', '🚤', '🤴🏻', '🎺']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4226, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 143395.01it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 91.48it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 95.46it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 97.46it/s]
2025-01-23 22:33:21.066 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.6022239353345789, 0.5095402022080573, 0.643891857516381, 0.5080309435368041, 0.2950880527496338], 'topk_tokens': ['state', ' a', ' first', ' days', ' print', ' that', ' Gov', ' preparation', 'that', ' Grow', ' Gov', ',', ' Grow', ' printed', 'ences', ' that', ' cable', ' cable', ' material', ' make'], 'evidence_proportions': [0.5750244140625, 0.7147102355957031, 0.6396484375, 0.6703338623046875, 0.3994903564453125]}, 'weight': {'score': [0.00108106369557588, 0.007475824983437029, 0.0007207547464678364, 0.007561272415595909, 0.0005596012753598831], 'topk_tokens': ['\n', '\n\n', ':', '\n\n', '?\n', '\n\n', '\n\n', '<|eot_id|>', 'b', 'Answer', '<|end_header_id|>', 'user', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'athroom', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0005194246768951416, 0.0002513378858566284, 0.001244525114695231, 0.001983642578125, 0.0014650672674179077]}, 'saliency': {'score': [0.00018274784088134766, 0.0001318133729247758, 7.98298466590143e-05, 0.00013191883710292476, 7.589161396026611e-05], 'topk_tokens': ['If', '<|end_header_id|>', ' Fourth', 'NEW', '\n\n', '***', '\n\n', '?\n', 'street', ' bathroom', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', 'b', 'Answer', '\n\n', 'athroom', '<|eot_id|>', '<|end_header_id|>', 'assistant'], 'evidence_proportions': [3.8719177246093745e-05, 3.115832805633545e-05, 0.0002525697151819865, 0.00027529895305633545, 0.0003170892596244812]}}, 25: {'grad': {'score': [0.8559256014616593, 0.8219550187363198, 1.0412969896870274, 0.8201379227386797, 0.5006572218502269], 'topk_tokens': [' fr', ' A', ' Hoe', ' to', ' as', '7', ' separate', 'SP', ' NEW', ' for', ' Ch', ' PA', ' was', 'fortunate', 'nes', ' news', ' adher', 'que', 'nes', 'de'], 'evidence_proportions': [1.258402633666992, 0.7739181518554688, 0.5899333953857422, 0.6011886596679688, 1.08856201171875]}, 'weight': {'score': [0.0015016573926676874, 0.007389965635083944, 0.0011686019359096404, 0.00746865531970762, 0.0007415462066145504], 'topk_tokens': ['Civil', ' bogus', '\n\n', ':', '\n\n', 'b', '<|eot_id|>', '<|end_header_id|>', 'user', '<|start_header_id|>', '?\n', '<|start_header_id|>', '<|eot_id|>', 'Answer', '<|eot_id|>', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0008567154407501221, 0.0008255019783973694, 0.0025111983219782514, 0.001995876431465149, 0.0009754598140716553]}, 'saliency': {'score': [0.00038327989370926565, 0.00025245040951325776, 7.019888970159715e-05, 0.00025308337187607017, 5.087694701026468e-05], 'topk_tokens': ['sur', ' Pioneer', ' random', ' especial', ' God', '<|start_header_id|>', 'b', 'rail', '<|start_header_id|>', ' Pioneer', '?\n', 'user', 'Answer', 'athroom', '<|eot_id|>', '<|eot_id|>', 'assistant', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [0.00020929574966430665, 0.00025741755962371826, 0.0007343490918477376, 0.00041671842336654663, 0.00016658008098602295]}}, 26: {'grad': {'score': [0.5617164943529211, 0.5974123982304484, 0.523869514465332, 0.598155647316235, 0.6289735681870404], 'topk_tokens': ['      ', ' ', '\n', ' for', ',\n', '\n', '?\n', '\n', 'description', '      ', '\n', ' $', '      ', ' $', '\n\n', '      ', '\n', ' for', ' Eagle', ' fore'], 'evidence_proportions': [0.57269287109375, 0.46671295166015625, 0.6517842610677084, 0.5897445678710938, 0.4798698425292969]}, 'weight': {'score': [0.001138987748519234, 0.007267012959456365, 0.0010710522051780454, 0.007346835433865325, 0.0036920834990108713], 'topk_tokens': ['ye', ' bogus', '\n\n', '<|start_header_id|>', ':', 'Answer', 'user', '<|end_header_id|>', '<|start_header_id|>', '\n\n', 'b', '\n\n', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', 'athroom', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0003040611743927002, 0.0003290623426437378, 0.0017871310313542683, 0.0012825727462768555, 0.0018767714500427246]}, 'saliency': {'score': [0.0001364842705104662, 0.00016152828409950483, 0.00010018002602361864, 0.00016212219788495555, 0.0002861868809251224], 'topk_tokens': ['stage', '\n\n', ' Stewart', ' Pioneer', 'b', '<|eot_id|>', 'doctor', '<|end_header_id|>', '<|start_header_id|>', ' Pioneer', ' bogus', 'ye', '<|start_header_id|>', ' bathroom', 'assistant', 'athroom', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1.8030405044555664e-05, 2.956390380859375e-05, 0.00038166840871175134, 1.7568469047546387e-05, 0.000142611563205719]}}, 27: {'grad': {'score': [0.9705730935801631, 1.1571416967692558, 1.0181387624432963, 1.1592030986957789, 0.9976284924675437], 'topk_tokens': [',', ' in', ',', ' days', ' in', ' in', ' in', ' In', ' in', ' in', ' in', ' in', ' in', 'In', ' in', ',', ',', ',', ';', ','], 'evidence_proportions': [1.4558593750000002, 0.59893798828125, 0.9317830403645833, 1.0286712646484375, 0.735687255859375]}, 'weight': {'score': [0.002249233100725257, 0.007357267875328371, 0.001365756796252343, 0.007429948039102874, 0.003984851872219759], 'topk_tokens': ['RI', '?\n', 'If', ' bogus', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', 'Answer', '\n\n', '\n\n', '<|eot_id|>', 'b', '<|eot_id|>', 'user', '\n\n', '<|start_header_id|>', '<|end_header_id|>', 'athroom', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [0.0007068395614624023, 0.0005181655287742615, 0.0038427114486694336, 0.0021743178367614746, 0.0035929903388023376]}, 'saliency': {'score': [0.00044919226480566937, 0.00030834758473525836, 0.0003633297258807767, 0.00030716257178771986, 0.0004737876793917488], 'topk_tokens': ['If', 'EF', 'stage', ' Joseph', 'Minnesota', 'had', ' John', '<|end_header_id|>', '<|eot_id|>', '\n\n', '<|end_header_id|>', ' Chicago', 'b', 'athroom', '<|start_header_id|>', ' bathroom', '<|begin_of_text|>', 'user', 'assistant', '<|start_header_id|>'], 'evidence_proportions': [0.0002043783664703369, 0.00010788440704345703, 0.0009518613417943318, 6.445497274398804e-05, 0.0007272511720657349]}}, 28: {'grad': {'score': [0.5367172904636549, 0.5372475201182043, 0.5038663187334614, 0.537498481893128, 0.5526119119980756], 'topk_tokens': [' *', ' Min', '!"', ' Min', ' Min', ' Min', 'Min', '8', ' Min', ' Min', ' "', ' *', 'Min', ' *', ' Min', ' Min', ' Min', ' Min', ' Min', 'Min'], 'evidence_proportions': [0.4753875732421875, 0.5678558349609375, 0.6119740804036459, 0.6219482421875, 0.384124755859375]}, 'weight': {'score': [0.0007062865340191385, 0.007020820332431026, 0.0005081526694759246, 0.007104024305325341, 0.0015299670836504769], 'topk_tokens': [' block', '\n\n', '\n', '<|start_header_id|>', '<|start_header_id|>', '?\n', '<|eot_id|>', ':', 'user', 'Answer', '\n\n', '<|eot_id|>', '<|eot_id|>', 'b', '<|start_header_id|>', 'athroom', 'assistant', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0001821577548980713, 0.0001525282859802246, 0.0012754350900650024, 0.0009069144725799561, 0.000860854983329773]}, 'saliency': {'score': [7.512647172679071e-05, 0.00024423998252960725, 2.739410246572187e-05, 0.0002467835666692154, 5.859048927531523e-05], 'topk_tokens': [' During', '?\n', ' block', '\n', ':', '<|start_header_id|>', 'Today', 'Answer', 'user', ' milk', '\n\n', '<|eot_id|>', '<|eot_id|>', 'During', 'athroom', '<|begin_of_text|>', 'assistant', 'b', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [5.388259887695313e-06, 1.3209879398345947e-05, 0.00020766258239746094, 5.961954593658447e-05, 4.0918588638305664e-05]}}, 29: {'grad': {'score': [1.3168547257133152, 1.2355403873066286, 1.3459073958858367, 1.2342720251183954, 0.5170963301378138], 'topk_tokens': [' border', ' B', ' they', ' be', ' leap', ' second', 'Bridge', ' im', ' Cl', ' l', ' pur', ' Bench', 'pl', 'b', 'ball', ' Sh', 'Sh', ' B', ' J', ' Gen'], 'evidence_proportions': [1.1767822265625, 1.449615478515625, 1.324915568033854, 1.3894119262695312, 1.2745361328125]}, 'weight': {'score': [0.001155306463656218, 0.007205427849512782, 0.0007483440060769358, 0.007286761116775776, 0.0026522962486042697], 'topk_tokens': ['\n', '<|start_header_id|>', '<|start_header_id|>', 'If', '<|eot_id|>', 'During', ':', '\n\n', '\n\n', 'user', 'b', '<|eot_id|>', '<|eot_id|>', 'Answer', '<|start_header_id|>', 'athroom', '\n\n', 'assistant', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.00030364990234375, 0.00014382600784301758, 0.0018607576688130696, 0.0024229586124420166, 0.0009055286645889282]}, 'saliency': {'score': [4.395194675611413e-05, 0.0002972904788449202, 7.247540258592175e-05, 0.0003003576057869316, 0.00035384867121191583], 'topk_tokens': ['could', 'A', '<|eot_id|>', ' S', ' He', '\n', '185', '\n\n', 'Answer', 'user', 'athroom', 'During', 'b', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'If', '<|end_header_id|>', 'assistant', '<|begin_of_text|>'], 'evidence_proportions': [2.1243095397949217e-05, 8.143484592437744e-06, 8.146464824676514e-05, 4.773586988449097e-05, 4.809349775314331e-05]}}, 30: {'grad': {'score': [0.8629214245340099, 0.9214474217224843, 0.9429268990793536, 0.9216104692146396, 0.57666454595678], 'topk_tokens': [' Adams', 'ed', ' heard', ' bogus', ' a', ' await', ' as', 'ed', ' As', ' bogus', ' raid', ' always', ' below', ' B', '9', ' B', ' based', 'ab', 'b', ' Associated'], 'evidence_proportions': [0.9147296905517578, 0.7533721923828125, 0.8171132405598959, 0.7963104248046875, 1.0430335998535156]}, 'weight': {'score': [0.002367898173954176, 0.006936722325308313, 0.001608830305837816, 0.007001498837313281, 0.006223163183997659], 'topk_tokens': [' bogus', 'NEW', '?\n', '<|end_header_id|>', ':', '\n\n', 'Answer', '<|eot_id|>', '<|start_header_id|>', '<|start_header_id|>', 'user', '<|eot_id|>', 'athroom', '<|eot_id|>', '\n\n', 'assistant', 'b', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0010090291500091553, 0.0004174932837486267, 0.003336787223815918, 0.004243612289428711, 0.0026878416538238525]}, 'saliency': {'score': [0.0001970374065896739, 0.00034472232766474137, 9.187767582554971e-05, 0.0003474152656781022, 0.0006844844011699452], 'topk_tokens': ['�', ' Stewart', '\n', '\n\n', '<|eot_id|>', 'user', 'Civil', ' bathroom', '\n\n', '<|eot_id|>', '\n\n', 'Answer', 'NEW', ' bogus', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'b', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [6.679892539978028e-05, 4.649162292480469e-05, 0.0003313322861989339, 0.0003259405493736267, 0.00018003582954406738]}}, 31: {'grad': {'score': [0.5196802719779636, 0.6286388276258578, 0.5907778586110761, 0.6295208354924349, 0.8571468381320729], 'topk_tokens': [' PA', 'ot', 'ot', '�', '.', '202', ' ', ' an', '202', ' PA', '<|end_header_id|>', 'user', '<|start_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.5429931640625, 0.35787391662597656, 0.6027882893880209, 0.21318721771240234, 0.8341765403747559]}, 'weight': {'score': [0.0014111555140951405, 0.006773794537971666, 0.0013113223737285984, 0.006843947302746521, 0.002153126194196589], 'topk_tokens': [' dropped', ' what', 'Just', '<|eot_id|>', ' One', '\n\n', 'If', 'user', ':', 'Answer', '<|eot_id|>', '<|start_header_id|>', '?\n', '<|eot_id|>', '\n\n', 'b', 'assistant', '<|end_header_id|>', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.0008798062801361084, 0.0002717450261116028, 0.002281586329142253, 0.0018520951271057129, 0.001468166708946228]}, 'saliency': {'score': [8.387021396471107e-05, 0.0002957678946961287, 8.138245151888939e-05, 0.000298529063295198, 0.00014707533752217014], 'topk_tokens': ['PA', '<|eot_id|>', ' You', '\n\n', ':', ' One', ' He', 'If', '\n\n', 'Bridge', 'user', '<|eot_id|>', '<|start_header_id|>', '?\n', '<|eot_id|>', 'assistant', '<|begin_of_text|>', 'athroom', 'b', '<|end_header_id|>'], 'evidence_proportions': [5.4824352264404294e-05, 1.0676681995391846e-05, 9.522338708241782e-05, 7.302314043045044e-05, 0.00018718838691711426]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-23 22:33:21.074 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:33:21.074 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-15_pid-3_1-2-5-6-8.pkl | len: 10 |  size: 9.76 KB
Processing depth (1, 2, 5, 6, 8):   4%|▍         | 4/100 [00:55<22:42, 14.19s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Processing depth (0, 1, 2, 5, 9):   4%|▍         | 4/100 [01:04<22:42, 14.19s/it]2025-01-23 22:33:30.392 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Mary moved to the bathroom.
2025-01-23 22:33:30.392 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-23 22:33:30.393 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Mary got the milk.
2025-01-23 22:33:30.395 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (412, 416) -->  Mary got the milk
2025-01-23 22:33:30.395 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-23 22:33:30.400 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (915, 921) --> . Mary journeyed to the
2025-01-23 22:33:30.400 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary left the milk.
2025-01-23 22:33:30.410 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2131, 2135) -->  Mary left the milk
2025-01-23 22:33:30.410 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Daniel left the apple.
2025-01-23 22:33:30.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3670, 3674) -->  Daniel left the apple
2025-01-23 22:33:30.428 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John moved to the garden.
2025-01-23 22:33:30.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (463, 468) --> . John moved to the
2025-01-23 22:33:30.431 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Sandra journeyed to the office.
2025-01-23 22:33:30.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (428, 434) -->  Sandra journeyed to the office
2025-01-23 22:33:30.433 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel picked up the apple.
2025-01-23 22:33:30.451 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3752, 3757) --> . Daniel picked up the
2025-01-23 22:33:30.451 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Daniel took the football.
2025-01-23 22:33:30.460 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (1787, 1791) -->  took the football.
2025-01-23 22:33:30.460 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:33:30.470 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (1974, 1979) --> . Sandra moved to the
2025-01-23 22:33:30.470 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  John went back to the office.
2025-01-23 22:33:30.482 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2291, 2297) -->  execution. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:33:31.024 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The bathroom.<|eot_id|>
2025-01-23 22:33:31.024 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4240])
your chose emoji: ['🧑🏼\u200d🏭', '🧑\u200d🦳', '🦸🏼\u200d♀', '🤸\u200d♀', '💂🏾\u200d♀️', '👨\u200d👨\u200d👦\u200d👦', '🏌️\u200d♂️', '👨🏼\u200d⚕️', '\U0001fa87', '🧝🏽\u200d♂️']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4243, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 225197.53it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 124.98it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.15it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.66it/s]
2025-01-23 22:33:34.171 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.08267232646112857, 0.10492378502782156, 0.08955932432605375, 0.10515966037487068, 0.1287510475691627], 'topk_tokens': [' of', '.', ' convention', ' in', ' in', '.', ' the', ' context', ' the', ' completed', ' the', ' conventions', 'the', ' the', ' the', ' conventions', ',', 'announcement', ' announcement', 'announcement'], 'evidence_proportions': [0.06951751708984374, 0.06265473365783691, 0.10562769571940105, 0.09610176086425781, 0.07127094268798828]}, 'weight': {'score': [0.07897454241047734, 0.0074058004968604076, 0.005088870563814717, 0.007029993326631529, 0.0010043480817009422], 'topk_tokens': ['Mary', '\n\n', '?\n', ' ', 'Answer', '.', '\n\n', '<|eot_id|>', ' milk', 'assistant', ':', ' bedroom', '<|eot_id|>', '\n\n', '<|start_header_id|>', 'b', '<|end_header_id|>', 'athroom', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [0.31914825439453126, 0.021123647689819336, 0.01152267058690389, 0.014125227928161621, 0.0026354193687438965]}, 'saliency': {'score': [0.0009724523710167927, 2.429771473911779e-05, 8.505678945972073e-05, 1.86421672550583e-05, 9.142300661872415e-06], 'topk_tokens': ['.\n\n', ' milk', ' journey', ' milk', ' location', '\n\n', '\n\n', '<|eot_id|>', '<|begin_of_text|>', ':', '<|eot_id|>', ' location', '\n\n', '<|end_header_id|>', ' ', '<|start_header_id|>', 'athroom', ' bedroom', 'b', ' bathroom'], 'evidence_proportions': [0.0037290871143341067, 0.00029737502336502075, 0.0002240488926569621, 0.0002697780728340149, 2.70158052444458e-05]}}, 25: {'grad': {'score': [0.12204808774201767, 0.17293323999639112, 0.10550705079109438, 0.17371160484891315, 0.0898768985972685], 'topk_tokens': [' the', ' of', ' the', ' com', ' under', ' the', ' the', ' considerable', 'of', 'ible', 'com', 'If', ' composing', ' under', ' of', ' of', 'iring', ' compos', ' composing', 'posit'], 'evidence_proportions': [0.098345947265625, 0.09947395324707031, 0.11220932006835938, 0.14401626586914062, 0.1670398712158203]}, 'weight': {'score': [0.02136850357055664, 0.007199575337435372, 0.003187743886824577, 0.007151468730991625, 0.000978003179325777], 'topk_tokens': ['\n\n', ' Mary', ' discarded', '.\n\n', ' prior', ' to', '<|eot_id|>', 'Answer', '?\n', 'Mary', '<|eot_id|>', 'b', 'assistant', ':', '<|start_header_id|>', ' bathroom', 'athroom', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.06562004089355469, 0.0190885066986084, 0.008299291133880615, 0.0073653459548950195, 0.0019410550594329834]}, 'saliency': {'score': [0.00017966394839079484, 2.3053385248842763e-05, 2.6089529837331463e-05, 2.217103541964533e-05, 1.565989326028263e-05], 'topk_tokens': [' Mary', 'E', ' PA', '<|eot_id|>', '.\n\n', ' prior', ' Mary', '�', 'Answer', ' to', 'athroom', ' bathroom', ':', '<|start_header_id|>', 'b', 'assistant', 'Mary', '<|end_header_id|>', '<|begin_of_text|>', '\n\n'], 'evidence_proportions': [0.0003542840480804443, 0.00027798116207122803, 9.777645270029704e-05, 0.00011814385652542114, 4.7422945499420166e-05]}}, 26: {'grad': {'score': [0.09820842742919922, 0.10054137306114483, 0.0931635518227854, 0.1006087806066029, 0.09909542869119083], 'topk_tokens': ['�', '\n', '\n', ' du', 'ER', ',\n', '\n', '\n', '\n', '\n', ',\n', ' Moore', '\n', '\n', '\n', '\n', '\n', '\n', ',\n', '\n'], 'evidence_proportions': [0.08176116943359375, 0.12317276000976562, 0.10636138916015625, 0.09192466735839844, 0.08785748481750488]}, 'weight': {'score': [0.04084265491236811, 0.007048966270444979, 0.0028944073184843985, 0.0068941647638196615, 0.001138973586699542], 'topk_tokens': [' prior', ' the', '\n\n', ' discarded', ' the', ' bedroom', '.\n\n', '<|eot_id|>', '?\n', '<|eot_id|>', 'Answer', 'assistant', 'b', '\n\n', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', 'athroom', ':', '<|begin_of_text|>'], 'evidence_proportions': [0.16171159744262695, 0.007361173629760742, 0.007006039222081502, 0.011368632316589355, 0.0034669041633605957]}, 'saliency': {'score': [0.0006331943947335948, 3.458124383732645e-05, 2.6935531247046684e-05, 3.1351096933455476e-05, 1.0177317787619198e-05], 'topk_tokens': [' a', ' Mary', '185', 'user', '***', '.\n\n', ' bedroom', ' kitchen', '?\n', 'Mary', 'Answer', '\n\n', 'athroom', 'assistant', '<|start_header_id|>', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', ':', 'b'], 'evidence_proportions': [0.0026677191257476808, 8.626282215118408e-05, 4.2632222175598145e-05, 9.897351264953613e-05, 5.7034194469451904e-05]}}, 27: {'grad': {'score': [0.19665228802224863, 0.17835655095524983, 0.2218696532710906, 0.17793408542067546, 0.10075211244470933], 'topk_tokens': [' said', ' hour', ' duration', '.', ' location', '.', ' work', '.', 'ers', ' actions', ' ground', 'Republicans', '.', '1', ' *\n\n', ' other', '!"', '!"', ' execution', ' convention'], 'evidence_proportions': [0.20909576416015624, 0.20557832717895508, 0.17409483591715494, 0.2180032730102539, 0.18465709686279297]}, 'weight': {'score': [0.07440996170043945, 0.007276342860825804, 0.0035258858434615596, 0.006935495626217826, 0.0012832168270559873], 'topk_tokens': [' discarded', 'RE', '<|eot_id|>', '<|eot_id|>', ' to', '?\n', 'Answer', 'Mary', ' bedroom', 'NEW', 'assistant', '\n\n', '.\n\n', 'b', ':', '<|start_header_id|>', '<|end_header_id|>', ' bathroom', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.31560573577880857, 0.011168360710144043, 0.00625687837600708, 0.008937478065490723, 0.0038589537143707275]}, 'saliency': {'score': [0.0006432300028593644, 4.229951854474753e-05, 0.00011949673775703676, 3.8428782107694016e-05, 6.3924228443818935e-06], 'topk_tokens': [' the', '\n\n', '?\n', ' the', ' the', ' the', ' prior', ' to', 'RE', 'Mary', 'assistant', 'athroom', ' bathroom', '<|start_header_id|>', 'NEW', ':', '<|begin_of_text|>', '<|end_header_id|>', 'b', '.\n\n'], 'evidence_proportions': [0.0022470057010650635, 0.00032191723585128784, 9.227792421976726e-05, 0.0003139898180961609, 0.00011549144983291626]}}, 28: {'grad': {'score': [0.16513272990351138, 0.14949548646242247, 0.14495025142546622, 0.1494432653325587, 0.09904209024765913], 'topk_tokens': ['.', ' it', "'clock", ' the', ' item', 'eward', '.', 'ivery', '.', ' the', "'clock", ' L', ' Peter', ' and', ' out', 'the', ' bend', ' in', ' it', 'ien'], 'evidence_proportions': [0.2242218017578125, 0.1324458122253418, 0.16613340377807617, 0.1551990509033203, 0.13239097595214844]}, 'weight': {'score': [0.01738726574441661, 0.0068485096830987946, 0.005273619005756993, 0.006802300616874704, 0.0006780845277449663], 'topk_tokens': [' to', '?\n', ' the', ' the', 'Answer', ' the', ' the', ' discarded', '<|eot_id|>', ' the', '<|eot_id|>', ' bathroom', 'assistant', '<|end_header_id|>', 'b', '\n\n', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.04536933898925782, 0.004415988922119141, 0.008556524912516275, 0.021256089210510254, 0.004758238792419434]}, 'saliency': {'score': [0.00024044902428336766, 2.816398133092013e-05, 2.2576701256536667e-05, 2.7039763067468136e-05, 3.686021356021657e-06], 'topk_tokens': ['<|end_header_id|>', ' the', ' the', ' to', ' was', '.\n\n', 'Bridge', '\n\n', ' the', ' milk', ' kitchen', 'Answer', ' garden', ' bathroom', 'b', 'assistant', ':', 'athroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0007936060428619385, 4.97475266456604e-05, 4.095832506815592e-05, 0.0002619549632072449, 1.7434358596801758e-05]}}, 29: {'grad': {'score': [0.09835832015327785, 0.13743593825952305, 0.10397266187975483, 0.13789813610727106, 0.09894614219665528], 'topk_tokens': [' employed', ' also', ' his', ' producing', 'itated', ' their', ' advantage', ' with', 'ely', ' available', ' impressed', ' give', ' which', ' very', ',', ' generally', ' who', ' wholly', ' its', ' enough'], 'evidence_proportions': [0.13797149658203126, 0.13162708282470703, 0.08259646097819011, 0.07437801361083984, 0.06319618225097656]}, 'weight': {'score': [0.005206963290338931, 0.007020977738609917, 0.00246313214302063, 0.007064667293580912, 0.0005033777040593764], 'topk_tokens': [' where', '.', '\n\n', ' Where', '\n\n', ' to', ' the', ' was', '<|eot_id|>', '?\n', 'Answer', '.\n\n', 'b', 'assistant', '<|end_header_id|>', 'athroom', ':', '\n\n', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.006117582321166992, 0.0038052797317504883, 0.004487872123718262, 0.009901762008666992, 0.0018542110919952393]}, 'saliency': {'score': [7.901502692181131e-05, 3.577087135099168e-05, 2.544541512766192e-05, 3.560984809121473e-05, 3.8048800300149357e-06], 'topk_tokens': ['G', ' a', ' Where', ' the', '.', ' place', ':', ' the', ' to', ' the', 'athroom', 'NEW', ' was', 'assistant', '<|end_header_id|>', ':', '\n\n', 'b', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [3.592371940612793e-05, 5.109608173370361e-05, 8.692344029744467e-05, 0.00021345168352127075, 1.449882984161377e-05]}}, 30: {'grad': {'score': [0.07561204744421918, 0.07833833714686782, 0.078830688230453, 0.07834966247023134, 0.07366553755367504], 'topk_tokens': ['ch', ' Dub', ' B', ' Bench', ' The', ' Out', 'at', 'Bridge', ' The', ' Knowledge', ' Collection', ' Fourth', ' Online', ' Clean', ' Gray', ' In', 'Civil', ' Bank', ' Note', ' The'], 'evidence_proportions': [0.06095771789550781, 0.10394859313964844, 0.08334628740946452, 0.05642056465148926, 0.07318353652954102]}, 'weight': {'score': [0.027974740318630054, 0.007072865218450909, 0.006520069414569485, 0.006962192872453345, 0.002260661475798663], 'topk_tokens': ['<|eot_id|>', ':', ' the', 'NEW', '\n\n', ' the', ' the', 'Mary', '.\n\n', 'Answer', '?\n', 'assistant', ' bathroom', '\n\n', 'b', '<|end_header_id|>', '<|start_header_id|>', ':', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.07482185363769531, 0.014499425888061523, 0.013576189676920574, 0.02676832675933838, 0.0056954026222229]}, 'saliency': {'score': [0.0011556018953738005, 9.316002285876646e-05, 5.992958622594034e-05, 8.757252714804118e-05, 2.108637024374569e-05], 'topk_tokens': [':', ' the', ' Mary', ' the', '<|eot_id|>', '.\n\n', 'E', ' garden', ' the', '?\n', 'assistant', 'Answer', 'Mary', '<|end_header_id|>', '<|begin_of_text|>', ':', ' bathroom', 'athroom', '<|start_header_id|>', 'b'], 'evidence_proportions': [0.004486715793609619, 0.0003974214196205139, 0.0001551757256189982, 0.00030546635389328003, 0.00010066479444503784]}}, 31: {'grad': {'score': [0.08055488311726114, 0.0772831968448647, 0.06872912568430747, 0.07732853640602778, 0.06215430743554059], 'topk_tokens': ['\n', 'L', 'sur', ':', ' Bench', ' of', '\u200d', 'RE', '\n', ' m', ' B', ' l', ' in', 'g', ' in', '\n', ' by', 'b', 'd', ' to'], 'evidence_proportions': [0.08702800869941711, 0.07294993847608566, 0.06589679419994354, 0.10024361312389374, 0.08236682415008545]}, 'weight': {'score': [0.007138480310854705, 0.006520011473904515, 0.002089965728021437, 0.006549399546206381, 0.0008084076292374555], 'topk_tokens': [' bedroom', ' the', ',', ':', ' bathroom', '.\n\n', ' Where', '<|eot_id|>', ' the', 'Answer', '?\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', 'b', ':', '<|end_header_id|>', '\n\n', 'athroom', '<|begin_of_text|>'], 'evidence_proportions': [0.01719818115234375, 0.005367279052734375, 0.00461729367574056, 0.004047513008117676, 0.0032078027725219727]}, 'saliency': {'score': [7.103318753449813e-05, 2.0119194542774466e-05, 2.896305053464828e-05, 1.9774200182650876e-05, 3.543671439675724e-06], 'topk_tokens': ['Bridge', ' the', ' was', ' location', ' the', ' bathroom', '.\n\n', 'Answer', ' the', '<|eot_id|>', ' the', ' the', 'b', '?\n', '<|begin_of_text|>', 'assistant', '<|end_header_id|>', ':', 'athroom', '<|start_header_id|>'], 'evidence_proportions': [0.0002021312713623047, 1.835078001022339e-05, 2.1134813626607258e-05, 7.510930299758911e-05, 3.061443567276001e-05]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-23 22:33:34.177 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:33:34.178 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/4-hop_sid-15_pid-4_0-1-2-5-9.pkl | len: 10 |  size: 9.36 KB
Processing depth (0, 1, 2, 5, 9):   5%|▌         | 5/100 [01:08<21:50, 13.80s/it]Processing depth (0, 1, 2, 5, 9):   5%|▌         | 5/100 [01:08<21:43, 13.73s/it]
2025-01-23 22:33:34.521 | INFO     | __main__:<module>:99 - Selected idx: 16
2025-01-23 22:33:34.521 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-23 22:33:34.522 | INFO     | __main__:<module>:101 - Answer: hallway
2025-01-23 22:33:34.522 | INFO     | __main__:<module>:102 - Tag: 3-hop
2025-01-23 22:33:34.522 | INFO     | __main__:<module>:103 - Needle: [' John went back to the office.', ' John journeyed to the bedroom.', ' Daniel grabbed the apple.', ' John moved to the garden.', ' Sandra journeyed to the office.', ' Sandra moved to the kitchen.', ' Sandra went back to the hallway.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-23 22:33:34.522 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel grabbed the apple.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-23 22:33:34.522 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Processing depth (1, 3, 5, 9):   0%|          | 0/100 [00:10<?, ?it/s]2025-01-23 22:33:45.227 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel grabbed the apple.
2025-01-23 22:33:45.229 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (446, 450) -->  Daniel grabbed the apple
2025-01-23 22:33:45.229 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-23 22:33:45.237 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1510, 1515) --> . Daniel travelled to the
2025-01-23 22:33:45.237 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the garden.
2025-01-23 22:33:45.248 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2160, 2166) -->  Daniel went back to the garden
2025-01-23 22:33:45.248 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-23 22:33:45.266 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3693, 3698) -->  cold. Mary travelled to
2025-01-23 22:33:45.266 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the office.
2025-01-23 22:33:45.282 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2482, 2488) --> . John went back to the
2025-01-23 22:33:45.282 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the bedroom.
2025-01-23 22:33:45.283 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (67, 73) --> . John journeyed to the
2025-01-23 22:33:45.283 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:33:45.287 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (803, 808) --> . John moved to the
2025-01-23 22:33:45.287 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:33:45.290 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (734, 740) --> . Sandra journeyed to the
2025-01-23 22:33:45.290 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:33:45.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3363, 3368) --> . Sandra moved to the
2025-01-23 22:33:45.307 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra went back to the hallway.
2025-01-23 22:33:45.317 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2013, 2019) -->  the ground. Sandra went back
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:33:45.791 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:33:45.791 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4222])
your chose emoji: ['🐋', '🖖🏽', '👬🏼', '🇨🇦', '\U0001fae2', '👨🏼\u200d❤️\u200d👨🏻', '👰\u200d♀️', '⛹🏽\u200d♀️', '🧑🏻\u200d❤️\u200d💋\u200d🧑🏽', '🕕']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4225, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 193956.25it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 112.67it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.82it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.57it/s]
2025-01-23 22:33:49.086 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.24643540382385254, 0.3594982404539571, 0.33444494359633503, 0.3602446007574242, 0.3853810855320522], 'topk_tokens': [' H', ' of', ' passengers', 'P', 'hand', 'ly', ' heard', "'s", ' him', ' him', ' head', ' hear', 'way', ' heard', ' hear', ' hung', ' headlines', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.2962150573730469, 0.16993865966796876, 0.1881866455078125, 0.3530069351196289]}, 'weight': {'score': [0.31601614505052567, 0.007469320805115107, 0.24258228347581975, 0.004073306128607754, 0.0033233142518377923], 'topk_tokens': ['\n\n', '\n\n', 'hall', ' \n', '<|eot_id|>', 'Answer', '<|start_header_id|>', '.', 'way', '<|end_header_id|>', '<|eot_id|>', 'assistant', 'y', 'ching', 'ible', '.', ' the', '.', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0006706938147544861, 1.2582899570465087, 0.0008369187513987223, 0.004233765602111817]}, 'saliency': {'score': [0.0011195734143257142, 0.00010449307204703608, 0.0009347112739787382, 9.285820613688229e-05, 0.00017255660775419954], 'topk_tokens': ['\u200d', '\n\n', '<|start_header_id|>', ' Where', 'Question', '\n\n', 'y', 'Answer', '<|eot_id|>', '<|eot_id|>', '\n\n', 'ching', 'ible', '<|begin_of_text|>', '.', ' the', '.', '.', 'way', 'hall'], 'evidence_proportions': [9.253621101379395e-06, 0.00433661937713623, 1.633167266845703e-05, 0.00011467337608337403]}}, 25: {'grad': {'score': [0.8214336395263672, 0.7725940735946746, 0.8235011661753935, 0.7719449169256796, 0.628135229085947], 'topk_tokens': [' at', ' extraordinary', ' random', ' at', ' customary', ' bag', ' grat', ' At', ' Adams', ' double', ' Charles', '-t', ' At', ' laid', ' At', ' Ramsey', ' Gal', ' at', ' laid', ' Gal'], 'evidence_proportions': [0.9071464538574219, 0.6219879150390626, 1.0140533447265625, 0.7211654663085938]}, 'weight': {'score': [0.37409027814865115, 0.007485080470700236, 0.2587231870959787, 0.0036792306555915124, 0.002610763172050575], 'topk_tokens': ['<|start_header_id|>', 'Answer', '?', '<|eot_id|>', '.\n\n', 'way', '<|eot_id|>', ' \n', 'assistant', '.', '\n\n', '<|end_header_id|>', 'y', 'ible', 'ching', '.', '.', ' the', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0013614296913146973, 1.492303657531738, 0.0009508083264032999, 0.001827341318130493]}, 'saliency': {'score': [0.0016508802771568298, 0.00011270168970322468, 0.0012407828779781565, 9.613052399945585e-05, 0.00011497619864228484], 'topk_tokens': ['      ', ' \n', '<|eot_id|>', 'Answer', '      ', '.\n\n', ' return', 'ENCES', ' write', 'y', 'ible', 'way', '.', 'ching', '.', ' the', '<|end_header_id|>', '<|begin_of_text|>', '.', '\n\n'], 'evidence_proportions': [7.623434066772461e-05, 0.006350082159042358, 6.0195724169413246e-05, 0.00012021660804748536]}}, 26: {'grad': {'score': [0.2614318370819092, 0.3126649466068787, 0.3138865302590763, 0.31290065227610064, 0.4783225864559025], 'topk_tokens': [' all', ' air', ' enraged', '�', ' Col', ' wield', ' all', ' air', '.\n\n', 'g', ' pen', ' Col', ' bend', 'pend', '3', ' Hale', ' Hill', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.3100433349609375, 0.1300443649291992, 0.3130772908528646, 0.29195556640625]}, 'weight': {'score': [0.26745991706848143, 0.007329816197502542, 0.20618213976130767, 0.004461539760535639, 0.007874882066404664], 'topk_tokens': ['.\n\n', 'hall', '<|start_header_id|>', ':', '.\n\n', 'Answer', '<|end_header_id|>', '<|eot_id|>', ' \n', '?', 'assistant', 'way', 'y', 'ching', 'ible', '.', ' the', '.', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0014179646968841553, 1.0605499267578127, 0.0020065704981486, 0.005747485160827637]}, 'saliency': {'score': [0.0005587071180343627, 0.0001480341239793766, 0.00046784211607540357, 0.000143458019541025, 0.0003259096826825823], 'topk_tokens': ['\u200d', ' *\n\n', ' *\n\n', ' the', '<|eot_id|>', ' *\n\n', ' *\n\n', '<|eot_id|>', '<|end_header_id|>', 'assistant', '.', '.', ' *\n\n', '.', 'ching', 'ible', 'y', '<|begin_of_text|>', 'way', 'hall'], 'evidence_proportions': [4.453212022781372e-05, 0.0019469082355499268, 9.551644325256348e-05, 0.00013767480850219728]}}, 27: {'grad': {'score': [0.25125765800476074, 0.2542922594420303, 0.3003395304960363, 0.25393145503371317, 0.2641996160730139], 'topk_tokens': [' bag', '600', ' garden', ' appear', ' ranks', ' block', '600', ' veto', ' Bench', ' courts', ' Ear', ' court', 'rail', ' court', ' mob', '240', ' board', '800', 'hall', ' Bank'], 'evidence_proportions': [0.3257713317871094, 0.10860805511474608, 0.31007893880208337, 0.26371078491210936]}, 'weight': {'score': [0.2971655696630478, 0.007462647567839312, 0.212955141768736, 0.004398441563347666, 0.0074143100094485595], 'topk_tokens': ['<|eot_id|>', '.\n\n', ':', ' \n', '.\n\n', 'Answer', '?', 'hall', '.', '<|end_header_id|>', 'way', 'assistant', 'y', 'ching', 'ible', '.', '.', ' the', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.001424446702003479, 1.18160400390625, 0.0015663305918375652, 0.004039120674133301]}, 'saliency': {'score': [0.0002627313137054443, 0.00010022333387792464, 0.0003679280771928675, 9.726190475558983e-05, 0.00023816583992598893], 'topk_tokens': [' *\n\n', 'RE', '<|begin_of_text|>', 'Question', ' *\n\n', ' *\n\n', 'y', ' *\n\n', 'ible', ' \n', ' *\n\n', 'assistant', ' *\n\n', '.', ' *\n\n', '?', 'hall', 'Answer', '<|end_header_id|>', 'way'], 'evidence_proportions': [7.621943950653076e-05, 0.0008516967296600341, 2.8694669405619305e-05, 0.00010381937026977539]}}, 28: {'grad': {'score': [0.3800363540649414, 0.48395171216253696, 0.4207228492288029, 0.48496539916845854, 0.5400801819640321], 'topk_tokens': ['.', ' hour', ' and', '.', ' hour', 'hour', '.', '.', ' hung', ' bend', ' headlines', 'hours', ' England', ' happened', ' law', ' Hale', ' Hill', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [0.40659332275390625, 0.257586669921875, 0.45863914489746094, 0.3869171142578125]}, 'weight': {'score': [0.3237869769334793, 0.007328582402517104, 0.21252711204921498, 0.004138479813543963, 0.0037107800508474373], 'topk_tokens': [':', '\n\n', 'Answer', ' \n', '.\n\n', '.', '.', '.\n\n', 'assistant', '<|end_header_id|>', 'way', '?', 'y', 'ible', 'ching', '.', '<|begin_of_text|>', '.', ' the', '.'], 'evidence_proportions': [0.0006309524178504944, 1.291014242172241, 0.00091916819413503, 0.0025259017944335937]}, 'saliency': {'score': [0.0014318734407424926, 7.758697109109552e-05, 0.0008003141950158512, 6.520182244413564e-05, 8.52489626252806e-05], 'topk_tokens': [':', ' \n', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'assistant', '.\n\n', 'y', 'hall', '<|end_header_id|>', '<|eot_id|>', 'ching', '\n\n', 'ible', '<|begin_of_text|>', '?', '.', '.', ' the', '.'], 'evidence_proportions': [4.157423973083496e-05, 0.005631387233734131, 1.9590059916178383e-05, 3.933906555175781e-05]}}, 29: {'grad': {'score': [0.4071746826171875, 0.4940479960244083, 0.364406753988827, 0.4955213281983124, 0.4956893425483208], 'topk_tokens': [' idea', ' idea', 'sur', '️', '5', 'blue', '9', '26', ' type', '25', '�', '4', '8', ' full', '❤', '100', '240', ' pur', ' pur', '50'], 'evidence_proportions': [0.4078521728515625, 0.270440673828125, 0.4290110270182292, 0.5171630859375]}, 'weight': {'score': [0.36434428244829176, 0.007452238861625717, 0.25636015394154715, 0.0037119583570822856, 0.0038742012791819387], 'topk_tokens': ['.\n\n', ':', '<|eot_id|>', '.', ':', '<|end_header_id|>', '\n\n', ' \n', '.', 'assistant', 'way', '?', 'y', 'ching', 'ible', '<|begin_of_text|>', '.', '.', ' the', '.'], 'evidence_proportions': [0.00035633891820907593, 1.4539298057556151, 0.0006753106911977131, 0.0023518800735473633]}, 'saliency': {'score': [0.0005081683397293091, 7.069057966830462e-05, 0.0004290026776930865, 6.56720789408861e-05, 0.00018839983197001668], 'topk_tokens': [' the', '.', 'way', '<|start_header_id|>', '?', 'assistant', '<|eot_id|>', '.', '.', '.', '.', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '\n\n', '<|eot_id|>', 'ible', 'y', '<|end_header_id|>', 'ching'], 'evidence_proportions': [5.885958671569824e-06, 0.0019228160381317139, 2.1134813626607258e-05, 7.978677749633789e-05]}}, 30: {'grad': {'score': [0.2770099639892578, 0.2519215311806583, 0.27500107709099264, 0.25161309837866275, 0.38401020347297965], 'topk_tokens': ['      ', '      ', ' air', ' S', ' loud', ' lyn', ' military', ' Republican', ' lin', '�', 's', ' S', ' Republican', ' S', 'd', ' S', 'ed', 'hall', ' item', ' item'], 'evidence_proportions': [0.19683837890625, 0.2643585205078125, 0.3023338317871094, 0.3234100341796875]}, 'weight': {'score': [0.20517016649246217, 0.007272242258286335, 0.1371696924462038, 0.0052644571249675365, 0.01984559405933727], 'topk_tokens': ['.', '.\n\n', 'hall', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|eot_id|>', '<|eot_id|>', ' \n', 'assistant', 'way', '?', 'y', 'ching', 'ible', '<|begin_of_text|>', '.', ' the', '.', '.'], 'evidence_proportions': [0.0017781257629394531, 0.8077017784118652, 0.0021559794743855796, 0.00896921157836914]}, 'saliency': {'score': [0.0013033971190452576, 0.00017184297008627266, 0.00044101389015422147, 0.00016422300023216334, 0.0007893342476386529], 'topk_tokens': [' *\n\n', ' Hale', ' *\n\n', '�', '�', ' Articles', '�', '.', '.', '.', '<|start_header_id|>', 'ching', ' \n', 'y', ' the', 'way', 'ible', '.', 'assistant', 'hall'], 'evidence_proportions': [7.587671279907227e-05, 0.0048232853412628176, 6.8436066309611e-05, 0.0002474784851074219]}}, 31: {'grad': {'score': [0.17722828984260558, 0.29197069156804734, 0.19952459896312041, 0.2932744592935511, 0.29486024534547484], 'topk_tokens': ['SP', ' five', ' item', ' N', ' to', 'D', ' to', ' the', ' We', ' item', ' it', ' an', ' the', ' time', ' in', ' in', 'D', 'v', 'f', ' second'], 'evidence_proportions': [0.13943257927894592, 0.12107162475585938, 0.2263654073079427, 0.204656982421875]}, 'weight': {'score': [0.23864903450012206, 0.0071433411547418176, 0.14733619374387405, 0.0048904831219108175, 0.006608911148913495], 'topk_tokens': ['.', '<|start_header_id|>', '<|eot_id|>', ':', 'y', 'assistant', '<|end_header_id|>', 'ching', '\n\n', '?', 'ible', ' \n', '<|eot_id|>', 'hall', 'way', '.', '<|begin_of_text|>', '.', ' the', '.'], 'evidence_proportions': [0.0007735788822174072, 0.9469714641571045, 0.0014693538347880046, 0.005242586135864258]}, 'saliency': {'score': [0.00027511268854141235, 0.00014433953889022918, 0.0002983843579011805, 0.00014245678011789737, 0.00019406382139627035], 'topk_tokens': ['Answer', '\u200d', '.', ' apple', '.', 'ible', '.', '.', '\n\n', '<|eot_id|>', ':', '<|start_header_id|>', '?', '<|end_header_id|>', ' \n', ' write', 'assistant', 'way', 'hall', '<|eot_id|>'], 'evidence_proportions': [3.318488597869873e-05, 0.0008313477039337158, 0.00011126200358072916, 0.00010904073715209961]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:33:49.093 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:33:49.093 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-16_pid-0_1-3-5-9.pkl | len: 10 |  size: 8.42 KB
Processing depth (1, 3, 5, 9):   1%|          | 1/100 [00:14<23:54, 14.49s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Processing depth (0, 2, 8, 9):   1%|          | 1/100 [00:24<23:54, 14.49s/it]2025-01-23 22:33:58.809 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel grabbed the apple.
2025-01-23 22:33:58.830 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-23 22:33:58.835 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (973, 978) --> . Daniel travelled to the
2025-01-23 22:33:58.835 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the garden.
2025-01-23 22:33:58.852 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (3360, 3366) --> . Daniel went back to the
2025-01-23 22:33:58.852 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-23 22:33:58.870 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3669, 3674) --> . Mary travelled to the
2025-01-23 22:33:58.871 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the office.
2025-01-23 22:33:58.883 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2479, 2485) --> . John went back to the
2025-01-23 22:33:58.883 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the bedroom.
2025-01-23 22:33:58.884 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (72, 78) --> . John journeyed to the
2025-01-23 22:33:58.884 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:33:58.888 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (801, 806) -->  John moved to the garden
2025-01-23 22:33:58.888 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:33:58.892 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (777, 783) --> . Sandra journeyed to the
2025-01-23 22:33:58.892 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:33:58.909 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3365, 3370) -->  the garden. Sandra moved
2025-01-23 22:33:58.909 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra went back to the hallway.
2025-01-23 22:33:58.919 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (2011, 2017) -->  the ground. Sandra went back
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:33:59.396 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - the bedroom<|eot_id|>
2025-01-23 22:33:59.396 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4220])
your chose emoji: ['🤏🏼', '🕳️', '☑️', '👨🏼\u200d❤️\u200d👨🏽', '🏃🏾\u200d♀️\u200d➡', '🏄🏾', '🚶🏽\u200d♀️\u200d➡', '📽️', '👊', '🦸🏻\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4223, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 191739.61it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.94it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.19it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.64it/s]
2025-01-23 22:34:02.459 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.22308814525604248, 0.2030128444677954, 0.25135430167703066, 0.20254200469839068, 0.18599650065104167], 'topk_tokens': [' the', ' of', 'ENCES', ' ST', ' the', ' Written', ' Daily', '26', ' IN', '***', ' make', ' OF', 'UL', ' December', 'D', ' DAYS', ' apple', ' Project', 'Daniel', ' Date'], 'evidence_proportions': [0.213336181640625, 0.24212265014648438, 0.2099987030029297]}, 'weight': {'score': [0.008608520030975342, 0.007569332922236095, 0.006043777746312758, 0.007577778137247253, 0.009246871868769328], 'topk_tokens': ['<|eot_id|>', ' question', ' balance', 'ern', '<|start_header_id|>', ' now', ' more', 'assistant', 'ian', 'ot', 'pay', ' most', ' men', ' some', ' time', 'nes', 'way', '<|end_header_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.017617130279541017, 0.005131840705871582, 0.0037719249725341798]}, 'saliency': {'score': [6.465241312980652e-05, 8.060273445530466e-05, 5.866850123685949e-05, 8.084260243293109e-05, 8.739113807678223e-05], 'topk_tokens': [':', '�', 'ION', ' old', 'Answer', 'ush', 'If', ' in', '\n\n', 'UG', ' which', ' second', ' location', 'cap', 'itol', '<|start_header_id|>', 'way', '<|end_header_id|>', 'assistant', 'hall'], 'evidence_proportions': [0.0001617729663848877, 1.1583169301350912e-05, 3.121495246887207e-05]}}, 25: {'grad': {'score': [0.2431168556213379, 0.24004097897210958, 0.2799559200511259, 0.23970397393422935, 0.20085381825764975], 'topk_tokens': [',', 'ting', ' as', '\n', ' ST', ' and', 'ION', ',', ',', '\n', '\n', ' journey', '\n', '\n', '\n', ' grabbed', ',\n', '\n', 'ine', 'MIN'], 'evidence_proportions': [0.3014129638671875, 0.2106793721516927, 0.22374572753906252]}, 'weight': {'score': [0.008260555565357208, 0.007572361058373602, 0.005559832734220168, 0.007586119709442252, 0.009549663464228312], 'topk_tokens': ['way', ' most', 'ian', ' services', ' men', ' some', 'Min', 'pay', '�', ' question', 'assistant', ' right', ' the', 'ern', 'ian', 'ot', 'nes', '<|begin_of_text|>', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.01785998344421387, 0.0041915178298950195, 0.0035439729690551758]}, 'saliency': {'score': [6.532855331897736e-05, 7.604659238811242e-05, 4.214924924513873e-05, 7.63638697166452e-05, 8.081515630086262e-05], 'topk_tokens': ['nes', 'ot', '<|begin_of_text|>', '�', ' question', 'ION', '<|start_header_id|>', 'ian', ' *\n\n', ' of', ' right', ' of', ' the', '<|start_header_id|>', 'assistant', '<|eot_id|>', 'way', '<|eot_id|>', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.00013037323951721192, 3.1148393948872886e-05, 4.130005836486817e-05]}}, 26: {'grad': {'score': [0.4252746105194092, 0.3732336587655214, 0.3492909038768095, 0.37322920116622926, 0.3872895304361979], 'topk_tokens': [' the', ' and', ' A', '<|eot_id|>', ' A', ' book', '***', ' was', '<|start_header_id|>', ' Ch', ' to', ' Pa', ' The', '<|end_header_id|>', '<|eot_id|>', ' H', ' at', ' as', '25', ' PA'], 'evidence_proportions': [0.3990196228027344, 0.4110692342122396, 0.46857604980468753]}, 'weight': {'score': [0.007918432354927063, 0.007565133864637407, 0.006260423099293429, 0.007574409539254484, 0.008767274220784506], 'topk_tokens': ['de', ' than', '�', ' question', ' more', ' most', 'way', 'pay', ' time', ' some', ' men', 'ian', 'nes', 'ot', '<|begin_of_text|>', 'hall', '<|eot_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.013535785675048827, 0.004562656084696451, 0.006328010559082031]}, 'saliency': {'score': [8.230097591876984e-05, 0.00011094015073132634, 9.064639315885656e-05, 0.00011121530399144253, 0.00012445370356241862], 'topk_tokens': [' *\n\n', '<|eot_id|>', ' when', 'out', 'hand', 'ARR', ' *\n\n', ' Think', ' *', ' *', '<|end_header_id|>', ' remin', 'iously', ' *', '<|eot_id|>', ' *', '<|start_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>'], 'evidence_proportions': [0.00014315843582153321, 6.402035554250081e-05, 4.3380260467529294e-05]}}, 27: {'grad': {'score': [0.3106546401977539, 0.28383383365877707, 0.32936371074003334, 0.2833600381338822, 0.24109882354736328], 'topk_tokens': [' printed', 'SP', 'ed', 'ch', ' WITH', '      ', ' CONNECT', ' INCIDENT', 'ir', 'otype', '\n', 'APER', 'AYS', 'ARR', ' producing', 'ERS', '.', 'AILY', ' EAR', ' Think'], 'evidence_proportions': [0.32394866943359374, 0.17461649576822916, 0.46060638427734374]}, 'weight': {'score': [0.00817231833934784, 0.00757281068318155, 0.005818012882681454, 0.007584809485654194, 0.009573142528533935], 'topk_tokens': [' of', ' one', ' not', 'way', ' may', ' most', ' so', ' there', ' men', 'pay', ' time', ' some', 'nes', 'ian', 'ot', 'hue', 'de', 'UG', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.016315460205078125, 0.005000948905944824, 0.003834819793701172]}, 'saliency': {'score': [0.00011387281119823456, 0.00014360159063915096, 0.00014104825608870562, 0.00014373637948309292, 0.0001883967717488607], 'topk_tokens': ['000', ' inhabit', ' *', ' *', ' brought', ' fifty', '<|start_header_id|>', 'hand', 'ARR', ' *', ' presses', 'way', ' producing', ' latter', '<|end_header_id|>', 'assistant', ' impressions', '<|start_header_id|>', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.0001759171485900879, 5.123515923817953e-05, 0.0001269936561584473]}}, 28: {'grad': {'score': [0.6734232902526855, 0.6116414595180966, 0.6625031863941866, 0.6109901755484008, 0.5666039911905925], 'topk_tokens': [',', ',', '.', '.', '\n', '\n', ' PA', '\n', '\n', ' It', '.', ',', '\n\n', '\n', ',\n', 'Cut', ' travelled', '.', '\n', ' the'], 'evidence_proportions': [0.7381744384765625, 0.4751853942871094, 0.8465576171875]}, 'weight': {'score': [0.008601672947406769, 0.007538759338610155, 0.00560099324759315, 0.007550472118230052, 0.009170618057250977], 'topk_tokens': [' now', ' not', 'hue', ' right', ' most', ' time', ' the', 'ern', ' some', 'nes', 'UG', ' question', ' men', 'ot', 'ian', '<|begin_of_text|>', 'pay', 'way', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.018194198608398438, 0.0049824317296346034, 0.003352236747741699]}, 'saliency': {'score': [7.313117384910583e-05, 7.419836145708579e-05, 7.425248622894287e-05, 7.420201225015662e-05, 9.152015050252279e-05], 'topk_tokens': ['ian', 'ern', 'ot', ' great', 'ian', ' not', '<|eot_id|>', '<|begin_of_text|>', ' right', ' of', ' be', 'way', ' question', ' the', '�', 'hall', 'street', '<|start_header_id|>', 'assistant', '<|end_header_id|>'], 'evidence_proportions': [0.00012619495391845702, 6.119906902313232e-05, 3.438591957092285e-05]}}, 29: {'grad': {'score': [0.9369354248046875, 1.1174654029089215, 0.9653840065002441, 1.1193966866682223, 1.0926910400390626], 'topk_tokens': [' minds', ' *', ':', '�', ' advantage', '\n', ' worth', '?', 'to', '?', ' tele', 'and', 'x', '♀', ' perpet', '�', 'cuts', ' sounded', ' *\n\n', '️'], 'evidence_proportions': [0.71514892578125, 0.7376810709635416, 1.3978271484375]}, 'weight': {'score': [0.008844396099448204, 0.007555694001956825, 0.005730144241276909, 0.007565626774135894, 0.009464802742004395], 'topk_tokens': ['ation', ' of', 'pay', 'for', ' question', 'ern', 'hue', ' so', ' be', ' first', ' time', 'ian', ' some', ' most', 'ot', 'nes', '<|begin_of_text|>', 'UG', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.020474386215209958, 0.0031060526768366494, 0.004100418090820312]}, 'saliency': {'score': [0.00015790946781635284, 0.00014335384251465595, 0.0001399893971050487, 0.00014332544595081686, 0.00026245633761088055], 'topk_tokens': [' *\n\n', 'posit', '<|eot_id|>', '<|start_header_id|>', 'ation', '.', ' setting', ' *', ' as', 'NEW', 'ern', 'assistant', 'asca', 'way', ' so', 'old', ' inhabit', ' the', 'hall', '<|end_header_id|>'], 'evidence_proportions': [0.0002720057964324951, 4.192193349202474e-05, 0.00018299818038940429]}}, 30: {'grad': {'score': [0.3655061721801758, 0.3452513810161615, 0.4202025918399586, 0.34456304700546575, 0.3173961893717448], 'topk_tokens': [' journey', '\n', '.', ' of', ' B', ' the', ' the', ' the', '\n', ' in', '\n\n', ' as', ' bedroom', '.', 'D', ' the', ' to', '\n', '\n', 'ed'], 'evidence_proportions': [0.4893341064453125, 0.23008219401041666, 0.40418701171875]}, 'weight': {'score': [0.008802168071269989, 0.007398033096924117, 0.005690988372353946, 0.007406557746108362, 0.007966008186340332], 'topk_tokens': [' men', ' FR', 'assistant', ' first', 'pay', ' most', ' time', 'for', ' some', 'ian', 'ot', 'nes', 'NEW', '<|begin_of_text|>', '<|eot_id|>', 'hall', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>'], 'evidence_proportions': [0.016923904418945312, 0.0010391672452290854, 0.00999603271484375]}, 'saliency': {'score': [0.00025212764739990234, 0.00014418657854104139, 0.0001652240753173828, 0.0001436013084734312, 0.0001434183120727539], 'topk_tokens': [' Think', ' STR', ' some', ' time', 'for', 'ian', 'nes', 'ot', '<|end_header_id|>', '***', '25', '<|eot_id|>', 'APER', ' Daily', '<|eot_id|>', ' FR', '<|end_header_id|>', '<|start_header_id|>', 'way', 'NEW'], 'evidence_proportions': [0.0003701388835906982, 1.444419225056966e-05, 0.0004193365573883056]}}, 31: {'grad': {'score': [0.5505437850952148, 0.5986600203336824, 0.5370549594654757, 0.5993464406148553, 0.5973318481445312], 'topk_tokens': ['      ', '\n', '\n', ' bend', '      ', '\n', '.', '      ', '\n', '\n', '<|eot_id|>', '\n', '\n', ' composing', ',', '\n', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [0.45427246093749996, 0.597826639811198, 0.59007568359375]}, 'weight': {'score': [0.009150072932243347, 0.006981194767374973, 0.0033584026729359348, 0.007002396032789072, 0.005846422513326009], 'topk_tokens': [' there', 'assistant', ' more', 'pay', ' be', ' would', ' time', ' first', ' men', ' most', ' some', 'ian', 'for', 'nes', 'ot', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>', 'hall', 'way'], 'evidence_proportions': [0.021983194351196292, 0.001415888468424479, 0.005597972869873047]}, 'saliency': {'score': [0.00013186223804950714, 0.00011882374361739632, 8.970148423138787e-05, 0.00011901102876194712, 9.957551956176757e-05], 'topk_tokens': ['\n\n', ' would', ' had', 'pay', 'nes', ' the', ' men', 'ot', ' time', 'ian', ' some', ' remin', 'for', 'assistant', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', 'hall', '<|eot_id|>', 'way'], 'evidence_proportions': [0.0002476811408996582, 9.819865226745605e-06, 0.0001624941825866699]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-23 22:34:02.465 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:34:02.466 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-16_pid-1_0-2-8-9.pkl | len: 10 |  size: 8.29 KB
Processing depth (0, 2, 8, 9):   2%|▏         | 2/100 [00:27<22:35, 13.83s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
Processing depth (1, 4, 6, 8):   2%|▏         | 2/100 [00:39<22:35, 13.83s/it]2025-01-23 22:34:13.833 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel grabbed the apple.
2025-01-23 22:34:13.835 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (433, 437) -->  Daniel grabbed the apple
2025-01-23 22:34:13.836 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-23 22:34:13.845 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1848, 1853) -->  Hon. Daniel travelled to
2025-01-23 22:34:13.845 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the garden.
2025-01-23 22:34:13.857 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2363, 2369) -->  Daniel went back to the garden
2025-01-23 22:34:13.857 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-23 22:34:13.873 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3326, 3331) --> . Mary travelled to the
2025-01-23 22:34:13.874 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the office.
2025-01-23 22:34:13.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2389, 2395) --> . John went back to the
2025-01-23 22:34:13.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the bedroom.
2025-01-23 22:34:13.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (67, 73) --> . John journeyed to the
2025-01-23 22:34:13.886 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:34:13.890 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (743, 748) --> . John moved to the
2025-01-23 22:34:13.890 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:34:13.894 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (677, 683) --> . Sandra journeyed to the
2025-01-23 22:34:13.894 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:34:13.910 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3330, 3335) -->  the bedroom. Sandra moved
2025-01-23 22:34:13.910 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra went back to the hallway.
2025-01-23 22:34:13.920 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1978, 1984) -->  Sandra went back to the hallway
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:34:14.497 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - Daniel's house.<|eot_id|>
2025-01-23 22:34:14.498 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4249])
your chose emoji: ['👩🏿\u200d❤\u200d💋\u200d👩🏻', '🧑🏻\u200d🚀', '👨🏿\u200d❤\u200d👨🏾', '🏳\u200d⚧', '⛪', '🌐', '👨🏽\u200d💼', '\U0001faf7🏽', '👨🏽\u200d🦯\u200d➡️', '🧑🏼\u200d❤️\u200d💋\u200d🧑🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4252, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 185383.60it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 125.57it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 129.84it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 126.92it/s]
2025-01-23 22:34:17.462 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.38106727600097656, 0.5026223585500206, 0.6173518685733571, 0.5022722628640243, 0.6546173095703125], 'topk_tokens': ['ess', ' H', 'iously', ' heard', 'hue', 'ace', '.', ' P', '.', 'AP', '�', 'ed', '�', 'hand', 'ly', ' hallway', ' hung', ' hallway', 'way', 'hall'], 'evidence_proportions': [0.3781585693359375, 0.6515029907226563, 0.24173990885416669, 0.2801513671875]}, 'weight': {'score': [0.0012412890791893005, 0.0073966553832491054, 0.11294332847875707, 0.0065711481037907735, 0.06571524160412642], 'topk_tokens': ['Answer', '<|eot_id|>', '\n\n', 'hall', 'way', ',', '<|end_header_id|>', 'assistant', 'and', ',', 'ian', ' in', ' a', '�', 'first', '�', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0009710639715194702, 0.0013857603073120118, 0.0013596365849177043, 0.001170980930328369]}, 'saliency': {'score': [7.525384426116943e-05, 0.0001631556455526056, 0.000599173061987933, 0.00016004307858429392, 0.00036512229305047257], 'topk_tokens': [' ', ' *', '<|eot_id|>', 'user', 'assistant', 'Just', ' a', '�', '�', 'Answer', 'first', '\n\n', '.', '�', '<|begin_of_text|>', 'Question', '.', ' Where', 'way', 'hall'], 'evidence_proportions': [2.812594175338745e-05, 7.198452949523926e-05, 0.00012328724066416422, 5.858540534973145e-05]}}, 25: {'grad': {'score': [1.1114120483398438, 0.7677266362244973, 0.9098050734576058, 0.7649385525874719, 0.8443211775559646], 'topk_tokens': ['�', ' at', ' galaxy', ' Gal', '�', '�', ' scramble', ' proof', ' Hor', ' Ramsey', ' Gal', ' regular', ' grat', ' Charleston', ' At', ' custom', ' curs', ' customary', ' laid', ' laid'], 'evidence_proportions': [1.045074462890625, 0.851824951171875, 1.3691609700520833, 1.1147705078125]}, 'weight': {'score': [0.0013492479920387268, 0.007392577371390945, 0.11769596531110652, 0.006528011244100977, 0.06971264429963552], 'topk_tokens': [' \n', 'way', 'v', ' the', 'assistant', ',', ',', 'ian', '<|end_header_id|>', 'and', '\n\n', ' in', ' a', '�', '�', 'first', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.001521289348602295, 0.0010304212570190429, 0.0019405881563822427, 0.0008208334445953369]}, 'saliency': {'score': [9.413361549377441e-05, 0.00015137071948172567, 0.0007089727065142463, 0.0001471273356133725, 0.00043191474217634933], 'topk_tokens': [' \n', 'ian', 'MIN', '<|eot_id|>', '      ', '      ', '<|eot_id|>', ' a', ' return', '<|start_header_id|>', '�', 'first', '�', 'way', '�', '.', '.', '<|begin_of_text|>', '<|end_header_id|>', '\n\n'], 'evidence_proportions': [8.278340101242065e-05, 7.851123809814453e-05, 7.5380007425944e-05, 0.0001413404941558838]}}, 26: {'grad': {'score': [0.45422210693359377, 0.4603327753851129, 0.4722458895514993, 0.46026540222822226, 0.7169415767376239], 'topk_tokens': [' genu', ' Col', '�', ' pen', '�', '�', ' all', ' Paul', ' garden', '�', ' hallway', '�', '3', '�', '�', '�', ' morning', '\u200d', ' Hill', 'hall'], 'evidence_proportions': [0.4128265380859375, 0.4261077880859375, 0.5235722859700521, 0.432232666015625]}, 'weight': {'score': [0.0024872034788131713, 0.007253517211885354, 0.09248946694766774, 0.0065858907191852206, 0.06081621004984929], 'topk_tokens': [':', ' \n', 'hall', '?', '<|eot_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', 'and', ',', 'ian', ' in', ' a', '�', '�', 'first', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.002603471279144287, 0.002392876148223877, 0.0025477210680643716, 0.002415895462036133]}, 'saliency': {'score': [0.0001157999038696289, 0.00019370057985452918, 0.0005266087896683637, 0.0001913754570308102, 0.0003491476751290835], 'topk_tokens': [' fall', ' a', '\n\n', '<|eot_id|>', ':', ' \n', 'ian', 'and', ',', 'Answer', '\n\n', ' in', ',', 'way', '<|eot_id|>', '�', '<|end_header_id|>', '.', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [0.00014227628707885742, 9.909868240356445e-05, 0.00011695921421051025, 0.00010992884635925293]}}, 27: {'grad': {'score': [0.33253135681152346, 0.47112099763254794, 0.37813977634205537, 0.4725343264415754, 0.4135473324702336], 'topk_tokens': ['.', '.', '.', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ' Bench', '.', '.'], 'evidence_proportions': [0.2718353271484375, 0.232281494140625, 0.2745234171549479, 0.5509475708007813]}, 'weight': {'score': [0.0023370325565338136, 0.007398902753110439, 0.10326652141178355, 0.006646577448092965, 0.07146457181527065], 'topk_tokens': ['?', 'hall', '<|eot_id|>', 'Answer', ',', ':', '<|end_header_id|>', 'way', 'and', ',', 'ian', ' in', ' a', '�', '�', 'first', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0021538734436035156, 0.0018371105194091798, 0.0030161142349243164, 0.002168583869934082]}, 'saliency': {'score': [9.277909994125366e-05, 0.00010989718524667304, 0.0002209850970436545, 0.00010907902724405536, 0.0002582947222086099], 'topk_tokens': [' Alexander', 'As', ' *\n\n', ' apple', ' in', '<|eot_id|>', ' a', ' Where', ' *\n\n', '.', 'assistant', ' *\n\n', 'NEW', ' \n', '<|begin_of_text|>', ':', '?', '<|end_header_id|>', 'hall', 'way'], 'evidence_proportions': [0.00013336539268493652, 7.497668266296387e-05, 0.00011682510375976562, 4.925727844238281e-05]}}, 28: {'grad': {'score': [0.5361854553222656, 0.5439238669394549, 0.5890601663028493, 0.5435951709065794, 0.7024067732004019], 'topk_tokens': [' *', 'ex', '.', '.', '.', '.\n\n', ' Hill', ' *', ' *', ' be', ' hallway', ' *', '.', ' messenger', ' *', ' so', '.', ' Hale', ' hallway', 'hall'], 'evidence_proportions': [0.23220062255859375, 0.557952880859375, 0.7154541015625, 0.5424835205078125]}, 'weight': {'score': [0.001430097222328186, 0.007251847160389596, 0.09601732913185568, 0.0065606629325981195, 0.07239459311732879], 'topk_tokens': ['Answer', '.\n\n', ' \n', '\n\n', ':', 'way', 'and', ',', '<|end_header_id|>', '?', 'ian', ' in', ' a', '�', '�', '.', 'first', '�', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.001022428274154663, 0.0004815995693206787, 0.0028685082991917925, 0.0009786367416381836]}, 'saliency': {'score': [7.707774639129639e-05, 0.00011543125643106642, 0.0008181342307259055, 0.00010992272119399421, 0.0006788908862150633], 'topk_tokens': [' \n', 'Just', 'During', '<|start_header_id|>', '<|eot_id|>', ' garden', '<|eot_id|>', 'assistant', ' a', '<|end_header_id|>', '\n\n', '�', '<|begin_of_text|>', '.', '?', '�', 'hall', '�', '.', 'first'], 'evidence_proportions': [8.65831971168518e-05, 2.2256374359130858e-05, 0.00015522042910257977, 3.052353858947754e-05]}}, 29: {'grad': {'score': [0.5193344116210937, 0.5162434331224645, 0.4803205658407772, 0.5165196498727276, 0.5268454184898963], 'topk_tokens': ['25', 'en', 'SP', ' type', 'SP', '186', '�', '�', 'u', ' full', '❤', ' H', '186', ' pur', '186', '185', '186', '❤', 'u', '50'], 'evidence_proportions': [0.5374526977539062, 0.55675048828125, 0.5247090657552083, 0.46097412109375]}, 'weight': {'score': [0.0005943745374679566, 0.007417253133606933, 0.11406385109705083, 0.006586017602679274, 0.08361152788767448], 'topk_tokens': ['If', '<|end_header_id|>', '<|eot_id|>', ':', ',', '\n\n', 'way', '?', 'and', ',', 'ian', ' in', ' a', '�', '�', 'first', '.', '�', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.00015535950660705566, 0.0004239499568939209, 0.0011768738428751626, 0.00041701197624206544]}, 'saliency': {'score': [1.9998848438262938e-05, 8.992627893880304e-05, 0.0005635338671067182, 8.642363258632608e-05, 0.0004216037117517911], 'topk_tokens': ['.', ',', ':', 'ian', '<|eot_id|>', 'assistant', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '�', '�', 'way', '.', 'first', '�', '<|eot_id|>', '<|begin_of_text|>', ' in', '?', '.'], 'evidence_proportions': [4.284083843231201e-06, 1.1366605758666992e-05, 4.380444685618083e-05, 1.2636184692382812e-05]}}, 30: {'grad': {'score': [0.396240234375, 0.37147157369418066, 0.4230225506950827, 0.37093605524929096, 0.4319877876685216], 'topk_tokens': ['hall', '      ', ' S', '      ', 'lyn', '      ', ' lyn', 'ed', ' ', ' journey', 'd', '      ', ' to', ' S', '      ', '      ', ' military', ' time', ' item', ' room'], 'evidence_proportions': [0.34590911865234375, 0.300811767578125, 0.406768798828125, 0.51929931640625]}, 'weight': {'score': [0.0029571175575256346, 0.007218470577911061, 0.05849043060751522, 0.006823515937463507, 0.05485759331629826], 'topk_tokens': ['<|start_header_id|>', 'assistant', 'hall', '<|eot_id|>', ' \n', '<|end_header_id|>', '?', ':', '\n\n', ' in', '<|eot_id|>', 'way', ' a', '�', 'first', '�', '<|begin_of_text|>', '.', '�', '.'], 'evidence_proportions': [0.0015946626663208008, 0.002835416793823242, 0.003373563289642334, 0.003669047355651855]}, 'saliency': {'score': [0.0001686394214630127, 0.00022661214937943954, 0.00032040301491232477, 0.0002261287203966407, 0.0010204722101871783], 'topk_tokens': ['.\n', '      ', '�', 'NEW', '?', ':', ' Where', '<|eot_id|>', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', ' \n', ' a', '�', '�', '.', '�', 'first', 'way', 'hall'], 'evidence_proportions': [7.361918687820435e-05, 0.00013406872749328612, 0.000256990393002828, 0.00017320513725280762]}}, 31: {'grad': {'score': [0.2981845855712891, 0.39005929080249, 0.32474416844985066, 0.39102599119901543, 0.3578033263866718], 'topk_tokens': [' time', ' convention', ' would', ' time', ' new', '1', ' to', ' would', ' item', ' the', ' in', ' item', ' the', ' in', ' it', ' the', ' the', 'D', ' an', ' second'], 'evidence_proportions': [0.13590621948242188, 0.31038818359375, 0.385040283203125, 0.31157684326171875]}, 'weight': {'score': [0.0014520406723022461, 0.007022529532072798, 0.07855088745846468, 0.006469753354773628, 0.049363683049495406], 'topk_tokens': ['<|eot_id|>', 'Answer', '.', 'assistant', '?', ':', ' \n', '<|end_header_id|>', ' a', '\n\n', '<|eot_id|>', 'hall', 'first', '�', '�', 'way', '.', '�', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0011901259422302246, 0.0011469364166259766, 0.0019116997718811035, 0.001415085792541504]}, 'saliency': {'score': [4.9583613872528076e-05, 0.0001517028205257929, 0.00017818194978377398, 0.00015197487715711588, 0.0002427917833511646], 'topk_tokens': [' was', '.', '�', '.\n', '�', '.', '?', '\n\n', 'first', '�', 'Answer', '<|begin_of_text|>', ' apple', ' a', ' write', '<|end_header_id|>', 'assistant', '<|eot_id|>', 'hall', 'way'], 'evidence_proportions': [7.396191358566284e-05, 2.8079748153686525e-05, 7.773935794830322e-05, 1.7797946929931642e-05]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-23 22:34:17.469 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:34:17.469 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-16_pid-2_1-4-6-8.pkl | len: 10 |  size: 8.25 KB
Processing depth (1, 4, 6, 8):   3%|▎         | 3/100 [00:42<23:13, 14.37s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Processing depth (1, 3, 6, 7):   3%|▎         | 3/100 [00:52<23:13, 14.37s/it]2025-01-23 22:34:26.650 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel grabbed the apple.
2025-01-23 22:34:26.652 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (433, 437) -->  grabbed the apple.
2025-01-23 22:34:26.652 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-23 22:34:26.659 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (1447, 1452) --> . Daniel travelled to the
2025-01-23 22:34:26.660 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the garden.
2025-01-23 22:34:26.672 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2348, 2354) --> . Daniel went back to the
2025-01-23 22:34:26.672 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-23 22:34:26.686 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (2845, 2850) --> . Mary travelled to the
2025-01-23 22:34:26.686 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the office.
2025-01-23 22:34:26.698 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2405, 2411) --> . John went back to the
2025-01-23 22:34:26.699 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the bedroom.
2025-01-23 22:34:26.699 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (67, 73) --> . John journeyed to the
2025-01-23 22:34:26.699 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:34:26.703 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (731, 736) --> . John moved to the
2025-01-23 22:34:26.703 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:34:26.706 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (665, 671) --> . Sandra journeyed to the
2025-01-23 22:34:26.706 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:34:26.723 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3301, 3306) --> . Sandra moved to the
2025-01-23 22:34:26.723 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra went back to the hallway.
2025-01-23 22:34:26.733 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1959, 1965) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:34:27.247 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - The hallway.<|eot_id|>
2025-01-23 22:34:27.248 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4202])
your chose emoji: ['🕷️', '🍄', '↕️', '⏲', '👣', '🕵🏽\u200d♂️', '🚴🏻\u200d♂', '👁\u200d🗨️', '👐🏼', '👷🏽\u200d♂']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4205, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 127.04it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.55it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 130.95it/s]
2025-01-23 22:34:30.304 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.3150783538818359, 0.3018654259716855, 0.30737798354204965, 0.30175661230856926, 0.44156800027479204], 'topk_tokens': [' cap', ' will', '�', ' cap', ' doctor', 'Dub', ' proceed', 'icing', 'ANK', ' writer', 'ured', ' William', ' well', 'rail', ' lin', 'hand', ' Project', 'hand', ' hand', ' hand'], 'evidence_proportions': [0.24825096130371094, 0.359515380859375, 0.2990875244140625, 0.343292236328125]}, 'weight': {'score': [0.01281556636095047, 0.00745288154883277, 0.27264511234620037, 0.0052549076766687484, 0.004329848707767955], 'topk_tokens': [' apple', ' the', ' Bench', ' boat', ':', 'Bridge', '<|start_header_id|>', ' bedroom', ' hallway', 'assistant', '<|eot_id|>', '.', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', ' hallway', 'way', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.025583505630493164, 0.01804838180541992, 0.004991923769315084, 0.0067567706108093255]}, 'saliency': {'score': [0.0004605725407600403, 0.00010504704168095175, 0.0009424633839551141, 9.647497094645324e-05, 7.763975544979698e-05], 'topk_tokens': [' apple', 'PA', ' bedroom', ' garden', ' ', ' PA', '\n\n', ' Bench', ' journey', ' boat', '.', 'Bridge', ' hallway', '<|end_header_id|>', '.', '<|begin_of_text|>', ' bedroom', 'way', ' hallway', 'hall'], 'evidence_proportions': [0.0011240765452384949, 0.0006754398345947266, 8.218983809153238e-05, 0.0001689612865447998]}}, 25: {'grad': {'score': [0.4204078674316406, 0.5599543187054102, 0.3870595202726476, 0.5620428159764749, 0.3847854681182326], 'topk_tokens': [' in', ' rest', ' laying', ' threw', ' getting', ' laid', ' a', ' employ', ' o', ' extra', ' most', ' to', ' to', 'ad', ' two', ' Project', ' extraordinary', ' large', '10', ' producing'], 'evidence_proportions': [0.5212860107421875, 0.4615966796875, 0.36190287272135413, 0.3687225341796875]}, 'weight': {'score': [0.01246044784784317, 0.0074627138062974925, 0.35158926511512084, 0.0046199632822476475, 0.008799775650626734], 'topk_tokens': ['26', ' PA', '.', '<|start_header_id|>', '<|end_header_id|>', ' Daniel', '<|start_header_id|>', 'RE', ':', ' hallway', 'assistant', 'hall', '<|eot_id|>', '��', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.02070915699005127, 0.019181156158447267, 0.005098327994346619, 0.007975316047668457]}, 'saliency': {'score': [0.000766591727733612, 9.015977878774672e-05, 0.002121054074343513, 7.026600740065605e-05, 0.00016087502763982404], 'topk_tokens': [' doctor', '<|eot_id|>', ' were', ' Joseph', ' apple', ' Bench', '<|eot_id|>', ' random', 'Dub', 'PA', ' Daniel', ' PA', 'RE', ' hallway', 'hall', ' Daniel', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', '.'], 'evidence_proportions': [0.0009460970759391785, 0.0014682114124298096, 0.00026498734951019287, 0.0005232930183410644]}}, 26: {'grad': {'score': [0.17100825309753417, 0.22665912569207045, 0.1933860217823702, 0.22719979131115514, 0.254289752558658], 'topk_tokens': ['hall', ' sure', ' they', ' determined', 'asca', ' city', 'ible', '3', ' proprietor', 'city', ' and', ' much', ' prepared', ' absolutely', ' double', ' true', 'ucci', 'outs', ' it', ' Cl'], 'evidence_proportions': [0.22820067405700684, 0.14892425537109374, 0.17953745524088544, 0.137103271484375]}, 'weight': {'score': [0.015638451278209686, 0.007304380963446836, 0.2830052551101236, 0.005006016442299578, 0.009763060954579135], 'topk_tokens': ['.\n\n', '<|eot_id|>', '<|end_header_id|>', ' the', ' \n', '��', '***', '<|start_header_id|>', 'assistant', '\n\n', '<|start_header_id|>', '<|eot_id|>', '<|eot_id|>', 'hall', ' hallway', ':', '<|end_header_id|>', 'way', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.017435550689697266, 0.0228330135345459, 0.009105727076530457, 0.014845478534698486]}, 'saliency': {'score': [0.00037021040916442873, 0.00014154708342943408, 0.0013534163727479823, 0.0001305191811525055, 0.000236494499340392], 'topk_tokens': [' Gray', '<|eot_id|>', ' the', ' Daniel', ' Gray', ' leve', ' Joseph', '***', 'assistant', '<|eot_id|>', ' hallway', 'way', '\n\n', 'Bridge', '<|begin_of_text|>', ':', '.', '<|end_header_id|>', ' hallway', 'hall'], 'evidence_proportions': [0.0006765425205230713, 0.0006107151508331298, 0.00010759135087331137, 0.0001997828483581543]}}, 27: {'grad': {'score': [0.4031855583190918, 0.4235013075022295, 0.3788049922269933, 0.4239652896036558, 0.32536878083881576], 'topk_tokens': [',', '.', '.', ',', ' *\n\n', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'], 'evidence_proportions': [0.26556873321533203, 0.39920654296875, 0.49007002512613934, 0.4129966735839844]}, 'weight': {'score': [0.011247511208057403, 0.007438824321370346, 0.36135946389506846, 0.004521581371902874, 0.012699947022555167], 'topk_tokens': ['<|eot_id|>', ' garden', 'assistant', '\n\n', '.', 'NEW', 'PA', '<|start_header_id|>', '��', 'RE', ' bedroom', '<|eot_id|>', '.\n\n', ':', 'hall', '<|end_header_id|>', ' hallway', 'way', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.015452146530151367, 0.016897630691528318, 0.00511603057384491, 0.009591460227966309]}, 'saliency': {'score': [0.0003661677241325378, 0.00013352826007339533, 0.0015132988200468175, 0.00012110595500948802, 0.0003285418476974755], 'topk_tokens': [' John', 'Bridge', ' Jul', '<|begin_of_text|>', '<|start_header_id|>', ' garden', ' journey', ' PA', ' Joseph', ':', 'assistant', 'NEW', 'PA', ' hallway', '.', 'way', '.\n\n', '.', '<|end_header_id|>', 'hall'], 'evidence_proportions': [0.0009021088480949402, 0.00040798783302307127, 0.00010175009568532309, 0.00021289587020874025]}}, 28: {'grad': {'score': [0.2156601905822754, 0.3064514087581748, 0.26930049587698546, 0.30719314699028233, 0.30138032478198673], 'topk_tokens': [' lines', 'antics', ' location', ' context', ' hallway', ' laid', ' room', ' location', 'eward', ' Brown', ' lowered', ' ranks', ' line', ' law', 'ivery', 'ching', '\n\n\n\n\n\n\n', ' hallway', 'ched', ' house'], 'evidence_proportions': [0.2553262710571289, 0.27260246276855465, 0.15152994791666666, 0.20394134521484375]}, 'weight': {'score': [0.016526925563812255, 0.007232060472123263, 0.38820578946786766, 0.004066798104575903, 0.004234351609882556], 'topk_tokens': [' the', '?', ' the', '.\n\n', ' the', 'Bridge', ' hallway', ' before', ' garden', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '\n\n', 'hall', 'way', ':', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.007674217224121094, 0.02230110168457031, 0.016199111938476562, 0.01822829246520996]}, 'saliency': {'score': [0.00031673461198806764, 0.00012357143527971012, 0.0013719218618729536, 0.00011241576723867484, 5.8438171420181007e-05], 'topk_tokens': [' the', '<|eot_id|>', ' hallway', ' the', ' the', 'assistant', ' garden', '?', ' the', '<|eot_id|>', ' apple', '<|end_header_id|>', '<|start_header_id|>', 'Bridge', '<|begin_of_text|>', '\n\n', 'way', ':', '.', 'hall'], 'evidence_proportions': [0.00035774707794189453, 0.00025041699409484864, 0.0004566460847854614, 0.00018234848976135255]}}, 29: {'grad': {'score': [0.24091224670410155, 0.20289573351920148, 0.2307472789988798, 0.20248443917807715, 0.2286631935521176], 'topk_tokens': [' by', '\n', ' composing', ' by', '\n', 'possible', ' George', 'pl', 'A', '\n', '.', ' regular', 'mail', ' pur', 'c', ' mail', ' pur', '\n', 'stage', 'hall'], 'evidence_proportions': [0.27129364013671875, 0.320013427734375, 0.14627456665039062, 0.2510711669921875]}, 'weight': {'score': [0.005253522098064423, 0.007344119812447166, 0.48190663316670584, 0.003467147155314634, 0.003033692376655445], 'topk_tokens': ['<|eot_id|>', '<|end_header_id|>', ' before', 'Answer', '?', ' the', '***', '.\n\n', '<|start_header_id|>', '<|eot_id|>', 'hall', 'assistant', '<|start_header_id|>', '<|eot_id|>', 'way', '<|end_header_id|>', '\n\n', ':', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.0032200217247009277, 0.007518744468688965, 0.005291437109311422, 0.004569602012634277]}, 'saliency': {'score': [0.0002566397190093994, 7.433490860901605e-05, 0.0019679542850045595, 5.794629020249806e-05, 6.961351946780556e-05], 'topk_tokens': [' garden', ' the', '.\n\n', ' to', 'If', 'NEW', '.', 'Answer', ' to', '<|end_header_id|>', '***', '<|start_header_id|>', 'way', ' before', '<|start_header_id|>', 'hall', '<|begin_of_text|>', ':', '\n\n', '.'], 'evidence_proportions': [6.416440010070801e-05, 0.0005325675010681152, 0.00017820795377095539, 0.00022881031036376954]}}, 30: {'grad': {'score': [0.17489933967590332, 0.22818587549234542, 0.13332829755895279, 0.22921957541189836, 0.20029502165944954], 'topk_tokens': [' S', ' Press', ' Herald', ' message', ' platform', ' stake', ' happened', ' Bank', ' Press', ' Hoe', ' States', ' efforts', ' Hoe', ' Reporter', ' room', ' arrival', ' Press', ' bring', 'ISC', ' for'], 'evidence_proportions': [0.29752349853515625, 0.11443157196044922, 0.16308212280273438, 0.1514484405517578]}, 'weight': {'score': [0.02184351086616516, 0.007248289945151094, 0.291701499153586, 0.004848069870107233, 0.016107350064997087], 'topk_tokens': ['NEW', ' bedroom', '��', '.\n\n', '?', ' \n', '<|eot_id|>', '<|start_header_id|>', 'assistant', 'hall', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', 'way', ':', '<|begin_of_text|>', '.'], 'evidence_proportions': [0.010397911071777344, 0.04318447113037109, 0.013560513655344646, 0.019598627090454103]}, 'saliency': {'score': [0.000511816143989563, 0.0001172726063609265, 0.0012698892284842098, 0.00010593080055394881, 0.0003807936844072844], 'topk_tokens': [' Joseph', ' Jul', ' Daily', ' hallway', '.', ' the', ' river', ' bedroom', '<|start_header_id|>', 'assistant', 'Bridge', '26', ' the', '<|begin_of_text|>', ' hallway', 'NEW', '.', ':', 'way', 'hall'], 'evidence_proportions': [0.0002739951014518738, 0.001208162307739258, 0.00023381908734639484, 0.0003393232822418213]}}, 31: {'grad': {'score': [0.17860492765903474, 0.232645584315097, 0.19442310753990621, 0.2332190310372068, 0.1401729683081309], 'topk_tokens': [' informed', ' family', ' had', ' delivered', 'wing', ' work', ' was', ' work', ' first', ' determined', 'the', ' of', ' day', ' had', ' had', 'ford', ' work', ' the', ' have', ' voice'], 'evidence_proportions': [0.13754406571388245, 0.20372440814971923, 0.20814792315165204, 0.1508825421333313]}, 'weight': {'score': [0.005991119146347046, 0.006743621372581237, 0.19032499018837423, 0.005243569217627665, 0.004311352445368181], 'topk_tokens': ['<|start_header_id|>', ':', ' Where', '.\n\n', ' before', '?', 'Answer', '<|eot_id|>', ' the', ' \n', '<|start_header_id|>', 'assistant', ':', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '.', '<|begin_of_text|>'], 'evidence_proportions': [0.0031157732009887695, 0.008454418182373045, 0.005080521106719971, 0.006920814514160156]}, 'saliency': {'score': [0.00010612905025482177, 7.93700907091465e-05, 0.0007536788197124706, 7.371803675178044e-05, 2.5915472131026418e-05], 'topk_tokens': [' boat', ' John', '?', ' bedroom', ' Where', '<|eot_id|>', '<|start_header_id|>', ':', ' garden', ' the', '<|begin_of_text|>', ' the', 'assistant', '<|end_header_id|>', ' hallway', '.', ':', ' hallway', 'way', 'hall'], 'evidence_proportions': [8.142739534378052e-05, 0.00013605356216430664, 0.00010958810647328694, 9.181499481201171e-05]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-23 22:34:30.312 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:34:30.313 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-16_pid-3_1-3-6-7.pkl | len: 10 |  size: 9.06 KB
Processing depth (1, 3, 6, 7):   4%|▍         | 4/100 [00:55<22:01, 13.77s/it]is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.08s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s][ALoading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Processing depth (3, 6, 7, 9):   4%|▍         | 4/100 [01:05<22:01, 13.77s/it]2025-01-23 22:34:40.142 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  Daniel grabbed the apple.
2025-01-23 22:34:40.150 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (1471, 1475) -->  grabbed the apple.
2025-01-23 22:34:40.150 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-23 22:34:40.162 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (2387, 2392) --> . Daniel travelled to the
2025-01-23 22:34:40.162 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  Daniel went back to the garden.
2025-01-23 22:34:40.180 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (2817, 2823) --> . Daniel went back to the
2025-01-23 22:34:40.180 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-23 22:34:40.198 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (3595, 3600) -->  Mary travelled to the bedroom
2025-01-23 22:34:40.198 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 0 -->  John went back to the office.
2025-01-23 22:34:40.210 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 0 at --> (2419, 2425) -->  the senate. John went back
2025-01-23 22:34:40.211 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 1 -->  John journeyed to the bedroom.
2025-01-23 22:34:40.211 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 1 at --> (67, 73) --> . John journeyed to the
2025-01-23 22:34:40.211 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 2 -->  John moved to the garden.
2025-01-23 22:34:40.215 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 2 at --> (785, 790) --> . John moved to the
2025-01-23 22:34:40.215 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 3 -->  Sandra journeyed to the office.
2025-01-23 22:34:40.219 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 3 at --> (716, 722) --> . Sandra journeyed to the
2025-01-23 22:34:40.219 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-23 22:34:40.235 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 4 at --> (3277, 3282) --> . Sandra moved to the
2025-01-23 22:34:40.235 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:670 - evidence 5 -->  Sandra went back to the hallway.
2025-01-23 22:34:40.245 | INFO     | test_jbb_retain_zero_embed:find_multi_needle_idx:678 - find evidence 5 at --> (1996, 2002) -->  the ground. Sandra went back
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-23 22:34:40.719 | INFO     | test_jbb_retain_zero_embed:begin_test:841 - hallway<|eot_id|>
2025-01-23 22:34:40.720 | INFO     | test_jbb_retain_zero_embed:begin_test:843 - torch.Size([1, 4195])
your chose emoji: ['🤙🏾', '🤤', '\U0001faf4🏻', '\U0001fa89', '🏄🏽\u200d♀️', '➰', '🦫', '🧝🏾', '🚶🏾', '🧗🏾']
Grad None!
ignore layer: 0 !
ignore layer: 1 !
ignore layer: 2 !
ignore layer: 3 !
ignore layer: 4 !
ignore layer: 5 !
ignore layer: 6 !
ignore layer: 7 !
ignore layer: 8 !
ignore layer: 9 !
ignore layer: 10 !
ignore layer: 11 !
ignore layer: 12 !
ignore layer: 13 !
ignore layer: 14 !
ignore layer: 15 !
ignore layer: 16 !
ignore layer: 17 !
ignore layer: 18 !
ignore layer: 19 !
ignore layer: 20 !
ignore layer: 21 !
ignore layer: 22 !
ignore layer: 23 !
Grad Not None! 0.01
torch.Size([1, 4198, 4096])

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 229824.88it/s]

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 128.20it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 132.35it/s]
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False

  0%|          | 0/8 [00:00<?, ?it/s][A100%|██████████| 8/8 [00:00<00:00, 131.98it/s]
2025-01-23 22:34:43.608 | INFO     | test_jbb_retain_zero_embed:begin_test:892 - {24: {'grad': {'score': [0.28557581901550294, 0.26197857831761406, 0.2739110273473403, 0.2617667906532877, 0.43650390625], 'topk_tokens': [' parade', ' competitors', 'sur', ' proceedings', ' contents', 'icing', ' announcement', 'hand', 'announcement', '�', ' upper', ' proceed', ' proceedings', ' lin', 'hom', 'announcement', ' hand', 'hand', ' Project', ' hand'], 'evidence_proportions': [0.2567291259765625, 0.3506431579589844, 0.24298270543416342, 0.29469757080078124]}, 'weight': {'score': [0.027837933599948884, 0.007429567276153637, 0.006109608446850497, 0.0073419010777270935, 0.006607679128646851], 'topk_tokens': ['<|eot_id|>', '�', ' the', ' ', ' apple', '\n\n', 'Answer', ':', '<|start_header_id|>', 'assistant', '.', '<|eot_id|>', ' hallway', '\n\n', '<|eot_id|>', '<|end_header_id|>', ' hallway', 'way', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.09930801391601562, 0.010528892278671265, 0.0031375984350840254, 0.017611312866210937]}, 'saliency': {'score': [0.0012774005532264709, 0.00010239081248719105, 0.00014450181933010325, 9.637441069002777e-05, 0.00024362266063690185], 'topk_tokens': [' random', ' bedroom', '<|start_header_id|>', ':', 'Bridge', '�', ' Bench', ' Daniel', '<|end_header_id|>', '\n\n', ' During', ' apple', ' bedroom', ' ', '.', ' hallway', '<|begin_of_text|>', 'way', ' hallway', 'hall'], 'evidence_proportions': [0.004316896200180054, 0.0003599822521209717, 4.9302975336710616e-05, 0.0012369394302368165]}}, 25: {'grad': {'score': [0.31212425231933594, 0.44840111443972575, 0.32669830322265625, 0.4500573482292499, 0.3125674819946289], 'topk_tokens': [' getting', ' great', 'g', ' o', ' two', ' to', ' had', ' a', ' to', '2', ' large', 'iture', ' a', ' threw', 'human', ' probably', ' two', ' extraordinary', '10', ' producing'], 'evidence_proportions': [0.41149139404296875, 0.29499359130859376, 0.2704569498697917, 0.299761962890625]}, 'weight': {'score': [0.029222701489925385, 0.007421651870196862, 0.003781706971280715, 0.007346298862032909, 0.007901111841201782], 'topk_tokens': ['<|start_header_id|>', ' the', ' apple', ' Daniel', '�', '.\n\n', 'Answer', ' Daniel', '<|start_header_id|>', ' hallway', ':', '<|eot_id|>', '.', 'assistant', 'hall', '<|eot_id|>', 'way', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [0.0972433090209961, 0.021510815620422362, 0.004405791560808817, 0.012298393249511718]}, 'saliency': {'score': [0.0010010287165641784, 6.699978084209819e-05, 6.140330258537742e-05, 6.253783623454193e-05, 0.00020508944988250732], 'topk_tokens': ['.\n\n', ' corner', ' Mary', '�', ' hallway', 'way', ' double', ' apple', '<|eot_id|>', 'assistant', ' During', '<|eot_id|>', ' random', ' Daniel', ' hallway', 'hall', ' Daniel', '<|end_header_id|>', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0016421601176261902, 0.001919347047805786, 0.00011248886585235596, 0.0006360530853271485]}}, 26: {'grad': {'score': [0.1576087236404419, 0.24603081022119908, 0.21332219067741842, 0.2467259199692936, 0.19695051193237303], 'topk_tokens': ['blue', ' true', ' time', ' upper', ' up', 're', 'ible', ' double', ' Marshall', ' exc', ' sure', 'asca', 'city', 'outs', ' it', 'ucci', ' Cl', ' prepared', ' and', 'hall'], 'evidence_proportions': [0.2349395751953125, 0.12273406982421875, 0.19980875651041669, 0.07997865676879884]}, 'weight': {'score': [0.02611175924539566, 0.007228398334417757, 0.0046345346114214725, 0.0071585441231152265, 0.008124778270721436], 'topk_tokens': [' apple', '<|eot_id|>', '�', '<|start_header_id|>', '.', '.\n\n', ' \n', ' hallway', 'Answer', 'assistant', '<|eot_id|>', '\n\n', ' hallway', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', ':', 'way', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [0.06975841522216797, 0.021439236402511597, 0.004232873519261679, 0.022121620178222653]}, 'saliency': {'score': [0.0009143635630607605, 0.00018410529478099473, 0.00015291133347679586, 0.00018083681729999748, 0.00018176019191741942], 'topk_tokens': ['.', ' the', '***', '.', ' Daniel', '<|start_header_id|>', 'Bridge', ' apple', '�', '<|eot_id|>', 'assistant', ' Daniel', 'way', '<|begin_of_text|>', ':', ' hallway', '\n\n', '<|end_header_id|>', ' hallway', 'hall'], 'evidence_proportions': [0.002141997218132019, 0.0010442912578582763, 0.0003913243611653646, 0.0004299759864807129]}}, 27: {'grad': {'score': [0.3124969482421875, 0.326860592330053, 0.3221420961267808, 0.32696862846728, 0.3307925224304199], 'topk_tokens': ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'], 'evidence_proportions': [0.266326904296875, 0.37700195312499996, 0.3103828430175781, 0.28746490478515624]}, 'weight': {'score': [0.025688168406486512, 0.007419759969361456, 0.004067931981647716, 0.007359092494178003, 0.010294978618621825], 'topk_tokens': [' before', ' Daniel', ' garden', ' \n', ' bedroom', 'Answer', '.', '<|eot_id|>', ' hallway', '\n\n', '<|eot_id|>', 'assistant', '<|start_header_id|>', '.\n\n', ':', 'hall', '<|end_header_id|>', ' hallway', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.06293678283691406, 0.019825899600982667, 0.003977636496225993, 0.027804183959960937]}, 'saliency': {'score': [0.0011443763971328736, 0.00013820955502299708, 0.0001493972890517291, 0.00013326174619114996, 0.00030844926834106446], 'topk_tokens': [' When', 'RE', '�', 'Answer', ' garden', '.', ' bedroom', 'NEW', '<|start_header_id|>', '<|begin_of_text|>', ' hallway', 'assistant', ' Daniel', ':', ' Daniel', ' hallway', 'way', '<|end_header_id|>', '.\n\n', 'hall'], 'evidence_proportions': [0.0008388012647628784, 0.0023276209831237793, 0.00020430982112884521, 0.0013336718082427978]}}, 28: {'grad': {'score': [0.2543375015258789, 0.3081222904926825, 0.3425433495465447, 0.3080994574259607, 0.2944824409484863], 'topk_tokens': [' huge', ' location', ' hallway', ' Brown', ' Hill', "'clock", "'clock", 'ivery', ' lowered', ' laid', ' and', ' location', ' hallway', 'ching', ' and', 'ched', ' line', ' house', ',', ' and'], 'evidence_proportions': [0.2387990951538086, 0.3243011474609375, 0.22051874796549478, 0.23738708496093752]}, 'weight': {'score': [0.0221241295337677, 0.00713205155785621, 0.006787973291733686, 0.0070625190051366, 0.0068978965282440186], 'topk_tokens': [' the', '.', ' hallway', ' apple', '?', ' garden', 'Answer', ' \n', '.\n\n', '<|eot_id|>', ' before', 'assistant', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '\n\n', 'hall', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.040782928466796875, 0.025888895988464354, 0.007619897524515788, 0.02083740234375]}, 'saliency': {'score': [0.00046368390321731566, 0.0001280998448180607, 0.0002745819442412432, 0.0001252783987397853, 0.00011702954769134522], 'topk_tokens': [':', '?', ' the', 'Bridge', ' grabbed', ' back', ' hallway', ' During', ' the', '.\n\n', ' hallway', '<|end_header_id|>', ' the', 'assistant', '<|start_header_id|>', 'way', '<|begin_of_text|>', '\n\n', ':', 'hall'], 'evidence_proportions': [0.0009117648005485535, 0.00046452879905700684, 0.0002406885226567586, 0.0003719687461853027]}}, 29: {'grad': {'score': [0.25728068351745603, 0.2013736632166731, 0.2330143171198228, 0.20084424173049484, 0.18442211151123047], 'topk_tokens': [' pur', ' a', ' the', ' marched', ' the', '185', ' regular', 'A', ' the', '\n', '\n', 'stage', '\n', '\n', 'hall', ' mail', 'mail', '\n', '\n', '\n'], 'evidence_proportions': [0.3259773254394531, 0.3116180419921875, 0.1532438596089681, 0.2728302001953125]}, 'weight': {'score': [0.013169269263744354, 0.007279944226763372, 0.003713883021298577, 0.0072807790675563705, 0.004992035627365113], 'topk_tokens': [' Where', '<|eot_id|>', '�', '<|end_header_id|>', ' before', ' \n', '.', 'Answer', '<|eot_id|>', '.\n\n', '<|start_header_id|>', '<|start_header_id|>', 'assistant', 'hall', '<|eot_id|>', ':', '<|end_header_id|>', 'way', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.03649711608886719, 0.010722535848617555, 0.002397000789642334, 0.009880447387695314]}, 'saliency': {'score': [0.00017527341842651367, 7.341371910409623e-05, 7.471266914816464e-05, 7.291146082997782e-05, 0.00029754698276519777], 'topk_tokens': ['.', ' the', '<|end_header_id|>', '<|eot_id|>', '.', '"The', '<|eot_id|>', ' the', '<|end_header_id|>', 'Answer', 'assistant', '<|start_header_id|>', 'way', ' before', '�', '<|start_header_id|>', ':', 'hall', '\n\n', '<|begin_of_text|>'], 'evidence_proportions': [0.0005280524492263794, 0.00014646649360656737, 5.103647708892822e-05, 7.094144821166992e-05]}}, 30: {'grad': {'score': [0.23595018386840821, 0.282052969239677, 0.1979056807125316, 0.28296587066760853, 0.3185210418701172], 'topk_tokens': [' Project', ' Herald', 'ENCES', ' Republicans', ' Bank', ' Pioneer', ' Herald', 'Civil', ' Square', ' Pioneer', 'Republicans', ' Pioneer', ' Articles', 'ISC', ' States', ' Out', ' Press', ' As', ' Pioneer', ' Fourth'], 'evidence_proportions': [0.3294830322265625, 0.25756378173828126, 0.21265188852945965, 0.16746826171875]}, 'weight': {'score': [0.025898198783397674, 0.00713004538875469, 0.007461021928226247, 0.0070367499567482, 0.01642155408859253], 'topk_tokens': ['.', '�', '�', '?', '<|eot_id|>', 'Answer', '.\n\n', '<|end_header_id|>', '<|eot_id|>', ' \n', 'hall', 'assistant', '\n\n', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>', ':', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.05180931091308594, 0.025419330596923827, 0.006325259804725647, 0.029135704040527344]}, 'saliency': {'score': [0.0010455802083015441, 7.291747281073161e-05, 0.00038203947684344123, 6.568692193068133e-05, 0.00024191200733184815], 'topk_tokens': ['.', ' before', '?', ' hallway', ' Sandra', 'NEW', '\n\n', '.', '<|eot_id|>', '�', 'assistant', ' Daniel', ' Daniel', '.', ' the', ' hallway', ':', '<|begin_of_text|>', 'way', 'hall'], 'evidence_proportions': [0.002257116138935089, 0.0014806807041168213, 0.00028273959954579675, 0.0005566596984863282]}}, 31: {'grad': {'score': [0.25646969527006147, 0.3033149284854169, 0.2894043019589256, 0.30365514710660607, 0.14862775802612305], 'topk_tokens': [' injunction', ' was', ' would', ' the', ' decided', ' DAYS', ' had', ' they', ' an', ' had', ' informed', ' voice', 'the', ' work', ' determined', 'ford', ' have', ' day', ' of', ' the'], 'evidence_proportions': [0.28620775789022446, 0.29923647046089175, 0.21216371655464172, 0.2430796444416046]}, 'weight': {'score': [0.007007125020027161, 0.006700500526446397, 0.0030879325726452995, 0.006728660473492154, 0.003343045711517334], 'topk_tokens': [':', '<|start_header_id|>', ' Where', ' apple', ' before', '?', ' the', '.\n\n', '<|eot_id|>', 'Answer', ' \n', '<|start_header_id|>', ':', 'assistant', '\n\n', '<|eot_id|>', '<|end_header_id|>', 'hall', 'way', '<|begin_of_text|>'], 'evidence_proportions': [0.013782978057861328, 0.0047711610794067385, 0.0022351642449696856, 0.009548759460449217]}, 'saliency': {'score': [0.00019848346710205078, 4.986583079083185e-05, 7.463027449215159e-05, 4.8945381029232125e-05, 1.9411444664001466e-05], 'topk_tokens': ['Question', ' the', '.', ':', '<|eot_id|>', ' Where', ' bedroom', ' the', '.\n\n', ' \n', ' apple', '<|start_header_id|>', ':', ' hallway', 'assistant', '<|end_header_id|>', '<|begin_of_text|>', 'hall', ' hallway', 'way'], 'evidence_proportions': [0.0004018545150756836, 9.762048721313476e-05, 4.1390458742777504e-05, 0.00032516121864318843]}}, 'pred_res': 'hallway<|eot_id|>', 'score': 100}
2025-01-23 22:34:43.614 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-23 22:34:43.614 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/SaliencyResults/preliminary/babilong_random5x100/results/information_flow_normal_max12k_sample100_gws/Meta-Llama-3.1-8B-Instruct_factor0.01/3900/label/3-hop_sid-16_pid-4_3-6-7-9.pkl | len: 10 |  size: 9.13 KB
Processing depth (3, 6, 7, 9):   5%|▌         | 5/100 [01:09<21:31, 13.60s/it]Processing depth (3, 6, 7, 9):   5%|▌         | 5/100 [01:09<21:55, 13.84s/it]
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:99 - Selected idx: 17
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:100 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:101 - Answer: hallway
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:102 - Tag: 4-hop
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:103 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' John went back to the office.', ' Sandra went back to the hallway.', ' Daniel went back to the garden.', ' Sandra moved to the kitchen.', ' John journeyed to the bedroom.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:104 - Real Needle: [' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' Daniel went back to the garden.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-23 22:34:43.824 | INFO     | __main__:<module>:105 - =============================================
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.01s/it][A