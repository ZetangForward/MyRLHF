/mnt/petrelfs/tangzecheng/.bashrc:è¡Œ90: proxy_on: æœªæ‰¾åˆ°å‘½ä»¤
/mnt/petrelfs/tangzecheng/.bashrc:è¡Œ97: svp: æœªæ‰¾åˆ°å‘½ä»¤
/mnt/petrelfs/tangzecheng/.bashrc:è¡Œ101: svp: æœªæ‰¾åˆ°å‘½ä»¤
2025-01-22 01:25:38.464 | INFO     | __main__:__init__:146 - loading from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-01-22 01:25:38.708 | INFO     | __main__:__init__:150 - layer number: 32, head number 32
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
