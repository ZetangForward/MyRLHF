nohup: 忽略输入
2025-01-22 03:45:52.832 | INFO     | test_jbb_embedding:<module>:7 - ['/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/analysis', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python310.zip', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages', '/mnt/petrelfs/tangzecheng/DeepSpeed', '/mnt/petrelfs/tangzecheng/modelzipper/src', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 03:45:55.382 | INFO     | __main__:<module>:11 - ['/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/faiss_attn', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect', '/mnt/petrelfs/tangzecheng/MyRLHF/reetrievalheaddetect/analysis', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python310.zip', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/lib-dynload', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages', '/mnt/petrelfs/tangzecheng/DeepSpeed', '/mnt/petrelfs/tangzecheng/modelzipper/src', '/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/setuptools/_vendor']
2025-01-22 03:45:56.188 | INFO     | __main__:<module>:82 - Selected idx: 0
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:86 - Needle: [' John went back to the bedroom.', ' John took the milk.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the kitchen.']
2025-01-22 03:45:56.189 | INFO     | __main__:<module>:88 - =============================================
ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-01-22 07:45:47
Process: 70521
begin to read data from ./haystack_for_detect/reasoning_needle_jbb_200.jsonl | file size: 247.16 KB | file type: jsonl
begin to testing with [11900]
  0%|          | 0/100 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.74s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (2, 3, 4, 9):   0%|          | 0/100 [00:23<?, ?it/s]2025-01-22 03:46:20.254 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:46:20.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4903, 4909) --> . Mary journeyed to the
2025-01-22 03:46:20.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:46:20.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4903, 4909) --> . Mary journeyed to the
2025-01-22 03:46:20.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:46:20.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9750, 9754) -->  Mary dropped the football
2025-01-22 03:46:20.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:46:20.456 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21460, 21466) --> . Daniel went back to the
2025-01-22 03:46:20.456 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:46:20.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6994, 7000) -->  John went back to the bedroom
2025-01-22 03:46:20.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John took the milk.
2025-01-22 03:46:20.549 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11768, 11772) -->  John took the milk
2025-01-22 03:46:20.549 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 03:46:20.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6538, 6544) --> . Sandra journeyed to the
2025-01-22 03:46:20.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:46:20.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4904, 4910) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-22 03:46:26.893 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 03:46:26.893 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 03:46:35.244 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [583.4886363636364, 24.499428314225405, 563.3821022727273, 23.497928069411618, 40.922247023809526], 'topk_indices': array([ 6540, 24141, 11176, 11434,  9754, 24144, 11435, 24154,  6539,
        4905, 24153, 24142,  7606,  6544, 11438, 24146,  9753, 24147,
       24149,     0]), 'topk_tokens': [' journey', 'Question', 'coop', ' journey', '.', ' was', 'ed', '<|eot_id|>', ' Sandra', ' journey', ':', ':', ' bathroom', ' bedroom', ' office', ' football', ' football', ' before', ' bathroom', '<|begin_of_text|>'], 'evidence_proportions': [724.9583333333334, 724.9583333333334, 871.875, 108.29166666666667]}, 'weight': {'score': [21.556463068181817, 23.43450705682712, 23.457741477272727, 23.436199050462328, 29.03249007936508], 'topk_indices': array([18764, 18808, 14646, 14610, 14691, 19473, 14655, 19415, 14574,
       14509, 20287, 20335, 18102, 18133, 23517, 23651, 21944, 21971,
       23599, 23733]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [3.4852128462357954, 0.14103604061685154, 3.56842041015625, 0.13485888981644173, 0.30307418581039186], 'topk_indices': array([24143, 24144, 24152,  6551, 24013,  4904,  6540, 24141, 11434,
        6999, 11176,  4905, 11438,  6539, 24147,  7606,  6544, 24146,
        9753, 24149]), 'topk_tokens': [' Where', ' was', 'Answer', 'ford', ' context', ' Mary', ' journey', 'Question', ' journey', ' bedroom', 'coop', ' journey', ' office', ' Sandra', ' before', ' bathroom', ' bedroom', ' football', ' football', ' bathroom'], 'evidence_proportions': [4.0654296875, 4.0654296875, 6.15966796875, 0.54180908203125]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:46:35.250 | INFO     | modelzipper.tutils:auto_save_data:296 - /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label not exist! --> Create data dir /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label
2025-01-22 03:46:35.254 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:46:35.254 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-0_2-3-4-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 3, 4, 9):   1%|          | 1/100 [00:38<1:04:04, 38.83s/it]is_0k: False
your chose emoji: ['💆🏻\u200d♂', '👫🏻', '🇺🇿', '❤️\u200d🔥', '🇯🇲', '👬🏿', '🕵🏽\u200d♂', '👩🏻\u200d🦲', '🚑', '📔']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (2, 5, 6, 8):   1%|          | 1/100 [00:56<1:04:04, 38.83s/it]2025-01-22 03:46:52.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:46:52.838 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4945, 4951) --> . Mary journeyed to the
2025-01-22 03:46:52.838 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:46:52.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4945, 4951) --> . Mary journeyed to the
2025-01-22 03:46:52.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:46:52.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14336, 14340) -->  Mary dropped the football
2025-01-22 03:46:52.939 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:46:53.042 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19249, 19255) -->  Daniel went back to the kitchen
2025-01-22 03:46:53.043 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:46:53.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6969, 6975) --> . John went back to the
2025-01-22 03:46:53.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John took the milk.
2025-01-22 03:46:53.134 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11624, 11628) -->  John took the milk
2025-01-22 03:46:53.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 03:46:53.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6514, 6520) --> . Sandra journeyed to the
2025-01-22 03:46:53.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:46:53.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4946, 4952) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:46:55.863 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 03:46:55.863 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24147])
2025-01-22 03:47:04.230 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [529.8125, 17.35143535964222, 363.3636363636364, 16.567929371499687, 27.158648574561404], 'topk_indices': array([   14, 24001,  6520, 24130, 11322, 24035,  6515, 24135, 14340,
       11820, 14338, 24134, 24146, 24141, 24137, 14337, 24145,     0,
       14339, 24142]), 'topk_tokens': ['\n', ' context', ' bedroom', ':', ' office', '.\n\n', ' Sandra', ' before', '.', ' bathroom', ' the', ' football', '\n\n', ':', ' bathroom', ' dropped', '<|end_header_id|>', '<|begin_of_text|>', ' football', '<|eot_id|>'], 'evidence_proportions': [463.0833333333333, 463.0833333333333, 1316.125, 139.0625]}, 'weight': {'score': [22.644886363636363, 23.43135585324444, 22.224431818181817, 23.433175171126322, 28.953673245614034], 'topk_indices': array([18758, 18802, 14654, 14618, 19404, 19462, 14663, 14699, 14582,
       14517, 20276, 20324, 18096, 18127, 23499, 23633, 21926, 21953,
       23581, 23715]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.052083333333332, 27.013671875, 24.91796875]}, 'saliency': {'score': [3.3117842240767046, 0.09847864875033645, 2.133644797585227, 0.0936885065420167, 0.19455973307291666], 'topk_indices': array([24116, 24035, 24146,  6516, 24144, 11318,  4947, 24045, 24140,
       24001, 24135, 24129, 11322,  6520,  6515, 24134, 14337, 11820,
       24137, 14339]), 'topk_tokens': [' return', '.\n\n', '\n\n', ' journey', 'assistant', ' journey', ' journey', ' location', 'Answer', ' context', ' before', 'Question', ' office', ' bedroom', ' Sandra', ' football', ' dropped', ' bathroom', ' bathroom', ' football'], 'evidence_proportions': [2.4892578125, 2.4892578125, 9.31982421875, 0.95147705078125]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:47:04.246 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:47:04.246 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-1_2-5-6-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (2, 5, 6, 8):   2%|▏         | 2/100 [01:07<53:58, 33.04s/it]  is_0k: False
your chose emoji: ['🏈', '🏊🏼', '📀', '💇🏾\u200d♀️', '🔉', '🧕🏾', '🏄🏽', '🤵\u200d♂', '🧑🏽\u200d🦱', '🇸🇪']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.38s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.34s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.35s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.51s/it]
Processing depth (0, 4, 5, 6):   2%|▏         | 2/100 [01:24<53:58, 33.04s/it]2025-01-22 03:47:20.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:47:20.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:47:20.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:47:21.047 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9716, 9722) --> . Mary journeyed to the
2025-01-22 03:47:21.047 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:47:21.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11900, 11904) -->  Mary dropped the football
2025-01-22 03:47:21.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:47:21.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14370, 14376) --> . Daniel went back to the
2025-01-22 03:47:21.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:47:21.228 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6999, 7005) --> . John went back to the
2025-01-22 03:47:21.228 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John took the milk.
2025-01-22 03:47:21.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11741, 11745) -->  John took the milk
2025-01-22 03:47:21.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 03:47:21.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6570, 6576) --> . Sandra journeyed to the
2025-01-22 03:47:21.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:47:21.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:47:24.143 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the football.<|eot_id|>
2025-01-22 03:47:24.144 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 03:47:32.547 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1217.659090909091, 35.99375284196602, 1782.4431818181818, 33.325987700335446, 47.97275641025641], 'topk_indices': array([   24,  6574,    30, 11386, 11385, 24184,    29,    36, 24185,
       11903,    33,  6576,    31,  6571,  6570,    32,  6573,  6572,
        6577,     0]), 'topk_tokens': ['\n\n', ' to', 'Mary', 'ed', ' journey', '<|eot_id|>', '\n\n', '.', '<|start_header_id|>', ' football', ' to', ' bedroom', ' journey', ' Sandra', '.', 'ed', 'ed', ' journey', '.', '<|begin_of_text|>'], 'evidence_proportions': [2425.1666666666665, 848.8333333333334, 1604.25, 121.25]}, 'weight': {'score': [21.417258522727273, 23.443672233475258, 21.17294034090909, 23.447587303805857, 29.37139423076923], 'topk_indices': array([18835, 18791, 14655, 14691, 19488, 19430, 14736, 14700, 14619,
       14554, 20308, 20356, 18129, 18160, 23659, 23525, 21964, 21991,
       23607, 23741]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [6.767500443892046, 0.20214999170016743, 9.972367720170455, 0.18726687992750135, 0.3564155774238782], 'topk_indices': array([ 6570, 24043,  6569,  6577,  9722, 24171, 11389, 11384, 24176,
           0, 24179,    30, 11385,    32,  6573, 11903,    31,  6576,
        6571,  6572]), 'topk_tokens': ['.', ' context', ' Gen', '.', ' bathroom', 'Question', ' office', ' John', ' football', '<|begin_of_text|>', ' bathroom', 'Mary', ' journey', 'ed', 'ed', ' football', ' journey', ' bedroom', ' Sandra', ' journey'], 'evidence_proportions': [12.337239583333334, 4.653483072916667, 10.8291015625, 0.6040445963541666]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:47:32.554 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:47:32.554 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-2_0-4-5-6.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 4, 5, 6):   3%|▎         | 3/100 [01:36<49:55, 30.88s/it]is_0k: False
your chose emoji: ['🥸', '👴🏼', '🦸🏼', '🤽🏽\u200d♂', '🏃🏼', '🔎', '🕵🏾\u200d♂️', '👩🏼\u200d❤\u200d💋\u200d👩🏽', '🛻', '👩🏽\u200d🦽\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]
Processing depth (1, 2, 3, 7):   3%|▎         | 3/100 [01:53<49:55, 30.88s/it]2025-01-22 03:47:50.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:47:50.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2974, 2980) -->  tragedy. Mary journeyed to
2025-01-22 03:47:50.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:47:50.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2974, 2980) -->  tragedy. Mary journeyed to
2025-01-22 03:47:50.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:47:50.368 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7560, 7564) -->  Mary dropped the football
2025-01-22 03:47:50.368 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:47:50.461 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16792, 16798) --> . Daniel went back to the
2025-01-22 03:47:50.461 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:47:50.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6996, 7002) --> . John went back to the
2025-01-22 03:47:50.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John took the milk.
2025-01-22 03:47:50.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11806, 11810) -->  John took the milk
2025-01-22 03:47:50.562 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 03:47:50.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6553, 6559) --> . Sandra journeyed to the
2025-01-22 03:47:50.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:47:50.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2976, 2982) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:47:53.248 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 03:47:53.248 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 03:48:01.624 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [370.75142045454544, 16.467243555261305, 307.3025568181818, 15.878899286987522, 28.976059422348484], 'topk_indices': array([18941,  4978, 24019,    23,     1, 24156, 24147, 24164, 24148,
       24152,    14, 24053, 24146, 24159, 24153,  7563, 24155, 24163,
           0, 24160]), 'topk_tokens': [' in', ' bathroom', ' context', '4', '<|start_header_id|>', '?', 'Question', '\n\n', ':', ' football', '\n', '.\n\n', '.\n\n', ':', ' before', ' football', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [410.9583333333333, 410.9583333333333, 674.9375, 87.546875]}, 'weight': {'score': [24.19637784090909, 23.437383622294863, 22.224431818181817, 23.437797628300792, 29.3046875], 'topk_indices': array([18797, 18841, 14656, 14620, 14701, 14665, 19494, 19436, 14519,
       14584, 20356, 20308, 18129, 18160, 23647, 23513, 21952, 21979,
       23729, 23595]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.891927083333332, 24.891927083333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [2.289584073153409, 0.0932606984246338, 1.8130243474786931, 0.0896892556304862, 0.21612340753728693], 'topk_indices': array([ 6559, 11490, 24156, 24149, 24063, 24162,  6997, 24053, 11486,
        2977, 24158, 24146, 18940, 24019, 24147, 24153,  4978, 24152,
        7563, 24155]), 'topk_tokens': [' bedroom', ' office', '?', ' Where', ' location', 'assistant', ' John', '.\n\n', ' journey', ' journey', 'Answer', '.\n\n', ' manner', ' context', 'Question', ' before', ' bathroom', ' football', ' football', ' bathroom'], 'evidence_proportions': [2.3758951822916665, 2.3758951822916665, 4.71923828125, 0.4971923828125]}}, 'pred_res': 'the office<|eot_id|>', 'score': 100}
2025-01-22 03:48:01.630 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:48:01.631 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-3_1-2-3-7.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 2, 3, 7):   4%|▍         | 4/100 [02:05<48:16, 30.17s/it]is_0k: False
your chose emoji: ['🤵🏼\u200d♂️', '👩🏽\u200d🍼', '👩🏾\u200d🦼\u200d➡', '🟧', '🐶', '🐴', '👲🏻', '🔮', '💇🏿\u200d♀️', '👨\u200d⚕️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.21s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.86s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]
Processing depth (2, 3, 5, 7):   4%|▍         | 4/100 [02:22<48:16, 30.17s/it]2025-01-22 03:48:19.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:48:19.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4929, 4935) --> . Mary journeyed to the
2025-01-22 03:48:19.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:48:19.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4929, 4935) --> . Mary journeyed to the
2025-01-22 03:48:19.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:48:19.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11866, 11870) -->  Mary dropped the football
2025-01-22 03:48:19.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:48:19.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16812, 16818) --> . Daniel went back to the
2025-01-22 03:48:19.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:48:19.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7019, 7025) --> . John went back to the
2025-01-22 03:48:19.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John took the milk.
2025-01-22 03:48:19.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11675, 11679) -->  John took the milk
2025-01-22 03:48:19.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 03:48:19.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6564, 6570) --> . Sandra journeyed to the
2025-01-22 03:48:19.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:48:19.777 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4930, 4936) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:48:22.448 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 03:48:22.449 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 03:48:30.820 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [326.49360795454544, 13.279891360499235, 253.36363636363637, 12.7759148708235, 25.440875771604937], 'topk_indices': array([    3, 24176, 24178,  7504, 24189, 24194, 11870, 24183, 24093,
       24083, 24182,    23,    24,    14,     1, 24185, 11869, 24190,
           0, 24193]), 'topk_tokens': ['<|end_header_id|>', '.\n\n', ':', ' bathroom', ':', '\n\n', '.', ' before', ' location', '.\n\n', ' football', '4', '\n\n', '\n', '<|start_header_id|>', ' bathroom', ' football', '<|eot_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [325.125, 325.125, 742.9375, 51.6015625]}, 'weight': {'score': [21.556463068181817, 23.44992664379882, 22.224431818181817, 23.452767578044135, 30.09104938271605], 'topk_indices': array([18851, 18807, 14698, 14662, 14743, 14707, 19504, 19446, 14626,
       14561, 20366, 20318, 18162, 18131, 23539, 23673, 21968, 21995,
       23621, 23755]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [1.9537464488636365, 0.07481333639629913, 1.5555697354403408, 0.07175312982762234, 0.19598539375964505], 'topk_indices': array([   20,     8, 24049, 24179, 24083, 24188, 11867,  6570, 24164,
          24,    23,  6565, 24192, 24177, 24183, 24093,  7504, 24182,
       11869, 24185]), 'topk_tokens': [' Jul', ' Date', ' context', ' Where', '.\n\n', 'Answer', ' dropped', ' bedroom', ' return', '\n\n', '4', ' Sandra', 'assistant', 'Question', ' before', ' location', ' bathroom', ' football', ' football', ' bathroom'], 'evidence_proportions': [1.7410888671875, 1.7410888671875, 5.0966796875, 0.2837727864583333]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 100}
2025-01-22 03:48:30.845 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:48:30.845 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-0_pid-4_2-3-5-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 3, 5, 7):   5%|▌         | 5/100 [02:34<47:13, 29.82s/it]Processing depth (2, 3, 5, 7):   5%|▌         | 5/100 [02:34<49:02, 30.97s/it]
2025-01-22 03:48:31.293 | INFO     | __main__:<module>:82 - Selected idx: 1
2025-01-22 03:48:31.293 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 03:48:31.294 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 03:48:31.294 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 03:48:31.294 | INFO     | __main__:<module>:86 - Needle: [' Mary journeyed to the office.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary got the apple.', ' John took the milk.', ' Mary journeyed to the bathroom.', ' Sandra journeyed to the bedroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 03:48:31.294 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the office.', ' Mary got the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the kitchen.']
2025-01-22 03:48:31.294 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏋\u200d♂️', '🧏\u200d♂', '🤵🏼\u200d♂', '🙎\u200d♀️', '🚟', '🧑🏽\u200d❤️\u200d🧑🏻', '💑🏽', '👩🏻\u200d❤️\u200d👨🏻', '🏔', '🤷🏼\u200d♂️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.91s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.54s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.48s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (2, 5, 6, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 03:48:47.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:48:47.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4967, 4973) --> . Mary journeyed to the
2025-01-22 03:48:47.866 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the apple.
2025-01-22 03:48:47.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11982, 11986) -->  Mary got the apple
2025-01-22 03:48:47.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:48:47.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4967, 4973) --> . Mary journeyed to the
2025-01-22 03:48:47.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:48:48.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19431, 19435) -->  Mary dropped the apple
2025-01-22 03:48:48.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 03:48:48.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21436, 21442) --> . Daniel went back to the
2025-01-22 03:48:48.147 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:48:48.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18103, 18109) --> . John went back to the
2025-01-22 03:48:48.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the office.
2025-01-22 03:48:48.269 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4968, 4974) -->  Mary journeyed to the office
2025-01-22 03:48:48.269 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John took the milk.
2025-01-22 03:48:48.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21915, 21919) -->  took the milk.
2025-01-22 03:48:48.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:48:48.434 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11654, 11660) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:48:51.239 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the apple.<|eot_id|>
2025-01-22 03:48:51.239 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24143])
2025-01-22 03:48:59.637 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [387.2782451923077, 15.643298301925865, 259.77627840909093, 15.019426692119351, 31.811604817708332], 'topk_indices': array([   24, 11983, 11984, 24127, 24115, 24022, 24116, 19435, 14426,
       19434, 19780, 24117, 24141, 11985, 24128, 24139, 11986, 19779,
       24138,     0]), 'topk_tokens': ['\n\n', ' got', ' the', ' the', '.\n\n', '.\n\n', 'Question', '.', ' bathroom', ' apple', 'ed', ':', '<|end_header_id|>', ' apple', ' apple', '<|start_header_id|>', '.', ' journey', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [377.375, 751.125, 377.375, 574.1875, 39.9140625]}, 'weight': {'score': [22.254807692307693, 23.42662041830607, 21.346946022727273, 23.429783466302858, 29.009440104166668], 'topk_indices': array([18720, 18764, 14625, 14589, 14670, 19423, 14634, 19365, 14553,
       14488, 20313, 20265, 18051, 18082, 23492, 23626, 21931, 21958,
       23708, 23574]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.353515625, 20.052083333333332, 26.755859375, 20.927083333333332]}, 'saliency': {'score': [2.379033015324519, 0.08873792290070408, 1.5286338112571023, 0.08495216608670815, 0.22873942057291666], 'topk_indices': array([24115, 24122, 24103, 11982, 19780,  4969, 19783, 24136, 24130,
       24126, 11656, 11660, 11983,  3392, 24116, 19434, 11985, 14426,
       24128, 19779]), 'topk_tokens': ['.\n\n', ' prior', ' return', ' Mary', 'ed', ' journey', ' office', 'Answer', ' discarded', ' where', ' journey', ' bedroom', ' got', '�', 'Question', ' apple', ' apple', ' bathroom', ' apple', ' journey'], 'evidence_proportions': [2.015380859375, 5.08251953125, 2.015380859375, 4.0361328125, 0.19927978515625]}}, 'pred_res': 'Mary dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:48:59.642 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:48:59.642 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-0_2-5-6-8-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 5, 6, 8, 9):   1%|          | 1/100 [00:28<46:25, 28.14s/it]is_0k: False
your chose emoji: ['🥥', '🇲🇼', '\U0001fae1', '☕', '👳🏿\u200d♂', '🐛', '👩🏻\u200d🦼\u200d➡️', '🏡', '🦳', '🔕']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (1, 2, 4, 7, 8):   1%|          | 1/100 [00:45<46:25, 28.14s/it]2025-01-22 03:49:16.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:49:16.789 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2913, 2919) --> . Mary journeyed to the
2025-01-22 03:49:16.789 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the apple.
2025-01-22 03:49:16.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4827, 4831) -->  Mary got the apple
2025-01-22 03:49:16.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:49:16.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2913, 2919) --> . Mary journeyed to the
2025-01-22 03:49:16.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:49:16.925 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16688, 16692) -->  Mary dropped the apple
2025-01-22 03:49:16.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 03:49:17.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19242, 19248) -->  Daniel went back to the kitchen
2025-01-22 03:49:17.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:49:17.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17858, 17864) --> . John went back to the
2025-01-22 03:49:17.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the office.
2025-01-22 03:49:17.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2914, 2920) -->  Mary journeyed to the office
2025-01-22 03:49:17.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John took the milk.
2025-01-22 03:49:17.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (22016, 22020) -->  John took the milk
2025-01-22 03:49:17.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:49:17.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11484, 11490) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:49:19.924 | INFO     | test_jbb_embedding:begin_test:693 - Mary<|eot_id|>
2025-01-22 03:49:19.924 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24155])
2025-01-22 03:49:28.323 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [547.4567307692307, 19.724691600778243, 230.8536931818182, 18.962906746443238, 21.82110821759259], 'topk_indices': array([24129, 24147,    19, 16690, 24034,     9, 16687, 16689, 24140,
          24,  4831,    14,    23, 24153, 16692,  4830, 16691,     0,
       24150, 24151]), 'topk_tokens': [':', '?\n', '26', ' the', '.\n\n', ':', '.', ' dropped', ' apple', '\n\n', '.', '\n', '4', '<|end_header_id|>', '.', ' apple', ' apple', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [426.1875, 1003.25, 426.1875, 1191.375, 56.854166666666664]}, 'weight': {'score': [23.17578125, 23.43318706793062, 22.224431818181817, 23.434567679600978, 29.85763888888889], 'topk_indices': array([18795, 18751, 14609, 14645, 19455, 19397, 14690, 14654, 14508,
       14573, 20276, 20324, 18120, 18089, 23504, 23638, 21947, 21920,
       23586, 23720]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.353515625, 20.052083333333332, 26.755859375, 24.91796875]}, 'saliency': {'score': [3.5830477201021633, 0.10884101754926108, 1.4494795365767046, 0.1038709473810675, 0.16484352394386573], 'topk_indices': array([   16,  2914,  2915, 24000,  4828, 24128,  4827,    22, 16688,
       24134,    24,    19,    20, 24142,  9681,    23, 16689, 24140,
        4830, 16691]), 'topk_tokens': [' Date', ' Mary', ' journey', ' context', ' got', 'Question', ' Mary', '202', ' Mary', ' prior', '\n\n', '26', ' Jul', ' discarded', ' bathroom', '4', ' dropped', ' apple', ' apple', ' apple'], 'evidence_proportions': [2.55126953125, 6.7529296875, 2.55126953125, 8.30224609375, 0.3872172037760417]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-22 03:49:28.335 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:49:28.336 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-1_1-2-4-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 4, 7, 8):   2%|▏         | 2/100 [00:56<46:29, 28.47s/it]is_0k: False
your chose emoji: ['🧙🏾\u200d♀️', '👦🏻', '🏖️', '👳🏽\u200d♂', '👨\u200d🏭', '☸️', '👸', '♏', '👩🏾\u200d💻', '🙆']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.44s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.88s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.33s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.80s/it]
Processing depth (0, 2, 4, 7, 9):   2%|▏         | 2/100 [01:18<46:29, 28.47s/it]2025-01-22 03:49:49.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:49:49.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:49:49.721 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the apple.
2025-01-22 03:49:49.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4965, 4969) -->  Mary got the apple
2025-01-22 03:49:49.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:49:49.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9745, 9751) --> . Mary journeyed to the
2025-01-22 03:49:49.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:49:49.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16737, 16741) -->  dropped the apple.
2025-01-22 03:49:49.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 03:49:50.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21520, 21526) --> . Daniel went back to the
2025-01-22 03:49:50.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:49:50.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17878, 17884) --> . John went back to the
2025-01-22 03:49:50.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the office.
2025-01-22 03:49:50.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:49:50.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John took the milk.
2025-01-22 03:49:50.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (22046, 22050) -->  John took the milk
2025-01-22 03:49:50.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:49:50.273 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11570, 11576) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:49:53.038 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the apple.<|eot_id|>
2025-01-22 03:49:53.038 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24239])
2025-01-22 03:50:01.451 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [293.2301682692308, 16.61376773854214, 235.8728693181818, 16.117106446079443, 47.5716552734375], 'topk_indices': array([16710, 24128,    13, 24232,    24,  9751, 24231, 24233, 24211,
       24118, 24238, 24237,    23, 24212,    14, 24213, 16711, 24234,
           0, 24235]), 'topk_tokens': ['\u200d', ' location', '3', 'Answer', '\n\n', ' bathroom', '?\n', ':', '.\n\n', '.\n\n', '\n\n', '<|end_header_id|>', '4', 'Question', '\n', ':', '❤', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [389.375, 330.8125, 344.3958333333333, 405.3125, 46.143229166666664]}, 'weight': {'score': [21.247295673076923, 23.452209067282702, 21.17294034090909, 23.456651329930146, 29.444010416666668], 'topk_indices': array([18879, 18835, 14667, 14703, 19474, 14748, 19532, 14712, 14566,
       14631, 20353, 20401, 18173, 18204, 23710, 23576, 22031, 22004,
       23792, 23658]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 26.353515625, 20.052083333333332, 20.97265625, 20.927083333333332]}, 'saliency': {'score': [1.6413691594050481, 0.09468618628756549, 1.3164783824573865, 0.09191293842178652, 0.34920819600423175], 'topk_indices': array([   39, 24231,    20, 24238, 24224, 24084, 24211, 24118, 24218,
       19763, 24128,    23,    31, 24226, 11576, 24232, 16710,  9751,
       24212, 16711]), 'topk_tokens': ['***', '?\n', ' Jul', '\n\n', ' apple', ' context', '.\n\n', '.\n\n', ' prior', ' journey', ' location', '4', ' journey', ' discarded', ' bedroom', 'Answer', '\u200d', ' bathroom', 'Question', '❤'], 'evidence_proportions': [2.134521484375, 2.3006591796875, 1.7674967447916667, 2.176513671875, 0.225799560546875]}}, 'pred_res': 'Mary dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:50:01.460 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:50:01.460 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-2_0-2-4-7-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 2, 4, 7, 9):   3%|▎         | 3/100 [01:29<49:27, 30.59s/it]is_0k: False
your chose emoji: ['🧑🏿\u200d🍳', '🤸🏻\u200d♂', '💏🏼', '👨🏼\u200d🤝\u200d👨🏿', '🚴🏻\u200d♀️', '🚣🏻\u200d♂', '🚃', '👩🏻\u200d❤️\u200d👨🏽', '🧙🏻\u200d♂', '👩🏼\u200d💼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.73s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.39s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (1, 2, 4, 5, 7):   3%|▎         | 3/100 [01:46<49:27, 30.59s/it]2025-01-22 03:50:18.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:50:18.128 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Mary journeyed to the
2025-01-22 03:50:18.129 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the apple.
2025-01-22 03:50:18.157 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4983, 4987) -->  Mary got the apple
2025-01-22 03:50:18.157 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:50:18.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3031, 3037) --> . Mary journeyed to the
2025-01-22 03:50:18.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:50:18.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11938, 11942) -->  Mary dropped the apple
2025-01-22 03:50:18.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 03:50:18.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16833, 16839) -->  affair. Daniel went back to
2025-01-22 03:50:18.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:50:18.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18143, 18149) --> . John went back to the
2025-01-22 03:50:18.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the office.
2025-01-22 03:50:18.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3032, 3038) -->  Mary journeyed to the office
2025-01-22 03:50:18.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John took the milk.
2025-01-22 03:50:18.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (22034, 22038) -->  John took the milk
2025-01-22 03:50:18.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:50:18.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11614, 11620) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:50:21.242 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 03:50:21.242 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24211])
2025-01-22 03:50:29.644 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [410.86778846153845, 18.90588475199273, 276.85298295454544, 18.24932172046348, 37.10889862804878], 'topk_indices': array([ 4986, 11939, 24205, 19830, 24100, 24203, 24198, 24185, 11988,
       11987, 24090, 12825,  4987, 11620, 11941, 11942, 24196,     0,
       24206, 24207]), 'topk_tokens': [' apple', ' dropped', ':', ' journey', ' location', '?\n', ' discarded', ':', 'IC', 'ANT', '.\n\n', 'ed', '.', ' bedroom', ' apple', '.', ' apple', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [395.9583333333333, 483.4375, 395.9583333333333, 791.25, 138.71875]}, 'weight': {'score': [23.213040865384617, 23.448323214801967, 22.224431818181817, 23.449690603662322, 29.8984375], 'topk_indices': array([18766, 18810, 14622, 14658, 19463, 14703, 14667, 19405, 14521,
       14586, 20288, 20336, 18122, 18091, 23684, 23550, 21965, 21938,
       23632, 23766]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.353515625, 20.052083333333332, 26.755859375, 25.079427083333332]}, 'saliency': {'score': [2.5374849759615383, 0.10639652067394065, 1.6527266068892046, 0.10237303374101231, 0.27596078267911583], 'topk_indices': array([24204, 24190, 12031, 11988, 24184, 19834, 12863, 11939, 24100,
       12824, 11615, 11616, 11987,  4986, 19830,  9763, 24198, 11941,
       11620, 24196]), 'topk_tokens': ['Answer', ' prior', 'ANT', 'IC', 'Question', ' office', 'present', ' dropped', ' location', 'present', ' Sandra', ' journey', 'ANT', ' apple', ' journey', ' bathroom', ' discarded', ' apple', ' bedroom', ' apple'], 'evidence_proportions': [2.1416015625, 3.338623046875, 2.1416015625, 5.46142578125, 0.8458658854166666]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:50:29.651 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:50:29.651 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-3_1-2-4-5-7.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 2, 4, 5, 7):   4%|▍         | 4/100 [01:58<47:25, 29.65s/it]is_0k: False
your chose emoji: ['🧂', '😥', '👩🏻\u200d❤\u200d💋\u200d👨🏾', '👫🏻', '🚶🏿\u200d♂️', '🧑🏾\u200d🤝\u200d🧑🏾', '🧝', '🏃\u200d♀️', '🙋🏾\u200d♀️', '💆🏻\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.39s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (0, 2, 3, 5, 6):   4%|▍         | 4/100 [02:14<47:25, 29.65s/it]2025-01-22 03:50:46.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:50:46.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:50:46.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the apple.
2025-01-22 03:50:46.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4815, 4819) -->  Mary got the apple
2025-01-22 03:50:46.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:50:46.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7437, 7443) --> . Mary journeyed to the
2025-01-22 03:50:46.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:50:46.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11760, 11764) -->  Mary dropped the apple
2025-01-22 03:50:46.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 03:50:46.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14264, 14270) --> . Daniel went back to the
2025-01-22 03:50:46.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 03:50:46.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17847, 17853) --> . John went back to the
2025-01-22 03:50:46.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the office.
2025-01-22 03:50:46.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:50:46.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John took the milk.
2025-01-22 03:50:47.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21956, 21960) -->  John took the milk
2025-01-22 03:50:47.016 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:50:47.090 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11412, 11418) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:50:49.881 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the apple.<|eot_id|>
2025-01-22 03:50:49.881 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24141])
2025-01-22 03:50:58.317 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [594.6586538461538, 25.779105227188005, 265.22585227272725, 24.946621446358165, 32.51429521276596], 'topk_indices': array([10842, 10843, 10818, 11763, 10697,  4819, 10841,  4818, 10819,
          14, 10864, 10886, 10865, 10820, 10889, 10888, 10866,     0,
       24136, 24137]), 'topk_tokens': [' I', ' could', '\n', ' apple', 'ing', '.', ' as', ' apple', 'walk', '\n', ' and', ' as', '\n', 'ed', ' I', ' as', 'walk', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [591.9166666666666, 1031.5, 422.625, 1159.0, 101.97916666666667]}, 'weight': {'score': [22.13701923076923, 23.424289649173673, 21.17294034090909, 23.427734293940652, 28.51994680851064], 'topk_indices': array([18784, 18740, 14645, 14609, 14690, 14654, 19379, 19437, 14573,
       14508, 20258, 20306, 18078, 18109, 23624, 23490, 21914, 21941,
       23572, 23706]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 26.353515625, 20.052083333333332, 26.755859375, 20.927083333333332]}, 'saliency': {'score': [3.7058997521033654, 0.1416115230896378, 1.5127674449573865, 0.13651349759740092, 0.23540675386469415], 'topk_indices': array([24030, 10842, 11760, 24114, 10841, 10696,    31, 24126, 11761,
       10886, 10843, 10820, 10889, 24128, 11763, 10888, 10887,  4818,
       10819, 10866]), 'topk_tokens': [' location', ' I', ' Mary', 'Question', ' as', 'count', ' journey', ' apple', ' dropped', ' as', ' could', 'ed', ' I', ' discarded', ' apple', ' as', ' rapidly', ' apple', 'walk', 'walk'], 'evidence_proportions': [3.2664388020833335, 7.15576171875, 2.2688802083333335, 7.75, 0.5864054361979166]}}, 'pred_res': 'Mary got the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:50:58.324 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:50:58.325 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-1_pid-4_0-2-3-5-6.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 2, 3, 5, 6):   5%|▌         | 5/100 [02:26<46:22, 29.29s/it]Processing depth (0, 2, 3, 5, 6):   5%|▌         | 5/100 [02:27<46:36, 29.44s/it]
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:82 - Selected idx: 2
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the bathroom? 
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:86 - Needle: [' Daniel went back to the kitchen.', ' Mary journeyed to the office.', ' Sandra journeyed to the bedroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' John moved to the bedroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the office.', ' Mary journeyed to the bathroom.', ' Mary dropped the football.', ' Daniel went back to the hallway.']
2025-01-22 03:50:58.708 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧓🏻', '🍹', '♻️', '👸🏾', '\U0001fabf', '🇸🇻', '🧗🏽', '🤷🏿', '🤦🏿', '🐈']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.69s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.40s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (0, 1, 2, 3):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 03:51:16.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:51:16.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:51:16.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:51:16.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3043, 3049) --> . Mary journeyed to the
2025-01-22 03:51:16.137 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:51:16.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (5020, 5024) -->  Mary dropped the football
2025-01-22 03:51:16.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:51:16.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7656, 7662) --> . Daniel went back to the
2025-01-22 03:51:16.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:51:16.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7656, 7662) --> . Daniel went back to the
2025-01-22 03:51:16.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:51:16.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8457, 8463) --> . Sandra journeyed to the
2025-01-22 03:51:16.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:51:16.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4377, 4383) -->  execution. John went back to
2025-01-22 03:51:16.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:51:16.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:51:16.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John took the milk.
2025-01-22 03:51:16.337 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2843, 2847) -->  John took the milk
2025-01-22 03:51:16.337 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:51:16.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3900, 3905) -->  the ground. John moved
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:51:19.209 | INFO     | test_jbb_embedding:begin_test:693 - St. Mary dropped the football.<|eot_id|>
2025-01-22 03:51:19.209 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24174])
2025-01-22 03:51:27.602 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1084.6448863636363, 31.212651493216413, 411.2405303030303, 29.731932237469426, 52.324554443359375], 'topk_indices': array([   39,  4994, 24161,  3050,  5022,  9339,    31,    40, 24170,
        3063,  5024,  5019, 24169,  5020,  5021,  4995,  4996,  3064,
        5023,     0]), 'topk_tokens': ['***', ' St', ' football', '.', ' the', ' cause', ' journey', '\n\n\n', '<|start_header_id|>', ' const', '.', '.', '<|eot_id|>', ' Mary', ' dropped', '.', ' *', 'ern', ' football', '<|begin_of_text|>'], 'evidence_proportions': [1188.1666666666667, 793.6666666666666, 2758.0, 156.53125]}, 'weight': {'score': [21.417258522727273, 23.4342581485771, 21.947443181818183, 23.438131905289996, 28.95166015625], 'topk_indices': array([18755, 18799, 14592, 14628, 14637, 14673, 19452, 19394, 14491,
       14556, 20266, 20314, 18066, 18097, 23518, 23652, 21938, 21965,
       23734, 23600]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [6.597523082386363, 0.17713953468832727, 2.271247632575758, 0.16841875174899465, 0.3765749931335449], 'topk_indices': array([24162,  5018,  4980,  4995,  9386,    30,  4994, 24164,    40,
        9339,    39,  3049,    31, 24161,  3063,  4996,  5021,  5020,
        3064,  5023]), 'topk_tokens': [' before', ' St', ' *\n\n', '.', 'did', 'Mary', ' St', ' bathroom', '\n\n\n', ' cause', '***', ' bathroom', ' journey', ' football', ' const', ' *', ' dropped', ' Mary', 'ern', ' football'], 'evidence_proportions': [6.151692708333333, 4.1259765625, 19.5537109375, 0.87744140625]}}, 'pred_res': 'St. Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:51:27.624 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:51:27.624 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-0_0-1-2-3.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 1, 2, 3):   1%|          | 1/100 [00:28<47:29, 28.78s/it]is_0k: False
your chose emoji: ['🇹🇩', '🏃🏿\u200d➡️', '🆕', '🇦🇺', '🧗🏿\u200d♂', '🦻🏻', '☑', '🕳', '🕵🏿\u200d♂', '👩🏼\u200d🦯']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.74s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (0, 2, 3, 7):   1%|          | 1/100 [00:46<47:29, 28.78s/it]2025-01-22 03:51:45.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:51:45.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:51:45.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:51:45.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5012, 5018) --> . Mary journeyed to the
2025-01-22 03:51:45.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:51:45.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7472, 7476) -->  Mary dropped the football
2025-01-22 03:51:45.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:51:45.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16781, 16787) --> . Daniel went back to the
2025-01-22 03:51:45.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:51:45.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16781, 16787) --> . Daniel went back to the
2025-01-22 03:51:45.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:51:45.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8364, 8370) --> . Sandra journeyed to the
2025-01-22 03:51:45.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:51:45.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4370, 4376) -->  execution. John went back to
2025-01-22 03:51:45.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:51:45.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:51:45.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John took the milk.
2025-01-22 03:51:45.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2843, 2847) -->  John took the milk
2025-01-22 03:51:45.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:51:45.644 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3893, 3898) -->  the ground. John moved
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:51:48.409 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the football.<|eot_id|>
2025-01-22 03:51:48.410 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24170])
2025-01-22 03:51:56.797 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [617.3920454545455, 21.115562427602185, 336.20075757575756, 20.140487000870756, 25.709519909274192], 'topk_indices': array([ 5012,    32,    29,     1,    36, 10046, 24058, 24166, 24157,
          35, 24024,    24,    31, 23049,    14,  5013, 24168,  7475,
       24165,     0]), 'topk_tokens': ['.', 'ed', '\n\n', '<|start_header_id|>', '.', 'or', '.\n\n', '<|start_header_id|>', ' football', ' office', ' context', '\n\n', ' journey', ' office', '\n', ' Mary', '<|end_header_id|>', ' football', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [904.9166666666666, 729.4583333333334, 805.0, 92.72916666666667]}, 'weight': {'score': [21.417258522727273, 23.439653835015722, 21.947443181818183, 23.443540539349836, 30.181199596774192], 'topk_indices': array([18773, 18817, 14667, 14631, 14712, 19412, 14676, 19470, 14530,
       14595, 20332, 20284, 18100, 18131, 23528, 23662, 21928, 21955,
       23610, 23744]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [3.5845447887073862, 0.11926252361009122, 1.8807003136837122, 0.11369119811751047, 0.19236681538243447], 'topk_indices': array([10046, 10081,  7473,  5011,  7825, 24152, 23045,    39,  3900,
        5014,    30,    35,  5018, 24024, 24160, 24157, 23049,    31,
        5013,  7475]), 'topk_tokens': ['or', 'po', ' dropped', ' St', ' Square', 'Question', ' journey', '***', ' bedroom', ' journey', 'Mary', ' office', ' bathroom', ' context', ' bathroom', ' football', ' office', ' journey', ' Mary', ' football'], 'evidence_proportions': [4.712565104166667, 4.086100260416667, 5.765625, 0.50091552734375]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:51:56.804 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:51:56.804 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-1_0-2-3-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 2, 3, 7):   2%|▏         | 2/100 [00:57<47:23, 29.02s/it]is_0k: False
your chose emoji: ['🚶\u200d♂️\u200d➡', '🙎\u200d♀️', '🧛🏼\u200d♂️', '🏋🏾\u200d♂', '🕵️\u200d♀️', '\U0001fac4🏽', '💪', '🙎', '🙋🏽', '📱']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.97s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.51s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]
Processing depth (4, 6, 8, 9):   2%|▏         | 2/100 [01:15<47:23, 29.02s/it]2025-01-22 03:52:14.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:52:14.517 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9614, 9620) --> . Mary journeyed to the
2025-01-22 03:52:14.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:52:14.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9614, 9620) --> . Mary journeyed to the
2025-01-22 03:52:14.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:52:14.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19128, 19132) -->  Mary dropped the football
2025-01-22 03:52:14.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:52:14.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18444, 18450) --> . Daniel went back to the
2025-01-22 03:52:14.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:52:14.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18444, 18450) --> . Daniel went back to the
2025-01-22 03:52:14.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:52:14.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8236, 8242) -->  St. Sandra journeyed to
2025-01-22 03:52:14.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:52:14.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4217, 4223) -->  John went back to the bedroom
2025-01-22 03:52:14.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:52:14.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9615, 9621) -->  Mary journeyed to the office
2025-01-22 03:52:14.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John took the milk.
2025-01-22 03:52:15.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2552, 2556) -->  John took the milk
2025-01-22 03:52:15.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:52:15.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3806, 3811) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:52:17.818 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the football.<|eot_id|>
2025-01-22 03:52:17.818 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 03:52:26.214 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [625.4602272727273, 29.216639771598807, 450.094696969697, 28.096649110438353, 45.77122395833333], 'topk_indices': array([24148, 24151, 24152, 24149,    14, 24155, 24064, 24054, 24020,
       19132, 24154, 14154, 24164, 14153, 24153, 19131, 24156, 24161,
           0, 24162]), 'topk_tokens': ['Question', ' was', ' the', ':', '\n', ' the', ' location', '.\n\n', ' context', '.', ' before', '.', '<|end_header_id|>', ' bathroom', ' football', ' football', ' bathroom', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [529.7083333333334, 529.7083333333334, 1369.0, 321.2708333333333]}, 'weight': {'score': [21.556463068181817, 23.432467519033434, 22.848958333333332, 23.43497769615975, 28.96640625], 'topk_indices': array([18769, 18813, 14638, 14674, 14683, 19413, 14719, 19471, 14602,
       14537, 20285, 20333, 18131, 18100, 23504, 23638, 21936, 21963,
       23586, 23720]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [3.8883833451704546, 0.16565363662047955, 2.725134647253788, 0.15875433222289326, 0.33158976236979165], 'topk_indices': array([24063,  9616, 24135, 24151,  3811, 14149, 19128,  8238, 24150,
       24054,  8243, 19129, 24064, 24148, 24020, 24154, 24153, 19131,
       14153, 24156]), 'topk_tokens': [' first', ' journey', ' return', ' was', ' bedroom', ' journey', ' Mary', ' Sandra', ' Where', '.\n\n', ' bedroom', ' dropped', ' location', 'Question', ' context', ' before', ' football', ' football', ' bathroom', ' bathroom'], 'evidence_proportions': [2.9099934895833335, 2.9099934895833335, 9.8046875, 1.9009602864583333]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:52:26.222 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:52:26.222 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-2_4-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (4, 6, 8, 9):   3%|▎         | 3/100 [01:27<47:12, 29.20s/it]is_0k: False
your chose emoji: ['⛹🏻', '🧛\u200d♀', '⛹️', '🧖🏽\u200d♂', '\U0001faf2🏿', '🦥', '🥷🏾', '😽', '🕴🏼', '\U0001faf1🏿\u200d\U0001faf2🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.32s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.59s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (0, 6, 8, 9):   3%|▎         | 3/100 [01:44<47:12, 29.20s/it]2025-01-22 03:52:43.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:52:43.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:52:43.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:52:43.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14240, 14246) -->  papers. Mary journeyed to
2025-01-22 03:52:43.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:52:43.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19222, 19226) -->  Mary dropped the football
2025-01-22 03:52:43.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:52:43.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18538, 18544) --> . Daniel went back to the
2025-01-22 03:52:43.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:52:43.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18538, 18544) --> . Daniel went back to the
2025-01-22 03:52:43.511 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:52:43.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8386, 8392) --> . Sandra journeyed to the
2025-01-22 03:52:43.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:52:43.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4295, 4301) --> . John went back to the
2025-01-22 03:52:43.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:52:43.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:52:43.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John took the milk.
2025-01-22 03:52:43.587 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2843, 2847) -->  John took the milk
2025-01-22 03:52:43.587 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:52:43.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3845, 3850) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:52:46.399 | INFO     | test_jbb_embedding:begin_test:693 - Mary dropped the football.<|eot_id|>
2025-01-22 03:52:46.400 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24254])
2025-01-22 03:52:54.883 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [737.5454545454545, 28.3447265625, 414.21022727272725, 27.173866782364364, 30.164212740384617], 'topk_indices': array([24239, 24245,    24, 24142, 14247,    14, 24237,    31, 24152,
       24244, 19223, 24108, 24242, 14686, 19226, 24241, 19225,     0,
       24250, 24249]), 'topk_tokens': [' was', '?', '\n\n', '.\n\n', ' bathroom', '\n', ':', ' journey', ' location', ' bathroom', ' dropped', ' context', ' before', ' accordance', '.', ' football', ' football', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [782.5833333333334, 499.0, 1500.625, 422.3333333333333]}, 'weight': {'score': [22.347301136363637, 23.452444755936675, 20.781013257575758, 23.457092101049543, 28.950420673076923], 'topk_indices': array([18907, 18863, 14768, 14732, 14813, 14777, 19565, 19507, 14696,
       14631, 20427, 20379, 18225, 18194, 23732, 23598, 22057, 22030,
       23680, 23814]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 23.462239583333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [4.581409801136363, 0.16040753689164536, 2.322176846590909, 0.15344086456267303, 0.2207942375769982], 'topk_indices': array([24142, 19222, 14438, 19006, 14456, 18540, 24247, 24238, 24236,
       24152, 14621,    31, 19223, 14247, 24242, 24108, 24244, 24241,
       14686, 19225]), 'topk_tokens': ['.\n\n', ' Mary', 'ord', ' manner', 'ible', ' went', 'Answer', ' Where', 'Question', ' location', ' accordance', ' journey', ' dropped', ' bathroom', ' before', ' context', ' bathroom', ' football', ' accordance', ' football'], 'evidence_proportions': [4.261393229166667, 2.8103841145833335, 10.728515625, 2.5743815104166665]}}, 'pred_res': 'Mary dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 03:52:54.896 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:52:54.896 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-3_0-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 6, 8, 9):   4%|▍         | 4/100 [01:56<46:23, 28.99s/it]is_0k: False
your chose emoji: ['\U0001faf1🏼\u200d\U0001faf2🏻', '👨🏿\u200d🏭', '👩\u200d💼', '🤸🏻\u200d♂️', '🚶🏾\u200d♂', '🧑🏼\u200d🤝\u200d🧑🏾', '🧚🏽', '🚴🏾', '👩🏿\u200d🤝\u200d👩🏼', '👨🏽\u200d⚖']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.63s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (2, 3, 6, 8):   4%|▍         | 4/100 [02:12<46:23, 28.99s/it]2025-01-22 03:53:11.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:53:11.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4873, 4879) --> . Mary journeyed to the
2025-01-22 03:53:11.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bathroom.
2025-01-22 03:53:11.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4873, 4879) --> . Mary journeyed to the
2025-01-22 03:53:11.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary dropped the football.
2025-01-22 03:53:11.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14547, 14551) -->  Mary dropped the football
2025-01-22 03:53:11.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 03:53:11.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18771, 18777) --> . Daniel went back to the
2025-01-22 03:53:11.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the kitchen.
2025-01-22 03:53:11.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18771, 18777) --> . Daniel went back to the
2025-01-22 03:53:11.896 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the bedroom.
2025-01-22 03:53:11.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8401, 8407) --> . Sandra journeyed to the
2025-01-22 03:53:11.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:53:11.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4244, 4250) --> . John went back to the
2025-01-22 03:53:11.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the office.
2025-01-22 03:53:11.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4874, 4880) -->  Mary journeyed to the office
2025-01-22 03:53:11.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John took the milk.
2025-01-22 03:53:12.002 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2780, 2784) -->  John took the milk
2025-01-22 03:53:12.002 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the bedroom.
2025-01-22 03:53:12.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3798, 3803) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:53:14.883 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Mary.<|eot_id|>
2025-01-22 03:53:14.884 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24210])
2025-01-22 03:53:23.288 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [555.9360795454545, 17.643659394102098, 236.1998106060606, 16.854869965227472, 23.066430068597562], 'topk_indices': array([24206, 24098, 24192,    24,    14, 24191, 14549, 24193, 24209,
       24108, 24204, 24198, 14551, 24200, 24197, 24208, 14548, 24205,
           0, 14550]), 'topk_tokens': ['<|start_header_id|>', '.\n\n', 'Question', '\n\n', '\n', '.\n\n', ' the', ':', '\n\n', ' location', ':', ' before', '.', ' bathroom', ' football', '<|end_header_id|>', ' dropped', '<|eot_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [531.375, 531.375, 1367.5, 64.015625]}, 'weight': {'score': [21.556463068181817, 23.449286510821082, 21.482007575757574, 23.453697750651983, 29.96455792682927], 'topk_indices': array([18713, 18757, 14583, 14619, 14628, 19359, 19417, 14664, 14477,
       14542, 20300, 20252, 18076, 18045, 23538, 23672, 21922, 21949,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.052083333333332, 27.013671875, 20.927083333333332]}, 'saliency': {'score': [3.403780850497159, 0.10000460612248059, 1.3722885594223484, 0.09525780611274683, 0.1728644022127477], 'topk_indices': array([24201, 24207, 24191, 24179, 14205, 24194, 24203,  4879, 24064,
       14547,  4875,  7544, 14235, 24192, 24108, 24198, 14548, 24197,
       24200, 14550]), 'topk_tokens': ['?', 'assistant', '.\n\n', ' return', 'rail', ' Where', 'Answer', ' office', ' context', ' Mary', ' journey', ' bathroom', 'rail', 'Question', ' location', ' before', ' dropped', ' football', ' bathroom', ' football'], 'evidence_proportions': [2.8203125, 2.8203125, 9.74462890625, 0.3434855143229167]}}, 'pred_res': 'The football was dropped by Mary.<|eot_id|>', 'score': 0}
2025-01-22 03:53:23.294 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:53:23.294 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-2_pid-4_2-3-6-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 3, 6, 8):   5%|▌         | 5/100 [02:24<45:33, 28.78s/it]Processing depth (2, 3, 6, 8):   5%|▌         | 5/100 [02:24<45:50, 28.95s/it]
2025-01-22 03:53:23.586 | INFO     | __main__:<module>:82 - Selected idx: 3
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:86 - Needle: [' Sandra journeyed to the bedroom.', ' Mary journeyed to the office.', ' John went back to the bedroom.', ' Mary picked up the apple.', ' John journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Mary journeyed to the bathroom.', ' John took the milk.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the office.', ' Mary picked up the apple.', ' Mary journeyed to the bathroom.', ' Mary dropped the apple.', ' Daniel went back to the hallway.']
2025-01-22 03:53:23.587 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👨🏻\u200d🎨', '👩🏿\u200d❤\u200d💋\u200d👨🏼', '🏞️', '🌩️', '▶️', '👷🏾\u200d♂', '🧹', '🙎\u200d♂️', '🦸🏾', '👩🏻\u200d❤\u200d💋\u200d👩🏿']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (1, 3, 6, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 03:53:40.660 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:53:40.675 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2921, 2927) --> . Mary journeyed to the
2025-01-22 03:53:40.675 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:53:40.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7535, 7540) --> . Mary picked up the
2025-01-22 03:53:40.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:53:40.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2921, 2927) --> . Mary journeyed to the
2025-01-22 03:53:40.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:53:40.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19200, 19204) -->  Mary dropped the apple
2025-01-22 03:53:40.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:53:40.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18128, 18134) --> . Daniel went back to the
2025-01-22 03:53:40.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:53:41.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (19598, 19604) --> . Sandra journeyed to the
2025-01-22 03:53:41.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:53:41.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15845, 15851) --> . John went back to the
2025-01-22 03:53:41.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:53:41.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2922, 2928) -->  Mary journeyed to the office
2025-01-22 03:53:41.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:53:41.186 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18128, 18134) --> . Daniel went back to the
2025-01-22 03:53:41.186 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:53:41.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19602, 19607) -->  to the bedroom. John
2025-01-22 03:53:41.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John took the milk.
2025-01-22 03:53:41.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (7177, 7181) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:53:43.952 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 03:53:43.952 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24237])
2025-01-22 03:53:52.333 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [224.60185185185185, 12.468519225215562, 126.56818181818181, 12.075910397452335, 21.174272017045453], 'topk_indices': array([24209,    20,  7540,     3,    22,     9, 19204, 24211, 24231,
          19, 24236,     1,    14, 19203,    24, 24235,    23,     0,
       24233, 24232]), 'topk_tokens': ['.\n\n', ' Jul', ' apple', '<|end_header_id|>', '202', ':', '.', ':', ':', '26', '\n\n', '<|start_header_id|>', '\n', ' apple', '\n\n', '<|end_header_id|>', '4', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [162.3125, 274.3, 162.3125, 516.6875, 113.04166666666667]}, 'weight': {'score': [21.61892361111111, 23.452803333470854, 21.667140151515152, 23.45728828477191, 30.09481534090909], 'topk_indices': array([18885, 18841, 14629, 14665, 19485, 14674, 19543, 14710, 14593,
       14528, 20370, 20418, 18210, 18179, 23716, 23582, 22021, 22048,
       23664, 23798]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 22.1, 20.052083333333332, 26.755859375, 20.927083333333332]}, 'saliency': {'score': [1.3639594184027777, 0.07028730936906948, 0.7950180516098485, 0.06785357507741739, 0.1589906865900213], 'topk_indices': array([24236, 19201, 24224, 24082,    16, 24222, 19604, 24234, 24216,
       24126,     0,    24,    22, 24210, 24230,    19,  7540,    20,
          23, 19203]), 'topk_tokens': ['\n\n', ' dropped', ' discarded', ' context', ' Date', ' apple', ' bedroom', 'assistant', ' prior', ' location', '<|begin_of_text|>', '\n\n', '202', 'Question', 'Answer', '26', ' apple', ' Jul', '4', ' apple'], 'evidence_proportions': [0.9509073893229166, 1.50322265625, 0.9509073893229166, 3.5341796875, 0.627197265625]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 03:53:52.345 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:53:52.346 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-0_1-3-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 3, 6, 8, 9):   1%|          | 1/100 [00:28<47:14, 28.63s/it]is_0k: False
your chose emoji: ['🤵\u200d♂️', '👨\u200d❤\u200d👨', '🎳', '🕰️', '\U0001faf6🏽', '🧑🏾\u200d❤️\u200d💋\u200d🧑🏻', '👩🏼\u200d❤\u200d💋\u200d👨🏼', '🧏🏼\u200d♀', '🧏🏿\u200d♀️', '👐🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.56s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.41s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.56s/it]
Processing depth (0, 1, 2, 4, 9):   1%|          | 1/100 [00:44<47:14, 28.63s/it]2025-01-22 03:54:08.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:54:08.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:54:08.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:54:08.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2967, 2972) -->  tragedy. Mary picked up
2025-01-22 03:54:08.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:54:08.823 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4964, 4970) --> . Mary journeyed to the
2025-01-22 03:54:08.823 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:54:08.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9749, 9753) -->  Mary dropped the apple
2025-01-22 03:54:08.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:54:08.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18121, 18127) --> . Daniel went back to the
2025-01-22 03:54:08.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:54:09.104 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (19586, 19592) --> . Sandra journeyed to the
2025-01-22 03:54:09.104 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:54:09.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15951, 15957) -->  John went back to the bedroom
2025-01-22 03:54:09.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:54:09.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:54:09.188 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:54:09.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18121, 18127) --> . Daniel went back to the
2025-01-22 03:54:09.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:54:09.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19590, 19595) -->  to the bedroom. John
2025-01-22 03:54:09.391 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John took the milk.
2025-01-22 03:54:09.425 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (7192, 7196) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:54:12.111 | INFO     | test_jbb_embedding:begin_test:693 - Mary's bathroom<|eot_id|>
2025-01-22 03:54:12.111 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24233])
2025-01-22 03:54:20.508 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [557.6736111111111, 27.98716989890654, 234.91950757575756, 27.113114012409515, 22.48855377906977], 'topk_indices': array([ 9750, 24216,  4970,    31, 17184, 24213, 24231,    14, 24219,
          23, 24220, 24112, 24218,  2974,    24,  2973,  9752,     0,
       24228, 24229]), 'topk_tokens': [' dropped', ' where', ' bathroom', ' journey', ' spear', ' to', '<|end_header_id|>', '\n', ' was', '4', ' discarded', '.\n\n', ' apple', '.', '\n\n', ' apple', ' apple', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [630.6666666666666, 598.35, 568.0833333333334, 1102.0, 77.48958333333333]}, 'weight': {'score': [22.58101851851852, 23.446423045182588, 21.788352272727273, 23.44965292140641, 29.34484011627907], 'topk_indices': array([18834, 18878, 14628, 14664, 14709, 19473, 19531, 14673, 14592,
       14527, 20358, 20406, 18203, 18172, 23570, 23704, 22009, 22036,
       23652, 23786]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 27.9078125, 20.052083333333332, 26.755859375, 20.927083333333332]}, 'saliency': {'score': [3.3359917534722223, 0.15616328399009696, 1.3853463837594697, 0.15093398053742244, 0.17279123705486918], 'topk_indices': array([   24, 24078, 24219, 13493, 24112,  4965,    39, 24226,  2970,
       24206, 24216,  9750, 24212,    31, 24218,  4970, 17184,  2973,
       24220,  9752]), 'topk_tokens': ['\n\n', ' context', ' was', 'nes', '.\n\n', ' Mary', '***', 'Answer', ' picked', 'Question', ' where', ' dropped', ' prior', ' journey', ' apple', ' bathroom', ' spear', ' apple', ' discarded', ' apple'], 'evidence_proportions': [3.4364420572916665, 3.8548828125, 2.99609375, 7.39892578125, 0.4344075520833333]}}, 'pred_res': "Mary's bathroom<|eot_id|>", 'score': 0}
2025-01-22 03:54:20.516 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:54:20.517 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-1_0-1-2-4-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 1, 2, 4, 9):   2%|▏         | 2/100 [00:56<46:19, 28.36s/it]is_0k: False
your chose emoji: ['👩🏼\u200d🤝\u200d👨🏻', '⛩', '🇸🇿', '🚵🏾', '🤵🏽\u200d♂️', '🧎🏼\u200d♀\u200d➡', '👩🏼\u200d❤\u200d👩🏾', '🖋', '🖖🏽', '👮🏽\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.86s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.32s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]
Processing depth (0, 1, 3, 4, 9):   2%|▏         | 2/100 [01:13<46:19, 28.36s/it]2025-01-22 03:54:37.107 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:54:37.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:54:37.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:54:37.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2983, 2988) -->  tragedy. Mary picked up
2025-01-22 03:54:37.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:54:37.161 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7618, 7624) -->  war. Mary journeyed to
2025-01-22 03:54:37.161 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:54:37.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9693, 9697) -->  Mary dropped the apple
2025-01-22 03:54:37.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:54:37.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18181, 18187) --> . Daniel went back to the
2025-01-22 03:54:37.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:54:37.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (19638, 19644) --> . Sandra journeyed to the
2025-01-22 03:54:37.398 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:54:37.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15956, 15962) --> . John went back to the
2025-01-22 03:54:37.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:54:37.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (31, 37) -->  journeyed to the office.
2025-01-22 03:54:37.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:54:37.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18181, 18187) --> . Daniel went back to the
2025-01-22 03:54:37.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:54:37.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19642, 19647) -->  to the bedroom. John
2025-01-22 03:54:37.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John took the milk.
2025-01-22 03:54:37.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (7283, 7287) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:54:40.560 | INFO     | test_jbb_embedding:begin_test:693 - Mary journeyed to the office.<|eot_id|>
2025-01-22 03:54:40.560 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24211])
2025-01-22 03:54:48.968 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [431.5509259259259, 24.074301511584686, 186.59753787878788, 23.396740311762514, 28.317578125], 'topk_indices': array([24184,     4, 24191, 10087,  7625, 10052,     9, 24056, 24183,
          24, 10059, 24209, 24205, 24185, 24203, 24090,    14,     0,
       24206, 24207]), 'topk_tokens': ['Question', '\n\n', ' to', ' as', ' bathroom', ' if', ':', ' context', '.\n\n', '\n\n', 'or', '<|end_header_id|>', ':', ':', '?\n', '.\n\n', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [651.4166666666666, 439.775, 427.375, 677.25, 45.208333333333336]}, 'weight': {'score': [23.222511574074073, 23.445504481063892, 20.966145833333332, 23.44914128162133, 30.06125], 'topk_indices': array([18842, 18798, 14658, 14622, 19495, 14703, 14667, 19437, 14521,
       14586, 20370, 20322, 18129, 18160, 23694, 23560, 21977, 22004,
       23642, 23776]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.541666666666668, 27.9078125, 22.938802083333332, 26.755859375, 20.927083333333332]}, 'saliency': {'score': [2.575961642795139, 0.13166087577566804, 1.0404404148910984, 0.1276868002777864, 0.21076273600260417], 'topk_indices': array([24183,    40,    39,    14, 10052, 24203, 10094, 10059, 10058,
       24090, 24204,  2989,  9696, 24190, 24196,    31, 24184, 24056,
       24198,  7625]), 'topk_tokens': ['.\n\n', '\n\n\n', '***', '\n', ' if', '?\n', 'po', 'or', 'po', '.\n\n', 'Answer', ' apple', ' apple', ' prior', ' apple', ' journey', 'Question', ' context', ' discarded', ' bathroom'], 'evidence_proportions': [3.4571940104166665, 2.8296875, 2.3877766927083335, 4.7138671875, 0.24620564778645834]}}, 'pred_res': 'Mary journeyed to the office.<|eot_id|>', 'score': 100}
2025-01-22 03:54:48.974 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:54:48.975 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-2_0-1-3-4-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 3, 4, 9):   3%|▎         | 3/100 [01:25<45:55, 28.40s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d❤️\u200d💋\u200d🧑🏿', '🔵', '♐', '🥍', '🏌\u200d♀', '💇🏻\u200d♂️', '🧛🏿', '🏃🏾\u200d♀\u200d➡', '🧑🏻\u200d🦯\u200d➡️', '⛓']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.72s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (2, 3, 4, 6, 8):   3%|▎         | 3/100 [01:42<45:55, 28.40s/it]2025-01-22 03:55:06.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:55:06.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4861, 4867) --> . Mary journeyed to the
2025-01-22 03:55:06.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:55:06.801 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7531, 7536) --> . Mary picked up the
2025-01-22 03:55:06.801 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:55:06.826 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4861, 4867) --> . Mary journeyed to the
2025-01-22 03:55:06.826 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:55:06.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14437, 14441) -->  dropped the apple.
2025-01-22 03:55:06.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:55:06.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18207, 18213) --> . Daniel went back to the
2025-01-22 03:55:06.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:55:07.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (19709, 19715) --> . Sandra journeyed to the
2025-01-22 03:55:07.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:55:07.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15973, 15979) -->  John went back to the bedroom
2025-01-22 03:55:07.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:55:07.193 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4862, 4868) -->  Mary journeyed to the office
2025-01-22 03:55:07.193 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:55:07.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18207, 18213) --> . Daniel went back to the
2025-01-22 03:55:07.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:55:07.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19713, 19718) -->  to the bedroom. John
2025-01-22 03:55:07.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John took the milk.
2025-01-22 03:55:07.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (7173, 7177) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:55:10.004 | INFO     | test_jbb_embedding:begin_test:693 - Mary<|eot_id|>
2025-01-22 03:55:10.004 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24197])
2025-01-22 03:55:18.390 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [292.6608796296296, 17.343272190999627, 157.72727272727272, 16.843406934835745, 47.07447725183823], 'topk_indices': array([    9, 24157, 24189,    19, 24190, 24184, 24176, 24191, 24177,
       24169, 24170, 24195, 24076,    14, 24171,    24,    23, 24192,
       24193,     0]), 'topk_tokens': [':', ' return', '?\n', '26', 'Answer', ' discarded', ' prior', ':', ' to', '.\n\n', 'Question', '<|end_header_id|>', '.\n\n', '\n', ':', '\n\n', '4', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [297.4270833333333, 339.7, 297.4270833333333, 552.1875, 70.91145833333333]}, 'weight': {'score': [20.76215277777778, 23.439899376007272, 22.48934659090909, 23.444193978105968, 29.75160845588235], 'topk_indices': array([18776, 18820, 14638, 14602, 14647, 19422, 19480, 14683, 14501,
       14566, 20331, 20379, 18107, 18138, 23674, 23540, 22006, 21979,
       23756, 23622]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 22.1, 20.052083333333332, 20.97265625, 20.927083333333332]}, 'saliency': {'score': [1.6482792607060186, 0.09865325255563247, 1.0058871182528408, 0.09567969856466066, 0.34505372888901653], 'topk_indices': array([ 9741,  7176, 24194, 24169, 24042,    22, 24086,    24,  7536,
       24076, 24157,    19,    20, 24182, 14439, 24190,    23, 24176,
       24170, 24184]), 'topk_tokens': [' bathroom', ' milk', 'assistant', '.\n\n', ' context', '202', ' location', '\n\n', ' apple', '.\n\n', ' return', '26', ' Jul', ' apple', ' apple', 'Answer', '4', ' prior', 'Question', ' discarded'], 'evidence_proportions': [1.716064453125, 1.9150390625, 1.716064453125, 3.013916015625, 0.3799845377604167]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-22 03:55:18.410 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:55:18.410 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-3_2-3-4-6-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 3, 4, 6, 8):   4%|▍         | 4/100 [01:54<46:05, 28.81s/it]is_0k: False
your chose emoji: ['♠️', 'Ⓜ', '👨🏽\u200d🚀', '🧝🏻\u200d♀️', '\U0001faf0🏻', '😮\u200d💨', '🧍🏾\u200d♀️', '🙆🏿\u200d♀️', '🦹🏼\u200d♂', '🧚🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.41s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (2, 3, 4, 8, 9):   4%|▍         | 4/100 [02:11<46:05, 28.81s/it]2025-01-22 03:55:35.308 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 03:55:35.337 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4987, 4993) --> . Mary journeyed to the
2025-01-22 03:55:35.338 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 03:55:35.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7547, 7552) --> . Mary picked up the
2025-01-22 03:55:35.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bathroom.
2025-01-22 03:55:35.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4987, 4993) --> . Mary journeyed to the
2025-01-22 03:55:35.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the apple.
2025-01-22 03:55:35.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19247, 19251) -->  dropped the apple.
2025-01-22 03:55:35.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 03:55:35.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18136, 18142) --> . Daniel went back to the
2025-01-22 03:55:35.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:55:35.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (19578, 19584) --> . Sandra journeyed to the
2025-01-22 03:55:35.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 03:55:35.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15938, 15944) -->  John went back to the bedroom
2025-01-22 03:55:35.787 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the office.
2025-01-22 03:55:35.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4988, 4994) -->  Mary journeyed to the office
2025-01-22 03:55:35.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 03:55:35.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18136, 18142) --> . Daniel went back to the
2025-01-22 03:55:35.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 03:55:36.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19582, 19587) -->  to the bedroom. John
2025-01-22 03:55:36.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John took the milk.
2025-01-22 03:55:36.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (7189, 7193) -->  John took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:55:38.727 | INFO     | test_jbb_embedding:begin_test:693 - Mary's hand<|eot_id|>
2025-01-22 03:55:38.727 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 03:55:47.143 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [595.3379629629629, 24.094288014532843, 258.8806818181818, 23.135241194486984, 37.50721914556962], 'topk_indices': array([24138, 18955,  7553,  4988,    23,    24, 18496, 19247, 24204,
       19087, 19250,  7552, 18956, 18957, 19249, 24214,     0, 18958,
       18959, 24215]), 'topk_tokens': [' the', ' from', '.', ' Mary', '4', '\n\n', ' Mr', ' dropped', ' apple', ' in', '.', ' apple', '\n', 'the', ' apple', '<|eot_id|>', '<|begin_of_text|>', ' manner', ' in', '<|start_header_id|>'], 'evidence_proportions': [569.6354166666666, 679.9, 569.6354166666666, 1255.375, 136.25]}, 'weight': {'score': [20.76215277777778, 23.444294207505884, 22.48934659090909, 23.448595812983733, 29.546875], 'topk_indices': array([18815, 18859, 14693, 14657, 14702, 19459, 14738, 19517, 14621,
       14556, 20398, 20350, 18184, 18153, 23696, 23562, 22001, 22028,
       23778, 23644]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 22.1, 20.052083333333332, 20.97265625, 20.927083333333332]}, 'saliency': {'score': [3.463523582175926, 0.13714623044697782, 1.7025479403409092, 0.13129090802972765, 0.2787367422369462], 'topk_indices': array([24192, 16059, 19527,  4989,  7549,  7192, 24108, 19086, 18462,
        7548, 24206, 18496, 19247,  4988, 24204,  7552, 18959, 18957,
       19249, 18958]), 'topk_tokens': ['Question', ' as', '�', ' journey', ' picked', ' milk', ' location', ' manner', ' Britain', ' Mary', ' discarded', ' Mr', ' dropped', ' Mary', ' apple', ' apple', ' in', 'the', ' apple', ' manner'], 'evidence_proportions': [3.3857421875, 4.03125, 3.3857421875, 7.0458984375, 0.7577311197916666]}}, 'pred_res': "Mary's hand<|eot_id|>", 'score': 0}
2025-01-22 03:55:47.151 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:55:47.151 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-3_pid-4_2-3-4-8-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (2, 3, 4, 8, 9):   5%|▌         | 5/100 [02:23<45:34, 28.79s/it]Processing depth (2, 3, 4, 8, 9):   5%|▌         | 5/100 [02:23<45:30, 28.74s/it]
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:82 - Selected idx: 4
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:86 - Needle: [' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' Mary journeyed to the office.']
2025-01-22 03:55:47.411 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🎰', '🧎🏼\u200d♀\u200d➡', '🐨', '👨🏿\u200d❤\u200d👨🏽', '👩🏾\u200d❤️\u200d👩🏿', '☠', '🙇🏻\u200d♂️', '🤞', '👩🏿\u200d🌾', '👵🏽']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.32s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.23s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.30s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.49s/it]
Processing depth (1, 4, 7, 9):   0%|          | 0/100 [00:15<?, ?it/s]2025-01-22 03:56:03.433 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:56:03.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2972, 2978) -->  tragedy. Daniel journeyed to
2025-01-22 03:56:03.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:56:03.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9712, 9717) -->  war. Daniel went to
2025-01-22 03:56:03.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:56:03.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16800, 16804) -->  Daniel left the apple
2025-01-22 03:56:03.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:56:03.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21493, 21499) --> . Mary journeyed to the
2025-01-22 03:56:03.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 03:56:03.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  got the football there.
2025-01-22 03:56:03.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:56:03.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18175, 18180) --> . Mary moved to the
2025-01-22 03:56:03.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:56:03.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6830, 6836) --> . John went back to the
2025-01-22 03:56:03.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:56:03.941 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17372, 17378) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:56:06.861 | INFO     | test_jbb_embedding:begin_test:693 - The apple was left on the table.<|eot_id|>
2025-01-22 03:56:06.861 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 03:56:15.293 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [200.67596726190476, 8.211378145531176, 98.43181818181819, 7.9619118796051, 17.97706842996988], 'topk_indices': array([24096, 24179,     1, 24181,  2975, 24188,  2980,     9, 16803,
       24185,    24, 24197,    14, 24186,    23,  2979, 24196,     0,
       24193, 24194]), 'topk_tokens': [' location', '.\n\n', '<|start_header_id|>', ':', ' journey', ' garden', '.', ':', ' apple', ' apple', '\n\n', '\n\n', '\n', ' before', '4', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [311.25, 166.64375, 349.0625, 19.537760416666668]}, 'weight': {'score': [23.464657738095237, 23.45148702532953, 20.642045454545453, 23.45403405445401, 30.0988328313253], 'topk_indices': array([18835, 18791, 14663, 14627, 14672, 19500, 19442, 14708, 14526,
       14591, 20374, 20326, 18154, 18123, 23550, 23684, 22016, 21989,
       23766, 23632]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.110677083333332, 23.1296875, 26.533203125, 20.052083333333332]}, 'saliency': {'score': [1.2542550223214286, 0.04597757311971871, 0.5257984508167614, 0.04449028585493806, 0.14237562432346573], 'topk_indices': array([    8, 24191, 24052,    19, 24195,    20,  9718,    24, 24197,
           0, 24096, 24180,    23,  2974, 24188,  2975, 24186, 16803,
       24185,  2979]), 'topk_tokens': [' Date', 'Answer', ' context', '26', 'assistant', ' Jul', ' garden', '\n\n', '\n\n', '<|begin_of_text|>', ' location', 'Question', '4', ' Daniel', ' garden', ' journey', ' before', ' apple', ' apple', ' bathroom'], 'evidence_proportions': [1.8511962890625, 0.984765625, 2.4293212890625, 0.0985107421875]}}, 'pred_res': 'The apple was left on the table.<|eot_id|>', 'score': 0}
2025-01-22 03:56:15.301 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:56:15.301 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-0_1-4-7-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 4, 7, 9):   1%|          | 1/100 [00:27<45:46, 27.74s/it]is_0k: False
your chose emoji: ['🚶🏼\u200d♀️\u200d➡️', '👩🏼\u200d⚕️', '🤾\u200d♀', '⛹🏿', '🚶\u200d♂\u200d➡', '💇🏼\u200d♀️', '🏃🏿\u200d♂\u200d➡️', '🇨🇳', '🇪🇭', '🏋️\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.71s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.90s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (1, 2, 5, 7):   1%|          | 1/100 [00:45<45:46, 27.74s/it]2025-01-22 03:56:32.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:56:32.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Daniel journeyed to the
2025-01-22 03:56:32.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:56:32.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4972, 4977) --> . Daniel went to the
2025-01-22 03:56:32.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:56:32.944 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11880, 11884) -->  Daniel left the apple
2025-01-22 03:56:32.944 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:56:33.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16750, 16756) --> . Mary journeyed to the
2025-01-22 03:56:33.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 03:56:33.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  got the football there.
2025-01-22 03:56:33.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:56:33.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18146, 18151) --> . Mary moved to the
2025-01-22 03:56:33.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:56:33.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6834, 6840) --> . John went back to the
2025-01-22 03:56:33.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:56:33.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17316, 17322) -->  Sandra journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:56:36.015 | INFO     | test_jbb_embedding:begin_test:693 - Daniel left the apple.<|eot_id|>
2025-01-22 03:56:36.015 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24222])
2025-01-22 03:56:44.383 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [149.52008928571428, 6.90687951496388, 64.0028409090909, 6.731088076875362, 9.986836965460526], 'topk_indices': array([24120,     4,     3, 24213, 24216,  4977, 11883, 24203, 24221,
          23, 24210,     1,    14, 24209,    24, 24212,     0, 24218,
       24220, 24217]), 'topk_tokens': [' location', '\n\n', '<|end_header_id|>', '?', ':', ' garden', ' apple', '.\n\n', '\n\n', '4', ' before', '<|start_header_id|>', '\n', ' apple', '\n\n', ' garden', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [185.875, 173.4, 245.96875, 28.966145833333332]}, 'weight': {'score': [21.257068452380953, 23.455673374613003, 21.875355113636363, 23.459020397403027, 29.79309210526316], 'topk_indices': array([18902, 18858, 14714, 14750, 14795, 14759, 19555, 19497, 14678,
       14613, 20369, 20417, 18196, 18227, 23580, 23714, 22046, 22019,
       23662, 23796]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 19.665625, 26.533203125, 20.052083333333332]}, 'saliency': {'score': [0.9104570661272321, 0.03877339869952915, 0.3468295010653409, 0.037736156483499476, 0.07168028982062088], 'topk_indices': array([   20, 24203,  3039,    23, 24215, 24204,  4973,  7800, 24120,
        3038, 24219,    24,     0,  3043,  4977, 11883,  6840, 24210,
       24209, 24212]), 'topk_tokens': [' Jul', '.\n\n', ' journey', '4', 'Answer', 'Question', ' Daniel', ' Square', ' location', ' Daniel', 'assistant', '\n\n', '<|begin_of_text|>', ' bathroom', ' garden', ' apple', ' bedroom', ' before', ' apple', ' garden'], 'evidence_proportions': [1.1003011067708333, 0.958984375, 1.6893310546875, 0.16092427571614584]}}, 'pred_res': 'Daniel left the apple.<|eot_id|>', 'score': 0}
2025-01-22 03:56:44.389 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:56:44.390 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-1_1-2-5-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 2, 5, 7):   2%|▏         | 2/100 [00:56<46:36, 28.53s/it]is_0k: False
your chose emoji: ['🤦\u200d♀️', '👩🏿\u200d🤝\u200d👨🏾', '☁', '👷🏻\u200d♀️', '🧗🏻\u200d♂️', '👩🏽\u200d❤️\u200d💋\u200d👨🏻', '🙆', '👨🏽\u200d❤️\u200d👨🏿', '🧑🏼\u200d🎄', '🩴']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.52s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 3, 5, 7):   2%|▏         | 2/100 [01:13<46:36, 28.53s/it]2025-01-22 03:57:01.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:57:01.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:57:01.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:57:01.429 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7547, 7552) --> . Daniel went to the
2025-01-22 03:57:01.429 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:57:01.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11920, 11924) -->  Daniel left the apple
2025-01-22 03:57:01.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:57:01.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (32, 38) --> ed to the bathroom. Mary
2025-01-22 03:57:01.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 03:57:01.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (36, 41) --> . Mary got the football
2025-01-22 03:57:01.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:57:01.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (33, 38) -->  to the bathroom. Mary
2025-01-22 03:57:01.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:57:01.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6816, 6822) --> . John went back to the
2025-01-22 03:57:01.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:57:01.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17300, 17306) -->  business. Sandra journeyed to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:57:04.279 | INFO     | test_jbb_embedding:begin_test:693 - The Press<|eot_id|>
2025-01-22 03:57:04.280 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24180])
2025-01-22 03:57:12.650 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [299.76785714285717, 9.347717146342472, 132.3662997159091, 8.982960238452776, 11.444679054054054], 'topk_indices': array([    1, 24164, 24162, 24174, 11923, 24165, 24161, 24172,    24,
          14, 24171, 24170, 11924, 24163, 24178, 24167, 24168,     0,
       24175, 24176]), 'topk_tokens': ['<|start_header_id|>', ' Where', 'Question', ':', ' apple', ' was', '.\n\n', ' \n', '\n\n', '\n', '?', ' garden', '.', ':', '<|end_header_id|>', ' apple', ' before', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [299.2604166666667, 242.05, 447.375, 249.96875]}, 'weight': {'score': [21.67373511904762, 23.440970929992144, 22.140980113636363, 23.44369304059652, 29.183699324324323], 'topk_indices': array([18798, 18842, 14658, 14622, 14667, 14703, 19495, 19437, 14521,
       14586, 20357, 20309, 18167, 18136, 23662, 23528, 21980, 21953,
       23744, 23610]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 19.665625, 26.533203125, 20.84375]}, 'saliency': {'score': [1.7264811197916667, 0.050992272181452365, 0.7202661687677557, 0.0489247787462944, 0.08507630631730363], 'topk_indices': array([24149, 11921, 11969,  7552,    35,    24,     0,    31, 24163,
       24165, 24161, 24171, 24173, 24172, 24164, 24162, 11923, 24170,
       24168, 24167]), 'topk_tokens': [' return', ' left', 'ANT', ' garden', ' bathroom', '\n\n', '<|begin_of_text|>', ' journey', ':', ' was', '.\n\n', '?', 'Answer', ' \n', ' Where', 'Question', ' apple', ' garden', ' before', ' apple'], 'evidence_proportions': [1.7222086588541667, 1.203271484375, 2.99560546875, 1.3206787109375]}}, 'pred_res': 'The Press<|eot_id|>', 'score': 0}
2025-01-22 03:57:12.655 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:57:12.655 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-2_0-3-5-7.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 3, 5, 7):   3%|▎         | 3/100 [01:25<45:55, 28.41s/it]is_0k: False
your chose emoji: ['👨🏾\u200d🦽', '🇧🇧', '🧎🏽\u200d♀\u200d➡️', '👩🏿\u200d🌾', '🇽🇰', '🦨', '🤦🏿', '🙇\u200d♂️', '🤵🏻', '🧝🏿\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (2, 4, 5, 7):   3%|▎         | 3/100 [01:42<45:55, 28.41s/it]2025-01-22 03:57:29.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:57:29.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4885, 4891) --> . Daniel journeyed to the
2025-01-22 03:57:29.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:57:29.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9686, 9691) -->  war. Daniel went to
2025-01-22 03:57:29.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:57:30.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11816, 11820) -->  Daniel left the apple
2025-01-22 03:57:30.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:57:30.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16672, 16678) --> . Mary journeyed to the
2025-01-22 03:57:30.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 03:57:30.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  got the football there.
2025-01-22 03:57:30.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:57:30.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18068, 18073) --> . Mary moved to the
2025-01-22 03:57:30.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:57:30.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6826, 6832) --> . John went back to the
2025-01-22 03:57:30.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:57:30.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17238, 17244) -->  Sandra journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:57:33.213 | INFO     | test_jbb_embedding:begin_test:693 - The apple was in the house.<|eot_id|>
2025-01-22 03:57:33.214 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24164])
2025-01-22 03:57:41.610 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [140.52380952380952, 6.240085912608102, 66.1747159090909, 6.0685339288675175, 9.240337949810606], 'topk_indices': array([24152, 11819, 24151, 24154, 24052, 24137, 24145,     4, 24158,
       24147,    26,    23, 24163,    14,     1,    24,     0, 24160,
       24162, 24159]), 'topk_tokens': [' before', ' apple', ' apple', ' garden', '.\n\n', '.', '.\n\n', '\n\n', ':', ':', '<|start_header_id|>', '4', '\n\n', '\n', '<|start_header_id|>', '\n\n', '<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [177.5, 135.70625, 229.0625, 48.536458333333336]}, 'weight': {'score': [22.081845238095237, 23.43457504034427, 21.875355113636363, 23.43717453314127, 28.71496212121212], 'topk_indices': array([18780, 18824, 14618, 14654, 14663, 19477, 14699, 19419, 14517,
       14582, 20351, 20303, 18118, 18149, 23508, 23642, 21947, 21974,
       23724, 23590]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 23.1296875, 26.533203125, 20.052083333333332]}, 'saliency': {'score': [0.8763224283854166, 0.03485310224169322, 0.3737099387428977, 0.03381157901784789, 0.06558857542095763], 'topk_indices': array([   20,  9692, 24163,  9688,     0,    23, 24062, 24133, 24152,
        6832, 24146,  4886, 24157,  4891,  4887, 24161, 11819,    24,
       24154, 24151]), 'topk_tokens': [' Jul', ' garden', '\n\n', ' Daniel', '<|begin_of_text|>', '4', ' location', ' return', ' before', ' bedroom', 'Question', ' Daniel', 'Answer', ' bathroom', ' journey', 'assistant', ' apple', '\n\n', ' garden', ' apple'], 'evidence_proportions': [1.0721842447916667, 0.827197265625, 1.547607421875, 0.2738749186197917]}}, 'pred_res': 'The apple was in the house.<|eot_id|>', 'score': 0}
2025-01-22 03:57:41.615 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:57:41.615 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-3_2-4-5-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (2, 4, 5, 7):   4%|▍         | 4/100 [01:54<45:48, 28.63s/it]is_0k: False
your chose emoji: ['👴🏼', '🧘🏿\u200d♂', '🛅', '👩🏾\u200d🤝\u200d👨🏽', '🇭🇷', '👃', '📦', '🍛', '🥷🏿', '🧑🏼\u200d🏫']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.37s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.01s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.57s/it]
Processing depth (0, 3, 8, 9):   4%|▍         | 4/100 [02:10<45:48, 28.63s/it]2025-01-22 03:57:58.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:57:58.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:57:58.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 03:57:58.523 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7553, 7558) --> . Daniel went to the
2025-01-22 03:57:58.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 03:57:58.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19266, 19270) -->  left the apple.
2025-01-22 03:57:58.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the office.
2025-01-22 03:57:58.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (32, 38) --> ed to the bathroom. Mary
2025-01-22 03:57:58.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 03:57:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (36, 41) --> . Mary got the football
2025-01-22 03:57:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 03:57:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (33, 38) -->  to the bathroom. Mary
2025-01-22 03:57:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:57:58.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6822, 6828) --> . John went back to the
2025-01-22 03:57:58.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 03:57:58.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17337, 17343) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:58:01.484 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's house.<|eot_id|>
2025-01-22 03:58:01.484 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24220])
2025-01-22 03:58:09.877 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [234.36904761904762, 8.363710368245057, 118.62642045454545, 8.067106079404466, 8.638297872340425], 'topk_indices': array([24205, 24118, 24212, 24214,  7559,    14,     1, 24211, 24202,
       24201, 19269, 24203,    24, 24210, 24207, 24208, 24218,     0,
       24216, 24215]), 'topk_tokens': [' was', ' location', ' \n', ':', '.', '\n', '<|start_header_id|>', '?', 'Question', '.\n\n', '.', ':', '\n\n', ' garden', ' apple', ' before', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [227.66666666666666, 192.875, 359.0625, 192.52083333333334]}, 'weight': {'score': [20.50967261904762, 23.452343846757213, 21.33877840909091, 23.456822528949544, 29.42752659574468], 'topk_indices': array([18818, 18774, 14627, 14591, 19476, 14682, 19418, 14646, 14490,
       14555, 20290, 20338, 18119, 18088, 23684, 23550, 21996, 21969,
       23632, 23766]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 19.665625, 20.421875, 20.84375]}, 'saliency': {'score': [1.2477911086309523, 0.046436805726672484, 0.6852028586647727, 0.04481226918715105, 0.06320717994202958], 'topk_indices': array([24217, 24189, 18917, 24205,    31,  7558, 24213, 24211, 24212,
          35, 24204,    24, 19268, 24201, 24118, 18774, 24202, 24208,
       24210, 24207]), 'topk_tokens': ['assistant', ' return', ' manner', ' was', ' journey', ' garden', 'Answer', '?', ' \n', ' bathroom', ' Where', '\n\n', ' apple', '.\n\n', ' location', 'untlet', 'Question', ' before', ' garden', ' apple'], 'evidence_proportions': [1.34912109375, 0.9533203125, 1.738525390625, 1.064697265625]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-22 03:58:09.884 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:58:09.884 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-4_pid-4_0-3-8-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 3, 8, 9):   5%|▌         | 5/100 [02:22<45:07, 28.50s/it]Processing depth (0, 3, 8, 9):   5%|▌         | 5/100 [02:22<45:10, 28.53s/it]
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:82 - Selected idx: 5
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:86 - Needle: [' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' Mary journeyed to the office.']
2025-01-22 03:58:10.204 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👩🏻\u200d❤\u200d💋\u200d👩🏻', '🚅', '👨🏼\u200d🦼\u200d➡️', '📓', '👮🏾\u200d♀️', '🤛🏿', '👩🏻\u200d🔬', '🇲🇼', '🤦🏿\u200d♀️', '🧑🏿\u200d🤝\u200d🧑🏽']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.78s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]
Processing depth (0, 1, 7, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 03:58:26.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:58:26.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:58:26.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:58:26.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2819, 2823) -->  Daniel grabbed the milk
2025-01-22 03:58:26.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:58:26.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16672, 16677) --> . Daniel went to the
2025-01-22 03:58:26.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:58:27.023 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19187, 19191) -->  left the milk.
2025-01-22 03:58:27.023 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:58:27.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21422, 21428) --> . Mary journeyed to the
2025-01-22 03:58:27.149 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:58:27.215 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12015, 12021) --> . Sandra journeyed to the
2025-01-22 03:58:27.215 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football there.
2025-01-22 03:58:27.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16927, 16932) --> . Mary got the football
2025-01-22 03:58:27.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:58:27.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16817, 16823) -->  affair. John went back to
2025-01-22 03:58:27.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:58:27.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18807, 18812) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:58:30.214 | INFO     | test_jbb_embedding:begin_test:693 - The bathroom.<|eot_id|>
2025-01-22 03:58:30.214 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 03:58:38.594 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [821.48375, 24.97287487073423, 131.65767045454547, 24.050302034980106, 24.697079613095237], 'topk_indices': array([  431,   542,    32,   513,   539,   543,    31,   426,   530,
         427,    35,   428,    76,   546,   549,   547, 24167,     0,
         544, 24168]), 'topk_tokens': ['10', ' ', 'ed', ',', '\n', '6', ' journey', '6', ' an', ',', ' bathroom', '000', 'UL', ' to', ',', ' ', '<|eot_id|>', '<|begin_of_text|>', ',', '<|start_header_id|>'], 'evidence_proportions': [2023.5, 806.5, 246.075, 930.125, 36.536458333333336]}, 'weight': {'score': [21.5490625, 23.43802740434333, 22.13387784090909, 23.441173765956567, 29.664434523809526], 'topk_indices': array([18793, 18749, 14636, 14600, 14645, 14681, 19457, 19399, 14499,
       14564, 20271, 20319, 18112, 18081, 23643, 23509, 21948, 21975,
       23591, 23725]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 28.263671875, 19.665625, 20.3515625, 20.052083333333332]}, 'saliency': {'score': [5.01003662109375, 0.1335179275465357, 0.7490178888494318, 0.1279039534712976, 0.18802630712115576], 'topk_indices': array([547,  71,  65, 543,  53, 431, 151, 426, 530,  39, 541, 545,  75,
       546,   0, 544,  31,  76, 428,  35]), 'topk_tokens': [' ', ' DAYS', 'ENCES', '6', ' Gutenberg', '10', '�', '6', ' an', '***', ' setting', '000', ' PA', ' to', '<|begin_of_text|>', ',', ' journey', 'UL', '000', ' bathroom'], 'evidence_proportions': [12.390625, 5.9345703125, 1.347607421875, 4.811767578125, 0.197296142578125]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-22 03:58:38.602 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:58:38.602 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-0_0-1-7-8-9.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 1, 7, 8, 9):   1%|          | 1/100 [00:28<46:39, 28.28s/it]is_0k: False
your chose emoji: ['🧎🏼\u200d♀️\u200d➡️', '🧕🏼', '🔸', '👩🏼\u200d🦯', '\U0001fad9', '🚣🏼\u200d♂️', '🖐️', '◾', '🍄', '🙇🏾\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.89s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.73s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.44s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (0, 3, 6, 8, 9):   1%|          | 1/100 [00:45<46:39, 28.28s/it]2025-01-22 03:58:56.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:58:56.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 03:58:56.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:58:56.337 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7657, 7661) -->  Daniel grabbed the milk
2025-01-22 03:58:56.338 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:58:56.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14525, 14530) -->  bonds. Daniel went to
2025-01-22 03:58:56.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:58:56.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19342, 19346) -->  Daniel left the milk
2025-01-22 03:58:56.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:58:56.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21534, 21540) --> . Mary journeyed to the
2025-01-22 03:58:56.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:58:56.675 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12151, 12157) --> . Sandra journeyed to the
2025-01-22 03:58:56.675 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football there.
2025-01-22 03:58:56.758 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17113, 17118) --> . Mary got the football
2025-01-22 03:58:56.758 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:58:56.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17010, 17016) --> . John went back to the
2025-01-22 03:58:56.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:58:56.936 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19105, 19110) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:58:59.617 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 03:58:59.618 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24168])
2025-01-22 03:59:07.999 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [169.6575, 8.56757839973522, 107.3465909090909, 8.310556914276239, 6.066454277663935], 'topk_indices': array([24167, 24162, 24013,   542,   425, 12157,    36, 24047,   426,
       24057, 14531,     1,    23,  2092,    14,    24, 24166,     0,
       24163, 24164]), 'topk_tokens': ['\n\n', ':', ' context', ' to', ' to', ' bedroom', '.', '.\n\n', ' ', ' location', ' garden', '<|start_header_id|>', '4', ' the', '\n', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [265.1666666666667, 215.78125, 168.54375, 183.78125, 34.911458333333336]}, 'weight': {'score': [23.4084375, 23.437502585743246, 21.001420454545453, 23.439754303287184, 29.764600409836067], 'topk_indices': array([18747, 18791, 14564, 14600, 14645, 19455, 14609, 19397, 14522,
       14457, 20269, 20317, 18055, 18086, 23517, 23651, 21920, 21947,
       23733, 23599]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 28.263671875, 24.0734375, 26.462890625, 20.052083333333332]}, 'saliency': {'score': [1.047210693359375, 0.04779349438311924, 0.5901378284801136, 0.046263192388236944, 0.046893385590099895], 'topk_indices': array([   20,  2045, 24128, 24165, 24047, 24161,  7660,    23,    31,
       19345,    65, 19110, 12152, 24141, 24013,    24,    35, 24057,
       14531, 12157]), 'topk_tokens': [' Jul', ' river', ' return', 'assistant', '.\n\n', 'Answer', ' milk', '4', ' journey', ' milk', 'ENCES', ' bathroom', ' Sandra', 'Question', ' context', '\n\n', ' bathroom', ' location', ' garden', ' bedroom'], 'evidence_proportions': [1.4360758463541667, 1.598876953125, 0.989501953125, 1.25103759765625, 0.2027740478515625]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 03:59:08.011 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:59:08.011 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-1_0-3-6-8-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 3, 6, 8, 9):   2%|▏         | 2/100 [00:57<47:16, 28.94s/it]is_0k: False
your chose emoji: ['🏋🏿\u200d♂', '🏪', '👨🏾\u200d❤\u200d👨🏿', '🧝', '🏚️', '👨\u200d🦯\u200d➡️', '✋🏾', '😏', '🇹🇱', '✂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.07s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (1, 4, 5, 7, 9):   2%|▏         | 2/100 [01:14<47:16, 28.94s/it]2025-01-22 03:59:24.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:59:24.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Daniel journeyed to the
2025-01-22 03:59:24.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:59:24.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9739, 9743) -->  Daniel grabbed the milk
2025-01-22 03:59:24.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:59:25.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11911, 11916) --> . Daniel went to the
2025-01-22 03:59:25.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:59:25.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16779, 16783) -->  Daniel left the milk
2025-01-22 03:59:25.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:59:25.251 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21502, 21508) --> . Mary journeyed to the
2025-01-22 03:59:25.251 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:59:25.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12068, 12074) -->  Sandra journeyed to the bedroom
2025-01-22 03:59:25.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football there.
2025-01-22 03:59:25.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16992, 16997) --> . Mary got the football
2025-01-22 03:59:25.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:59:25.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16947, 16953) --> . John went back to the
2025-01-22 03:59:25.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:59:25.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19107, 19112) -->  order. Mary moved to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:59:28.228 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 03:59:28.229 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24204])
2025-01-22 03:59:36.612 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [161.4784375, 8.17914885157186, 59.81800426136363, 7.973497484219784, 9.520853935917721], 'topk_indices': array([24189, 24093, 24177, 11963, 11962, 24176, 24196,     3,    24,
       24198, 24178,  3032, 24083,    14, 24049,    23, 24202,     0,
       24199, 24200]), 'topk_tokens': [' milk', ' location', 'Question', 'IC', 'ANT', '.\n\n', '?\n', '<|end_header_id|>', '\n\n', ':', ':', ' Daniel', '.\n\n', '\n', ' context', '4', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [263.0416666666667, 280.46875, 107.4625, 182.84375, 11.358072916666666]}, 'weight': {'score': [22.366875, 23.44292198537613, 22.953125, 23.4444814517798, 29.16376582278481], 'topk_indices': array([18838, 18794, 14640, 14604, 19497, 14685, 14649, 19439, 14568,
       14503, 20311, 20359, 18132, 18163, 23687, 23553, 21980, 22007,
       23635, 23769]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 28.263671875, 19.665625, 26.462890625, 20.052083333333332]}, 'saliency': {'score': [1.054044189453125, 0.04559044028941783, 0.36865650523792615, 0.04425274173155526, 0.06842465943928007], 'topk_indices': array([16782,  9740,    19,  9742, 11963,    20,  3037,  3033, 24093,
       24183, 24197, 24189, 11962, 24083,    23,  9739, 24191, 24177,
        3032, 24049]), 'topk_tokens': [' milk', ' grabbed', '26', ' milk', 'IC', ' Jul', ' bathroom', ' journey', ' location', ' prior', 'Answer', ' milk', 'ANT', '.\n\n', '4', ' Daniel', ' discarded', 'Question', ' Daniel', ' context'], 'evidence_proportions': [1.6028238932291667, 2.13677978515625, 0.5662841796875, 1.2523193359375, 0.0577239990234375]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 03:59:36.624 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 03:59:36.624 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-2_1-4-5-7-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 4, 5, 7, 9):   3%|▎         | 3/100 [01:26<46:32, 28.79s/it]is_0k: False
your chose emoji: ['🇬🇮', '👨🏽\u200d🚀', '👩🏾\u200d❤️\u200d💋\u200d👨🏾', '🇹🇫', '🚶🏽\u200d♂\u200d➡', '👦🏻', '🙆🏽\u200d♂', '💸', '🕺🏻', '🤫']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.90s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.12s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.54s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.30s/it]
Processing depth (3, 6, 7, 8, 9):   3%|▎         | 3/100 [01:45<46:32, 28.79s/it]2025-01-22 03:59:56.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 03:59:56.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7607, 7613) --> . Daniel journeyed to the
2025-01-22 03:59:56.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 03:59:56.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14506, 14510) -->  Daniel grabbed the milk
2025-01-22 03:59:56.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 03:59:56.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16746, 16751) --> . Daniel went to the
2025-01-22 03:59:56.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 03:59:56.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19312, 19316) -->  Daniel left the milk
2025-01-22 03:59:56.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 03:59:56.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21418, 21424) --> . Mary journeyed to the
2025-01-22 03:59:56.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 03:59:56.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12131, 12137) -->  Sandra journeyed to the bedroom
2025-01-22 03:59:56.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football there.
2025-01-22 03:59:56.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17021, 17026) --> . Mary got the football
2025-01-22 03:59:56.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 03:59:56.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16926, 16932) --> . John went back to the
2025-01-22 03:59:56.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 03:59:56.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19075, 19080) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 03:59:59.548 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 03:59:59.548 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24136])
2025-01-22 04:00:07.865 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [334.66125, 11.278300934172915, 175.7840909090909, 10.79250892412419, 18.429166666666667], 'topk_indices': array([16747, 24055, 24108, 24025, 14510,    14,  7610, 14507, 16748,
          24,  7607, 19080, 14506,  7609, 16751, 24134, 24132,  7608,
           0, 24131]), 'topk_tokens': [' Daniel', ' the', '.\n\n', ' location', '.', '\n', 'ed', ' grabbed', ' went', '\n\n', '.', ' bathroom', ' Daniel', ' journey', ' garden', '<|end_header_id|>', '<|start_header_id|>', ' Daniel', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [528.6458333333334, 415.8125, 398.4, 318.9375, 43.942708333333336]}, 'weight': {'score': [22.366875, 23.427309022743277, 22.234730113636363, 23.429498446060933, 29.298958333333335], 'topk_indices': array([18761, 18717, 14578, 14542, 19367, 14623, 19425, 14587, 14501,
       14436, 20247, 20295, 18049, 18080, 23475, 23609, 21902, 21929,
       23691, 23557]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 28.263671875, 19.665625, 26.462890625, 20.052083333333332]}, 'saliency': {'score': [2.10845703125, 0.0640368955256535, 1.0535333806818181, 0.06101184878538519, 0.1361941867404514], 'topk_indices': array([23981, 24129, 19312, 12131, 24121, 14509,     0, 19315, 24025,
       24109, 12136, 16747,  7613, 16748, 14507, 14506,  7609, 19080,
       16751,  7608]), 'topk_tokens': [' context', 'Answer', ' Daniel', ' Sandra', ' milk', ' milk', '<|begin_of_text|>', ' milk', ' location', 'Question', ' bedroom', ' Daniel', ' bathroom', ' went', ' grabbed', ' Daniel', ' journey', ' bathroom', ' garden', ' Daniel'], 'evidence_proportions': [3.1817220052083335, 3.237060546875, 2.15830078125, 2.1119384765625, 0.23893229166666666]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:00:07.870 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:00:07.870 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-3_3-6-7-8-9.pkl | len: 3 |  size: 2.12 KB
Processing depth (3, 6, 7, 8, 9):   4%|▍         | 4/100 [01:57<47:37, 29.76s/it]is_0k: False
your chose emoji: ['⚫', '🧩', '😪', '🙃', '🌥️', '🇦🇬', '🔞', '🙅🏽\u200d♂️', '🤣', '🏃🏼\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.38s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 2, 4, 8, 9):   4%|▍         | 4/100 [02:14<47:37, 29.76s/it]2025-01-22 04:00:24.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:00:24.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 04:00:24.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:00:24.940 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4995, 4999) -->  Daniel grabbed the milk
2025-01-22 04:00:24.940 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:00:24.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9675, 9680) --> . Daniel went to the
2025-01-22 04:00:24.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:00:25.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19219, 19223) -->  left the milk.
2025-01-22 04:00:25.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the office.
2025-01-22 04:00:25.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21440, 21446) --> . Mary journeyed to the
2025-01-22 04:00:25.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the bedroom.
2025-01-22 04:00:25.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12019, 12025) --> . Sandra journeyed to the
2025-01-22 04:00:25.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football there.
2025-01-22 04:00:25.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16863, 16868) --> . Mary got the football
2025-01-22 04:00:25.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:00:25.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16822, 16828) --> . John went back to the
2025-01-22 04:00:25.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:00:25.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18839, 18844) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:00:28.162 | INFO     | test_jbb_embedding:begin_test:693 - The bathroom<|eot_id|>
2025-01-22 04:00:28.163 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 04:00:36.557 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1149.59625, 59.481884432499484, 370.96590909090907, 58.06894600836855, 31.455882352941178], 'topk_indices': array([   31,  7464,   549,  2092,    35,  7508,  7463,    14,    36,
        1628,  9259,  1706,   542,   425,   426,  7465,  7509,     0,
       24177, 24178]), 'topk_tokens': [' journey', ' or', ' per', ' the', ' bathroom', ' two', ' two', '\n', '.', ' work', ' a', "'s", ' to', ' to', ' ', ' three', ' or', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [3295.8333333333335, 1192.625, 196.05, 718.25, 56.859375]}, 'weight': {'score': [21.5490625, 23.442363551788297, 21.001420454545453, 23.446549204055845, 29.977481617647058], 'topk_indices': array([18825, 18781, 14598, 14634, 19489, 19431, 14679, 14643, 14497,
       14562, 20303, 20351, 18119, 18150, 23527, 23661, 21993, 21966,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 28.263671875, 19.665625, 20.3515625, 20.052083333333332]}, 'saliency': {'score': [6.73670166015625, 0.327637021591379, 2.1227361505681817, 0.31936298079257164, 0.24071906594669118], 'topk_indices': array([ 542,  425, 7455, 7494, 7464,   40, 1705, 7495, 7500, 9201, 7456,
       1706, 7508, 7463,  549,   31, 1628,   35, 7509, 7465]), 'topk_tokens': [' to', ' to', ' tele', ' or', ' or', '\n\n\n', ' summer', ' five', ' tele', ' few', 'graph', "'s", ' two', ' two', ' per', ' journey', ' work', ' bathroom', ' or', ' three'], 'evidence_proportions': [18.236328125, 8.875, 1.16572265625, 3.948974609375, 0.3125101725260417]}}, 'pred_res': 'The bathroom<|eot_id|>', 'score': 100}
2025-01-22 04:00:36.565 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:00:36.565 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-5_pid-4_0-2-4-8-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 2, 4, 8, 9):   5%|▌         | 5/100 [02:26<46:30, 29.38s/it]Processing depth (0, 2, 4, 8, 9):   5%|▌         | 5/100 [02:26<46:24, 29.31s/it]
2025-01-22 04:00:36.871 | INFO     | __main__:<module>:82 - Selected idx: 6
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:86 - Needle: [' John went back to the bedroom.', ' Mary journeyed to the office.', ' Daniel journeyed to the bathroom.', ' Mary moved to the bathroom.', ' Mary got the football there.', ' John journeyed to the office.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel went to the garden.', ' Daniel left the apple.', ' John moved to the bedroom.']
2025-01-22 04:00:36.872 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🕵🏾\u200d♂️', '🙇\u200d♀️', '🧎🏽\u200d♂\u200d➡', '🧑\u200d🦼\u200d➡️', '\U0001fac3', '👲🏻', '🛒', '👳\u200d♀', '💆🏽\u200d♀', '☝🏽']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (2, 4, 5, 6):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:00:54.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:00:54.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4899, 4905) --> . Daniel journeyed to the
2025-01-22 04:00:54.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 04:00:54.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9726, 9731) --> . Daniel went to the
2025-01-22 04:00:54.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 04:00:54.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11891, 11895) -->  Daniel left the apple
2025-01-22 04:00:54.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the bedroom.
2025-01-22 04:00:54.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14380, 14385) --> . John moved to the
2025-01-22 04:00:54.269 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:00:54.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8583, 8589) --> . John went back to the
2025-01-22 04:00:54.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 04:00:54.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6419, 6425) -->  John journeyed to the office
2025-01-22 04:00:54.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:00:54.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19262, 19267) -->  Mary moved to the bathroom
2025-01-22 04:00:54.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary got the football there.
2025-01-22 04:00:54.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18709, 18714) --> . Mary got the football
2025-01-22 04:00:54.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John journeyed to the office.
2025-01-22 04:00:54.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6418, 6424) --> . John journeyed to the
2025-01-22 04:00:54.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 04:00:54.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16294, 16300) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:00:57.441 | INFO     | test_jbb_embedding:begin_test:693 - Daniel left the apple.<|eot_id|>
2025-01-22 04:00:57.442 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24223])
2025-01-22 04:01:05.824 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [219.61171875, 7.23095795839181, 84.24333639705883, 6.946908807194275, 9.38355863764045], 'topk_indices': array([24222,    23, 24211,  4906, 11895,    26,    14,     3,  4902,
       24210,  4900,    24, 11894,  4905,     1,  4901,     0, 24221,
       24219, 24218]), 'topk_tokens': ['\n\n', '4', ' before', '.', '.', '<|start_header_id|>', '\n', '<|end_header_id|>', 'ed', ' apple', ' Daniel', '\n\n', ' apple', ' bathroom', '<|start_header_id|>', ' journey', '<|begin_of_text|>', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [337.6666666666667, 197.625, 295.78125, 38.996875]}, 'weight': {'score': [21.024609375, 23.448637311153306, 21.868106617647058, 23.452866113168128, 29.422401685393258], 'topk_indices': array([18805, 18849, 14664, 14700, 19534, 14709, 19476, 14745, 14628,
       14563, 20396, 20348, 18168, 18137, 23569, 23703, 21992, 22019,
       23651, 23785]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 19.665625, 26.533203125, 18.88125]}, 'saliency': {'score': [1.306396484375, 0.040397944482264664, 0.45210400749655333, 0.03877135268430254, 0.07046723097897648], 'topk_indices': array([24204, 11957, 24121, 24205,  4902, 11892,     0, 24220,    24,
       11891,  9727, 24211, 24213,  9731,  8589, 24210, 11894,  4900,
        4901,  4905]), 'topk_tokens': ['.\n\n', ' MILL', ' location', 'Question', 'ed', ' left', '<|begin_of_text|>', 'assistant', '\n\n', ' Daniel', ' Daniel', ' before', ' garden', ' garden', ' bedroom', ' apple', ' apple', ' Daniel', ' journey', ' bathroom'], 'evidence_proportions': [1.962646484375, 1.0287109375, 2.0654296875, 0.18935546875]}}, 'pred_res': 'Daniel left the apple.<|eot_id|>', 'score': 0}
2025-01-22 04:01:05.829 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:01:05.829 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-0_2-4-5-6.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 4, 5, 6):   1%|          | 1/100 [00:28<47:34, 28.83s/it]is_0k: False
your chose emoji: ['☪', '👩🏾\u200d❤\u200d💋\u200d👩🏿', '🤹🏾\u200d♀', '🧏🏼', '👨🏾\u200d❤\u200d👨🏽', '🧛🏿\u200d♂', '🏣', '🇬🇧', '👩🏿\u200d🦼\u200d➡', '🧝🏻\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (1, 3, 5, 9):   1%|          | 1/100 [00:45<47:34, 28.83s/it]2025-01-22 04:01:22.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:01:22.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Daniel journeyed to the
2025-01-22 04:01:22.835 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 04:01:22.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7621, 7626) --> . Daniel went to the
2025-01-22 04:01:22.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 04:01:22.936 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11901, 11905) -->  Daniel left the apple
2025-01-22 04:01:22.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the bedroom.
2025-01-22 04:01:23.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21403, 21408) --> . John moved to the
2025-01-22 04:01:23.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:01:23.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8650, 8656) -->  John went back to the bedroom
2025-01-22 04:01:23.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 04:01:23.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6489, 6495) -->  John journeyed to the office
2025-01-22 04:01:23.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:01:23.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19172, 19177) -->  Mary moved to the bathroom
2025-01-22 04:01:23.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary got the football there.
2025-01-22 04:01:23.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18585, 18590) --> . Mary got the football
2025-01-22 04:01:23.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John journeyed to the office.
2025-01-22 04:01:23.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6488, 6494) --> . John journeyed to the
2025-01-22 04:01:23.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 04:01:23.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16276, 16282) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:01:26.447 | INFO     | test_jbb_embedding:begin_test:693 - The FIVE MILLION LOAN ELECTION.<|eot_id|>
2025-01-22 04:01:26.447 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 04:01:34.810 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [187.516796875, 8.41101909707854, 91.73713235294117, 8.144960561649802, 13.828207759533898], 'topk_indices': array([24146, 24154, 24051, 11905, 24162, 24144,    24,    14, 11904,
       11957,  7627, 24150, 24153, 24151,     0, 11914, 11913, 24161,
       24158, 24159]), 'topk_tokens': [':', '?', '.\n\n', '.', '\n\n', '.\n\n', '\n\n', '\n', ' apple', 'CE', '.', ' apple', ' garden', ' before', '<|begin_of_text|>', 'LE', 'CE', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [159.09375, 237.9, 364.25, 29.8546875]}, 'weight': {'score': [21.024609375, 23.43686377555243, 22.666130514705884, 23.43995145052256, 30.048993644067796], 'topk_indices': array([18785, 18741, 14572, 14608, 19444, 14653, 14617, 19386, 14536,
       14471, 20278, 20326, 18067, 18098, 23489, 23623, 21928, 21955,
       23705, 23571]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 19.665625, 26.533203125, 18.88125]}, 'saliency': {'score': [1.0773605346679687, 0.04707213299741889, 0.5402769200942096, 0.045522086099828406, 0.10526767019498146], 'topk_indices': array([11902,    24, 24154, 24051, 24156, 11889, 11912, 11930, 24160,
       11956, 24144, 24145,  7626, 11957, 11904, 24151, 24150, 24153,
       11914, 11913]), 'topk_tokens': [' left', '\n\n', '?', '.\n\n', 'Answer', 'LECTION', '--', 'ANT', 'assistant', '--', '.\n\n', 'Question', ' garden', 'CE', ' apple', ' before', ' apple', ' garden', 'LE', 'CE'], 'evidence_proportions': [0.90185546875, 1.17685546875, 2.38623046875, 0.141375732421875]}}, 'pred_res': 'The FIVE MILLION LOAN ELECTION.<|eot_id|>', 'score': 0}
2025-01-22 04:01:34.833 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:01:34.834 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-1_1-3-5-9.pkl | len: 3 |  size: 1.99 KB
Processing depth (1, 3, 5, 9):   2%|▏         | 2/100 [00:57<47:15, 28.93s/it]is_0k: False
your chose emoji: ['🖋️', '☯️', '🎗', '🌎', '🚋', '🏃\u200d♂\u200d➡', '🧑🏿\u200d🦰', '🅱', '😅', '🧑🏽\u200d❤️\u200d💋\u200d🧑🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.37s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.56s/it]
Processing depth (2, 4, 6, 7):   2%|▏         | 2/100 [01:14<47:15, 28.93s/it]2025-01-22 04:01:51.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:01:51.483 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4903, 4909) --> . Daniel journeyed to the
2025-01-22 04:01:51.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 04:01:51.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9690, 9695) --> . Daniel went to the
2025-01-22 04:01:51.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 04:01:51.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14234, 14238) -->  Daniel left the apple
2025-01-22 04:01:51.610 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the bedroom.
2025-01-22 04:01:51.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16637, 16642) --> . John moved to the
2025-01-22 04:01:51.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:01:51.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8547, 8553) --> . John went back to the
2025-01-22 04:01:51.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 04:01:51.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6409, 6415) -->  John journeyed to the office
2025-01-22 04:01:51.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:01:51.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19149, 19154) --> . Mary moved to the
2025-01-22 04:01:51.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary got the football there.
2025-01-22 04:01:51.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18487, 18492) --> . Mary got the football
2025-01-22 04:01:51.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John journeyed to the office.
2025-01-22 04:01:52.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6408, 6414) --> . John journeyed to the
2025-01-22 04:01:52.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 04:01:52.104 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16198, 16204) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:01:54.886 | INFO     | test_jbb_embedding:begin_test:693 - Daniel left the apple.<|eot_id|>
2025-01-22 04:01:54.886 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24161])
2025-01-22 04:02:03.296 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [287.53125, 10.230164837154444, 100.4439338235294, 9.872915154500207, 11.73700161637931], 'topk_indices': array([   24, 24049,    14,    23, 24089,  4904,  4905, 24151,  4909,
       24059, 24149, 24148, 14238, 24160, 14237, 24161, 24159,     0,
       24157, 24156]), 'topk_tokens': ['\n\n', '.\n\n', '\n', '4', ' the', ' Daniel', ' journey', ' garden', ' bathroom', ' location', ' before', ' apple', '.', '\n\n', ' apple', 'b', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [369.4791666666667, 200.9125, 512.625, 95.7375]}, 'weight': {'score': [21.024609375, 23.431571759642445, 21.031479779411764, 23.4369530277893, 29.060075431034484], 'topk_indices': array([18791, 18835, 14636, 14672, 19494, 14717, 14681, 19436, 14600,
       14535, 20308, 20356, 18123, 18154, 23647, 23513, 21979, 21952,
       23729, 23595]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 19.665625, 26.533203125, 18.88125]}, 'saliency': {'score': [1.73289794921875, 0.057549771992620735, 0.5473973891314339, 0.0554692335219761, 0.08468891012257543], 'topk_indices': array([24049,  9691, 24015, 24143,  9695, 24145, 24058, 14235,  8553,
       24160, 14234, 24149,  4904, 24059, 24151,  4905, 24161,  4909,
       24148, 14237]), 'topk_tokens': ['.\n\n', ' Daniel', ' context', 'Question', ' garden', ' Where', ' first', ' left', ' bedroom', '\n\n', ' Daniel', ' before', ' Daniel', ' location', ' garden', ' journey', 'b', ' bathroom', ' apple', ' apple'], 'evidence_proportions': [2.137939453125, 1.09736328125, 3.4931640625, 0.474169921875]}}, 'pred_res': 'Daniel left the apple.<|eot_id|>', 'score': 0}
2025-01-22 04:02:03.302 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:02:03.303 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-2_2-4-6-7.pkl | len: 3 |  size: 2.04 KB
Processing depth (2, 4, 6, 7):   3%|▎         | 3/100 [01:26<46:25, 28.72s/it]is_0k: False
your chose emoji: ['🇶🇦', '🦸🏼', '🙎\u200d♀️', '🟫', '⏭', '🇸🇩', '🚵\u200d♂️', '🤾🏾\u200d♂', '🤌🏾', '🕵🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.60s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (0, 3, 4, 7):   3%|▎         | 3/100 [01:43<46:25, 28.72s/it]2025-01-22 04:02:20.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:02:20.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 04:02:20.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 04:02:20.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7539, 7544) --> . Daniel went to the
2025-01-22 04:02:20.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 04:02:20.415 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9701, 9705) -->  Daniel left the apple
2025-01-22 04:02:20.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the bedroom.
2025-01-22 04:02:20.511 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16779, 16784) --> . John moved to the
2025-01-22 04:02:20.511 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:02:20.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8631, 8637) --> . John went back to the
2025-01-22 04:02:20.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 04:02:20.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6501, 6507) -->  John journeyed to the office
2025-01-22 04:02:20.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:02:20.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19220, 19225) -->  Mary moved to the bathroom
2025-01-22 04:02:20.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary got the football there.
2025-01-22 04:02:20.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18633, 18638) --> . Mary got the football
2025-01-22 04:02:20.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John journeyed to the office.
2025-01-22 04:02:20.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6500, 6506) --> . John journeyed to the
2025-01-22 04:02:20.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 04:02:20.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16290, 16296) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:02:23.624 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom.<|eot_id|>
2025-01-22 04:02:23.624 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24171])
2025-01-22 04:02:31.956 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [204.3109375, 11.169626975262679, 102.62132352941177, 10.880564624792703, 7.457736545138889], 'topk_indices': array([   23, 13909,     1,  9678,  8637,  7804, 24161, 24158,    24,
       18930, 24170,  9704, 24159, 18931, 18932, 24169, 18933,     0,
       24167, 24166]), 'topk_tokens': ['4', ',', '<|start_header_id|>', ' company', ' bedroom', ' Square', ' garden', ' apple', '\n\n', '\n', '\n\n', ' apple', ' before', 'the', ' manner', '<|end_header_id|>', ' in', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [264.1875, 198.45, 312.28125, 51.94375]}, 'weight': {'score': [21.224609375, 23.436253826425084, 21.868106617647058, 23.44029818356136, 29.498511904761905], 'topk_indices': array([18789, 18833, 14652, 14616, 19492, 19434, 14661, 14697, 14580,
       14515, 20354, 20306, 18097, 18128, 23527, 23661, 21993, 21966,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 19.665625, 26.533203125, 18.88125]}, 'saliency': {'score': [1.1953643798828124, 0.062390685111069745, 0.5729693244485294, 0.060731516469572706, 0.05748912266322544], 'topk_indices': array([18464, 24153,    31,    24, 24170,  7544,    35, 24069,    65,
       24025,  9678,  7804, 24159, 18933, 24161, 18931, 24158,  8637,
        9704, 18932]), 'topk_tokens': [' Mr', 'Question', ' journey', '\n\n', '\n\n', ' garden', ' bathroom', ' location', 'ENCES', ' context', ' company', ' Square', ' before', ' in', ' garden', 'the', ' apple', ' bedroom', ' apple', ' manner'], 'evidence_proportions': [1.489501953125, 1.00810546875, 2.1614990234375, 0.25675048828125]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 04:02:31.963 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:02:31.963 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-3_0-3-4-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 3, 4, 7):   4%|▍         | 4/100 [01:54<45:54, 28.70s/it]is_0k: False
your chose emoji: ['🇬🇮', '🎥', '🤹🏾\u200d♀', '🧘🏽', '🔕', '💆🏼\u200d♂️', '🙍🏾\u200d♀', '👨🏻\u200d🍳', '🚣\u200d♀️', '⚔️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.72s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.19s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.81s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (4, 5, 8, 9):   4%|▍         | 4/100 [02:12<45:54, 28.70s/it]2025-01-22 04:02:49.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:02:49.758 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9677, 9683) --> . Daniel journeyed to the
2025-01-22 04:02:49.758 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 04:02:49.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11856, 11861) --> . Daniel went to the
2025-01-22 04:02:49.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 04:02:49.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19214, 19218) -->  left the apple.
2025-01-22 04:02:49.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the bedroom.
2025-01-22 04:02:50.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21471, 21476) --> . John moved to the
2025-01-22 04:02:50.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:02:50.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8534, 8540) --> . John went back to the
2025-01-22 04:02:50.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the office.
2025-01-22 04:02:50.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6338, 6344) -->  John journeyed to the office
2025-01-22 04:02:50.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:02:50.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19216, 19221) -->  apple. Mary moved to
2025-01-22 04:02:50.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary got the football there.
2025-01-22 04:02:50.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18626, 18631) --> . Mary got the football
2025-01-22 04:02:50.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John journeyed to the office.
2025-01-22 04:02:50.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6337, 6343) --> . John journeyed to the
2025-01-22 04:02:50.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the bedroom.
2025-01-22 04:02:50.371 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16287, 16293) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:02:53.316 | INFO     | test_jbb_embedding:begin_test:693 - The apple was on Daniel's left hand.<|eot_id|>
2025-01-22 04:02:53.316 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24191])
2025-01-22 04:03:01.727 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [282.11875, 11.3658164214268, 200.39981617647058, 10.875252433719966, 14.798239511986301], 'topk_indices': array([18610, 24089, 18537, 24045, 24174, 19222, 24176, 18528, 19216,
       24180, 24182, 18529, 19217, 24189, 24178,     0, 24179, 24181,
       24187, 24186]), 'topk_tokens': ['G', ' location', 'oth', ' context', ':', ' bathroom', ' was', ' rapid', ' apple', ' the', '?', 'ity', '.', '<|end_header_id|>', ' apple', '<|begin_of_text|>', ' before', ' garden', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [219.33333333333334, 261.95, 672.875, 65.025]}, 'weight': {'score': [19.80234375, 23.440150450524925, 21.660386029411764, 23.445671085335544, 29.325128424657535], 'topk_indices': array([18782, 18826, 14643, 14607, 19432, 19490, 14664, 14700, 14506,
       14571, 20304, 20352, 18114, 18145, 23655, 23521, 21987, 21960,
       23603, 23737]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 19.665625, 20.421875, 18.88125]}, 'saliency': {'score': [1.476806640625, 0.06385768787263887, 1.135986328125, 0.06117701874158554, 0.10734474495665668], 'topk_indices': array([18510, 19214, 24184, 11861, 18610, 24176, 24175, 24182, 24089,
       18602, 24173, 18537, 24045, 18529, 19216, 19222, 18528, 24179,
       24178, 24181]), 'topk_tokens': [' one', ' left', 'Answer', ' garden', 'G', ' was', ' Where', '?', ' location', ' rapid', 'Question', 'oth', ' context', 'ity', ' apple', ' bathroom', ' rapid', ' before', ' apple', ' garden'], 'evidence_proportions': [1.2368570963541667, 1.4099609375, 3.372314453125, 0.315185546875]}}, 'pred_res': "The apple was on Daniel's left hand.<|eot_id|>", 'score': 0}
2025-01-22 04:03:01.738 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:03:01.738 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-6_pid-4_4-5-8-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (4, 5, 8, 9):   5%|▌         | 5/100 [02:24<46:03, 29.09s/it]Processing depth (4, 5, 8, 9):   5%|▌         | 5/100 [02:25<45:56, 29.01s/it]
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:82 - Selected idx: 7
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:86 - Needle: [' John journeyed to the office.', ' Daniel journeyed to the bathroom.', ' John went back to the bedroom.', ' Daniel grabbed the milk.', ' Mary moved to the bathroom.', ' Daniel went to the garden.', ' Sandra journeyed to the bedroom.', ' Mary got the football there.', ' Mary journeyed to the office.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-22 04:03:02.060 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel journeyed to the bathroom.', ' Daniel grabbed the milk.', ' Daniel went to the garden.', ' Daniel left the milk.', ' John moved to the bedroom.']
2025-01-22 04:03:02.061 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏌️\u200d♂️', '🏳\u200d⚧', '🇧🇪', '🏙️', '🚶🏽\u200d♂️\u200d➡', '🖕', '📄', '👏🏾', '🧕🏽', '🧑🏻\u200d🤝\u200d🧑🏽']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.34s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.57s/it]
Processing depth (0, 3, 4, 6, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:03:18.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:03:18.461 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 04:03:18.461 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:03:18.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7559, 7563) -->  Daniel grabbed the milk
2025-01-22 04:03:18.504 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:03:18.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9700, 9705) -->  war. Daniel went to
2025-01-22 04:03:18.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:03:18.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14356, 14360) -->  Daniel left the milk
2025-01-22 04:03:18.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:03:18.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16801, 16806) --> . John moved to the
2025-01-22 04:03:18.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the office.
2025-01-22 04:03:18.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (966, 972) --> . John journeyed to the
2025-01-22 04:03:18.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 04:03:18.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17193, 17199) --> . John went back to the
2025-01-22 04:03:18.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:03:18.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10107, 10112) --> . Mary moved to the
2025-01-22 04:03:18.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 04:03:18.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2887, 2893) --> . Sandra journeyed to the
2025-01-22 04:03:18.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary got the football there.
2025-01-22 04:03:18.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15772, 15777) --> . Mary got the football
2025-01-22 04:03:18.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:03:18.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (967, 973) -->  John journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:03:21.647 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:03:21.647 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24207])
2025-01-22 04:03:29.988 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [215.650390625, 11.878748451053283, 82.44404871323529, 11.576920874824031, 7.636586782094595], 'topk_indices': array([ 7351,    31, 14359, 24096,  7453,  7563, 24086, 22870,  7418,
        7436,  7463,    24, 24205,  7419, 24202,  7464,  7465,  7420,
           0, 24203]), 'topk_tokens': ['nes', ' journey', ' milk', ' location', '\n', '.', '.\n\n', 'nes', ' the', '.\n', ' the', '\n\n', '<|end_header_id|>', ' Min', '<|eot_id|>', ' Min', 'nes', 'nes', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [315.6041666666667, 310.6875, 135.55, 303.4375, 29.546875]}, 'weight': {'score': [23.107747395833332, 23.44194547707559, 21.031479779411764, 23.445670907171248, 29.56440033783784], 'topk_indices': array([18788, 18832, 14638, 14674, 14719, 19485, 19427, 14683, 14602,
       14537, 20347, 20299, 18157, 18126, 23690, 23556, 22022, 21995,
       23638, 23772]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 28.263671875, 23.1296875, 26.462890625, 18.88125]}, 'saliency': {'score': [1.3980662027994792, 0.06700796941152933, 0.45448841768152576, 0.06513981220539772, 0.05708044928473395], 'topk_indices': array([ 2893, 24186, 16795,    30, 16756,    24,  7562, 24192,  7351,
       24096, 24194, 22870, 14359,    31, 24180,    35,  7419,  7465,
        7464,  7420]), 'topk_tokens': [' bedroom', ' prior', 'present', 'Daniel', 'present', '\n\n', ' milk', ' milk', 'nes', ' location', ' discarded', 'nes', ' milk', ' journey', 'Question', ' bathroom', ' Min', 'nes', ' Min', 'nes'], 'evidence_proportions': [1.9123942057291667, 2.3267822265625, 0.74248046875, 2.072509765625, 0.1539306640625]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:03:29.993 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:03:29.993 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-0_0-3-4-6-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 3, 4, 6, 7):   1%|          | 1/100 [00:27<45:51, 27.80s/it]is_0k: False
your chose emoji: ['🕚', '🌌', '🦸🏽', '🙇🏾\u200d♂️', '🧑🏼\u200d❤\u200d💋\u200d🧑🏾', '🤽🏻\u200d♀️', '👩\u200d🦼\u200d➡', '👸🏽', '👩\u200d🔧', '🍗']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.19s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.51s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]
Processing depth (1, 4, 6, 7, 8):   1%|          | 1/100 [00:44<45:51, 27.80s/it]2025-01-22 04:03:47.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:03:47.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2875, 2881) --> . Daniel journeyed to the
2025-01-22 04:03:47.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:03:47.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9697, 9701) -->  Daniel grabbed the milk
2025-01-22 04:03:47.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:03:47.485 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14357, 14362) --> . Daniel went to the
2025-01-22 04:03:47.485 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:03:47.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16769, 16773) -->  Daniel left the milk
2025-01-22 04:03:47.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:03:47.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19255, 19260) -->  John moved to the bedroom
2025-01-22 04:03:47.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the office.
2025-01-22 04:03:47.674 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (832, 838) -->  John journeyed to the office
2025-01-22 04:03:47.674 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 04:03:47.767 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17159, 17165) --> . John went back to the
2025-01-22 04:03:47.767 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:03:47.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10105, 10110) --> . Mary moved to the
2025-01-22 04:03:47.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 04:03:47.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2806, 2812) --> . Sandra journeyed to the
2025-01-22 04:03:47.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary got the football there.
2025-01-22 04:03:47.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15781, 15786) --> . Mary got the football
2025-01-22 04:03:47.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:03:47.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (833, 839) -->  journeyed to the office.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:03:50.575 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:03:50.575 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 04:03:58.918 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [169.30208333333334, 7.321318586857757, 60.759880514705884, 7.085148616688747, 9.835859375], 'topk_indices': array([    1, 24181, 14362, 24128, 24088, 24098,  9674,     3, 16772,
          14, 24194,  9700,  2881,    23,  9701,    24, 24207, 24205,
           0, 24204]), 'topk_tokens': ['<|start_header_id|>', '.\n\n', ' garden', ' the', '.\n\n', ' location', ' company', '<|end_header_id|>', ' milk', '\n', ' milk', ' milk', ' bathroom', '4', '.', '\n\n', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [147.51041666666666, 292.15625, 193.95, 220.59375, 31.4875]}, 'weight': {'score': [23.349934895833332, 23.444312221212623, 21.053998161764707, 23.447770684979712, 29.875625], 'topk_indices': array([18764, 18808, 14601, 14637, 14646, 19409, 19467, 14682, 14565,
       14500, 20281, 20329, 18123, 18092, 23542, 23676, 21952, 21925,
       23624, 23758]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 28.263671875, 19.665625, 26.462890625, 24.3078125]}, 'saliency': {'score': [1.0866190592447917, 0.04137954195136967, 0.33289426915785847, 0.03993062048328725, 0.07391855875651042], 'topk_indices': array([24088,  2876, 24202, 24196,    20, 24206, 24188,    23, 24054,
       14358,    24,  9698,  9697, 24098, 14362,  9674, 16772,  9700,
       24194,  2881]), 'topk_tokens': ['.\n\n', ' Daniel', 'Answer', ' discarded', ' Jul', 'assistant', ' prior', '4', ' context', ' Daniel', '\n\n', ' grabbed', ' Daniel', ' location', ' garden', ' company', ' milk', ' milk', ' milk', ' bathroom'], 'evidence_proportions': [0.8372395833333334, 2.184814453125, 1.0765625, 1.5078125, 0.180419921875]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:03:58.924 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:03:58.925 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-1_1-4-6-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (1, 4, 6, 7, 8):   2%|▏         | 2/100 [00:56<46:29, 28.46s/it]is_0k: False
your chose emoji: ['🏃🏻\u200d♀', '😵', '🧑🏼\u200d🦽\u200d➡️', '💂🏻\u200d♀', '🏴\U000e0067\U000e0062\U000e0065\U000e006e\U000e0067\U000e007f', '💂🏼', '💘', '㊙', '🦬', '🧑🏾\u200d⚕️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.20s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (1, 3, 4, 5, 9):   2%|▏         | 2/100 [01:14<46:29, 28.46s/it]2025-01-22 04:04:17.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:04:17.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2968, 2974) -->  tragedy. Daniel journeyed to
2025-01-22 04:04:17.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:04:17.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7530, 7534) -->  grabbed the milk.
2025-01-22 04:04:17.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:04:17.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9687, 9692) --> . Daniel went to the
2025-01-22 04:04:17.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:04:17.232 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11812, 11816) -->  Daniel left the milk
2025-01-22 04:04:17.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:04:17.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21464, 21469) --> . John moved to the
2025-01-22 04:04:17.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the office.
2025-01-22 04:04:17.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (959, 965) --> . John journeyed to the
2025-01-22 04:04:17.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 04:04:17.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17157, 17163) --> . John went back to the
2025-01-22 04:04:17.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:04:17.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10075, 10080) --> . Mary moved to the
2025-01-22 04:04:17.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 04:04:17.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2904, 2910) --> . Sandra journeyed to the
2025-01-22 04:04:17.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary got the football there.
2025-01-22 04:04:17.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15742, 15747) --> . Mary got the football
2025-01-22 04:04:17.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:04:17.602 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (960, 966) -->  John journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:04:20.249 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:04:20.249 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24183])
2025-01-22 04:04:28.635 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [297.3828125, 12.476497250475482, 139.64246323529412, 12.01390630180703, 38.15172946068548], 'topk_indices': array([24072,  9691,  7530,  9687,  9689,  2911,  2910,    14, 24062,
        9690,  7533,  9665,  7532,    24, 24181,  2975,     0,  9692,
       24178, 24179]), 'topk_tokens': [' location', ' the', ' grabbed', '.', ' went', '.', ' bedroom', '\n', '.\n\n', ' to', '.', ' company', ' milk', '\n\n', '<|end_header_id|>', ' bathroom', '<|begin_of_text|>', ' garden', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [262.8229166666667, 491.5625, 504.4, 195.609375, 57.9125]}, 'weight': {'score': [22.410807291666668, 23.43479698999421, 21.031479779411764, 23.43920218470242, 29.368195564516128], 'topk_indices': array([18796, 18752, 14608, 14644, 19449, 14653, 19391, 14689, 14507,
       14572, 20281, 20329, 18121, 18090, 23660, 23526, 21947, 21974,
       23742, 23608]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.110677083333332, 22.15234375, 19.665625, 26.462890625, 18.88125]}, 'saliency': {'score': [1.6617813110351562, 0.07098486081824196, 0.7714915556066176, 0.06841538376137812, 0.2843008964292465], 'topk_indices': array([ 2913, 24028, 16726, 24170, 24156, 24072,    24,  2905,  2970,
        9689, 24168,  7529, 24162,  9688,  7530,  9665,  2910,  7532,
        2975,  9692]), 'topk_tokens': ['ible', ' context', 'present', ' discarded', 'Question', ' location', '\n\n', ' Sandra', ' Daniel', ' went', ' milk', ' Daniel', ' prior', ' Daniel', ' grabbed', ' company', ' bedroom', ' milk', ' bathroom', ' garden'], 'evidence_proportions': [1.5843912760416667, 2.8355712890625, 2.5123046875, 1.2568359375, 0.28905029296875]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:04:28.654 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:04:28.655 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-2_1-3-4-5-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 3, 4, 5, 9):   3%|▎         | 3/100 [01:26<46:57, 29.04s/it]is_0k: False
your chose emoji: ['💆🏿\u200d♀', '🪶', '🧝🏾\u200d♀', '🚶🏾\u200d♀️', '🏃🏿\u200d➡', '🧔', '🇧🇦', '⚠', '💇🏿\u200d♂', '🧔🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.07s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.49s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.49s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]
Processing depth (1, 3, 5, 6, 7):   3%|▎         | 3/100 [01:44<46:57, 29.04s/it]2025-01-22 04:04:47.125 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:04:47.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2947, 2953) --> . Daniel journeyed to the
2025-01-22 04:04:47.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:04:47.185 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7581, 7585) -->  Daniel grabbed the milk
2025-01-22 04:04:47.186 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:04:47.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11957, 11962) --> . Daniel went to the
2025-01-22 04:04:47.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:04:47.327 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14488, 14492) -->  Daniel left the milk
2025-01-22 04:04:47.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:04:47.411 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16878, 16883) -->  affair. John moved to
2025-01-22 04:04:47.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the office.
2025-01-22 04:04:47.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (901, 907) --> . John journeyed to the
2025-01-22 04:04:47.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 04:04:47.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17282, 17288) -->  John went back to the bedroom
2025-01-22 04:04:47.507 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:04:47.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10235, 10240) --> . Mary moved to the
2025-01-22 04:04:47.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 04:04:47.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2884, 2890) --> . Sandra journeyed to the
2025-01-22 04:04:47.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary got the football there.
2025-01-22 04:04:47.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15935, 15940) -->  Mary got the football there
2025-01-22 04:04:47.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:04:47.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (902, 908) -->  John journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:04:50.332 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:04:50.332 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24253])
2025-01-22 04:04:58.726 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [134.62386067708334, 7.782734168865435, 78.1861213235294, 7.558008894071824, 9.954776095360824], 'topk_indices': array([   19,     4,     9, 24252, 24225, 24227, 24247, 24172,     3,
           1, 24132, 24142, 24098,    14,  7844,    24,    23, 24248,
       24251,     0]), 'topk_tokens': ['26', '\n\n', ':', '\n\n', '.\n\n', ':', ':', ' the', '<|end_header_id|>', '<|start_header_id|>', '.\n\n', ' location', ' context', '\n', ' Square', '\n\n', '4', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [143.53125, 248.03125, 80.39375, 198.6875, 36.18828125]}, 'weight': {'score': [23.257486979166668, 23.456095914412927, 22.27734375, 23.457949133192827, 29.890463917525775], 'topk_indices': array([18888, 18844, 14690, 14654, 14735, 14699, 19483, 19541, 14618,
       14553, 20421, 20373, 18187, 18156, 23596, 23730, 22062, 22035,
       23678, 23812]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.270833333333332, 28.263671875, 19.665625, 26.462890625, 23.8640625]}, 'saliency': {'score': [0.8725026448567709, 0.04410365263201631, 0.42772898954503674, 0.0427430097165507, 0.07291837082695715], 'topk_indices': array([24238, 24132,  7843, 24096,  7582, 24246,    19,  2890,  8031,
        2953,  7584,    20, 24232, 24226,    24,    23, 24142, 24240,
       24098,  7844]), 'topk_tokens': [' milk', '.\n\n', 'Bridge', ' provided', ' grabbed', 'Answer', '26', ' bedroom', 'circ', ' bathroom', ' milk', ' Jul', ' prior', 'Question', '\n\n', '4', ' location', ' discarded', ' context', ' Square'], 'evidence_proportions': [0.81976318359375, 1.865234375, 0.4317138671875, 1.335693359375, 0.2118408203125]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:04:58.734 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:04:58.734 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-3_1-3-5-6-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 3, 5, 6, 7):   4%|▍         | 4/100 [01:56<47:07, 29.45s/it]is_0k: False
your chose emoji: ['👩🏼\u200d❤\u200d💋\u200d👨🏻', '🙎\u200d♂️', '🧓', '👱🏽\u200d♂', '🧑🏿\u200d❤️\u200d💋\u200d🧑🏾', '🤹🏾\u200d♀️', '🚴🏼\u200d♂️', '🫀', '👩🏻\u200d🦽\u200d➡', '🧝🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.50s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.35s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]
Processing depth (1, 2, 4, 8, 9):   4%|▍         | 4/100 [02:12<47:07, 29.45s/it]2025-01-22 04:05:15.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the bathroom.
2025-01-22 04:05:15.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2988, 2994) -->  tragedy. Daniel journeyed to
2025-01-22 04:05:15.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 04:05:15.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4939, 4943) -->  Daniel grabbed the milk
2025-01-22 04:05:15.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 04:05:15.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9690, 9695) -->  war. Daniel went to
2025-01-22 04:05:15.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 04:05:15.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19460, 19464) -->  Daniel left the milk
2025-01-22 04:05:15.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:05:15.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21512, 21517) --> . John moved to the
2025-01-22 04:05:15.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the office.
2025-01-22 04:05:15.379 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (921, 927) --> . John journeyed to the
2025-01-22 04:05:15.379 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the bedroom.
2025-01-22 04:05:15.475 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17219, 17225) -->  John went back to the bedroom
2025-01-22 04:05:15.475 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bathroom.
2025-01-22 04:05:15.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10097, 10102) --> . Mary moved to the
2025-01-22 04:05:15.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 04:05:15.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2924, 2930) --> . Sandra journeyed to the
2025-01-22 04:05:15.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary got the football there.
2025-01-22 04:05:15.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15791, 15796) --> . Mary got the football
2025-01-22 04:05:15.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:05:15.637 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (922, 928) -->  John journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:05:18.293 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 04:05:18.294 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24223])
2025-01-22 04:05:26.708 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [266.0130208333333, 10.58255593164369, 121.65073529411765, 10.172648233200928, 12.224133003048781], 'topk_indices': array([19464,    14, 24068,  4940,  4943,    24, 24222, 24217, 24195,
       19463,  2990,  4942,    23,  2995,     1, 24219, 24221,  4939,
           0, 24218]), 'topk_tokens': ['.', '\n', ' context', ' grabbed', '.', '\n\n', '\n\n', ':', '.\n\n', ' milk', ' Daniel', ' milk', '4', ' bathroom', '<|start_header_id|>', '<|start_header_id|>', '<|end_header_id|>', ' Daniel', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [262.3645833333333, 579.46875, 163.7875, 354.46875, 51.0875]}, 'weight': {'score': [24.151041666666668, 23.444576591265584, 21.829503676470587, 23.446147152226086, 29.365853658536587], 'topk_indices': array([18755, 18799, 14647, 14611, 14656, 14692, 19452, 19394, 14575,
       14510, 20271, 20319, 18124, 18093, 23572, 23706, 22022, 21995,
       23654, 23788]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.110677083333332, 28.263671875, 23.1296875, 26.462890625, 18.88125]}, 'saliency': {'score': [1.848876953125, 0.06040842854139148, 0.7093685374540442, 0.05771942290199075, 0.08975201118283155], 'topk_indices': array([24196,  9692, 24210,  4897, 24195,    23, 24202,     0,  2930,
       10102, 24220, 24068, 24216, 24208, 19463,  4940,  4942,  2990,
        2995,  4939]), 'topk_tokens': ['Question', ' Daniel', ' discarded', ' Buchanan', '.\n\n', '4', ' prior', '<|begin_of_text|>', ' bedroom', ' bathroom', 'assistant', ' context', 'Answer', ' milk', ' milk', ' grabbed', ' milk', ' Daniel', ' bathroom', ' Daniel'], 'evidence_proportions': [1.69189453125, 4.5338134765625, 1.03017578125, 2.4249267578125, 0.24716796875]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:05:26.716 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:05:26.716 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-7_pid-4_1-2-4-8-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (1, 2, 4, 8, 9):   5%|▌         | 5/100 [02:24<45:47, 28.92s/it]Processing depth (1, 2, 4, 8, 9):   5%|▌         | 5/100 [02:24<45:50, 28.95s/it]
2025-01-22 04:05:26.966 | INFO     | __main__:<module>:82 - Selected idx: 8
2025-01-22 04:05:26.966 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the kitchen? 
2025-01-22 04:05:26.966 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:05:26.967 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:05:26.967 | INFO     | __main__:<module>:86 - Needle: [' Mary journeyed to the office.', ' Daniel went back to the kitchen.', ' John moved to the bedroom.', ' Sandra picked up the milk.', ' Mary moved to the bathroom.', ' John went back to the bedroom.', ' John journeyed to the office.', ' Sandra travelled to the hallway.', ' Mary got the football there.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-22 04:05:26.967 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra picked up the milk.', ' Sandra travelled to the hallway.', ' Sandra went to the kitchen.', ' Daniel went back to the hallway.']
2025-01-22 04:05:26.967 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🤸', '🏊🏿\u200d♀️', '🚽', '👩🏽\u200d🤝\u200d👨🏾', '💇🏻\u200d♂', '🧎🏽\u200d♂️\u200d➡', '🫖', '👋🏾', '👩🏼\u200d🤝\u200d👨🏽', '🚻']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.62s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (0, 1, 4, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:05:43.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 04:05:43.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-22 04:05:43.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 04:05:43.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3051, 3056) --> . Sandra travelled to the
2025-01-22 04:05:43.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:05:44.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7762, 7767) -->  back to the kitchen.
2025-01-22 04:05:44.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 04:05:44.059 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7758, 7764) -->  city. Daniel went back to
2025-01-22 04:05:44.059 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 04:05:44.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (602, 608) --> . Mary journeyed to the
2025-01-22 04:05:44.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:05:44.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7758, 7764) -->  city. Daniel went back to
2025-01-22 04:05:44.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the bedroom.
2025-01-22 04:05:44.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8450, 8455) --> . John moved to the
2025-01-22 04:05:44.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:05:44.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (190, 195) --> . Mary moved to the
2025-01-22 04:05:44.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:05:44.217 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12378, 12384) --> . John went back to the
2025-01-22 04:05:44.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the office.
2025-01-22 04:05:44.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (603, 609) -->  Mary journeyed to the office
2025-01-22 04:05:44.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary got the football there.
2025-01-22 04:05:44.340 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (22290, 22295) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:05:47.044 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 04:05:47.045 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 04:05:55.432 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [238.25074404761904, 10.743736313114619, 86.2363782051282, 10.423884594275536, 10.136477953767123], 'topk_indices': array([   23, 24182, 24188,   105, 13516,    66,     1,    36,    70,
          14,    24,    67, 24187,    68, 24197,   106,    69, 24194,
           0, 24195]), 'topk_tokens': ['4', ':', ' the', ' P', 'nes', ' OF', '<|start_header_id|>', '.', 'ER', '\n', '\n\n', ' P', ' before', 'ION', '<|end_header_id|>', 'ION', 'E', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [538.35, 172.15, 160.875, 107.73177083333333]}, 'weight': {'score': [21.85751488095238, 23.44143820758615, 21.34294871794872, 23.446205976617513, 29.35980308219178], 'topk_indices': array([18803, 18847, 14645, 14681, 19449, 19507, 14726, 14690, 14609,
       14544, 20369, 20321, 18172, 18141, 23679, 23545, 21992, 21965,
       23761, 23627]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.025, 22.15, 19.121875, 23.75390625]}, 'saliency': {'score': [1.2953287760416667, 0.060284586743218536, 0.4715779622395833, 0.05854585881597035, 0.07421629396203445], 'topk_indices': array([24183,    31,    66,  3056,    35,    32, 24189,    24,    70,
       24186, 13490,    65, 24181, 13516,     0,    67, 24187,    69,
          68,   106]), 'topk_tokens': [' Where', 'andra', ' OF', ' hallway', ' milk', ' picked', ' kitchen', '\n\n', 'ER', ' milk', 'nes', 'ENCES', 'Question', 'nes', '<|begin_of_text|>', ' P', ' before', 'E', 'ION', 'ION'], 'evidence_proportions': [2.755078125, 1.1025390625, 0.821240234375, 0.6346028645833334]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 04:05:55.437 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:05:55.438 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-0_0-1-4-8.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 1, 4, 8):   1%|          | 1/100 [00:28<46:41, 28.30s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d⚕️', '👩🏽\u200d❤\u200d💋\u200d👨🏿', '🙆🏽', '🛐', '🧢', '👩🏾\u200d✈', '✌🏾', '👫', '🧑\u200d🍼', '🧔🏼\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (3, 4, 5, 9):   1%|          | 1/100 [00:45<46:41, 28.30s/it]2025-01-22 04:06:13.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 04:06:13.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7514, 7519) --> . Sandra picked up the
2025-01-22 04:06:13.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 04:06:13.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9731, 9736) --> . Sandra travelled to the
2025-01-22 04:06:13.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:06:13.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7645, 7650) -->  back to the kitchen.
2025-01-22 04:06:13.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 04:06:13.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7642, 7648) --> . Daniel went back to the
2025-01-22 04:06:13.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 04:06:13.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (595, 601) --> . Mary journeyed to the
2025-01-22 04:06:13.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:06:13.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7642, 7648) --> . Daniel went back to the
2025-01-22 04:06:13.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the bedroom.
2025-01-22 04:06:13.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8383, 8388) --> . John moved to the
2025-01-22 04:06:13.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:06:13.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (183, 188) --> . Mary moved to the
2025-01-22 04:06:13.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:06:13.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12473, 12479) --> . John went back to the
2025-01-22 04:06:13.660 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the office.
2025-01-22 04:06:13.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (596, 602) -->  Mary journeyed to the office
2025-01-22 04:06:13.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary got the football there.
2025-01-22 04:06:13.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (22245, 22250) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:06:16.461 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:06:16.461 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 04:06:24.852 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [230.91666666666666, 9.8894816442606, 148.28445512820514, 9.473185776487663, 9.844539388020833], 'topk_indices': array([  189,  7519,   185, 24060, 24155,    23, 24166, 24070,  7520,
          24, 24159, 24160,    14, 24026,  7515, 24162,  9736,     0,
       24170, 24167]), 'topk_tokens': ['.', ' milk', ' moved', '.\n\n', ':', '4', ':', ' location', '.', '\n\n', ' milk', ' before', '\n', ' context', ' Sandra', ' kitchen', ' hallway', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [349.5, 212.825, 205.175, 168.625]}, 'weight': {'score': [21.399553571428573, 23.434581178903827, 20.908052884615383, 23.44043936605847, 29.20078125], 'topk_indices': array([18824, 18780, 14638, 14602, 14683, 19419, 14647, 19477, 14566,
       14501, 20291, 20339, 18119, 18088, 23650, 23516, 21942, 21969,
       23732, 23598]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.49375, 22.15, 19.121875, 20.927083333333332]}, 'saliency': {'score': [1.3496442522321428, 0.056298393872802485, 0.8000785632011218, 0.05396923402916364, 0.07551676432291667], 'topk_indices': array([   23, 24060,    24,  7516, 24141, 24024,  7648,   188, 24165,
       24154, 24070, 21422,  7519,   185, 24160, 24026, 24159, 24162,
        7515,  9736]), 'topk_tokens': ['4', '.\n\n', '\n\n', ' picked', ' return', ' provided', ' kitchen', ' bathroom', 'Answer', 'Question', ' location', ' hallway', ' milk', ' moved', ' before', ' context', ' milk', ' kitchen', ' Sandra', ' hallway'], 'evidence_proportions': [2.260546875, 1.2380859375, 1.049609375, 0.9335530598958334]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:06:24.859 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:06:24.860 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-1_3-4-5-9.pkl | len: 3 |  size: 2.02 KB
Processing depth (3, 4, 5, 9):   2%|▏         | 2/100 [00:57<47:18, 28.96s/it]is_0k: False
your chose emoji: ['👷🏻\u200d♀', '🌀', '👨\u200d🔬', '🇬🇬', '🧝🏽', '🙆🏻', '👆🏾', '🌝', '🦸🏿', '🏊🏻\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.51s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (0, 4, 5, 6):   2%|▏         | 2/100 [01:14<47:18, 28.96s/it]2025-01-22 04:06:41.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 04:06:41.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  picked up the milk.
2025-01-22 04:06:41.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 04:06:41.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9705, 9710) -->  war. Sandra travelled to
2025-01-22 04:06:41.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:06:41.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7578, 7583) -->  back to the kitchen.
2025-01-22 04:06:41.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 04:06:41.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7575, 7581) --> . Daniel went back to the
2025-01-22 04:06:41.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 04:06:41.739 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (602, 608) --> . Mary journeyed to the
2025-01-22 04:06:41.739 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:06:41.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7575, 7581) --> . Daniel went back to the
2025-01-22 04:06:41.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the bedroom.
2025-01-22 04:06:41.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8292, 8297) --> . John moved to the
2025-01-22 04:06:41.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:06:41.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (190, 195) --> . Mary moved to the
2025-01-22 04:06:41.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:06:41.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12366, 12372) --> . John went back to the
2025-01-22 04:06:41.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the office.
2025-01-22 04:06:41.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (603, 609) -->  Mary journeyed to the office
2025-01-22 04:06:41.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary got the football there.
2025-01-22 04:06:41.990 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (22292, 22297) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:06:44.752 | INFO     | test_jbb_embedding:begin_test:693 - The living room.<|eot_id|>
2025-01-22 04:06:44.752 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24197])
2025-01-22 04:06:53.175 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [180.57886904761904, 11.251301652892561, 85.19951923076923, 10.984530343827672, 11.25100368923611], 'topk_indices': array([24180,    23,   338,    65,    36,    14,   105,    66,    24,
         518, 24185,    70,    67, 24195,    68,   106, 24192,    69,
           0, 24193]), 'topk_tokens': [':', '4', ' century', 'ENCES', '.', '\n', ' P', ' OF', '\n\n', ' an', ' before', 'ER', ' P', '<|end_header_id|>', 'ION', 'ION', '<|eot_id|>', 'E', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [404.5, 93.59375, 138.25, 101.73958333333333]}, 'weight': {'score': [21.874627976190474, 23.444896694214876, 20.908052884615383, 23.450361174399337, 30.02300347222222], 'topk_indices': array([18856, 18812, 14688, 14652, 19451, 19509, 14697, 14733, 14616,
       14551, 20371, 20323, 18181, 18150, 23547, 23681, 21967, 21994,
       23763, 23629]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.025, 25.6140625, 19.121875, 20.927083333333332]}, 'saliency': {'score': [0.9628644670758929, 0.06333063803428461, 0.45188708183092946, 0.06192036828623395, 0.08659225040011936], 'topk_indices': array([   64,    53, 24187,   366, 24095,    24,   105,    66, 24184,
       24179,   518,   338,    65,     0,    70, 24185,    67,    69,
         106,    68]), 'topk_tokens': ['ISC', ' Gutenberg', ' kitchen', ' past', ' location', '\n\n', ' P', ' OF', ' milk', 'Question', ' an', ' century', 'ENCES', '<|begin_of_text|>', 'ER', ' before', ' P', 'E', 'ION', 'ION'], 'evidence_proportions': [2.0603515625, 0.63768310546875, 0.685986328125, 0.5500081380208334]}}, 'pred_res': 'The living room.<|eot_id|>', 'score': 0}
2025-01-22 04:06:53.179 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:06:53.180 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-2_0-4-5-6.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 4, 5, 6):   3%|▎         | 3/100 [01:26<46:20, 28.67s/it]is_0k: False
your chose emoji: ['😥', '👩🏿\u200d❤️\u200d👩🏼', '👩🏻\u200d❤\u200d💋\u200d👩🏽', '🤼\u200d♂', '🐋', '🗓️', '🦊', '🤙🏾', '😶\u200d🌫️', '🧗\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.76s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (1, 5, 7, 9):   3%|▎         | 3/100 [01:42<46:20, 28.67s/it]2025-01-22 04:07:10.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 04:07:10.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2975, 2980) -->  tragedy. Sandra picked up
2025-01-22 04:07:10.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 04:07:10.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11917, 11922) --> . Sandra travelled to the
2025-01-22 04:07:10.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:07:10.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7697, 7702) -->  back to the kitchen.
2025-01-22 04:07:10.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 04:07:10.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7694, 7700) --> . Daniel went back to the
2025-01-22 04:07:10.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 04:07:10.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (375, 381) --> . Mary journeyed to the
2025-01-22 04:07:10.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:07:10.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7694, 7700) --> . Daniel went back to the
2025-01-22 04:07:10.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the bedroom.
2025-01-22 04:07:10.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8413, 8418) --> . John moved to the
2025-01-22 04:07:10.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:07:10.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (183, 188) --> . Mary moved to the
2025-01-22 04:07:10.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:07:10.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12347, 12353) --> . John went back to the
2025-01-22 04:07:10.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the office.
2025-01-22 04:07:10.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (376, 382) -->  Mary journeyed to the office
2025-01-22 04:07:10.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary got the football there.
2025-01-22 04:07:10.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (22285, 22290) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:07:13.192 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:07:13.192 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 04:07:21.562 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [194.96130952380952, 10.538843952729225, 126.02163461538461, 10.191853330433702, 21.690095248287673], 'topk_indices': array([24185, 19092, 24192, 24179, 18961, 24181, 11922,    24, 24189,
          14, 18962, 24186, 24052, 24188, 24194, 18963, 18964, 24196,
           0, 24193]), 'topk_tokens': [' milk', ' in', ':', '.\n\n', '\n', ':', ' hallway', '\n\n', '?', '\n', 'the', ' before', ' context', ' kitchen', '<|start_header_id|>', ' manner', ' in', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [219.3, 156.2875, 241.05, 168.5]}, 'weight': {'score': [22.782366071428573, 23.441686293954795, 20.908052884615383, 23.446352936912305, 29.351669520547944], 'topk_indices': array([18864, 18820, 14648, 14684, 19459, 19517, 14729, 14693, 14547,
       14612, 20379, 20331, 18189, 18158, 23556, 23690, 22009, 21982,
       23638, 23772]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [29.3015625, 22.15, 19.121875, 20.927083333333332]}, 'saliency': {'score': [1.1399042038690477, 0.06009777569237841, 0.6687215169270834, 0.0581752264649287, 0.1653836394009525], 'topk_indices': array([  185,  2981, 18501,   188, 24050, 24189,  7700, 24191, 21462,
        2977,  7709, 24185, 24180, 18962, 18964, 24186, 24052, 11922,
       24188, 18963]), 'topk_tokens': [' moved', ' milk', ' Mr', ' bathroom', ' provided', '?', ' kitchen', 'Answer', ' hallway', ' Sandra', '�', ' milk', 'Question', 'the', ' in', ' before', ' context', ' hallway', ' kitchen', ' manner'], 'evidence_proportions': [1.549853515625, 0.907421875, 1.2361328125, 0.9118245442708334]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:07:21.569 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:07:21.569 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-3_1-5-7-9.pkl | len: 3 |  size: 2.01 KB
Processing depth (1, 5, 7, 9):   4%|▍         | 4/100 [01:54<45:41, 28.56s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d🤝\u200d🧑🏾', '🕹️', '🐈\u200d⬛', '🤔', '🧎🏿\u200d♂\u200d➡️', '🚶🏾\u200d♀\u200d➡️', '🇨🇵', '💸', '🏋🏾', '🦆']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (1, 4, 5, 8):   4%|▍         | 4/100 [02:11<45:41, 28.56s/it]2025-01-22 04:07:38.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra picked up the milk.
2025-01-22 04:07:38.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3044, 3049) --> . Sandra picked up the
2025-01-22 04:07:38.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra travelled to the hallway.
2025-01-22 04:07:38.508 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9743, 9748) --> . Sandra travelled to the
2025-01-22 04:07:38.508 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:07:38.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7687, 7692) -->  back to the kitchen.
2025-01-22 04:07:38.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the hallway.
2025-01-22 04:07:38.589 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7684, 7690) --> . Daniel went back to the
2025-01-22 04:07:38.589 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the office.
2025-01-22 04:07:38.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (595, 601) --> . Mary journeyed to the
2025-01-22 04:07:38.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:07:38.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7684, 7690) --> . Daniel went back to the
2025-01-22 04:07:38.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the bedroom.
2025-01-22 04:07:38.678 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8395, 8400) --> . John moved to the
2025-01-22 04:07:38.678 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bathroom.
2025-01-22 04:07:38.679 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (183, 188) --> . Mary moved to the
2025-01-22 04:07:38.679 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:07:38.744 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12479, 12485) --> . John went back to the
2025-01-22 04:07:38.744 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the office.
2025-01-22 04:07:38.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (596, 602) -->  Mary journeyed to the office
2025-01-22 04:07:38.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary got the football there.
2025-01-22 04:07:38.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (22229, 22234) --> . Mary got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:07:41.541 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:07:41.542 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24152])
2025-01-22 04:07:49.892 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [193.36904761904762, 13.480180086938523, 126.45713141025641, 13.140534213529778, 12.34673828125], 'topk_indices': array([ 7874,  9217,  7377,  7419,  7307,  9748,  7374,  7875,  7375,
        9218,  9219, 24147,  9276, 24150,  7420,  7876,     0,  7421,
        9277,  7376]), 'topk_tokens': ['�', ' in', 'ot', ' the', 'nes', ' hallway', ' the', '�', ' Min', ' a', ' few', '<|eot_id|>', ' in', '<|end_header_id|>', ' Min', '�', '<|begin_of_text|>', 'nes', ' a', 'nes'], 'evidence_proportions': [231.2, 187.775, 207.65, 154.60416666666666]}, 'weight': {'score': [21.399553571428573, 23.43130304284827, 20.908052884615383, 23.4371579295497, 29.5628125], 'topk_indices': array([18792, 18748, 14596, 14632, 19452, 14677, 19394, 14641, 14495,
       14560, 20314, 20266, 18113, 18082, 23500, 23634, 21926, 21953,
       23716, 23582]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.49375, 22.15, 19.121875, 20.927083333333332]}, 'saliency': {'score': [1.1164318266369047, 0.07678123605199492, 0.6808456029647436, 0.07489739406353756, 0.09384719848632812], 'topk_indices': array([ 7694,  7377,  9218, 24006,  9216,  9276,  9275, 24142,  7228,
        7875,  7307,  7874,  9277,  9748,  7375,  9219,  7876,  7420,
        7421,  7376]), 'topk_tokens': ['nes', 'ot', ' a', ' context', ' draft', ' in', ' draft', ' kitchen', 'nes', '�', 'nes', '�', ' a', ' hallway', ' Min', ' few', '�', ' Min', 'nes', 'nes'], 'evidence_proportions': [1.4568359375, 1.07275390625, 1.125439453125, 0.8616536458333334]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:07:49.899 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:07:49.899 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-8_pid-4_1-4-5-8.pkl | len: 3 |  size: 1.96 KB
Processing depth (1, 4, 5, 8):   5%|▌         | 5/100 [02:22<45:05, 28.48s/it]Processing depth (1, 4, 5, 8):   5%|▌         | 5/100 [02:23<45:20, 28.63s/it]
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:82 - Selected idx: 9
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:86 - Needle: [' Mary got the football there.', ' Sandra travelled to the hallway.', ' Mary moved to the bathroom.', ' Daniel went back to the kitchen.', ' Sandra picked up the milk.', ' John went back to the bedroom.', ' John moved to the bedroom.', ' Sandra went to the kitchen.', ' Mary journeyed to the office.', ' John journeyed to the office.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra travelled to the hallway.', ' Sandra picked up the milk.', ' Sandra went to the kitchen.', ' Sandra left the milk.', ' Daniel went back to the hallway.']
2025-01-22 04:07:50.295 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['✌', '💇🏿', '🪨', '🎿', '⌨️', '👩🏽\u200d💻', '🕵🏾', '🤷🏼\u200d♂', '💚', '🧍🏼\u200d♀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.27s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.70s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (0, 1, 2, 4, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:08:07.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 04:08:07.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-22 04:08:07.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:08:07.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2974, 2979) -->  tragedy. Sandra picked up
2025-01-22 04:08:07.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:08:07.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2389, 2394) -->  back to the kitchen.
2025-01-22 04:08:07.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the milk.
2025-01-22 04:08:07.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9759, 9763) -->  Sandra left the milk
2025-01-22 04:08:07.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 04:08:07.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2386, 2392) --> . Daniel went back to the
2025-01-22 04:08:07.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 04:08:07.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12191, 12196) --> . Mary got the football
2025-01-22 04:08:07.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 04:08:07.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1390, 1395) --> . Mary moved to the
2025-01-22 04:08:07.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 04:08:07.645 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2386, 2392) --> . Daniel went back to the
2025-01-22 04:08:07.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:08:07.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12851, 12857) --> . John went back to the
2025-01-22 04:08:07.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:08:07.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4071, 4076) --> . John moved to the
2025-01-22 04:08:07.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:08:07.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9023, 9029) --> . Mary journeyed to the
2025-01-22 04:08:07.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John journeyed to the office.
2025-01-22 04:08:07.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9024, 9030) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:08:10.457 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 04:08:10.457 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24239])
2025-01-22 04:08:18.835 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [333.6675, 15.999177563732365, 158.51041666666666, 15.440833350566631, 11.637456849563954], 'topk_indices': array([24218, 24221,    25,    14, 24223, 24158, 24128,  1396, 24219,
       24231,    32,  2981, 24118, 20753,    24, 14997,    35,     0,
       24234, 24235]), 'topk_tokens': [' prior', ' place', '<|eot_id|>', '\n', ' the', ' the', ' location', '.', ' to', '?\n', ' travelled', '.', '.\n\n', 'nes', '\n\n', ' it', ' hallway', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [621.15, 320.2125, 208.425, 400.125, 165.375]}, 'weight': {'score': [23.6125, 23.44864027307978, 20.908052884615383, 23.452568903031683, 29.525072674418606], 'topk_indices': array([18826, 18870, 14661, 14625, 14670, 14706, 19491, 19549, 14524,
       14589, 20363, 20411, 18195, 18164, 23708, 23574, 22040, 22013,
       23790, 23656]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 29.3015625, 19.121875, 27.876953125, 20.927083333333332]}, 'saliency': {'score': [2.1355078125, 0.08884995605831306, 0.8614721053685898, 0.08548744839704484, 0.08669201163358467], 'topk_indices': array([24222,    40,  4940, 24221,    24, 24118, 24224,    31,  9762,
        2980, 16821, 24128, 24218, 14997,  2976,  1395, 24226, 20753,
          32,    35]), 'topk_tokens': [' where', '\n\n\n', ' kitchen', ' place', '\n\n', '.\n\n', ' milk', 'andra', ' milk', ' milk', ' hallway', ' location', ' prior', ' it', ' Sandra', ' bathroom', ' discarded', 'nes', ' travelled', ' hallway'], 'evidence_proportions': [4.000390625, 2.28916015625, 1.071044921875, 2.797119140625, 0.8993733723958334]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:08:18.844 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:08:18.844 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-0_0-1-2-4-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 2, 4, 7):   1%|          | 1/100 [00:28<46:53, 28.42s/it]is_0k: False
your chose emoji: ['🦵🏽', '🪕', '👷🏿\u200d♂', '🧍🏻\u200d♂️', '👨\u200d🚀', '📏', '🧑🏾\u200d❤️\u200d💋\u200d🧑🏼', '🧑🏾\u200d🦯\u200d➡', '👊🏿', '🏌🏼\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (1, 2, 4, 5, 9):   1%|          | 1/100 [00:45<46:53, 28.42s/it]2025-01-22 04:08:35.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 04:08:35.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2930, 2935) --> . Sandra travelled to the
2025-01-22 04:08:35.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:08:35.896 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4888, 4893) --> . Sandra picked up the
2025-01-22 04:08:35.896 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:08:35.909 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2204, 2209) -->  back to the kitchen.
2025-01-22 04:08:35.909 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the milk.
2025-01-22 04:08:35.967 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11872, 11876) -->  Sandra left the milk
2025-01-22 04:08:35.967 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 04:08:35.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2201, 2207) --> . Daniel went back to the
2025-01-22 04:08:35.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 04:08:36.041 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12065, 12070) -->  Mary got the football there
2025-01-22 04:08:36.041 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 04:08:36.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1286, 1291) -->  Mary moved to the bathroom
2025-01-22 04:08:36.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 04:08:36.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2201, 2207) --> . Daniel went back to the
2025-01-22 04:08:36.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:08:36.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12719, 12725) -->  John went back to the bedroom
2025-01-22 04:08:36.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:08:36.147 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4038, 4043) --> . John moved to the
2025-01-22 04:08:36.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:08:36.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (8998, 9004) --> . Mary journeyed to the
2025-01-22 04:08:36.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John journeyed to the office.
2025-01-22 04:08:36.246 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8999, 9005) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:08:38.884 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:08:38.884 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24234])
2025-01-22 04:08:47.237 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [341.605, 11.356802100094896, 184.28125, 10.73626437554296, 20.335553850446427], 'topk_indices': array([ 4893,  9708, 24219, 24213, 11875, 24214,  4894,  1291,    14,
          24, 24230,  2903, 24232,  2932,  2930,  2902,  2935, 24229,
        2931,     0]), 'topk_tokens': [' milk', ' Sandra', ' milk', ' prior', ' milk', ' to', '.', '.', '\n', '\n\n', '<|start_header_id|>', ' During', '<|end_header_id|>', ' travelled', '.', '.', ' hallway', '<|eot_id|>', ' Sandra', '<|begin_of_text|>'], 'evidence_proportions': [652.2, 288.125, 234.525, 346.0625, 213.60416666666666]}, 'weight': {'score': [22.4359375, 23.446393943144777, 22.723557692307693, 23.448605171575725, 29.303013392857142], 'topk_indices': array([18842, 18798, 14680, 14644, 14725, 14689, 19437, 19495, 14543,
       14608, 20357, 20309, 18136, 18167, 23687, 23553, 21987, 21960,
       23769, 23635]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.15, 23.49375, 19.121875, 27.876953125, 20.927083333333332]}, 'saliency': {'score': [2.1473046875, 0.0645339360807469, 1.0894744090544872, 0.060726301644332, 0.14605340503510975], 'topk_indices': array([ 9000,  2207,  1287, 24216, 11872,  4893,  2901, 24219,  2929,
       11875,  4889, 24213,  1290, 21482, 24221,  9708,  2903,  2932,
        2935,  2931]), 'topk_tokens': [' journey', ' kitchen', ' moved', ' place', ' Sandra', ' milk', ' Gov', ' milk', ' Gov', ' milk', ' Sandra', ' prior', ' bathroom', ' hallway', ' discarded', ' Sandra', ' During', ' travelled', ' hallway', ' Sandra'], 'evidence_proportions': [4.2970703125, 1.81171875, 1.2283203125, 2.4915771484375, 1.1717936197916667]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:08:47.271 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:08:47.272 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-1_1-2-4-5-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 2, 4, 5, 9):   2%|▏         | 2/100 [00:56<46:25, 28.42s/it]is_0k: False
your chose emoji: ['🧮', '\U0001faf1🏼\u200d\U0001faf2🏽', '🤤', '🚴🏽\u200d♀', '💂', '🧑🏾\u200d⚖', '👇🏼', '🚣\u200d♀', '🧑🏻\u200d❤️\u200d🧑🏼', '👨🏽\u200d❤\u200d💋\u200d👨🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.96s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.36s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.26s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.01s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]
Processing depth (0, 1, 2, 5, 8):   2%|▏         | 2/100 [01:12<46:25, 28.42s/it]2025-01-22 04:09:03.562 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 04:09:03.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-22 04:09:03.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:09:03.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2974, 2979) -->  tragedy. Sandra picked up
2025-01-22 04:09:03.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:09:03.592 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2389, 2394) -->  back to the kitchen.
2025-01-22 04:09:03.592 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the milk.
2025-01-22 04:09:03.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11877, 11881) -->  Sandra left the milk
2025-01-22 04:09:03.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 04:09:03.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2386, 2392) --> . Daniel went back to the
2025-01-22 04:09:03.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 04:09:03.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12101, 12106) --> . Mary got the football
2025-01-22 04:09:03.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 04:09:03.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1390, 1395) --> . Mary moved to the
2025-01-22 04:09:03.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 04:09:03.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2386, 2392) --> . Daniel went back to the
2025-01-22 04:09:03.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:09:03.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12673, 12679) --> . John went back to the
2025-01-22 04:09:03.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:09:03.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4055, 4060) --> . John moved to the
2025-01-22 04:09:03.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:09:03.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9035, 9041) --> . Mary journeyed to the
2025-01-22 04:09:03.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John journeyed to the office.
2025-01-22 04:09:03.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9036, 9042) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:09:06.573 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 04:09:06.579 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24235])
2025-01-22 04:09:14.955 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [274.50625, 13.435239860962126, 129.34935897435898, 12.978243670886076, 14.006568545386905], 'topk_indices': array([   40,    14,   452,   571,   453,    39, 24154, 24114,    36,
         548, 24215, 24124,   430,    35,    24,   569, 24230,   431,
           0, 24231]), 'topk_tokens': ['\n\n\n', '\n', ' em', ' per', 's', '***', ' the', '.\n\n', '.', 's', ' to', ' location', ' em', ' hallway', '\n\n', ' em', '<|eot_id|>', 's', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [587.9, 233.70625, 180.3, 255.625, 138.4375]}, 'weight': {'score': [23.6125, 23.447886583051407, 20.908052884615383, 23.451813867688426, 29.561383928571427], 'topk_indices': array([18895, 18851, 14709, 14745, 14790, 19555, 14754, 19497, 14608,
       14673, 20417, 20369, 18220, 18189, 23720, 23586, 22040, 22013,
       23802, 23668]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 29.3015625, 19.121875, 27.876953125, 20.927083333333332]}, 'saliency': {'score': [1.726611328125, 0.07595209001619357, 0.6931778345352564, 0.07324925701425085, 0.1016485123407273], 'topk_indices': array([24222, 24080,    40,   547,    24,   571,  2976,  4952,   126,
         452, 24214, 19287,  1395, 24124,    39,    32,   431,   430,
         569,    35]), 'topk_tokens': [' discarded', ' context', '\n\n\n', ' em', '\n\n', ' per', ' Sandra', ' kitchen', ' Daily', ' em', ' prior', ' hallway', ' bathroom', ' location', '***', ' travelled', 's', ' em', ' em', ' hallway'], 'evidence_proportions': [3.6615234375, 1.71259765625, 0.910205078125, 1.816162109375, 0.7465006510416666]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:09:14.962 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:09:14.963 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-2_0-1-2-5-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 1, 2, 5, 8):   3%|▎         | 3/100 [01:24<45:24, 28.09s/it]is_0k: False
your chose emoji: ['🙋🏽\u200d♂', '🧖🏻\u200d♀', '🤽\u200d♂', '🌋', '🧝🏽', '👩🏾\u200d🦼\u200d➡', '👩🏻\u200d❤️\u200d👩🏽', '👩🏻\u200d❤\u200d👨🏼', '👈🏽', '😗']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.59s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.37s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (0, 3, 4, 5, 6):   3%|▎         | 3/100 [01:41<45:24, 28.09s/it]2025-01-22 04:09:31.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 04:09:31.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-22 04:09:31.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:09:31.771 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7659, 7664) --> . Sandra picked up the
2025-01-22 04:09:31.771 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:09:31.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2415, 2420) -->  back to the kitchen.
2025-01-22 04:09:31.784 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the milk.
2025-01-22 04:09:31.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12031, 12035) -->  Sandra left the milk
2025-01-22 04:09:31.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 04:09:31.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2412, 2418) --> . Daniel went back to the
2025-01-22 04:09:31.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 04:09:31.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12315, 12320) --> . Mary got the football
2025-01-22 04:09:31.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 04:09:31.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1416, 1421) --> . Mary moved to the
2025-01-22 04:09:31.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 04:09:31.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2412, 2418) --> . Daniel went back to the
2025-01-22 04:09:31.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:09:32.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12879, 12885) --> . John went back to the
2025-01-22 04:09:32.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:09:32.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4091, 4096) --> . John moved to the
2025-01-22 04:09:32.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:09:32.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9045, 9051) --> . Mary journeyed to the
2025-01-22 04:09:32.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John journeyed to the office.
2025-01-22 04:09:32.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9046, 9052) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:09:34.776 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 04:09:34.776 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24247])
2025-01-22 04:09:44.161 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [252.565, 15.075144329896908, 115.93429487179488, 14.667025862068966, 10.425645616319445], 'topk_indices': array([ 9242,    35, 14489, 10843, 10911, 10844, 10910, 10889, 10908,
        9349,  9243,  9301,  9244, 10890,  9302, 10912,     0, 24242,
       10913, 24243]), 'topk_tokens': [' in', ' hallway', ' hallway', 'walk', ' rapidly', 'ed', ' as', '\n', ' I', ' cause', ' a', ' in', ' few', 'walk', ' a', ' as', '<|begin_of_text|>', '<|eot_id|>', ' I', '<|start_header_id|>'], 'evidence_proportions': [453.3, 306.475, 190.075, 172.6875, 145.6875]}, 'weight': {'score': [22.4509375, 23.451012886597937, 20.908052884615383, 23.456147151244522, 29.574826388888887], 'topk_indices': array([18894, 18850, 14652, 14688, 14697, 14733, 19513, 19571, 14616,
       14551, 20385, 20433, 18188, 18219, 23730, 23596, 22029, 22056,
       23812, 23678]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 23.49375, 19.121875, 27.876953125, 20.927083333333332]}, 'saliency': {'score': [1.53763671875, 0.08458162854381443, 0.6279437725360577, 0.0822034965306311, 0.07422086927625868], 'topk_indices': array([24220, 10910, 10908, 10906, 10867,    32,  9396, 17581,  9837,
        7660, 10843,  9302,    35,  9349, 10911, 14489,  9244, 10912,
       10890, 10913]), 'topk_tokens': ['Question', ' as', ' I', 'few', ' could', ' travelled', 'did', 'ente', ' kitchen', ' Sandra', 'walk', ' a', ' hallway', ' cause', ' rapidly', ' hallway', ' few', ' as', 'walk', ' I'], 'evidence_proportions': [2.803515625, 2.01337890625, 0.98115234375, 1.1546630859375, 0.8053385416666666]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:09:44.168 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:09:44.168 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-3_0-3-4-5-6.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 3, 4, 5, 6):   4%|▍         | 4/100 [01:53<45:38, 28.53s/it]is_0k: False
your chose emoji: ['🧓🏽', '🧎🏻\u200d♂️\u200d➡', '🏃🏼\u200d♂\u200d➡', '\U0001faf7🏻', '👨🏿\u200d❤️\u200d👨🏼', '🕞', '👩🏼\u200d❤\u200d💋\u200d👨🏾', '👨🏽\u200d🔧', '🍠', '🌻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.26s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.69s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (0, 1, 2, 3, 7):   4%|▍         | 4/100 [02:10<45:38, 28.53s/it]2025-01-22 04:10:01.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra travelled to the hallway.
2025-01-22 04:10:01.287 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  travelled to the hallway.
2025-01-22 04:10:01.287 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:10:01.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2986, 2991) -->  tragedy. Sandra picked up
2025-01-22 04:10:01.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the kitchen.
2025-01-22 04:10:01.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2401, 2406) -->  back to the kitchen.
2025-01-22 04:10:01.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the milk.
2025-01-22 04:10:01.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7616, 7620) -->  Sandra left the milk
2025-01-22 04:10:01.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the hallway.
2025-01-22 04:10:01.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2398, 2404) --> . Daniel went back to the
2025-01-22 04:10:01.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football there.
2025-01-22 04:10:01.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12050, 12055) -->  Mary got the football there
2025-01-22 04:10:01.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary moved to the bathroom.
2025-01-22 04:10:01.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1402, 1407) --> . Mary moved to the
2025-01-22 04:10:01.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the kitchen.
2025-01-22 04:10:01.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2398, 2404) --> . Daniel went back to the
2025-01-22 04:10:01.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:10:01.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12607, 12613) --> . John went back to the
2025-01-22 04:10:01.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the bedroom.
2025-01-22 04:10:01.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4083, 4088) --> . John moved to the
2025-01-22 04:10:01.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary journeyed to the office.
2025-01-22 04:10:01.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9026, 9032) --> . Mary journeyed to the
2025-01-22 04:10:01.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John journeyed to the office.
2025-01-22 04:10:01.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9027, 9033) -->  Mary journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:10:04.329 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:10:04.329 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24145])
2025-01-22 04:10:12.704 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [263.375, 11.712108663243333, 110.45032051282051, 11.290984159608039, 55.2412359775641], 'topk_indices': array([ 4938,  2993, 24034, 24125,     9,    36,    88, 24024,  1408,
          35,    14,   124, 16711,    24, 24143,    86,    87, 24140,
       24141,     0]), 'topk_tokens': [' kitchen', '.', ' location', ' to', ':', '.', 'ER', '.\n\n', '.', ' hallway', '\n', 'ION', ' hallway', '\n\n', '<|end_header_id|>', 'ION', 'E', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [465.1, 218.9, 210.0875, 269.09375, 172.92708333333334]}, 'weight': {'score': [23.6125, 23.426733062779526, 21.298477564102566, 23.429986583416376, 30.06690705128205], 'topk_indices': array([18808, 18764, 14633, 14669, 14714, 14678, 19403, 19461, 14597,
       14532, 20323, 20275, 18102, 18133, 23494, 23628, 21933, 21960,
       23710, 23576]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 29.3015625, 19.121875, 27.876953125, 20.927083333333332]}, 'saliency': {'score': [1.611767578125, 0.06628810243731365, 0.5903680263421475, 0.06383518332402346, 0.41521903796073717], 'topk_indices': array([   37,    88, 24127,  2988, 24024,  2992,  4934, 24130,    24,
       24034, 24132, 24124,  4938,    32,   124,  1407,    87,    86,
          35, 16711]), 'topk_tokens': ['�', 'ER', ' place', ' Sandra', '.\n\n', ' milk', ' Sandra', ' milk', '\n\n', ' location', ' discarded', ' prior', ' kitchen', ' travelled', 'ION', ' bathroom', 'E', 'ION', ' hallway', ' hallway'], 'evidence_proportions': [2.810546875, 1.548681640625, 1.011767578125, 1.94873046875, 0.9407145182291666]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:10:12.709 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:10:12.709 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-9_pid-4_0-1-2-3-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 1, 2, 3, 7):   5%|▌         | 5/100 [02:22<45:10, 28.53s/it]Processing depth (0, 1, 2, 3, 7):   5%|▌         | 5/100 [02:22<45:09, 28.52s/it]
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:82 - Selected idx: 10
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:86 - Needle: [' Daniel picked up the apple.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' John went back to the office.', ' Daniel took the football.', ' Sandra moved to the kitchen.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' Mary left the apple.', ' Daniel dropped the football.']
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel dropped the football.']
2025-01-22 04:10:13.022 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['*⃣', '🌵', '🦂', '🧎\u200d♀️\u200d➡', '🧗\u200d♀', '🫐', '🇮🇴', '♌', '🔋', '🟤']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.59s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (0, 2, 3, 6):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 04:10:30.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:10:30.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:10:30.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:10:30.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5003, 5009) --> . Mary journeyed to the
2025-01-22 04:10:30.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:10:30.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7609, 7613) -->  Mary left the apple
2025-01-22 04:10:30.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 04:10:30.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14369, 14373) -->  Daniel dropped the football
2025-01-22 04:10:30.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:10:30.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11136, 11141) --> . Daniel picked up the
2025-01-22 04:10:30.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the garden.
2025-01-22 04:10:30.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7526, 7531) --> . John moved to the
2025-01-22 04:10:30.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:10:30.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15957, 15963) --> . John went back to the
2025-01-22 04:10:30.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:10:30.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3599, 3603) -->  Daniel took the football
2025-01-22 04:10:30.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:10:30.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21987, 21992) -->  state. Sandra moved to
2025-01-22 04:10:30.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:10:30.897 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (758, 762) -->  Daniel left the apple
2025-01-22 04:10:30.897 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 04:10:30.951 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10217, 10223) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:10:33.928 | INFO     | test_jbb_embedding:begin_test:693 - Mary moved to the bathroom. PAUL***<|eot_id|>
2025-01-22 04:10:33.928 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24160])
2025-01-22 04:10:42.327 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [685.5370065789474, 18.947256652733518, 177.12287946428572, 18.19229576480775, 21.58521792763158], 'topk_indices': array([    1, 24146, 24152, 24142,  5004,    24, 24151, 24150, 24145,
          14,    35, 24144, 24143, 24147,    34,  5009, 24148,     0,
       24156, 24155]), 'topk_tokens': ['<|start_header_id|>', ' the', ' \n', 'Question', ' Mary', '\n\n', '?', ' bedroom', ' was', '\n', '.', ' Where', ':', ' apple', ' bathroom', ' bedroom', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [1238.6, 790.25, 487.5, 35.17578125]}, 'weight': {'score': [23.055098684210527, 23.4326139138352, 22.50870535714286, 23.434252703865777, 29.145559210526315], 'topk_indices': array([18760, 18804, 14611, 14647, 14692, 14656, 19399, 19457, 14510,
       14575, 20319, 20271, 18098, 18129, 23520, 23654, 21974, 21947,
       23736, 23602]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 20.052083333333332, 26.205078125, 27.341796875]}, 'saliency': {'score': [4.020636307565789, 0.10542675817624053, 1.0704158238002233, 0.10094031748043229, 0.16476721512643913], 'topk_indices': array([   27, 24153, 24141,    24,    30,  7612,   105, 24143, 24152,
       24151, 24145,    31, 24142,  5004, 24144, 24150, 24147, 24148,
          34,  5009]), 'topk_tokens': ['user', 'Answer', '.\n\n', '\n\n', 'Mary', ' apple', 'ION', ':', ' \n', '?', ' was', ' moved', 'Question', ' Mary', ' Where', ' bedroom', ' apple', ' before', ' bathroom', ' bedroom'], 'evidence_proportions': [7.3041015625, 4.315755208333333, 3.2548828125, 0.2393798828125]}}, 'pred_res': 'Mary moved to the bathroom. PAUL***<|eot_id|>', 'score': 100}
2025-01-22 04:10:42.334 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:10:42.334 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-0_0-2-3-6.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 2, 3, 6):   1%|          | 1/100 [00:29<48:09, 29.19s/it]is_0k: False
your chose emoji: ['🎈', '🤴🏽', '🧕🏻', '🧛\u200d♂', '\U0001faf8🏿', '☀️', '🇵🇷', '📶', '👩🏼\u200d❤️\u200d👨🏽', '👸']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.74s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (0, 3, 5, 8):   1%|          | 1/100 [00:46<48:09, 29.19s/it]2025-01-22 04:10:59.918 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:10:59.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:10:59.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:10:59.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7561, 7567) --> . Mary journeyed to the
2025-01-22 04:10:59.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:11:00.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11896, 11900) -->  Mary left the apple
2025-01-22 04:11:00.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 04:11:00.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19269, 19273) -->  dropped the football.
2025-01-22 04:11:00.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:11:00.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11168, 11173) -->  paper. Daniel picked up
2025-01-22 04:11:00.174 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the garden.
2025-01-22 04:11:00.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7465, 7470) --> . John moved to the
2025-01-22 04:11:00.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:11:00.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15974, 15980) --> . John went back to the
2025-01-22 04:11:00.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:11:00.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3589, 3593) -->  Daniel took the football
2025-01-22 04:11:00.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:11:00.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21981, 21986) -->  state. Sandra moved to
2025-01-22 04:11:00.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:11:00.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (632, 636) -->  Daniel left the apple
2025-01-22 04:11:00.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 04:11:00.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10304, 10310) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:11:03.171 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 04:11:03.171 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24170])
2025-01-22 04:11:11.536 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [379.4893092105263, 15.029672827121168, 188.84933035714286, 14.490329524959575, 29.721081149193548], 'topk_indices': array([   26, 24156, 24162,    25,     1, 24168,   636,    24, 24155,
       24161, 24153, 24160,    14, 24154,    34, 24157, 24158,     0,
       24166, 24165]), 'topk_tokens': ['<|start_header_id|>', ' the', ' \n', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '.', '\n\n', ' was', '?', ':', ' bedroom', '\n', ' Where', ' bathroom', ' apple', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [697.9, 361.0, 323.3125, 65.38671875]}, 'weight': {'score': [21.768503289473685, 23.437929197865387, 23.024553571428573, 23.439844171089184, 29.721018145161292], 'topk_indices': array([18815, 18771, 14676, 14640, 14685, 19421, 14721, 19479, 14539,
       14604, 20297, 20345, 18140, 18109, 23520, 23654, 21941, 21968,
       23736, 23602]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 20.052083333333332, 26.205078125, 21.23046875]}, 'saliency': {'score': [2.224496941817434, 0.08408015068141625, 1.1749437604631696, 0.08081102072685985, 0.22795917141822078], 'topk_indices': array([ 7558, 24153,    24, 24163, 24024,   632,    30, 24155, 24161,
         635, 11174, 24162,    31, 24152,  7567, 24154, 24158, 24160,
       24157,    34]), 'topk_tokens': ['nes', ':', '\n\n', 'Answer', ' context', ' Daniel', 'Mary', ' was', '?', ' apple', ' apple', ' \n', ' moved', 'Question', ' bedroom', ' Where', ' before', ' bedroom', ' apple', ' bathroom'], 'evidence_proportions': [4.1560546875, 1.8916015625, 2.14794921875, 0.3859405517578125]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 04:11:11.543 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:11:11.543 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-1_0-3-5-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 3, 5, 8):   2%|▏         | 2/100 [00:58<47:41, 29.20s/it]is_0k: False
your chose emoji: ['🧎🏾\u200d♀️\u200d➡', '🇦🇨', '👨🏼\u200d🦼\u200d➡️', '🍗', '7⃣', '❤', '💜', '🚻', '🧙🏻\u200d♂️', '👷\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.65s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (1, 2, 6, 9):   2%|▏         | 2/100 [01:14<47:41, 29.20s/it]2025-01-22 04:11:28.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:11:28.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2973, 2978) -->  tragedy. Mary moved to
2025-01-22 04:11:28.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:11:28.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4935, 4941) --> . Mary journeyed to the
2025-01-22 04:11:28.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:11:28.411 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14368, 14372) -->  Mary left the apple
2025-01-22 04:11:28.411 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 04:11:28.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21456, 21460) -->  Daniel dropped the football
2025-01-22 04:11:28.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:11:28.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11170, 11175) -->  Daniel picked up the apple
2025-01-22 04:11:28.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the garden.
2025-01-22 04:11:28.602 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7542, 7547) --> . John moved to the
2025-01-22 04:11:28.602 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:11:28.680 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15943, 15949) -->  John went back to the office
2025-01-22 04:11:28.680 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:11:28.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3595, 3599) -->  Daniel took the football
2025-01-22 04:11:28.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:11:28.803 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21967, 21972) -->  Sandra moved to the kitchen
2025-01-22 04:11:28.803 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:11:28.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (752, 756) -->  Daniel left the apple
2025-01-22 04:11:28.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 04:11:28.857 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10322, 10328) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:11:31.715 | INFO     | test_jbb_embedding:begin_test:693 - The apple was in the kitchen.<|eot_id|>
2025-01-22 04:11:31.715 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24218])
2025-01-22 04:11:40.120 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [164.3499177631579, 7.748956226002229, 93.98270089285714, 7.5009486399325525, 17.208848110465116], 'topk_indices': array([   25, 24209,     1,  7807,   756,    14, 24203, 24202,  2979,
          24, 24201,  4942, 24205,  4941, 24208, 24216, 24206,     0,
       24214, 24213]), 'topk_tokens': ['<|eot_id|>', '?', '<|start_header_id|>', ' Square', '.', '\n', ' was', ' Where', ' bathroom', '\n\n', ':', '.', ' apple', ' bedroom', ' bedroom', '<|end_header_id|>', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [208.98125, 249.9375, 128.140625, 16.388671875]}, 'weight': {'score': [24.30386513157895, 23.44569278725073, 23.9890625, 23.44423115715645, 29.065952034883722], 'topk_indices': array([18857, 18813, 14650, 14686, 14695, 19510, 19452, 14731, 14549,
       14614, 20378, 20330, 18131, 18162, 23680, 23546, 22012, 21985,
       23762, 23628]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4546875, 20.052083333333332, 26.205078125, 27.341796875]}, 'saliency': {'score': [0.9089773077713815, 0.04363526569950921, 0.6211449759347099, 0.042118555364854614, 0.12600322102391442], 'topk_indices': array([11174, 24201, 24211, 24209,   257,    24, 24116, 24203,  2976,
       24200,   755,   752, 24072,  7807, 24202, 24205,  2979, 24206,
        4941, 24208]), 'topk_tokens': [' apple', ':', 'Answer', '?', 'hue', '\n\n', ' location', ' was', ' moved', 'Question', ' apple', ' Daniel', ' context', ' Square', ' Where', ' apple', ' bathroom', ' before', ' bedroom', ' bedroom'], 'evidence_proportions': [1.2140625, 1.2384440104166667, 0.82537841796875, 0.1170196533203125]}}, 'pred_res': 'The apple was in the kitchen.<|eot_id|>', 'score': 0}
2025-01-22 04:11:40.127 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:11:40.127 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-2_1-2-6-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 6, 9):   3%|▎         | 3/100 [01:26<46:45, 28.92s/it]is_0k: False
your chose emoji: ['🤛🏻', '👨🏽\u200d🍳', '8⃣', '👨🏽\u200d🦼', '🟤', '🤹🏽\u200d♂️', '🕐', '👩🏾\u200d❤️\u200d👨🏽', '🇬🇶', '👨🏿\u200d❤\u200d💋\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 3, 6, 9):   3%|▎         | 3/100 [01:43<46:45, 28.92s/it]2025-01-22 04:11:57.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:11:57.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:11:57.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:11:57.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7447, 7453) --> . Mary journeyed to the
2025-01-22 04:11:57.074 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:11:57.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14344, 14348) -->  Mary left the apple
2025-01-22 04:11:57.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 04:11:57.260 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21446, 21450) -->  Daniel dropped the football
2025-01-22 04:11:57.260 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:11:57.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11083, 11088) --> . Daniel picked up the
2025-01-22 04:11:57.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the garden.
2025-01-22 04:11:57.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7351, 7356) --> . John moved to the
2025-01-22 04:11:57.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:11:57.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15913, 15919) -->  John went back to the office
2025-01-22 04:11:57.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:11:57.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3599, 3603) -->  Daniel took the football
2025-01-22 04:11:57.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:11:57.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21957, 21962) -->  Sandra moved to the kitchen
2025-01-22 04:11:57.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:11:57.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (758, 762) -->  Daniel left the apple
2025-01-22 04:11:57.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 04:11:57.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10128, 10134) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:12:00.341 | INFO     | test_jbb_embedding:begin_test:693 - The bathroom.<|eot_id|>
2025-01-22 04:12:00.341 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24184])
2025-01-22 04:12:08.709 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [437.49013157894734, 19.78179652292554, 229.444921875, 19.14885955887685, 21.398069519927535], 'topk_indices': array([  762,    34,     1, 24177, 24178,    14, 24165, 24170, 24176,
       24174, 24166, 24175, 24169, 24168, 24171, 24167,     0, 24172,
       24180, 24179]), 'topk_tokens': ['.', ' bathroom', '<|start_header_id|>', 'Answer', ':', '\n', '.\n\n', ' the', ' \n', ' bedroom', 'Question', '?', ' was', ' Where', ' apple', ':', '<|begin_of_text|>', ' before', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [823.5, 425.7083333333333, 369.46875, 40.671875]}, 'weight': {'score': [23.055098684210527, 23.438148592218962, 23.33080357142857, 23.438605850909543, 29.12205615942029], 'topk_indices': array([18853, 18809, 14662, 14626, 14707, 19448, 14671, 19506, 14590,
       14525, 20320, 20368, 18147, 18178, 23536, 23670, 21975, 22002,
       23752, 23618]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 20.052083333333332, 26.205078125, 27.341796875]}, 'saliency': {'score': [2.5587897049753288, 0.10803828793618472, 1.4384692600795201, 0.10417928321448579, 0.1609913922738338], 'topk_indices': array([11088, 24072, 24170, 20710,   761, 24021,    27,  7453, 24165,
       24177, 24167, 24176, 24175, 24169,    34, 24166, 24174, 24168,
       24171, 24172]), 'topk_tokens': [' apple', '.\n\n', ' the', 'nes', ' apple', '�', 'user', ' bedroom', '.\n\n', 'Answer', ':', ' \n', '?', ' was', ' bathroom', 'Question', ' bedroom', ' Where', ' apple', ' before'], 'evidence_proportions': [5.0244140625, 2.06689453125, 2.484130859375, 0.2892608642578125]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-22 04:12:08.717 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:12:08.717 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-3_0-3-6-9.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 3, 6, 9):   4%|▍         | 4/100 [01:55<46:03, 28.79s/it]is_0k: False
your chose emoji: ['🇹🇱', '🤦🏾', '👩🏿\u200d🦱', '👩🏼\u200d🦯\u200d➡', '🤝🏼', '☸️', '🚶🏾\u200d♀️\u200d➡️', '🪀', '🚬', '🏄🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.53s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (0, 2, 8, 9):   4%|▍         | 4/100 [02:12<46:03, 28.79s/it]2025-01-22 04:12:26.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:12:26.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:12:26.170 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:12:26.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4975, 4981) --> . Mary journeyed to the
2025-01-22 04:12:26.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:12:26.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19224, 19228) -->  left the apple.
2025-01-22 04:12:26.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 04:12:26.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21476, 21480) -->  Daniel dropped the football
2025-01-22 04:12:26.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:12:26.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11157, 11162) --> . Daniel picked up the
2025-01-22 04:12:26.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the garden.
2025-01-22 04:12:26.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7597, 7602) -->  war. John moved to
2025-01-22 04:12:26.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:12:26.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15973, 15979) --> . John went back to the
2025-01-22 04:12:26.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:12:26.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3599, 3603) -->  Daniel took the football
2025-01-22 04:12:26.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:12:26.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21997, 22002) -->  state. Sandra moved to
2025-01-22 04:12:26.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:12:26.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (758, 762) -->  Daniel left the apple
2025-01-22 04:12:26.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the office.
2025-01-22 04:12:26.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10312, 10318) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:12:29.479 | INFO     | test_jbb_embedding:begin_test:693 - The bathroom.<|eot_id|>
2025-01-22 04:12:29.479 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 04:12:37.845 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [496.0394736842105, 36.001499110872174, 141.00435267857142, 35.48689632595225, 26.811627012310606], 'topk_indices': array([13477, 13492, 13482, 11956, 13507, 13502, 13505, 24174, 13508,
       13462, 13483, 13454, 13480, 13501, 24173, 13509,     0, 13485,
       13484, 13510]), 'topk_tokens': ['age', ' The', ' the', 'LECTION', ' was', 'pass', ' the', '<|start_header_id|>', ' the', ' a', ' Min', ' a', ' amendment', '\n', '<|eot_id|>', ' Min', '<|begin_of_text|>', 'ot', 'nes', 'nes'], 'evidence_proportions': [1070.4, 462.625, 280.90625, 43.34375]}, 'weight': {'score': [21.837582236842106, 23.437308734130102, 23.00357142857143, 23.43919772195051, 29.2265625], 'topk_indices': array([18792, 18836, 14639, 14675, 14684, 14720, 19436, 19494, 14603,
       14538, 20308, 20356, 18130, 18161, 23664, 23530, 21984, 21957,
       23612, 23746]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 20.052083333333332, 20.421875, 27.341796875]}, 'saliency': {'score': [2.740462453741776, 0.20467356295361028, 0.8636160714285714, 0.2017207309926703, 0.1997851747455019], 'topk_indices': array([    0, 13454, 13503, 13462, 13426, 13476,    34, 13418, 13493,
       13477, 13450, 13507, 11956, 13502, 13483, 13480, 13485, 13509,
       13484, 13510]), 'topk_tokens': ['<|begin_of_text|>', ' a', 'age', ' a', ' later', 'pass', ' bathroom', ' little', ' only', 'age', ' ende', ' was', 'LECTION', 'pass', ' Min', ' amendment', 'ot', ' Min', 'nes', 'nes'], 'evidence_proportions': [6.2861328125, 2.3075358072916665, 1.389404296875, 0.3088226318359375]}}, 'pred_res': 'The bathroom.<|eot_id|>', 'score': 100}
2025-01-22 04:12:37.852 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:12:37.852 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-10_pid-4_0-2-8-9.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 2, 8, 9):   5%|▌         | 5/100 [02:24<45:46, 28.91s/it]Processing depth (0, 2, 8, 9):   5%|▌         | 5/100 [02:24<45:54, 29.00s/it]
2025-01-22 04:12:38.141 | INFO     | __main__:<module>:82 - Selected idx: 11
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:86 - Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' John moved to the garden.', ' John went back to the office.', ' Daniel took the football.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary picked up the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel dropped the football.']
2025-01-22 04:12:38.142 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🍩', '👩🏾\u200d🦰', '🇨🇲', '🚶🏽\u200d♀', '👨🏾\u200d💻', '♊', '🦸🏽\u200d♂️', '🍜', '🧛🏿\u200d♀️', '🤶🏾']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.97s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.91s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.52s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (0, 2, 5, 6, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:12:55.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:12:55.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:12:55.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk.
2025-01-22 04:12:55.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4832, 4837) --> . Mary picked up the
2025-01-22 04:12:55.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:12:55.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11866, 11872) --> . Mary journeyed to the
2025-01-22 04:12:55.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:12:55.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14380, 14384) -->  Mary left the milk
2025-01-22 04:12:55.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel dropped the football.
2025-01-22 04:12:55.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16801, 16805) -->  Daniel dropped the football
2025-01-22 04:12:55.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:12:55.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14095, 14100) --> . John moved to the
2025-01-22 04:12:55.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 04:12:55.440 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8427, 8433) --> . John went back to the
2025-01-22 04:12:55.440 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel took the football.
2025-01-22 04:12:55.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18624, 18628) -->  Daniel took the football
2025-01-22 04:12:55.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:12:55.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6923, 6927) -->  Daniel left the apple
2025-01-22 04:12:55.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:12:55.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1853, 1859) --> . Sandra journeyed to the
2025-01-22 04:12:55.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 04:12:55.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2987, 2992) -->  tragedy. Daniel picked up
2025-01-22 04:12:55.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 04:12:55.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (189, 194) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:12:58.257 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 04:12:58.257 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24187])
2025-01-22 04:13:06.634 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [133.41015625, 6.677444191814799, 67.44263392857142, 6.463264641436327, 3.9018050905257935], 'topk_indices': array([24161, 24075, 17664, 17622, 24105,    23, 24106, 24066,    34,
       17665,     4, 24076,    14,     1,    24, 24185, 24182, 11872,
           0, 24183]), 'topk_tokens': [':', ' first', 'ar', 'ed', ' to', '4', ' the', '.\n\n', ' bathroom', 'ison', '\n\n', ' location', '\n', '<|start_header_id|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', ' bedroom', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [256.85, 112.325, 117.61458333333333, 138.75, 23.8203125]}, 'weight': {'score': [22.844401041666668, 23.43327046300124, 22.902901785714285, 23.434625391094443, 28.831349206349206], 'topk_indices': array([18778, 18822, 14658, 14622, 19417, 14703, 19475, 14667, 14586,
       14521, 20315, 20363, 18111, 18142, 23536, 23670, 22002, 21975,
       23752, 23618]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 22.1, 20.052083333333332, 26.134765625, 27.341796875]}, 'saliency': {'score': [0.7808570861816406, 0.037138469517846995, 0.40245797293526786, 0.03586892290065165, 0.028577441260928198], 'topk_indices': array([17664, 24180,    20, 24174,    30,  4837, 24160, 17621, 24066,
       24075, 24166,    31,     4, 24104,    24,     0, 17665, 24076,
          34, 11872]), 'topk_tokens': ['ar', 'Answer', ' Jul', ' discarded', 'Mary', ' milk', 'Question', 'ison', '.\n\n', ' first', ' prior', ' moved', '\n\n', ' moved', '\n\n', '<|begin_of_text|>', 'ison', ' location', ' bathroom', ' bedroom'], 'evidence_proportions': [1.585205078125, 0.638671875, 0.56036376953125, 0.90203857421875, 0.16271209716796875]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:13:06.642 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:13:06.642 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-0_0-2-5-6-7.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 2, 5, 6, 7):   1%|          | 1/100 [00:28<46:49, 28.38s/it]is_0k: False
your chose emoji: ['🇱🇨', '🦸🏾\u200d♀', '🐈\u200d⬛', '👨🏻\u200d⚕️', '😨', '🐉', '👩🏼\u200d🚀', '🚵🏻\u200d♂️', '🪕', '🕣']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.53s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.18s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.91s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.43s/it]
Processing depth (0, 2, 3, 4, 7):   1%|          | 1/100 [00:44<46:49, 28.38s/it]2025-01-22 04:13:22.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:13:22.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:13:22.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk.
2025-01-22 04:13:22.763 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4982, 4987) --> . Mary picked up the
2025-01-22 04:13:22.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:13:22.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7614, 7620) -->  war. Mary journeyed to
2025-01-22 04:13:22.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:13:22.861 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9768, 9772) -->  Mary left the milk
2025-01-22 04:13:22.861 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel dropped the football.
2025-01-22 04:13:22.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16719, 16723) -->  Daniel dropped the football
2025-01-22 04:13:22.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:13:23.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14082, 14087) --> . John moved to the
2025-01-22 04:13:23.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 04:13:23.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8478, 8484) --> . John went back to the
2025-01-22 04:13:23.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel took the football.
2025-01-22 04:13:23.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18494, 18498) -->  Daniel took the football
2025-01-22 04:13:23.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:13:23.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7012, 7016) -->  left the apple.
2025-01-22 04:13:23.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:13:23.246 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2010, 2016) -->  cable. Sandra journeyed to
2025-01-22 04:13:23.246 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 04:13:23.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3087, 3092) -->  ranks. Daniel picked up
2025-01-22 04:13:23.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 04:13:23.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (189, 194) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:13:25.901 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 04:13:25.902 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 04:13:34.245 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [195.966796875, 11.017008120505826, 94.72276785714286, 10.711755354405733, 9.267607276119403], 'topk_indices': array([    4, 24074,    14, 17577, 24174, 24175,    34,    24,     1,
       17724,   185, 17588, 17578, 17681, 17682, 17725, 24190,  7621,
           0, 24191]), 'topk_tokens': ['\n\n', '.\n\n', '\n', 'arp', ' prior', ' to', ' bathroom', '\n\n', '<|start_header_id|>', 'ar', 'ION', 'ente', 'ente', 'ison', 'ed', 'ison', '<|eot_id|>', ' bedroom', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [364.8, 202.075, 167.734375, 179.671875, 35.93359375]}, 'weight': {'score': [23.566080729166668, 23.44028948673444, 22.69486607142857, 23.441245235925265, 29.776585820895523], 'topk_indices': array([18816, 18860, 14706, 14670, 19461, 14715, 19519, 14751, 14569,
       14634, 20333, 20381, 18149, 18180, 23672, 23538, 21977, 22004,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 22.1, 22.938802083333332, 26.134765625, 27.341796875]}, 'saliency': {'score': [1.1759084065755208, 0.06233745650305552, 0.5552782331194196, 0.060515565373211205, 0.06958497460208722], 'topk_indices': array([   24, 24180, 16501,    31,   337, 24084,  4987, 17587, 17724,
       24182, 17577, 24174,   185, 17682,    34, 17588, 17578, 17681,
       17725,  7621]), 'topk_tokens': ['\n\n', ' milk', 'round', ' moved', ' century', ' location', ' milk', 'arp', 'ar', ' discarded', 'arp', ' prior', 'ION', 'ed', ' bathroom', 'ente', 'ente', 'ison', 'ison', ' bedroom'], 'evidence_proportions': [2.19306640625, 1.170166015625, 0.94189453125, 1.19000244140625, 0.248565673828125]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:13:34.252 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:13:34.252 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-1_0-2-3-4-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 2, 3, 4, 7):   2%|▏         | 2/100 [00:55<45:36, 27.93s/it]is_0k: False
your chose emoji: ['\U0001fac4🏼', '🏿', '🪢', '👩🏿\u200d❤\u200d💋\u200d👩🏿', '🧏🏾\u200d♀', '🎑', '💂🏽\u200d♀️', '🏌🏾\u200d♀️', '🦃', '👮🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.51s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (0, 1, 4, 5, 7):   2%|▏         | 2/100 [01:12<45:36, 27.93s/it]2025-01-22 04:13:51.270 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:13:51.270 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:13:51.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk.
2025-01-22 04:13:51.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3005, 3010) -->  tragedy. Mary picked up
2025-01-22 04:13:51.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:13:51.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9727, 9733) -->  war. Mary journeyed to
2025-01-22 04:13:51.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:13:51.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11906, 11910) -->  Mary left the milk
2025-01-22 04:13:51.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel dropped the football.
2025-01-22 04:13:51.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16743, 16747) -->  Daniel dropped the football
2025-01-22 04:13:51.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:13:51.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14114, 14119) --> . John moved to the
2025-01-22 04:13:51.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 04:13:51.581 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8459, 8465) --> . John went back to the
2025-01-22 04:13:51.581 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel took the football.
2025-01-22 04:13:51.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18502, 18506) -->  Daniel took the football
2025-01-22 04:13:51.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:13:51.703 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6955, 6959) -->  Daniel left the apple
2025-01-22 04:13:51.703 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:13:51.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2010, 2016) -->  cable. Sandra journeyed to
2025-01-22 04:13:51.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 04:13:51.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3094, 3099) --> . Daniel picked up the
2025-01-22 04:13:51.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 04:13:51.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (189, 194) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:13:54.359 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 04:13:54.359 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24213])
2025-01-22 04:14:02.716 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [403.1575520833333, 18.941816619177402, 192.5, 18.30863724800265, 18.291683799342106], 'topk_indices': array([    1,    24, 24193, 24092,  7239,     4, 24187, 24207,    31,
         547,   430,  3012,  7317,    14, 24205,    34,   431,     0,
       24208, 24209]), 'topk_tokens': ['<|start_header_id|>', '\n\n', ' to', '.\n\n', 'nes', '\n\n', ':', ':', ' moved', ' to', ' to', '.', 'nes', '\n', '?\n', ' bathroom', ' ', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [855.4, 371.4125, 167.15625, 577.5625, 57.1328125]}, 'weight': {'score': [24.776041666666668, 23.44518861496531, 22.705580357142857, 23.444937996750426, 29.80674342105263], 'topk_indices': array([18848, 18804, 14666, 14702, 14711, 19501, 19443, 14747, 14630,
       14565, 20315, 20363, 18168, 18137, 23674, 23540, 21973, 22000,
       23622, 23756]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 27.9078125, 22.938802083333332, 26.134765625, 27.341796875]}, 'saliency': {'score': [2.4858601888020835, 0.1037953745070408, 1.1521780831473214, 0.09990983613946657, 0.14031038786235608], 'topk_indices': array([   27,     4,  7465,   190,   554,    38,  7316, 11909, 24186,
        3011,    30, 24205, 24198, 24192, 24206,  7239,    31, 24200,
        7317,    34]), 'topk_tokens': ['user', '\n\n', 'nes', ' Sandra', ' per', '***', ' Min', ' milk', 'Question', ' milk', 'Mary', '?\n', ' milk', ' prior', 'Answer', 'nes', ' moved', ' discarded', 'nes', ' bathroom'], 'evidence_proportions': [5.169921875, 2.37119140625, 0.957763671875, 3.67236328125, 0.3797607421875]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:14:02.723 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:14:02.723 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-2_0-1-4-5-7.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 1, 4, 5, 7):   3%|▎         | 3/100 [01:24<45:32, 28.18s/it]is_0k: False
your chose emoji: ['🔺', '🧑🏾\u200d❤️\u200d💋\u200d🧑🏼', '🕐', '🧎🏾\u200d♂\u200d➡️', '🌆', '🇸🇳', '💧', '👩\u200d🦳', '👏🏾', '🚶🏾\u200d♂️\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.60s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (2, 4, 5, 6, 9):   3%|▎         | 3/100 [01:41<45:32, 28.18s/it]2025-01-22 04:14:19.703 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:14:19.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4924, 4929) --> . Mary moved to the
2025-01-22 04:14:19.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk.
2025-01-22 04:14:19.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9804, 9809) --> . Mary picked up the
2025-01-22 04:14:19.784 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:14:19.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11932, 11938) --> . Mary journeyed to the
2025-01-22 04:14:19.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:14:19.924 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14574, 14578) -->  Mary left the milk
2025-01-22 04:14:19.924 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel dropped the football.
2025-01-22 04:14:20.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21486, 21490) -->  Daniel dropped the football
2025-01-22 04:14:20.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:14:20.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14201, 14206) --> . John moved to the
2025-01-22 04:14:20.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 04:14:20.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8497, 8503) --> . John went back to the
2025-01-22 04:14:20.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel took the football.
2025-01-22 04:14:20.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18661, 18665) -->  Daniel took the football
2025-01-22 04:14:20.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:14:20.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7008, 7012) -->  left the apple.
2025-01-22 04:14:20.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:14:20.306 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2004, 2010) -->  cable. Sandra journeyed to
2025-01-22 04:14:20.306 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 04:14:20.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3051, 3056) -->  Daniel picked up the apple
2025-01-22 04:14:20.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 04:14:20.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (183, 188) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:14:22.970 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 04:14:22.970 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24221])
2025-01-22 04:14:31.377 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [190.88118489583334, 9.172373601593462, 116.6984375, 8.83616736499069, 16.2156005859375], 'topk_indices': array([  185, 24195, 24206,  9817, 14578, 24213, 24100, 14577, 22884,
        9810,    14,     1,    24,  9818, 24219,  4929, 11938,     0,
       24216, 24217]), 'topk_tokens': [' moved', ':', ' milk', '\u200d', '.', '?\n', '.\n\n', ' milk', 'nes', '.', '\n', '<|start_header_id|>', '\n\n', '�', '<|end_header_id|>', ' bathroom', ' bedroom', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [253.225, 170.675, 229.6875, 253.875, 17.005859375]}, 'weight': {'score': [22.623046875, 23.444525573811095, 22.665401785714284, 23.44646990740741, 29.39375], 'topk_indices': array([18859, 18815, 14610, 14646, 19454, 14655, 14691, 19512, 14504,
       14569, 20338, 20386, 18179, 18148, 23704, 23570, 22009, 22036,
       23786, 23652]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 22.1, 20.052083333333332, 26.134765625, 27.341796875]}, 'saliency': {'score': [1.0622520446777344, 0.051701811534725375, 0.6866115025111608, 0.049778573596401046, 0.12222030162811279], 'topk_indices': array([11934,  9811, 24100,   188, 24214, 24208,    24,   184, 24200,
        9809,  4926,   185, 24194, 22884, 24206, 14577,  9817,  9818,
        4929, 11938]), 'topk_tokens': [' journey', '�', '.\n\n', ' kitchen', 'Answer', ' discarded', '\n\n', ' Sandra', ' prior', ' milk', ' moved', ' moved', 'Question', 'nes', ' milk', ' milk', '\u200d', '�', ' bathroom', ' bedroom'], 'evidence_proportions': [1.28427734375, 0.9384765625, 1.1927083333333333, 1.6859130859375, 0.12009429931640625]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:14:31.385 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:14:31.385 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-3_2-4-5-6-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 4, 5, 6, 9):   4%|▍         | 4/100 [01:53<45:23, 28.37s/it]is_0k: False
your chose emoji: ['🤾🏾\u200d♂', '🇧🇴', '⚓', '👩🏼\u200d🏭', '🤵\u200d♂', '🧍🏿\u200d♂️', '👳🏻', '🧑🏾\u200d❤\u200d🧑🏿', '🧑🏿\u200d🦽\u200d➡', '👖']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.50s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.53s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.29s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.59s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.19s/it]
Processing depth (1, 4, 5, 8, 9):   4%|▍         | 4/100 [02:11<45:23, 28.37s/it]2025-01-22 04:14:50.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:14:50.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3044, 3049) --> . Mary moved to the
2025-01-22 04:14:50.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk.
2025-01-22 04:14:50.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9738, 9743) --> . Mary picked up the
2025-01-22 04:14:50.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:14:50.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11928, 11934) --> . Mary journeyed to the
2025-01-22 04:14:50.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:14:50.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19292, 19296) -->  left the milk.
2025-01-22 04:14:50.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel dropped the football.
2025-01-22 04:14:50.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21460, 21464) -->  Daniel dropped the football
2025-01-22 04:14:50.666 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:14:50.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14130, 14135) -->  papers. John moved to
2025-01-22 04:14:50.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 04:14:50.778 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8483, 8489) --> . John went back to the
2025-01-22 04:14:50.778 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel took the football.
2025-01-22 04:14:50.866 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18498, 18502) -->  Daniel took the football
2025-01-22 04:14:50.866 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:14:50.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6994, 6998) -->  left the apple.
2025-01-22 04:14:50.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:14:50.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2004, 2010) -->  cable. Sandra journeyed to
2025-01-22 04:14:50.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel picked up the apple.
2025-01-22 04:14:50.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3087, 3092) -->  ranks. Daniel picked up
2025-01-22 04:14:50.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the kitchen.
2025-01-22 04:14:50.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (183, 188) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:14:53.585 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 04:14:53.585 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 04:15:01.962 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [149.32356770833334, 8.398812660137201, 93.046875, 8.13596513940097, 8.644881063432836], 'topk_indices': array([  189,     4, 24188, 24040,     9, 24193, 24168, 24189, 24167,
       24074,     1,  3049, 24187, 24169,    14,    24, 11934,     0,
       24190, 24191]), 'topk_tokens': ['.', '\n\n', 'Answer', ' context', ':', '<|end_header_id|>', 'Question', ':', '.\n\n', '.\n\n', '<|start_header_id|>', ' bathroom', '?\n', ':', '\n', '\n\n', ' bedroom', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [173.75, 129.9125, 188.3125, 221.28125, 12.61328125]}, 'weight': {'score': [21.6591796875, 23.435565439292503, 23.279464285714287, 23.437557932702266, 28.93027052238806], 'topk_indices': array([18844, 18800, 14621, 14657, 14702, 14666, 19508, 19450, 14520,
       14585, 20334, 20382, 18164, 18133, 23678, 23544, 22010, 21983,
       23626, 23760]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 22.1, 20.052083333333332, 20.3515625, 27.341796875]}, 'saliency': {'score': [0.7775802612304688, 0.045989740524462246, 0.5445111955915178, 0.04453953863439675, 0.062203022971082086], 'topk_indices': array([   20, 24038, 24169, 19294,    16,     8,   188, 24182,  9743,
       24180, 24174, 24040, 24167, 24074, 24187, 24188,    24, 24168,
        3049, 11934]), 'topk_tokens': [' Jul', ' provided', ':', ' milk', ' Date', ' Date', ' kitchen', ' discarded', ' milk', ' milk', ' prior', ' context', '.\n\n', '.\n\n', '?\n', 'Answer', '\n\n', 'Question', ' bathroom', ' bedroom'], 'evidence_proportions': [0.85400390625, 0.72470703125, 0.94384765625, 1.190673828125, 0.0856475830078125]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:15:01.969 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:15:01.969 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-11_pid-4_1-4-5-8-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 4, 5, 8, 9):   5%|▌         | 5/100 [02:23<46:10, 29.17s/it]Processing depth (1, 4, 5, 8, 9):   5%|▌         | 5/100 [02:24<45:36, 28.80s/it]
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:82 - Selected idx: 12
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:86 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John went back to the office.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel journeyed to the kitchen.']
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel journeyed to the kitchen.']
2025-01-22 04:15:02.286 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🇨🇽', '🤞🏾', '👨🏼\u200d🤝\u200d👨🏿', '⚧', '👩🏻\u200d❤\u200d💋\u200d👨🏽', '💜', '🛸', '♂️', '🤏🏽', '🪠']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.88s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.12s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.75s/it]
Processing depth (0, 1, 2, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:15:19.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:15:19.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:15:19.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:15:19.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2981, 2987) -->  tragedy. Mary journeyed to
2025-01-22 04:15:19.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:15:19.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4958, 4962) -->  Mary left the apple
2025-01-22 04:15:19.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 04:15:19.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16831, 16837) --> . Daniel journeyed to the
2025-01-22 04:15:19.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:15:19.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3867, 3872) --> . John moved to the
2025-01-22 04:15:19.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:15:19.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7227, 7233) -->  cold. Sandra journeyed to
2025-01-22 04:15:19.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:15:19.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16719, 16724) --> . Daniel picked up the
2025-01-22 04:15:19.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 04:15:19.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3757, 3762) --> . Sandra moved to the
2025-01-22 04:15:19.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:15:19.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1403, 1409) --> . John went back to the
2025-01-22 04:15:19.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:15:19.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20561, 20565) -->  Daniel left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:15:22.431 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:15:22.431 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24229])
2025-01-22 04:15:30.787 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [681.6785714285714, 33.68740714757346, 331.3508064516129, 32.743015922249796, 27.605720766129032], 'topk_indices': array([   26, 24223, 24210, 17342, 17343, 24215, 17372, 24221, 24219,
       17344, 24211, 24220, 24214, 24213, 24216, 24212, 24217,     0,
       24225, 24224]), 'topk_tokens': ['<|start_header_id|>', ':', '.\n\n', ' the', ' service', ' the', ' service', ' \n', ' bedroom', ' of', 'Question', '?', ' was', ' Where', ' apple', ':', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [1561.6, 450.8645833333333, 724.75, 150.51041666666666]}, 'weight': {'score': [22.825892857142858, 23.448652608121492, 22.108366935483872, 23.450911781430936, 29.079973118279568], 'topk_indices': array([18828, 18784, 14635, 14671, 14716, 14680, 19481, 19423, 14599,
       14534, 20295, 20343, 18122, 18153, 23587, 23721, 21992, 22019,
       23669, 23803]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 24.891927083333332, 26.205078125, 20.270833333333332]}, 'saliency': {'score': [3.9747837611607144, 0.18585686669486629, 1.9404493762600807, 0.18031675782057743, 0.20157434607064853], 'topk_indices': array([17342, 24117, 17344,    31, 24210,  2988,    27, 24222, 24212,
       17343, 24220, 24214, 24221, 17372,    34, 24213, 24219, 24211,
       24216, 24217]), 'topk_tokens': [' the', '.\n\n', ' of', ' moved', '.\n\n', ' bedroom', 'user', 'Answer', ':', ' service', '?', ' was', ' \n', ' service', ' bathroom', ' Where', ' bedroom', 'Question', ' apple', ' before'], 'evidence_proportions': [8.946484375, 2.4493815104166665, 4.748046875, 0.8415934244791666]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:15:30.792 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:15:30.792 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-0_0-1-2-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 1, 2, 7):   1%|          | 1/100 [00:28<46:46, 28.35s/it]is_0k: False
your chose emoji: ['🦙', '👨🏾\u200d🎨', '🧑🏿\u200d🤝\u200d🧑🏼', '🏄🏿', '🧑🏽\u200d❤\u200d🧑🏿', '🙎🏻\u200d♂️', '\U0001faf1🏾\u200d\U0001faf2🏽', '🙍🏻', '🧑🏻\u200d🚒', '🏄']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.30s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.33s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.49s/it]
Processing depth (1, 5, 6, 7):   1%|          | 1/100 [00:44<46:46, 28.35s/it]2025-01-22 04:15:47.026 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:15:47.043 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3038, 3043) --> . Mary moved to the
2025-01-22 04:15:47.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:15:47.109 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11911, 11917) --> . Mary journeyed to the
2025-01-22 04:15:47.109 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:15:47.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14385, 14389) -->  Mary left the apple
2025-01-22 04:15:47.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 04:15:47.270 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16751, 16757) --> . Daniel journeyed to the
2025-01-22 04:15:47.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:15:47.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3900, 3905) -->  the ground. John moved
2025-01-22 04:15:47.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:15:47.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7276, 7282) --> . Sandra journeyed to the
2025-01-22 04:15:47.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:15:47.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16655, 16660) --> . Daniel picked up the
2025-01-22 04:15:47.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 04:15:47.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3772, 3777) --> . Sandra moved to the
2025-01-22 04:15:47.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:15:47.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1501, 1507) --> . John went back to the
2025-01-22 04:15:47.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:15:47.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20479, 20483) -->  Daniel left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:15:50.200 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:15:50.200 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24225])
2025-01-22 04:15:58.589 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [103.96130952380952, 6.242958818309394, 84.58845766129032, 6.057618156953176, 5.383907108516484], 'topk_indices': array([   26, 24153, 24216, 24206,    23, 24210,    24, 24219,     1,
       24212, 24207, 24217,     3, 24209, 24213, 24208, 24223,     0,
       24221, 24220]), 'topk_tokens': ['<|start_header_id|>', ' the', '?', '.\n\n', '4', ' was', '\n\n', ':', '<|start_header_id|>', ' apple', 'Question', ' \n', '<|end_header_id|>', ' Where', ' before', ':', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [114.975, 82.90625, 149.15625, 85.70833333333333]}, 'weight': {'score': [21.190104166666668, 23.45160558032029, 21.982610887096776, 23.455453626530442, 29.60164835164835], 'topk_indices': array([18860, 18904, 14667, 14703, 19557, 14712, 14748, 19499, 14631,
       14566, 20371, 20419, 18198, 18229, 23715, 23581, 22047, 22020,
       23797, 23663]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 20.052083333333332, 26.205078125, 20.270833333333332]}, 'saliency': {'score': [0.5745965866815477, 0.03428333742854342, 0.5032579975743448, 0.033212656062774355, 0.03987322796832074], 'topk_indices': array([16660, 24222, 24079, 24123, 11917,    23,    24, 24216, 24206,
        3043, 24210, 24218, 16656, 24208, 24217, 24215, 24209, 24212,
       24207, 24213]), 'topk_tokens': [' apple', 'assistant', ' context', ' location', ' bedroom', '4', '\n\n', '?', '.\n\n', ' bathroom', ' was', 'Answer', ' Daniel', ':', ' \n', ' bedroom', ' Where', ' apple', 'Question', ' before'], 'evidence_proportions': [0.571875, 0.4130859375, 0.99652099609375, 0.45709228515625]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:15:58.594 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:15:58.594 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-1_1-5-6-7.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 5, 6, 7):   2%|▏         | 2/100 [00:56<45:46, 28.03s/it]is_0k: False
your chose emoji: ['👨🏽\u200d🌾', '👩🏽\u200d❤️\u200d💋\u200d👩🏽', '\U0001fa8f', '🏓', '9⃣', '👩🏾\u200d❤\u200d👩🏾', '🇺🇲', '👩🏾\u200d❤\u200d💋\u200d👨🏼', '🤦🏾\u200d♀️', '🗨️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (4, 5, 7, 8):   2%|▏         | 2/100 [01:12<45:46, 28.03s/it]2025-01-22 04:16:15.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:16:15.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9697, 9702) --> . Mary moved to the
2025-01-22 04:16:15.610 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:16:15.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11907, 11913) --> . Mary journeyed to the
2025-01-22 04:16:15.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:16:15.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16787, 16791) -->  Mary left the apple
2025-01-22 04:16:15.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 04:16:15.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19441, 19447) --> . Daniel journeyed to the
2025-01-22 04:16:15.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:16:15.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3808, 3813) --> . John moved to the
2025-01-22 04:16:15.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:16:15.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7179, 7185) -->  cold. Sandra journeyed to
2025-01-22 04:16:15.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:16:16.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16674, 16679) --> . Daniel picked up the
2025-01-22 04:16:16.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 04:16:16.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3630, 3635) --> . Sandra moved to the
2025-01-22 04:16:16.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:16:16.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1383, 1389) --> . John went back to the
2025-01-22 04:16:16.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:16:16.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20531, 20535) -->  Daniel left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:16:18.798 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:16:18.798 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24197])
2025-01-22 04:16:27.143 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [145.60119047619048, 7.091289385330579, 122.92036290322581, 6.822140420531721, 12.446580762987013], 'topk_indices': array([24184, 24190, 24196,     9, 24179,     1, 11913,    23, 24178,
       24187, 24185,  9702, 24180,    14, 24191,    24, 24193, 24195,
           0, 24192]), 'topk_tokens': [' apple', 'Answer', '\n\n', ':', 'Question', '<|start_header_id|>', ' bedroom', '4', '.\n\n', ' bedroom', ' before', ' bathroom', ':', '\n', ':', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [184.1, 161.625, 116.0, 117.22916666666667]}, 'weight': {'score': [21.190104166666668, 23.441268078512397, 22.108366935483872, 23.444936886595162, 29.095373376623378], 'topk_indices': array([18737, 18781, 14594, 14630, 14675, 19434, 19376, 14639, 14493,
       14558, 20333, 20285, 18075, 18106, 23679, 23545, 21984, 22011,
       23761, 23627]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 20.052083333333332, 26.205078125, 20.270833333333332]}, 'saliency': {'score': [0.7946544828869048, 0.03965035178444602, 0.7363320627520161, 0.03809940678721452, 0.09155967018821022], 'topk_indices': array([19447, 24188,     8, 16676, 24181,    23, 24189, 24178, 20534,
          24, 24166, 16679, 24194, 24184, 24190, 24185, 24179, 11913,
       24187,  9702]), 'topk_tokens': [' kitchen', '?', ' Date', ' picked', ' Where', '4', ' \n', '.\n\n', ' apple', '\n\n', ' return', ' apple', 'assistant', ' apple', 'Answer', ' before', 'Question', ' bedroom', ' bedroom', ' bathroom'], 'evidence_proportions': [0.92900390625, 0.8079833984375, 0.77227783203125, 0.6842854817708334]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:16:27.151 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:16:27.151 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-2_4-5-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (4, 5, 7, 8):   3%|▎         | 3/100 [01:24<45:42, 28.27s/it]is_0k: False
your chose emoji: ['👩🏾\u200d🚒', '🗓️', '👨🏻\u200d❤\u200d👨🏽', '👨🏼\u200d💻', '👩\u200d🏭', '🔗', '\U0001faf0🏽', '🧜🏿\u200d♂', '🕺🏿', '🧍🏻\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.51s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (3, 4, 8, 9):   3%|▎         | 3/100 [01:41<45:42, 28.27s/it]2025-01-22 04:16:44.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:16:44.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7612, 7617) -->  war. Mary moved to
2025-01-22 04:16:44.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:16:44.637 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9805, 9811) --> . Mary journeyed to the
2025-01-22 04:16:44.638 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:16:44.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19441, 19445) -->  Mary left the apple
2025-01-22 04:16:44.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 04:16:44.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21454, 21460) --> . Daniel journeyed to the
2025-01-22 04:16:44.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:16:44.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3846, 3851) --> . John moved to the
2025-01-22 04:16:44.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:16:44.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7274, 7280) --> . Sandra journeyed to the
2025-01-22 04:16:44.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:16:45.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16678, 16683) --> . Daniel picked up the
2025-01-22 04:16:45.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 04:16:45.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3754, 3759) --> . Sandra moved to the
2025-01-22 04:16:45.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:16:45.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1389, 1395) --> . John went back to the
2025-01-22 04:16:45.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:16:45.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20518, 20522) -->  Daniel left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:16:47.812 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:16:47.813 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 04:16:56.165 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [187.24404761904762, 7.482820775405495, 137.76360887096774, 7.158816371910764, 11.879034323770492], 'topk_indices': array([    1,    14,  9811, 24148, 24147, 24150, 24149,    24, 24146,
       24154, 24152,  7618, 24156,  9812, 24163, 24155, 24153,     0,
       24161, 24160]), 'topk_tokens': ['<|start_header_id|>', '\n', ' bedroom', ':', 'Question', ' was', ' Where', '\n\n', '.\n\n', ' the', ' apple', ' bathroom', '?', '.', '<|end_header_id|>', ' bedroom', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [158.65, 216.45833333333334, 201.71875, 172.20833333333334]}, 'weight': {'score': [22.014880952380953, 23.43263302714333, 21.459929435483872, 23.43640341215376, 28.868084016393443], 'topk_indices': array([18780, 18736, 14606, 14642, 14651, 19375, 14687, 19433, 14505,
       14570, 20300, 20252, 18105, 18074, 23513, 23647, 21938, 21965,
       23729, 23595]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.1109375, 20.052083333333332, 26.205078125, 20.270833333333332]}, 'saliency': {'score': [1.0073707217261905, 0.04188602754352346, 0.8109209614415323, 0.04005673323565009, 0.08732529937243852], 'topk_indices': array([16680,    23, 19444, 20521, 16679, 16683, 24162,    24, 24134,
       24146, 24158, 24150, 24156, 24149, 24147, 24152,  9811, 24153,
        7618, 24155]), 'topk_tokens': [' picked', '4', ' apple', ' apple', ' Daniel', ' apple', 'assistant', '\n\n', ' return', '.\n\n', 'Answer', ' was', '?', ' Where', 'Question', ' apple', ' bedroom', ' before', ' bathroom', ' bedroom'], 'evidence_proportions': [0.900732421875, 1.002685546875, 1.3389892578125, 0.8798421223958334]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:16:56.172 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:16:56.172 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-3_3-4-8-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (3, 4, 8, 9):   4%|▍         | 4/100 [01:53<45:42, 28.57s/it]is_0k: False
your chose emoji: ['🏴\u200d☠️', '🏄🏿', '🧔🏼', '🕔', '🤾🏽', '👨🏻\u200d🏫', '🍘', '🙋🏼', '👴🏼', '🤽🏻\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.20s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (3, 4, 5, 8):   4%|▍         | 4/100 [02:10<45:42, 28.57s/it]2025-01-22 04:17:13.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:17:13.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7459, 7464) --> . Mary moved to the
2025-01-22 04:17:13.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:17:13.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9686, 9692) -->  war. Mary journeyed to
2025-01-22 04:17:13.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:17:13.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11865, 11869) -->  Mary left the apple
2025-01-22 04:17:13.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 04:17:13.623 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19248, 19254) -->  Daniel journeyed to the kitchen
2025-01-22 04:17:13.623 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:17:13.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3840, 3845) --> . John moved to the
2025-01-22 04:17:13.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:17:13.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7131, 7137) -->  Sandra journeyed to the office
2025-01-22 04:17:13.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:17:13.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16609, 16614) --> . Daniel picked up the
2025-01-22 04:17:13.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the kitchen.
2025-01-22 04:17:13.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3748, 3753) --> . Sandra moved to the
2025-01-22 04:17:13.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:17:13.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1501, 1507) --> . John went back to the
2025-01-22 04:17:13.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel left the apple.
2025-01-22 04:17:13.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20398, 20402) -->  left the apple.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:17:16.550 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:17:16.550 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24151])
2025-01-22 04:17:24.902 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [205.53720238095238, 7.354201167508487, 153.39012096774192, 6.993693469421625, 18.32060185185185], 'topk_indices': array([24136,     1, 24138,    23,    14, 24145, 24142, 24134, 24133,
          24,  9693, 24132,  9694, 24141, 24139,  7464, 24149, 24147,
           0, 24146]), 'topk_tokens': [' was', '<|start_header_id|>', ' apple', '4', '\n', ':', '?', ':', 'Question', '\n\n', ' bedroom', '.\n\n', '.', ' bedroom', ' before', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [215.525, 188.33854166666666, 206.53125, 213.75]}, 'weight': {'score': [23.155133928571427, 23.431514966465183, 21.318800403225808, 23.43447315056842, 29.322627314814813], 'topk_indices': array([18801, 18757, 14613, 14649, 19403, 14658, 14694, 19461, 14512,
       14577, 20283, 20331, 18095, 18126, 23499, 23633, 21965, 21938,
       23581, 23715]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 22.938802083333332, 26.205078125, 24.26171875]}, 'saliency': {'score': [1.1903018043154763, 0.041231989751956195, 0.8872956306703629, 0.039142601362017415, 0.13307020399305555], 'topk_indices': array([24120,  7132,    23, 24148, 16611, 19253, 16614,    24, 24142,
       24144, 24132, 24135, 16610, 11868, 24138, 24133,  9693, 24139,
       24141,  7464]), 'topk_tokens': [' return', ' journey', '4', 'assistant', ' picked', ' kitchen', ' apple', '\n\n', '?', 'Answer', '.\n\n', ' Where', ' Daniel', ' apple', ' apple', 'Question', ' bedroom', ' before', ' bedroom', ' bathroom'], 'evidence_proportions': [1.0638671875, 0.9783935546875, 1.3948974609375, 1.3711751302083333]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:17:24.911 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:17:24.911 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-12_pid-4_3-4-5-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (3, 4, 5, 8):   5%|▌         | 5/100 [02:22<45:19, 28.63s/it]Processing depth (3, 4, 5, 8):   5%|▌         | 5/100 [02:22<45:11, 28.55s/it]
2025-01-22 04:17:25.181 | INFO     | __main__:<module>:82 - Selected idx: 13
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:86 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Mary got the football.', ' Sandra journeyed to the office.', ' Daniel left the apple.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the football.', ' Mary journeyed to the bedroom.', ' Mary left the football.', ' Daniel journeyed to the kitchen.']
2025-01-22 04:17:25.182 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🙍\u200d♂️', '🌌', '5⃣', '👊', '🚴', '🛁', '🙋🏿', '🧑🏻\u200d🎄', '👮\u200d♂️', '🧘🏾\u200d♀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]
Processing depth (2, 3, 6, 7, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:17:41.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:17:41.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4971, 4976) --> . Mary moved to the
2025-01-22 04:17:41.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 04:17:41.830 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7660, 7664) -->  Mary got the football
2025-01-22 04:17:41.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:17:41.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14442, 14448) --> . Mary journeyed to the
2025-01-22 04:17:41.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 04:17:41.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16794, 16798) -->  Mary left the football
2025-01-22 04:17:41.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 04:17:42.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19288, 19294) -->  Daniel journeyed to the kitchen
2025-01-22 04:17:42.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:17:42.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6371, 6376) --> . Daniel picked up the
2025-01-22 04:17:42.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:17:42.216 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20277, 20282) -->  Sandra moved to the kitchen
2025-01-22 04:17:42.217 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:17:42.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21100, 21105) --> . John moved to the
2025-01-22 04:17:42.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:17:42.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7605, 7611) -->  war. Sandra journeyed to
2025-01-22 04:17:42.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:17:42.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17547, 17551) -->  Daniel left the apple
2025-01-22 04:17:42.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:17:42.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6519, 6525) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:17:45.144 | INFO     | test_jbb_embedding:begin_test:693 - the football<|eot_id|>
2025-01-22 04:17:45.144 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24213])
2025-01-22 04:17:53.539 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [123.80875, 7.354704147051536, 58.90877016129032, 7.168051531456953, 9.23594000400641], 'topk_indices': array([24171,     1,  7663, 24198, 24077, 24206,    23,    24, 24186,
       14448,    14, 24207, 24185, 24187, 24092, 24205, 24211,     0,
       24208, 24209]), 'topk_tokens': ['.\n\n', '<|start_header_id|>', ' football', ' football', '.', 'Answer', '4', '\n\n', 'Question', ' bedroom', '\n', ':', '.\n\n', ':', '.\n\n', '?\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [117.6375, 208.65625, 113.10416666666667, 170.59375, 51.901041666666664]}, 'weight': {'score': [23.0565625, 23.444414333498514, 22.79107862903226, 23.445653973509934, 29.600360576923077], 'topk_indices': array([18841, 18797, 14611, 14647, 19501, 14692, 14656, 19443, 14510,
       14575, 20321, 20369, 18166, 18135, 23672, 23538, 21971, 21998,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.611328125, 20.052083333333332, 26.462890625, 24.26171875]}, 'saliency': {'score': [0.75563232421875, 0.03956950061345675, 0.36646221530052925, 0.03840910141041737, 0.06743538685334034], 'topk_indices': array([24102, 24188,  7607, 24187, 24173,  4976, 24210,    23, 24200,
          24, 16797, 24192, 24185, 24205,  7663, 24198, 24092, 24206,
       24186, 14448]), 'topk_tokens': [' location', ' Where', ' Sandra', ':', ' return', ' bathroom', 'assistant', '4', ' discarded', '\n\n', ' football', ' prior', '.\n\n', '?\n', ' football', ' football', '.\n\n', 'Answer', 'Question', ' bedroom'], 'evidence_proportions': [0.6191162109375, 1.4554443359375, 0.5587158203125, 1.140380859375, 0.3432718912760417]}}, 'pred_res': 'the football<|eot_id|>', 'score': 0}
2025-01-22 04:17:53.546 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:17:53.546 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-0_2-3-6-7-8.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 3, 6, 7, 8):   1%|          | 1/100 [00:28<46:34, 28.22s/it]is_0k: False
your chose emoji: ['🥀', '👩🏻\u200d❤️\u200d👨🏿', '🐅', '🦹🏻\u200d♂', '🏄🏼\u200d♂️', '👰🏾', '🙍🏼\u200d♀️', '🙎🏼\u200d♂️', '🇻🇬', '🇽🇰']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.59s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (5, 6, 7, 8, 9):   1%|          | 1/100 [00:45<46:34, 28.22s/it]2025-01-22 04:18:10.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:18:10.709 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11887, 11892) --> . Mary moved to the
2025-01-22 04:18:10.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 04:18:10.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14394, 14398) -->  Mary got the football
2025-01-22 04:18:10.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:18:10.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16740, 16746) --> . Mary journeyed to the
2025-01-22 04:18:10.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 04:18:10.983 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19296, 19300) -->  left the football.
2025-01-22 04:18:10.983 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 04:18:11.090 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21511, 21517) --> . Daniel journeyed to the
2025-01-22 04:18:11.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:18:11.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6337, 6342) --> . Daniel picked up the
2025-01-22 04:18:11.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:18:11.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20209, 20214) --> . Sandra moved to the
2025-01-22 04:18:11.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:18:11.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21059, 21064) --> . John moved to the
2025-01-22 04:18:11.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:18:11.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7464, 7470) --> . Sandra journeyed to the
2025-01-22 04:18:11.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:18:11.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17536, 17540) -->  Daniel left the apple
2025-01-22 04:18:11.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:18:11.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6425, 6431) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:18:14.129 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 04:18:14.129 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24241])
2025-01-22 04:18:22.504 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [182.120625, 7.081220482387395, 85.13961693548387, 6.800263172854308, 8.684634001358695], 'topk_indices': array([24213, 24215, 24233, 24235, 19299,    14,    23,    24, 24120,
           1, 24226, 14398, 19298, 11892, 24239, 14397, 16746,     0,
       24236, 24237]), 'topk_tokens': ['.\n\n', ':', '?\n', ':', '.', '\n', '4', '\n\n', '.\n\n', '<|start_header_id|>', ' football', '.', ' football', ' bathroom', '<|end_header_id|>', ' football', ' bedroom', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [202.3, 344.15625, 116.91666666666667, 325.90625, 26.627604166666668]}, 'weight': {'score': [21.1734375, 23.450477437716547, 21.459929435483872, 23.455382059285597, 29.46263586956522], 'topk_indices': array([18848, 18804, 14636, 14672, 19506, 19448, 14737, 14701, 14535,
       14600, 20338, 20386, 18173, 18142, 23562, 23696, 22022, 21995,
       23644, 23778]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.611328125, 20.052083333333332, 20.6796875, 20.270833333333332]}, 'saliency': {'score': [1.042716064453125, 0.03910912981207979, 0.49944280808971775, 0.03748185523821536, 0.06382759757663893], 'topk_indices': array([11889, 24213, 24201,    20, 23744, 24238,     0, 14395,    24,
       24220,  7465,    23, 24234, 24120, 24214, 24226, 19298, 14397,
       11892, 16746]), 'topk_tokens': [' moved', '.\n\n', ' return', ' Jul', ' heart', 'assistant', '<|begin_of_text|>', ' got', '\n\n', ' prior', ' Sandra', '4', 'Answer', '.\n\n', 'Question', ' football', ' football', ' football', ' bathroom', ' bedroom'], 'evidence_proportions': [0.999609375, 2.385986328125, 0.5811767578125, 1.7860107421875, 0.14913431803385416]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:18:22.520 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:18:22.520 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-1_5-6-7-8-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (5, 6, 7, 8, 9):   2%|▏         | 2/100 [00:57<46:49, 28.66s/it]is_0k: False
your chose emoji: ['🧘🏽\u200d♂', '🧍🏼\u200d♂️', '👨🏿\u200d❤️\u200d👨🏿', '🏊\u200d♂', '🏃🏽\u200d➡️', '👻', '💪🏾', '🧑🏽\u200d🎨', '👬🏻', '🧑🏿\u200d🤝\u200d🧑🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.59s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (1, 2, 4, 5, 8):   2%|▏         | 2/100 [01:14<46:49, 28.66s/it]2025-01-22 04:18:39.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:18:39.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary moved to
2025-01-22 04:18:39.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 04:18:39.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4952, 4956) -->  Mary got the football
2025-01-22 04:18:39.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:18:39.787 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9784, 9790) --> . Mary journeyed to the
2025-01-22 04:18:39.787 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 04:18:39.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11934, 11938) -->  Mary left the football
2025-01-22 04:18:39.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 04:18:39.955 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19351, 19357) --> . Daniel journeyed to the
2025-01-22 04:18:39.955 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:18:39.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6350, 6355) --> . Daniel picked up the
2025-01-22 04:18:39.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:18:40.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20255, 20260) -->  Sandra moved to the kitchen
2025-01-22 04:18:40.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:18:40.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21078, 21083) --> . John moved to the
2025-01-22 04:18:40.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:18:40.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7584, 7590) -->  war. Sandra journeyed to
2025-01-22 04:18:40.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:18:40.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17577, 17581) -->  Daniel left the apple
2025-01-22 04:18:40.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:18:40.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6498, 6504) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:18:42.999 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 04:18:42.999 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 04:18:51.369 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [178.48, 7.297001570507522, 57.87096774193548, 7.054774233637117, 11.938907398897058], 'topk_indices': array([   22,  2957,  4956,    20,    19,     1,    14,     9, 11937,
        2960, 24178,  4955, 24038,    24, 24189, 24191,    23, 24188,
        9790,     0]), 'topk_tokens': ['202', ' moved', '.', ' Jul', '26', '<|start_header_id|>', '\n', ':', ' football', ' bathroom', ' football', ' football', ' context', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '4', '<|eot_id|>', ' bedroom', '<|begin_of_text|>'], 'evidence_proportions': [213.19375, 272.3125, 171.6875, 238.8125, 53.567708333333336]}, 'weight': {'score': [23.2603125, 23.441878306331624, 22.79107862903226, 23.44290208160729, 30.0546875], 'topk_indices': array([18807, 18763, 14598, 14634, 19467, 14679, 14643, 19409, 14562,
       14497, 20299, 20347, 18114, 18083, 23674, 23540, 22006, 21979,
       23756, 23622]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4546875, 26.611328125, 20.052083333333332, 26.462890625, 20.270833333333332]}, 'saliency': {'score': [1.084638671875, 0.04176680441434018, 0.3589753181703629, 0.04027942824976051, 0.08887313393985524], 'topk_indices': array([24153, 24186, 24180, 24082, 24166,  2944,    16, 24172,    22,
        2957,    24,    19,    20,    23, 11937, 24038, 24178,  4955,
        2960,  9790]), 'topk_tokens': [' return', 'Answer', ' discarded', ' location', 'Question', 'lyn', ' Date', ' prior', '202', ' moved', '\n\n', '26', ' Jul', '4', ' football', ' context', ' football', ' football', ' bathroom', ' bedroom'], 'evidence_proportions': [1.196630859375, 1.8763427734375, 0.850830078125, 1.6785888671875, 0.3013509114583333]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 04:18:51.383 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:18:51.383 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-2_1-2-4-5-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 4, 5, 8):   3%|▎         | 3/100 [01:26<46:29, 28.76s/it]is_0k: False
your chose emoji: ['👳\u200d♂', '🙂\u200d↔️', '🍢', '🧗🏼\u200d♀️', '🏊🏿\u200d♂', '👩🏼\u200d❤️\u200d💋\u200d👨🏼', '🎵', '🪠', '🇨🇳', '🗑️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.22s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.59s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 4, 7, 8, 9):   3%|▎         | 3/100 [01:43<46:29, 28.76s/it]2025-01-22 04:19:08.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:19:08.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:19:08.576 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 04:19:08.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9814, 9818) -->  Mary got the football
2025-01-22 04:19:08.627 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:19:08.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16706, 16712) --> . Mary journeyed to the
2025-01-22 04:19:08.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 04:19:08.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19208, 19212) -->  left the football.
2025-01-22 04:19:08.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 04:19:08.933 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21433, 21439) --> . Daniel journeyed to the
2025-01-22 04:19:08.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:19:08.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6357, 6362) --> . Daniel picked up the
2025-01-22 04:19:08.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:19:09.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20169, 20174) --> . Sandra moved to the
2025-01-22 04:19:09.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:19:09.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21019, 21024) --> . John moved to the
2025-01-22 04:19:09.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:19:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7591, 7597) -->  war. Sandra journeyed to
2025-01-22 04:19:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:19:09.326 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17502, 17506) -->  Daniel left the apple
2025-01-22 04:19:09.326 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:19:09.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6505, 6511) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:19:12.007 | INFO     | test_jbb_embedding:begin_test:693 - the football<|eot_id|>
2025-01-22 04:19:12.007 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 04:19:20.357 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [267.015625, 12.167557973710318, 154.88407258064515, 11.720283627154458, 22.314512310606062], 'topk_indices': array([24096,    23, 10063,    24, 10028,    14, 24076, 24107,  9817,
       10035, 24187, 24078,    34, 24106, 24068, 24108, 16712, 24185,
       24184,     0]), 'topk_tokens': [' a', '4', ' as', '\n\n', ' if', '\n', ' the', ' to', ' football', 'or', '<|end_header_id|>', ' location', ' bathroom', ' moved', '.\n\n', ' the', ' bedroom', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [453.975, 384.125, 204.91666666666666, 355.375, 36.3359375]}, 'weight': {'score': [21.3859375, 23.4375568369709, 22.018649193548388, 23.44150432704259, 29.456202651515152], 'topk_indices': array([18776, 18820, 14636, 14600, 19478, 19420, 14645, 14681, 14499,
       14564, 20298, 20346, 18139, 18108, 23654, 23520, 21959, 21986,
       23602, 23736]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 26.611328125, 20.052083333333332, 20.6796875, 20.270833333333332]}, 'saliency': {'score': [1.55693115234375, 0.06869934727905919, 0.8975160660282258, 0.06609331838413703, 0.16444605047052557], 'topk_indices': array([24069,  7782, 24072,  9822,    31, 24074, 24071, 10028, 10034,
       10035, 24162, 10070, 24174, 24068, 19210, 24078,  9817, 24106,
          34, 16712]), 'topk_tokens': ['If', ' Square', ' obtained', ' barric', ' moved', ' item', ' person', ' if', 'po', 'or', 'Question', 'po', ' football', '.\n\n', ' football', ' location', ' football', ' moved', ' bathroom', ' bedroom'], 'evidence_proportions': [2.6353515625, 2.6435546875, 1.0106608072916667, 1.9864501953125, 0.193756103515625]}}, 'pred_res': 'the football<|eot_id|>', 'score': 0}
2025-01-22 04:19:20.370 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:19:20.370 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-3_0-4-7-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 4, 7, 8, 9):   4%|▍         | 4/100 [01:55<46:09, 28.85s/it]is_0k: False
your chose emoji: ['💬', '🧔🏽', '🈳', '🥎', '🇽🇰', '📭', '🧛🏽\u200d♀', '🧖🏼\u200d♀️', '🧑🏻\u200d🦽\u200d➡️', '🧎🏼\u200d♂\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.88s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.84s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.39s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (1, 2, 5, 7, 8):   4%|▍         | 4/100 [02:13<46:09, 28.85s/it]2025-01-22 04:19:38.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:19:38.722 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary moved to the
2025-01-22 04:19:38.722 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 04:19:38.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4964, 4968) -->  Mary got the football
2025-01-22 04:19:38.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:19:38.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11894, 11900) --> . Mary journeyed to the
2025-01-22 04:19:38.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 04:19:38.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16772, 16776) -->  Mary left the football
2025-01-22 04:19:38.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the kitchen.
2025-01-22 04:19:38.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19333, 19339) --> . Daniel journeyed to the
2025-01-22 04:19:38.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:19:39.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6362, 6367) --> . Daniel picked up the
2025-01-22 04:19:39.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:19:39.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20320, 20325) --> . Sandra moved to the
2025-01-22 04:19:39.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:19:39.242 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21060, 21065) --> . John moved to the
2025-01-22 04:19:39.242 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:19:39.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7529, 7535) --> . Sandra journeyed to the
2025-01-22 04:19:39.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:19:39.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17515, 17519) -->  Daniel left the apple
2025-01-22 04:19:39.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:19:39.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6510, 6516) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:19:42.448 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 04:19:42.449 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24183])
2025-01-22 04:19:51.332 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [330.3475, 19.39386938724882, 116.36189516129032, 18.947128833402402, 25.28125], 'topk_indices': array([   24, 24062, 17614, 16776,    14,  4967, 10693, 17206, 24168,
       16775, 17657, 24178,    23,  3036, 17658, 24179,     0, 17615,
       17616, 17659]), 'topk_tokens': ['\n\n', '.\n\n', 'ar', '.', '\n', ' football', 'ing', ' floats', ' football', ' football', ' cap', '<|eot_id|>', '4', ' bathroom', 'ar', '<|start_header_id|>', '<|begin_of_text|>', 'ison', 'ed', 'ison'], 'evidence_proportions': [459.6, 471.625, 167.20833333333334, 653.9375, 75.86458333333333]}, 'weight': {'score': [22.09875, 23.43786953196064, 21.459929435483872, 23.441798008184833, 29.810019841269842], 'topk_indices': array([18745, 18789, 14581, 14617, 14662, 14626, 19391, 19449, 14480,
       14545, 20311, 20263, 18114, 18083, 23520, 23654, 21931, 21958,
       23736, 23602]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.611328125, 20.052083333333332, 26.462890625, 20.270833333333332]}, 'saliency': {'score': [2.0143359375, 0.11105321696306748, 0.6818670457409274, 0.1083479829512342, 0.1938384525359623], 'topk_indices': array([ 7795,    22, 24028,    19, 23628, 10692,    20, 17613, 24156,
          23,  4967, 17206, 17657, 24168, 16775, 17658,  3036, 17616,
       17615, 17659]), 'topk_tokens': [' Square', '202', ' context', '26', ' ammunition', 'count', ' Jul', ' cap', 'Question', '4', ' football', ' floats', ' cap', ' football', ' football', 'ar', ' bathroom', 'ed', 'ison', 'ison'], 'evidence_proportions': [2.24931640625, 3.322509765625, 0.8722330729166666, 4.527587890625, 0.4130045572916667]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 04:19:51.336 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:19:51.336 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-13_pid-4_1-2-5-7-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 5, 7, 8):   5%|▌         | 5/100 [02:26<46:53, 29.61s/it]Processing depth (1, 2, 5, 7, 8):   5%|▌         | 5/100 [02:26<46:19, 29.26s/it]
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:82 - Selected idx: 14
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:86 - Needle: [' Daniel picked up the apple.', ' Sandra moved to the kitchen.', ' Mary moved to the bathroom.', ' John moved to the garden.', ' Daniel took the football.', ' Mary journeyed to the bedroom.', ' Sandra journeyed to the office.', ' John went back to the office.', ' Mary left the apple.', ' Daniel left the apple.']
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary journeyed to the bedroom.', ' Mary left the apple.', ' Daniel left the apple.']
2025-01-22 04:19:51.614 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏄🏼', '🍤', '👨\u200d👩\u200d👦', '\U0001fac4🏾', '🏈', '🏋🏼\u200d♀️', '◻️', '👩\u200d❤\u200d👩', '🏵️', '🤾🏾\u200d♀️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.86s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.38s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.02s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (5, 7, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:20:08.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:20:08.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11874, 11879) --> . Mary moved to the
2025-01-22 04:20:08.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:20:08.708 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16735, 16741) --> . Mary journeyed to the
2025-01-22 04:20:08.709 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:20:08.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19305, 19309) -->  Mary left the apple
2025-01-22 04:20:08.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:20:08.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21447, 21451) -->  Daniel left the apple
2025-01-22 04:20:08.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:20:09.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20121, 20126) --> . Daniel picked up the
2025-01-22 04:20:09.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:20:09.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19881, 19886) --> . Sandra moved to the
2025-01-22 04:20:09.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:20:09.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (105, 110) --> . John moved to the
2025-01-22 04:20:09.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:20:09.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21326, 21330) -->  Daniel took the football
2025-01-22 04:20:09.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:20:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6519, 6525) --> . Sandra journeyed to the
2025-01-22 04:20:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:20:09.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13738, 13744) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:20:11.947 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:20:11.947 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24175])
2025-01-22 04:20:20.291 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [134.75, 5.08670588965175, 75.45514112903226, 4.894189971195292, 8.715601679104477], 'topk_indices': array([24144, 24174,     9, 24162, 24157, 24166, 24158,    23, 24156,
           1, 16741, 24169, 16742,    14,    24, 24165, 24163,     0,
       24173, 24170]), 'topk_tokens': [' return', '\n\n', ':', ' apple', 'Question', '?', ':', '4', '.\n\n', '<|start_header_id|>', ' bedroom', ':', '.', '\n', '\n\n', ' bedroom', ' before', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [119.1625, 153.41666666666666, 167.53125, 93.453125]}, 'weight': {'score': [22.605263157894736, 23.442059930515345, 21.497731854838708, 23.445216980789954, 30.01795708955224], 'topk_indices': array([18716, 18760, 14573, 14609, 19418, 19360, 14654, 14618, 14537,
       14472, 20298, 20250, 18085, 18054, 23503, 23637, 21931, 21904,
       23719, 23585]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 20.052083333333332, 26.205078125, 26.533203125]}, 'saliency': {'score': [0.7582301089638158, 0.028636906182305452, 0.4460961126512097, 0.027526018157878036, 0.06589052570399953], 'topk_indices': array([24063,    20, 24160, 24029,     8,    23, 24166, 24156, 24172,
       24159, 24168, 19308, 24144, 11879,    24, 24162, 24157, 24163,
       16741, 24165]), 'topk_tokens': ['.\n\n', ' Jul', ' was', ' context', ' Date', '4', '?', '.\n\n', 'assistant', ' Where', 'Answer', ' apple', ' return', ' bathroom', '\n\n', ' apple', 'Question', ' before', ' bedroom', ' bedroom'], 'evidence_proportions': [0.600634765625, 0.7381591796875, 1.0968017578125, 0.646759033203125]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:20:20.298 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:20:20.299 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-0_5-7-8-9.pkl | len: 3 |  size: 2.0 KB
Processing depth (5, 7, 8, 9):   1%|          | 1/100 [00:28<47:06, 28.55s/it]is_0k: False
your chose emoji: ['🏃🏿\u200d♂️\u200d➡', '📞', '🙅\u200d♀️', '🪧', '✂️', '🙆🏿\u200d♀️', '👉🏽', '⏯', '🏁', '👩🏻\u200d❤\u200d💋\u200d👨🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.19s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.51s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]
Processing depth (1, 2, 3, 6):   1%|          | 1/100 [00:44<47:06, 28.55s/it]2025-01-22 04:20:36.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:20:36.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3042) --> . Mary moved to the
2025-01-22 04:20:36.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:20:36.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4929, 4935) --> . Mary journeyed to the
2025-01-22 04:20:36.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:20:37.003 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7620, 7624) -->  Mary left the apple
2025-01-22 04:20:37.003 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:20:37.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14366, 14370) -->  Daniel left the apple
2025-01-22 04:20:37.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:20:37.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20090, 20095) --> . Daniel picked up the
2025-01-22 04:20:37.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:20:37.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19822, 19827) --> . Sandra moved to the
2025-01-22 04:20:37.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:20:37.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (105, 110) --> . John moved to the
2025-01-22 04:20:37.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:20:37.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21331, 21335) -->  Daniel took the football
2025-01-22 04:20:37.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:20:37.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6532, 6538) --> . Sandra journeyed to the
2025-01-22 04:20:37.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:20:37.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13559, 13565) -->  John went back to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:20:40.151 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 04:20:40.151 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 04:20:48.506 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [125.47697368421052, 7.059423622030953, 86.62449596774194, 6.863850115068834, 9.802502241290984], 'topk_indices': array([24157,    23,  7450, 24146,    14, 24148, 24150, 24147,     1,
        4856, 17668,     3, 24154, 24153,    24, 24161, 24151,     0,
       24159, 24158]), 'topk_tokens': [':', '4', 'nes', ':', '\n', ' was', ' apple', ' Where', '<|start_header_id|>', '�', 'ison', '<|end_header_id|>', '?', ' bedroom', '\n\n', '<|end_header_id|>', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [127.9125, 126.69791666666667, 150.90625, 95.171875]}, 'weight': {'score': [22.605263157894736, 23.43695170901266, 22.14516129032258, 23.439267498756013, 29.652920081967213], 'topk_indices': array([18798, 18754, 14608, 14644, 14653, 14689, 19393, 19451, 14507,
       14572, 20283, 20331, 18092, 18123, 23647, 23513, 21952, 21979,
       23729, 23595]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 20.052083333333332, 26.205078125, 26.533203125]}, 'saliency': {'score': [0.7202533922697368, 0.03968052705036906, 0.47581728043094756, 0.03858369823157797, 0.07348995521420339], 'topk_indices': array([ 7623,  3404,   106, 24148, 24156,  7405,  4935,  7819, 24145,
          24,  7449,  4856, 24154,  7450,  3042, 24147, 17668, 24150,
       24151, 24153]), 'topk_tokens': [' apple', '�', ' John', ' was', 'Answer', 'nes', ' bedroom', ' Square', 'Question', '\n\n', ' Min', '�', '?', 'nes', ' bathroom', ' Where', 'ison', ' apple', ' before', ' bedroom'], 'evidence_proportions': [0.628125, 0.6252034505208334, 1.021728515625, 0.676513671875]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 04:20:48.512 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:20:48.513 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-1_1-2-3-6.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 2, 3, 6):   2%|▏         | 2/100 [00:56<46:18, 28.35s/it]is_0k: False
your chose emoji: ['⛅', '🧓🏻', '▫', '🥌', '💂🏼\u200d♀️', '👨🏾\u200d🔬', '🤦🏼\u200d♂️', '🚶🏻\u200d♂', '⛓', '👷\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.72s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.34s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (2, 3, 5, 6):   2%|▏         | 2/100 [01:13<46:18, 28.35s/it]2025-01-22 04:21:05.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:21:05.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4957, 4962) --> . Mary moved to the
2025-01-22 04:21:05.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:21:05.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7597, 7603) -->  war. Mary journeyed to
2025-01-22 04:21:05.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:21:05.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11902, 11906) -->  Mary left the apple
2025-01-22 04:21:05.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:21:05.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14392, 14396) -->  Daniel left the apple
2025-01-22 04:21:05.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:21:05.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20176, 20181) --> . Daniel picked up the
2025-01-22 04:21:05.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:21:05.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19998, 20003) --> . Sandra moved to the
2025-01-22 04:21:05.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:21:05.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (105, 110) --> . John moved to the
2025-01-22 04:21:05.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:21:05.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21381, 21385) -->  Daniel took the football
2025-01-22 04:21:05.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:21:05.887 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6511, 6517) --> . Sandra journeyed to the
2025-01-22 04:21:05.887 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:21:05.966 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13584, 13590) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:21:08.616 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:21:08.616 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 04:21:16.983 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [145.8503289473684, 6.888990380645694, 107.78225806451613, 6.650368970296211, 9.55882417485955], 'topk_indices': array([ 7604, 11995,    23,  7605, 11952,  7429, 24210,    14,  7428,
       11951,     1,  7384, 24209,     3, 24207,    24, 24215, 24217,
           0, 24214]), 'topk_tokens': [' bedroom', 'ANT', '4', '.', 'IC', 'nes', '?', '\n', ' Min', 'ANT', '<|start_header_id|>', 'nes', ' bedroom', '<|end_header_id|>', ' before', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [184.775, 114.99479166666667, 166.328125, 123.0]}, 'weight': {'score': [23.51685855263158, 23.449869952935348, 21.497731854838708, 23.45232086711898, 29.455231741573034], 'topk_indices': array([18814, 18770, 14670, 14634, 19409, 14715, 19467, 14679, 14533,
       14598, 20353, 20305, 18108, 18139, 23689, 23555, 22001, 21974,
       23637, 23771]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 22.938802083333332, 26.205078125, 26.533203125]}, 'saliency': {'score': [0.8596641138980263, 0.03897325290380388, 0.6094911636844758, 0.03759648715859905, 0.07181840532281425], 'topk_indices': array([11905,  6512, 24216, 24210, 24201, 18913, 11995, 11952, 24212,
       24206, 24203,    24,  7429, 11951,  7604,  7428,  4962, 24207,
        7384, 24209]), 'topk_tokens': [' apple', ' Sandra', 'assistant', '?', 'Question', ' manner', 'ANT', 'IC', 'Answer', ' apple', ' Where', '\n\n', 'nes', 'ANT', ' bedroom', ' Min', ' bathroom', ' before', 'nes', ' bedroom'], 'evidence_proportions': [0.9693359375, 0.59381103515625, 1.102294921875, 0.87872314453125]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:21:16.995 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:21:16.996 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-2_2-3-5-6.pkl | len: 3 |  size: 2.02 KB
Processing depth (2, 3, 5, 6):   3%|▎         | 3/100 [01:25<45:56, 28.41s/it]is_0k: False
your chose emoji: ['🏂🏾', '👨🏽\u200d🍳', '🕺🏾', '🧔🏼\u200d♂️', '🙋🏾\u200d♂️', '👩🏼\u200d❤\u200d💋\u200d👩🏼', '🙎', '🧚🏾', '🧛🏿', '👩🏽\u200d🦼\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]
Processing depth (4, 5, 7, 8):   3%|▎         | 3/100 [01:42<45:56, 28.41s/it]2025-01-22 04:21:34.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:21:34.398 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9659, 9664) -->  war. Mary moved to
2025-01-22 04:21:34.399 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:21:34.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11802, 11808) --> . Mary journeyed to the
2025-01-22 04:21:34.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:21:34.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16693, 16697) -->  Mary left the apple
2025-01-22 04:21:34.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:21:34.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19130, 19134) -->  Daniel left the apple
2025-01-22 04:21:34.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:21:34.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20010, 20015) --> . Daniel picked up the
2025-01-22 04:21:34.752 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:21:34.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19811, 19816) -->  Rev. Sandra moved to
2025-01-22 04:21:34.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:21:34.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (105, 110) --> . John moved to the
2025-01-22 04:21:34.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:21:34.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21303, 21307) -->  Daniel took the football
2025-01-22 04:21:34.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:21:35.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6533, 6539) --> . Sandra journeyed to the
2025-01-22 04:21:35.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:21:35.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13479, 13485) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:21:37.719 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 04:21:37.719 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 04:21:46.059 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [112.62582236842105, 5.175624094577815, 60.5546875, 5.01974252125674, 7.731394800646552], 'topk_indices': array([    4,    21, 24145, 24140, 24153,     9, 24150,  9665, 24147,
       11808, 24151, 24156,    23,    14,     1,     3,    24, 24155,
           0, 24152]), 'topk_tokens': ['\n\n', ' ', ' before', ':', '<|start_header_id|>', ':', 'Answer', ' bathroom', ' bedroom', ' bedroom', ':', '\n\n', '4', '\n', '<|start_header_id|>', '<|end_header_id|>', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [126.003125, 101.0625, 122.21875, 103.65625]}, 'weight': {'score': [23.51685855263158, 23.431345716059603, 22.205645161290324, 23.43285430060141, 28.806842672413794], 'topk_indices': array([18771, 18815, 14608, 14644, 19415, 14689, 14653, 19473, 14507,
       14572, 20347, 20299, 18134, 18103, 23509, 23643, 21975, 21948,
       23591, 23725]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.1109375, 20.052083333333332, 26.205078125, 26.533203125]}, 'saliency': {'score': [0.6766614412006579, 0.028975829067609166, 0.34991455078125, 0.02805276282938226, 0.05581467727134968], 'topk_indices': array([24149,    19,     8, 24126,  9662, 16696,    16, 24156,    22,
       24144,    20, 24145, 24139,    23, 24154, 24150,    24, 24147,
        9665, 11808]), 'topk_tokens': [' \n', '26', ' Date', ' return', ' moved', ' apple', ' Date', '\n\n', '202', ' apple', ' Jul', ' before', 'Question', '4', 'assistant', 'Answer', '\n\n', ' bedroom', ' bathroom', ' bedroom'], 'evidence_proportions': [0.710498046875, 0.524169921875, 0.8050537109375, 0.734710693359375]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:21:46.066 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:21:46.067 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-3_4-5-7-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (4, 5, 7, 8):   4%|▍         | 4/100 [01:54<45:52, 28.67s/it]is_0k: False
your chose emoji: ['🤹🏼\u200d♂️', '👨🏼\u200d⚕', '🐪', '⏹', '🙎\u200d♂', '🤦🏼\u200d♀', '⛽', '🚣', '🇷🇸', '🤱🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.40s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.64s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (0, 5, 6, 7):   4%|▍         | 4/100 [02:10<45:52, 28.67s/it]2025-01-22 04:22:02.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:22:02.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:22:02.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the bedroom.
2025-01-22 04:22:02.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11910, 11916) --> . Mary journeyed to the
2025-01-22 04:22:02.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary left the apple.
2025-01-22 04:22:03.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14443, 14447) -->  Mary left the apple
2025-01-22 04:22:03.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the apple.
2025-01-22 04:22:03.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16794, 16798) -->  Daniel left the apple
2025-01-22 04:22:03.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 04:22:03.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20104, 20109) --> . Daniel picked up the
2025-01-22 04:22:03.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:22:03.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19916, 19921) --> . Sandra moved to the
2025-01-22 04:22:03.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:22:03.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (111, 116) --> . John moved to the
2025-01-22 04:22:03.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:22:03.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21345, 21349) -->  Daniel took the football
2025-01-22 04:22:03.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the office.
2025-01-22 04:22:03.498 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6527, 6533) --> . Sandra journeyed to the
2025-01-22 04:22:03.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:22:03.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13775, 13781) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:22:06.259 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 04:22:06.260 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 04:22:14.648 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [193.61184210526315, 7.839393712265322, 74.95262096774194, 7.606914574009614, 9.842193161231885], 'topk_indices': array([24170, 24095,     1, 24163, 24106, 24067, 12006,     3,    23,
          14, 11963, 11962, 24169,    24, 24167, 24107, 24177, 24175,
       24174,     0]), 'topk_tokens': ['?', ' a', '<|start_header_id|>', ' Where', ' to', '.\n\n', 'ANT', '<|end_header_id|>', '4', '\n', 'IC', 'ANT', ' bedroom', '\n\n', ' before', ' the', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [245.6, 152.5625, 234.9375, 148.875]}, 'weight': {'score': [22.88486842105263, 23.437463816061534, 21.497731854838708, 23.44039067679844, 29.015172101449274], 'topk_indices': array([18750, 18794, 14609, 14645, 19389, 14654, 14690, 19447, 14508,
       14573, 20297, 20345, 18119, 18088, 23661, 23527, 21946, 21973,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 20.052083333333332, 26.205078125, 26.533203125]}, 'saliency': {'score': [1.1387361225328947, 0.04411083584574787, 0.41189378307711694, 0.04277654313021546, 0.07463792441547781], 'topk_indices': array([ 7549, 24161,  2369, 24067,    20, 24170, 24077,    23, 14446,
       24105, 12006,    24, 11963,    34, 24166, 24163, 11916, 11962,
       24167, 24169]), 'topk_tokens': ['nes', 'Question', 'nes', '.\n\n', ' Jul', '?', ' location', '4', ' apple', ' moved', 'ANT', '\n\n', 'IC', ' bathroom', ' apple', ' Where', ' bedroom', 'ANT', ' before', ' bedroom'], 'evidence_proportions': [1.37392578125, 0.7513834635416666, 1.53955078125, 1.02496337890625]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 04:22:14.672 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:22:14.679 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-14_pid-4_0-5-6-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 5, 6, 7):   5%|▌         | 5/100 [02:22<45:21, 28.65s/it]Processing depth (0, 5, 6, 7):   5%|▌         | 5/100 [02:23<45:22, 28.66s/it]
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:82 - Selected idx: 15
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:86 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Mary moved to the bathroom.', ' Daniel picked up the apple.', ' Daniel took the football.', ' Mary got the milk.', ' Sandra moved to the kitchen.', ' Mary journeyed to the bedroom.', ' John went back to the office.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary got the milk.', ' Mary journeyed to the bedroom.', ' Mary left the milk.', ' Daniel left the apple.']
2025-01-22 04:22:15.032 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👷🏿\u200d♂️', '🙋🏾\u200d♂', '🧑\u200d🚀', '\U0001faf7🏼', '🤲🏼', '🪓', '\U0001faf6', '🧑🏼\u200d⚖️', '🖐🏽', '🙇🏽\u200d♀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.78s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.97s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.48s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.12s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (0, 1, 4, 5, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:22:32.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:22:32.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:22:32.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 04:22:32.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3038, 3042) -->  Mary got the milk
2025-01-22 04:22:32.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:22:32.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9768, 9774) --> . Mary journeyed to the
2025-01-22 04:22:32.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:22:32.475 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11974, 11978) -->  Mary left the milk
2025-01-22 04:22:32.476 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:22:32.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16794, 16798) -->  Daniel left the apple
2025-01-22 04:22:32.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:22:32.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10059, 10064) --> . John moved to the
2025-01-22 04:22:32.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:22:32.708 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18390, 18396) --> . Sandra journeyed to the
2025-01-22 04:22:32.708 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:22:32.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13444, 13449) --> . Daniel picked up the
2025-01-22 04:22:32.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:22:32.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14577, 14581) -->  Daniel took the football
2025-01-22 04:22:32.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:22:32.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (20969, 20974) -->  Sandra moved to the kitchen
2025-01-22 04:22:32.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:22:33.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (19367, 19373) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:22:35.694 | INFO     | test_jbb_embedding:begin_test:693 - the bathroom<|eot_id|>
2025-01-22 04:22:35.694 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24231])
2025-01-22 04:22:44.073 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [545.1603260869565, 17.013996348105966, 98.6696068548387, 16.406937228598014, 14.976695667613637], 'topk_indices': array([ 3041,    14, 24150, 23494, 11978,    39,    30, 11977, 20691,
          29, 24226,   185,    32,    31,  3042,    35,  9774,    34,
       24227,     0]), 'topk_tokens': [' milk', '\n', ' the', 're', '.', '\n\n\n', 'Mary', ' milk', 'nes', '\n\n', '<|eot_id|>', 'ION', ' to', ' moved', '.', '.', ' bedroom', ' bathroom', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1013.3, 507.6875, 437.9583333333333, 584.125, 119.296875]}, 'weight': {'score': [23.463654891304348, 23.450885120079228, 22.27016129032258, 23.452386721980975, 29.759943181818183], 'topk_indices': array([18779, 18823, 14613, 14649, 19425, 14694, 14658, 19483, 14572,
       14507, 20345, 20297, 18110, 18141, 23684, 23550, 21996, 21969,
       23766, 23632]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 26.283203125, 20.052083333333332, 26.134765625, 26.533203125]}, 'saliency': {'score': [3.2737771739130435, 0.0967645634909889, 0.569214359406502, 0.09313688633163869, 0.112934025851163], 'topk_indices': array([   36,    29,  3056, 24204, 11975,  9770, 23494, 24224,    39,
          38, 24216,   185, 24218,  3041, 20691,    30, 11977,    31,
        9774,    34]), 'topk_tokens': [' PA', '\n\n', 'ern', 'Question', ' left', ' journey', 're', 'Answer', '\n\n\n', '***', ' milk', 'ION', ' discarded', ' milk', 'nes', 'Mary', ' milk', ' moved', ' bedroom', ' bathroom'], 'evidence_proportions': [5.8234375, 3.43701171875, 2.2615559895833335, 3.9091796875, 0.806396484375]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-22 04:22:44.081 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:22:44.081 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-0_0-1-4-5-7.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 1, 4, 5, 7):   1%|          | 1/100 [00:28<47:40, 28.89s/it]is_0k: False
your chose emoji: ['🖖', '🤟🏿', '💇🏼', '👩🏽\u200d❤\u200d💋\u200d👨🏾', '😩', '🏋🏿\u200d♀️', '💦', '👩🏻\u200d❤\u200d👩🏼', '👩🏻\u200d🏫', '🧑🏿\u200d❤\u200d💋\u200d🧑🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.91s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (1, 5, 6, 7, 8):   1%|          | 1/100 [00:45<47:40, 28.89s/it]2025-01-22 04:23:01.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:23:01.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2953, 2958) --> . Mary moved to the
2025-01-22 04:23:01.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 04:23:01.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11834, 11838) -->  Mary got the milk
2025-01-22 04:23:01.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:23:01.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14358, 14364) --> . Mary journeyed to the
2025-01-22 04:23:01.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:23:01.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16799, 16803) -->  Mary left the milk
2025-01-22 04:23:01.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:23:01.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19256, 19260) -->  left the apple.
2025-01-22 04:23:01.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:23:01.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10085, 10090) --> . John moved to the
2025-01-22 04:23:01.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:23:01.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18325, 18331) --> . Sandra journeyed to the
2025-01-22 04:23:01.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:23:01.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13383, 13388) -->  Daniel picked up the apple
2025-01-22 04:23:01.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:23:01.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14482, 14486) -->  Daniel took the football
2025-01-22 04:23:01.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:23:01.763 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (20956, 20961) --> . Sandra moved to the
2025-01-22 04:23:01.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:23:01.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (19258, 19264) -->  apple. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:23:04.534 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 04:23:04.535 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24233])
2025-01-22 04:23:12.903 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [192.9266304347826, 6.634478410216207, 68.33492943548387, 6.378195494065834, 12.88724543539326], 'topk_indices': array([16803, 11835, 24112, 11834, 24219, 16801,  2958,    23,     1,
          14, 24218, 17963, 16802, 11837, 11838, 24231, 24229, 14364,
           0, 24228]), 'topk_tokens': ['.', ' got', '.\n\n', ' Mary', ' was', ' the', ' bathroom', '4', '<|start_header_id|>', '\n', ' milk', ' Broadway', ' milk', ' milk', '.', '<|end_header_id|>', '<|start_header_id|>', ' bedroom', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [171.1875, 296.0, 190.29166666666666, 275.09375, 38.8125]}, 'weight': {'score': [22.16983695652174, 23.451283730813664, 22.930695564516128, 23.453169906852203, 29.74877106741573], 'topk_indices': array([18796, 18840, 14648, 14684, 14693, 19533, 14729, 19475, 14612,
       14547, 20395, 20347, 18158, 18127, 23710, 23576, 22042, 22015,
       23658, 23792]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.283203125, 20.052083333333332, 26.134765625, 20.421875]}, 'saliency': {'score': [1.1560244352921196, 0.03778885071536144, 0.4120089623235887, 0.036245544044899096, 0.0956022712621796], 'topk_indices': array([24118,    23,  2954, 10090, 24078, 24226,  2955, 24216, 14360,
       16800, 24206, 11835, 11834, 17954, 24218,  2958, 11837, 16802,
       17963, 14364]), 'topk_tokens': [' item', '4', ' Mary', ' garden', ' context', 'Answer', ' moved', ' where', ' journey', ' left', 'Question', ' got', ' Mary', ' Eighth', ' milk', ' bathroom', ' milk', ' milk', ' Broadway', ' bedroom'], 'evidence_proportions': [0.91611328125, 2.0289306640625, 0.9833170572916666, 1.7877197265625, 0.2103729248046875]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:23:12.908 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:23:12.908 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-1_1-5-6-7-8.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 5, 6, 7, 8):   2%|▏         | 2/100 [00:57<47:07, 28.85s/it]is_0k: False
your chose emoji: ['🏴\U000e0067\U000e0062\U000e0065\U000e006e\U000e0067\U000e007f', '🧑🏾\u200d🦳', '🧎🏻\u200d♂️\u200d➡️', '👮🏻\u200d♀️', '🎅🏿', '🩱', '🧍🏿\u200d♂️', '👨\u200d🦼', '🩴', '👱🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.32s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Processing depth (0, 2, 3, 4, 8):   2%|▏         | 2/100 [01:15<47:07, 28.85s/it]2025-01-22 04:23:30.664 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:23:30.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 04:23:30.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 04:23:30.689 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4926, 4930) -->  Mary got the milk
2025-01-22 04:23:30.689 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:23:30.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7562, 7568) --> . Mary journeyed to the
2025-01-22 04:23:30.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:23:30.774 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9724, 9728) -->  Mary left the milk
2025-01-22 04:23:30.775 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:23:30.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19242, 19246) -->  left the apple.
2025-01-22 04:23:30.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:23:30.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10032, 10037) --> . John moved to the
2025-01-22 04:23:30.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:23:31.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18295, 18301) --> . Sandra journeyed to the
2025-01-22 04:23:31.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:23:31.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13456, 13461) --> . Daniel picked up the
2025-01-22 04:23:31.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:23:31.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14481, 14485) -->  Daniel took the football
2025-01-22 04:23:31.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:23:31.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (20982, 20987) --> . Sandra moved to the
2025-01-22 04:23:31.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:23:31.361 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (19244, 19250) -->  apple. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:23:34.128 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the milk.<|eot_id|>
2025-01-22 04:23:34.128 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24203])
2025-01-22 04:23:42.499 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [505.8641304347826, 13.411120951417004, 90.09274193548387, 12.843733179446836, 10.313912056587839], 'topk_indices': array([24048,    14, 10355, 10384,    24,  7567, 24188,  9728, 10391,
          31,  4929,    35,  4930,    34,  7568, 10356, 24198, 10392,
           0, 24199]), 'topk_tokens': [' context', '\n', ' V', 't', '\n\n', ' the', ' milk', '.', ' V', ' moved', ' milk', '.', '.', ' bathroom', ' bedroom', 'icks', '<|eot_id|>', 'icks', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [702.55, 601.25, 555.4166666666666, 512.625, 83.53125]}, 'weight': {'score': [22.400815217391305, 23.4425297446914, 22.1875, 23.445132649470022, 29.58720439189189], 'topk_indices': array([18854, 18810, 14647, 14683, 19519, 14692, 14728, 19461, 14611,
       14546, 20381, 20333, 18172, 18141, 23552, 23686, 22018, 21991,
       23768, 23634]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 26.283203125, 20.052083333333332, 26.134765625, 20.421875]}, 'saliency': {'score': [3.0153543223505435, 0.0757936264752928, 0.5038511214717742, 0.0724448488026354, 0.0792643572833087], 'topk_indices': array([  185, 10355, 24182, 10357, 10391, 24190,    30,     0,  7564,
        4926,  9727, 24048,  9724,    31, 24188,  4929,    34, 10356,
       10392,  7568]), 'topk_tokens': ['ION', ' V', ' prior', 'burg', ' V', ' discarded', 'Mary', '<|begin_of_text|>', ' journey', ' Mary', ' milk', ' context', ' Mary', ' moved', ' milk', ' milk', ' bathroom', 'icks', 'icks', ' bedroom'], 'evidence_proportions': [4.0455078125, 4.1357421875, 2.7716471354166665, 3.55908203125, 0.429107666015625]}}, 'pred_res': 'Mary got the milk.<|eot_id|>', 'score': 0}
2025-01-22 04:23:42.515 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:23:42.515 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-2_0-2-3-4-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 2, 3, 4, 8):   3%|▎         | 3/100 [01:27<47:12, 29.20s/it]is_0k: False
your chose emoji: ['👨🏽\u200d❤️\u200d👨🏿', '🕴🏼', '\U0001faf1🏻\u200d\U0001faf2🏽', '🙍🏾\u200d♂️', '⌚', '🤵🏼\u200d♂️', '🖌️', '⁉️', '🇬🇹', '\U0001f6de']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.87s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.79s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (1, 2, 4, 6, 8):   3%|▎         | 3/100 [01:44<47:12, 29.20s/it]2025-01-22 04:23:59.496 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:23:59.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary moved to the
2025-01-22 04:23:59.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 04:23:59.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4994, 4998) -->  Mary got the milk
2025-01-22 04:23:59.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:23:59.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9840, 9846) --> . Mary journeyed to the
2025-01-22 04:23:59.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:23:59.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14552, 14556) -->  Mary left the milk
2025-01-22 04:23:59.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:23:59.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19357, 19361) -->  Daniel left the apple
2025-01-22 04:23:59.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:23:59.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10153, 10158) --> . John moved to the
2025-01-22 04:23:59.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:23:59.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18379, 18385) --> . Sandra journeyed to the
2025-01-22 04:23:59.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:23:59.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13733, 13738) --> . Daniel picked up the
2025-01-22 04:23:59.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:24:00.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14701, 14705) -->  Daniel took the football
2025-01-22 04:24:00.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:24:00.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (20976, 20981) --> . Sandra moved to the
2025-01-22 04:24:00.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:24:00.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (19360, 19366) -->  apple. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:24:02.940 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 04:24:02.940 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 04:24:11.337 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [223.9796195652174, 7.386209575830167, 55.17023689516129, 7.11868108519952, 7.166484882305195], 'topk_indices': array([ 4994,  4958,  4996,    14,     1, 24098,  4995, 24088,    23,
          24, 14556,  3036,  4997, 14555, 24194, 24207,  4998, 24204,
           0, 24205]), 'topk_tokens': [' Mary', '4', ' the', '\n', '<|start_header_id|>', ' location', ' got', '.\n\n', '4', '\n\n', '.', ' bathroom', ' milk', ' milk', ' milk', '<|end_header_id|>', '.', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [218.725, 448.9375, 113.66666666666667, 361.15625, 33.8828125]}, 'weight': {'score': [23.23267663043478, 23.44021301420783, 22.1875, 23.442018107355743, 28.989448051948052], 'topk_indices': array([18812, 18768, 14624, 14588, 19477, 14633, 14669, 19419, 14547,
       14482, 20321, 20369, 18130, 18099, 23678, 23544, 21983, 22010,
       23760, 23626]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.283203125, 20.052083333333332, 26.134765625, 26.533203125]}, 'saliency': {'score': [1.3737315302309783, 0.04127098851099145, 0.30366663778981856, 0.039665687675359225, 0.05261696159065544], 'topk_indices': array([   22, 24182, 24192,    19, 14553,  3051,    20, 24088,  4998,
          23, 24188,  9846,    24,  4994, 24098,  4995,  4997, 14555,
        3036, 24194]), 'topk_tokens': ['202', 'Question', ' where', '26', ' left', 'ern', ' Jul', '.\n\n', '.', '4', ' prior', ' bedroom', '\n\n', ' Mary', ' location', ' got', ' milk', ' milk', ' bathroom', ' milk'], 'evidence_proportions': [1.075390625, 2.99609375, 0.6066080729166666, 2.4180908203125, 0.230621337890625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:24:11.362 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:24:11.362 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-3_1-2-4-6-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (1, 2, 4, 6, 8):   4%|▍         | 4/100 [01:56<46:29, 29.06s/it]is_0k: False
your chose emoji: ['👨🏼\u200d🤝\u200d👨🏽', '🧑🏼\u200d🦯', '🚶🏻\u200d♀\u200d➡', '🧑🏽\u200d❤\u200d🧑🏿', '⚱', '👳', '🥏', '🦧', '😴', '👩🏽\u200d🦽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.37s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.47s/it]
Processing depth (4, 5, 6, 7, 9):   4%|▍         | 4/100 [02:12<46:29, 29.06s/it]2025-01-22 04:24:27.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 04:24:27.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9577, 9582) --> . Mary moved to the
2025-01-22 04:24:27.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 04:24:27.691 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11692, 11696) -->  Mary got the milk
2025-01-22 04:24:27.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the bedroom.
2025-01-22 04:24:27.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14196, 14202) --> . Mary journeyed to the
2025-01-22 04:24:27.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the milk.
2025-01-22 04:24:27.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16643, 16647) -->  Mary left the milk
2025-01-22 04:24:27.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel left the apple.
2025-01-22 04:24:27.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21402, 21406) -->  Daniel left the apple
2025-01-22 04:24:27.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:24:28.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9807, 9812) --> . John moved to the
2025-01-22 04:24:28.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:24:28.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18183, 18189) --> . Sandra journeyed to the
2025-01-22 04:24:28.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel picked up the apple.
2025-01-22 04:24:28.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13178, 13183) --> . Daniel picked up the
2025-01-22 04:24:28.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 04:24:28.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14324, 14328) -->  Daniel took the football
2025-01-22 04:24:28.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:24:28.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (20861, 20866) --> . Sandra moved to the
2025-01-22 04:24:28.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the office.
2025-01-22 04:24:28.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (19110, 19116) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:24:31.163 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:24:31.164 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24143])
2025-01-22 04:24:39.519 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [129.53566576086956, 5.3923624772219005, 47.89213709677419, 5.219160211377221, 7.0869362571022725], 'topk_indices': array([   23,     1, 24123, 24136, 24142, 24115,     3, 24116,    14,
       23988, 24135, 24137, 24117,    24, 24022, 24141, 14202,     0,
       24138, 24139]), 'topk_tokens': ['4', '<|start_header_id|>', ' to', 'Answer', '\n\n', '.\n\n', '<|end_header_id|>', 'Question', '\n', ' context', '?\n', ':', ':', '\n\n', '.\n\n', '<|end_header_id|>', ' bedroom', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [168.65, 150.71875, 130.9375, 172.765625, 14.126953125]}, 'weight': {'score': [23.23267663043478, 23.42486850824153, 21.497731854838708, 23.427531701394653, 28.940696022727273], 'topk_indices': array([18752, 18796, 14642, 14606, 14687, 14651, 19456, 19398, 14570,
       14505, 20318, 20270, 18083, 18114, 23620, 23486, 21925, 21952,
       23568, 23702]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 26.283203125, 20.052083333333332, 26.134765625, 26.533203125]}, 'saliency': {'score': [0.7586417820142664, 0.02982220301758055, 0.29070798812373994, 0.02879072743833372, 0.05223569003018466], 'topk_indices': array([24103, 24142, 17919, 24135, 24115,  9579, 18184, 24032,    24,
       24128, 24140, 11695, 16646, 24022, 24122,  9582, 23988, 24136,
       24116, 14202]), 'topk_tokens': [' return', '\n\n', ' Broadway', '?\n', '.\n\n', ' moved', ' Sandra', ' location', '\n\n', ' milk', 'assistant', ' milk', ' milk', '.\n\n', ' prior', ' bathroom', ' context', 'Answer', 'Question', ' bedroom'], 'evidence_proportions': [0.8748046875, 1.02239990234375, 0.6706136067708334, 1.1427001953125, 0.09766387939453125]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 04:24:39.524 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:24:39.524 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-15_pid-4_4-5-6-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (4, 5, 6, 7, 9):   5%|▌         | 5/100 [02:24<45:29, 28.74s/it]Processing depth (4, 5, 6, 7, 9):   5%|▌         | 5/100 [02:24<45:51, 28.96s/it]
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:82 - Selected idx: 16
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:86 - Needle: [' John went back to the office.', ' John journeyed to the bedroom.', ' Daniel grabbed the apple.', ' John moved to the garden.', ' Sandra journeyed to the office.', ' Sandra moved to the kitchen.', ' Sandra went back to the hallway.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-22 04:24:39.977 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel grabbed the apple.', ' Daniel travelled to the hallway.', ' Daniel went back to the garden.', ' Mary travelled to the bedroom.']
2025-01-22 04:24:39.978 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['♻', '🆖', '🙆🏿\u200d♂️', '🏡', '🤝', '🌶', '\U0001fab8', '💃🏽', '👬🏼', '\U0001faf3🏽']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.47s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.82s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (0, 4, 7, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 04:24:57.516 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:24:57.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:24:57.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9747, 9752) --> . Daniel travelled to the
2025-01-22 04:24:57.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:24:57.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16749, 16755) --> . Daniel went back to the
2025-01-22 04:24:57.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:24:57.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19214, 19219) --> . Mary travelled to the
2025-01-22 04:24:57.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:24:57.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (630, 636) --> . John went back to the
2025-01-22 04:24:57.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:24:57.932 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6642, 6648) --> . John journeyed to the
2025-01-22 04:24:57.933 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:24:57.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7009, 7014) --> . John moved to the
2025-01-22 04:24:57.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:24:58.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20598, 20604) --> . Sandra journeyed to the
2025-01-22 04:24:58.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:24:58.186 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22541, 22546) --> . Sandra moved to the
2025-01-22 04:24:58.186 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:24:58.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2203, 2209) --> . Sandra went back to the
2025-01-22 04:24:58.197 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 04:24:58.197 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 04:24:58.197 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 04:24:58.197 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['👩🏼\u200d❤️\u200d👨🏾', '💆🏾', '🏴\U000e0067\U000e0062\U000e0077\U000e006c\U000e0073\U000e007f', '👨\u200d🦼', '\U0001faf0🏻', '👨🏾\u200d⚖️', '👩🏾\u200d❤️\u200d💋\u200d👩🏽', '◽', '🏳️', '🖱️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.82s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.56s/it]
Processing depth (0, 2, 7, 9):   0%|          | 0/100 [00:34<?, ?it/s]2025-01-22 04:25:14.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:25:14.580 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:25:14.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4878, 4883) --> . Daniel travelled to the
2025-01-22 04:25:14.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:25:14.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16709, 16715) --> . Daniel went back to the
2025-01-22 04:25:14.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:25:14.804 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21447, 21452) --> . Mary travelled to the
2025-01-22 04:25:14.804 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:25:14.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (630, 636) --> . John went back to the
2025-01-22 04:25:14.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:25:14.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6572, 6578) --> . John journeyed to the
2025-01-22 04:25:14.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:25:14.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6939, 6944) --> . John moved to the
2025-01-22 04:25:14.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:25:14.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20540, 20546) --> . Sandra journeyed to the
2025-01-22 04:25:14.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:25:15.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22489, 22494) --> . Sandra moved to the
2025-01-22 04:25:15.089 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:25:15.099 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2127, 2133) --> . Sandra went back to the
2025-01-22 04:25:15.099 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 04:25:15.099 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 04:25:15.100 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 04:25:15.100 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🍄\u200d🟫', '🇩🇯', '💂🏼\u200d♂', '🙅🏿\u200d♀️', '🤽🏿\u200d♂', '🧖🏽\u200d♂', '👷🏿\u200d♂', '🦋', '😮\u200d💨', '🤵\u200d♀️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (0, 4, 5, 9):   0%|          | 0/100 [00:51<?, ?it/s]2025-01-22 04:25:31.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:25:31.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:25:31.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9705, 9710) --> . Daniel travelled to the
2025-01-22 04:25:31.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:25:31.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11865, 11871) --> . Daniel went back to the
2025-01-22 04:25:31.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:25:31.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21477, 21482) --> . Mary travelled to the
2025-01-22 04:25:31.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:25:31.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (630, 636) --> . John went back to the
2025-01-22 04:25:31.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:25:31.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6572, 6578) --> . John journeyed to the
2025-01-22 04:25:31.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:25:31.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6939, 6944) --> . John moved to the
2025-01-22 04:25:31.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:25:32.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20570, 20576) --> . Sandra journeyed to the
2025-01-22 04:25:32.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:25:32.175 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22519, 22524) --> . Sandra moved to the
2025-01-22 04:25:32.175 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:25:32.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2379, 2385) --> . Sandra went back to the
2025-01-22 04:25:32.190 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 04:25:32.190 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 04:25:32.190 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 04:25:32.190 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['👩🏾\u200d❤\u200d💋\u200d👩🏽', '👷\u200d♂️', '👩🏼\u200d❤️\u200d👨🏾', '🧍🏽\u200d♂', '👰🏻', '👨🏿\u200d⚖', '🚹', '🖍️', '👮\u200d♂️', '💥']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.51s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (1, 2, 5, 7):   0%|          | 0/100 [01:08<?, ?it/s]2025-01-22 04:25:48.676 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:25:48.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2990, 2994) -->  Daniel grabbed the apple
2025-01-22 04:25:48.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:25:48.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4984, 4989) --> . Daniel travelled to the
2025-01-22 04:25:48.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:25:48.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11827, 11833) --> . Daniel went back to the
2025-01-22 04:25:48.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:25:48.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16680, 16685) --> . Mary travelled to the
2025-01-22 04:25:48.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:25:48.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (625, 631) --> . John went back to the
2025-01-22 04:25:48.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:25:48.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6706, 6712) --> . John journeyed to the
2025-01-22 04:25:48.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:25:48.951 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7125, 7130) --> . John moved to the
2025-01-22 04:25:48.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:25:49.059 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20530, 20536) --> . Sandra journeyed to the
2025-01-22 04:25:49.059 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:25:49.178 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22473, 22478) --> . Sandra moved to the
2025-01-22 04:25:49.178 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:25:49.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2202, 2208) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:25:51.932 | INFO     | test_jbb_embedding:begin_test:693 - The ballroom.<|eot_id|>
2025-01-22 04:25:51.932 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 04:26:00.307 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [167.4734375, 11.575100597898047, 136.48851102941177, 11.269675421954052, 17.064029947916666], 'topk_indices': array([10359,    24, 24147, 24154, 24148, 24164,  3014, 24019, 24150,
       24156,    14,  2994,  2993, 24152, 24155, 24163, 24153, 24160,
           0, 24161]), 'topk_tokens': ['t', '\n\n', 'Question', ' the', ':', '\n\n', ' its', ' context', ' was', '?', '\n', '.', ' apple', ' apple', ' garden', '<|end_header_id|>', ' before', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [482.625, 102.25, 114.72916666666667, 43.86875]}, 'weight': {'score': [22.388671875, 23.433641592188017, 20.441636029411764, 23.438726919528076, 28.998958333333334], 'topk_indices': array([18774, 18818, 14668, 14632, 19491, 14677, 19433, 14713, 14596,
       14531, 20305, 20353, 18143, 18112, 23657, 23523, 21956, 21983,
       23739, 23605]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [28.333984375, 21.01875, 20.927083333333332, 20.75625]}, 'saliency': {'score': [1.1080108642578126, 0.06504863194778478, 0.7581939697265625, 0.06320629197388253, 0.1290845235188802], 'topk_indices': array([10359, 24162,  3012, 24063, 24164,  7130, 24149,  2991, 24156,
       24150,  2990,  3014,  2203, 24019, 24147,  2208,  2993, 24152,
       24153, 24155]), 'topk_tokens': ['t', 'assistant', ' any', ' location', '\n\n', ' garden', ' Where', ' grabbed', '?', ' was', ' Daniel', ' its', ' Sandra', ' context', 'Question', ' hallway', ' apple', ' apple', ' before', ' garden'], 'evidence_proportions': [3.578125, 0.5795654296875, 0.606689453125, 0.26195068359375]}}, 'pred_res': 'The ballroom.<|eot_id|>', 'score': 0}
2025-01-22 04:26:00.313 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:26:00.314 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-0_1-2-5-7.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 2, 5, 7):   1%|          | 1/100 [01:20<2:12:22, 80.22s/it]is_0k: False
your chose emoji: ['🧜🏼\u200d♂️', '👨🏿\u200d🦽\u200d➡', '📽', '🗽', '\U0001fabc', '🇭🇺', '👊🏻', '🕗', '👷🏽\u200d♂️', '🏎']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.66s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.59s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.38s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.06s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (1, 2, 3, 6):   1%|          | 1/100 [01:37<2:12:22, 80.22s/it]2025-01-22 04:26:17.440 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:26:17.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3046, 3050) -->  Daniel grabbed the apple
2025-01-22 04:26:17.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:26:17.483 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4972, 4977) --> . Daniel travelled to the
2025-01-22 04:26:17.483 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:26:17.525 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7653, 7659) --> . Daniel went back to the
2025-01-22 04:26:17.525 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:26:17.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14557, 14562) -->  bonds. Mary travelled to
2025-01-22 04:26:17.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:26:17.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (625, 631) --> . John went back to the
2025-01-22 04:26:17.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:26:17.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6694, 6700) --> . John journeyed to the
2025-01-22 04:26:17.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:26:17.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7113, 7118) --> . John moved to the
2025-01-22 04:26:17.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:26:17.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20546, 20552) --> . Sandra journeyed to the
2025-01-22 04:26:17.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:26:17.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22489, 22494) --> . Sandra moved to the
2025-01-22 04:26:17.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:26:17.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2374, 2380) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:26:20.707 | INFO     | test_jbb_embedding:begin_test:693 - Daniel grabbed the apple.<|eot_id|>
2025-01-22 04:26:20.708 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24181])
2025-01-22 04:26:29.095 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [186.1197265625, 14.427254331376117, 143.08363970588235, 14.10366682423332, 10.623405905330882], 'topk_indices': array([ 2553, 24180,  2425,  2426, 24170, 24166,  2571, 24172,  2645,
       24179, 24168,  2427,  2428, 24171,  2430, 24169,  2429, 24176,
       24177,     0]), 'topk_tokens': ['engers', '\n\n', ' *', '      ', ' the', ' was', 'ared', '?', ' ne', '<|end_header_id|>', ' apple', ' *', '      ', ' garden', '      ', ' before', ' *', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [517.75, 111.2, 154.60416666666666, 33.55390625]}, 'weight': {'score': [23.490625, 23.439391746609328, 20.441636029411764, 23.443573223166183, 29.366498161764707], 'topk_indices': array([18726, 18770, 14632, 14596, 14641, 19371, 19429, 14677, 14554,
       14489, 20291, 20243, 18095, 18064, 23673, 23539, 21999, 21972,
       23621, 23755]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [28.333984375, 21.01875, 20.927083333333332, 25.1640625]}, 'saliency': {'score': [1.2192214965820312, 0.08171897229476359, 0.7769703584558824, 0.07979652730449324, 0.07834350361543543], 'topk_indices': array([24166,  2570, 24172,  2552,  2425, 24163,  2426,  2627,  3049,
        2571,  2553,  2380,  2645,  2427,  2428, 24168,  2430, 24169,
       24171,  2429]), 'topk_tokens': [' was', ' ne', '?', ' mess', ' *', 'Question', '      ', ' mess', ' apple', 'ared', 'engers', ' hallway', ' ne', ' *', '      ', ' apple', '      ', ' before', ' garden', ' *'], 'evidence_proportions': [3.8438720703125, 0.641259765625, 0.7925211588541666, 0.209503173828125]}}, 'pred_res': 'Daniel grabbed the apple.<|eot_id|>', 'score': 0}
2025-01-22 04:26:29.102 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:26:29.102 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-1_1-2-3-6.pkl | len: 3 |  size: 2.01 KB
Processing depth (1, 2, 3, 6):   2%|▏         | 2/100 [01:49<1:21:36, 49.97s/it]is_0k: False
your chose emoji: ['👩🏼\u200d🦳', '💆🏿\u200d♀', '✴', '🔹', '🐣', '👨🏼\u200d🤝\u200d👨🏽', '🤵🏼\u200d♂️', '🍀', '🏃🏿\u200d♂️', '🈳']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.11s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.52s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (0, 1, 5, 9):   2%|▏         | 2/100 [02:05<1:21:36, 49.97s/it]2025-01-22 04:26:45.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:26:46.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:26:46.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2942, 2947) --> . Daniel travelled to the
2025-01-22 04:26:46.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:26:46.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11883, 11889) --> . Daniel went back to the
2025-01-22 04:26:46.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:26:46.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21515, 21520) --> . Mary travelled to the
2025-01-22 04:26:46.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:26:46.200 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (630, 636) --> . John went back to the
2025-01-22 04:26:46.200 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:26:46.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6622, 6628) --> . John journeyed to the
2025-01-22 04:26:46.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:26:46.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7020, 7025) -->  John moved to the garden
2025-01-22 04:26:46.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:26:46.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20600, 20606) --> . Sandra journeyed to the
2025-01-22 04:26:46.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:26:46.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22547, 22552) --> . Sandra moved to the
2025-01-22 04:26:46.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:26:46.495 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2195, 2201) --> . Sandra went back to the
2025-01-22 04:26:46.495 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 04:26:46.495 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 04:26:46.495 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 04:26:46.495 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['👩🏻\u200d❤️\u200d💋\u200d👨🏻', '👩\u200d🏫', '👨🏻\u200d❤️\u200d💋\u200d👨🏻', '🇪🇸', '🙎🏿\u200d♂', '🧍🏿\u200d♂', '🫕', '🎆', '🇷🇸', '💇\u200d♀️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.56s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.33s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  2.97s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
Processing depth (1, 3, 4, 7):   2%|▏         | 2/100 [02:22<1:21:36, 49.97s/it]2025-01-22 04:27:02.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:27:02.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2982, 2986) -->  Daniel grabbed the apple
2025-01-22 04:27:02.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:27:02.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7475, 7480) --> . Daniel travelled to the
2025-01-22 04:27:02.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:27:02.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9713, 9719) --> . Daniel went back to the
2025-01-22 04:27:02.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:27:02.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16764, 16769) --> . Mary travelled to the
2025-01-22 04:27:02.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:27:02.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (625, 631) --> . John went back to the
2025-01-22 04:27:02.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:27:02.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6580, 6586) --> . John journeyed to the
2025-01-22 04:27:02.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:27:02.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6969, 6974) --> . John moved to the
2025-01-22 04:27:02.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:27:02.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20630, 20636) --> . Sandra journeyed to the
2025-01-22 04:27:02.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:27:02.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22559, 22564) --> . Sandra moved to the
2025-01-22 04:27:02.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:27:02.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2194, 2200) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:27:05.667 | INFO     | test_jbb_embedding:begin_test:693 - The ballroom<|eot_id|>
2025-01-22 04:27:05.667 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24257])
2025-01-22 04:27:14.097 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [166.52109375, 11.50550288540808, 108.80583639705883, 11.240753519272081, 8.264726746757075], 'topk_indices': array([   23,  2660,  2644, 24242, 24246,  2200,    24,  2986,  2985,
       24256,    14,  2589,  2663, 24244, 24247, 24245, 24252, 24253,
       24255,     0]), 'topk_tokens': ['4', 'as', ',', ' was', ' the', ' hallway', '\n\n', '.', ' apple', '\n\n', '\n', 'ared', ' ne', ' apple', ' garden', ' before', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [370.03125, 168.725, 120.77083333333333, 56.409375]}, 'weight': {'score': [22.388671875, 23.461235057708162, 20.441636029411764, 23.4663626141246, 29.741450471698112], 'topk_indices': array([18904, 18860, 14728, 14692, 19557, 14773, 19499, 14737, 14656,
       14591, 20419, 20371, 18198, 18229, 23609, 23743, 22022, 22049,
       23825, 23691]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [28.333984375, 21.01875, 20.927083333333332, 20.75625]}, 'saliency': {'score': [1.05074462890625, 0.0654031651332569, 0.6006281235638786, 0.06383725263792099, 0.06091517322468308], 'topk_indices': array([ 2983, 24254, 24226, 24155, 24256,  2570, 24239,  2195,  2645,
        2588, 24111,  7480,  2571,  2589,  2985,  2663,  2200, 24245,
       24244, 24247]), 'topk_tokens': [' grabbed', 'assistant', ' return', ' location', '\n\n', ' mess', 'Question', ' Sandra', ' mess', ' ne', ' context', ' hallway', 'engers', 'ared', ' apple', ' ne', ' hallway', ' before', ' apple', ' garden'], 'evidence_proportions': [2.737548828125, 0.921240234375, 0.6269938151041666, 0.339306640625]}}, 'pred_res': 'The ballroom<|eot_id|>', 'score': 0}
2025-01-22 04:27:14.105 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:27:14.105 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-2_1-3-4-7.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 3, 4, 7):   3%|▎         | 3/100 [02:34<1:17:06, 47.70s/it]is_0k: False
your chose emoji: ['👩🏾\u200d❤️\u200d💋\u200d👨🏾', '👨🏽\u200d🤝\u200d👨🏾', '🧑🏾\u200d💼', '👨🏻\u200d🎓', '👨🏾\u200d❤️\u200d👨🏼', '👩\u200d🦽\u200d➡️', '💇\u200d♂️', '👱🏽\u200d♀️', '⏲', '👷🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.26s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.12s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (2, 4, 6, 9):   3%|▎         | 3/100 [02:50<1:17:06, 47.70s/it]2025-01-22 04:27:30.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:27:30.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4894, 4898) -->  Daniel grabbed the apple
2025-01-22 04:27:30.877 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:27:30.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9717, 9722) --> . Daniel travelled to the
2025-01-22 04:27:30.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:27:31.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14315, 14321) --> . Daniel went back to the
2025-01-22 04:27:31.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:27:31.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21437, 21442) --> . Mary travelled to the
2025-01-22 04:27:31.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:27:31.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (625, 631) --> . John went back to the
2025-01-22 04:27:31.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:27:31.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6610, 6616) --> . John journeyed to the
2025-01-22 04:27:31.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:27:31.215 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7007, 7012) --> . John moved to the
2025-01-22 04:27:31.215 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:27:31.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20530, 20536) --> . Sandra journeyed to the
2025-01-22 04:27:31.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:27:31.434 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22479, 22484) --> . Sandra moved to the
2025-01-22 04:27:31.434 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:27:31.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2374, 2380) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:27:34.125 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom.<|eot_id|>
2025-01-22 04:27:34.125 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24191])
2025-01-22 04:27:42.492 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [182.01875, 9.221125330660495, 121.47725183823529, 8.919855206607291, 12.810841181506849], 'topk_indices': array([    4,  2380,    23, 24079, 24174, 24190,  4895,  4898,  4894,
          14, 24181,  6612, 24179,    24, 24187, 24178,  4897, 24189,
       24186,     0]), 'topk_tokens': ['\n\n', ' hallway', '4', '.\n\n', ':', '\n\n', ' grabbed', '.', ' Daniel', '\n', ' garden', ' journey', ' before', '\n\n', '<|start_header_id|>', ' apple', ' apple', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [508.5625, 154.6125, 121.86458333333333, 20.375]}, 'weight': {'score': [22.388671875, 23.43972420848144, 20.441636029411764, 23.44481766518227, 29.015625], 'topk_indices': array([18794, 18838, 14660, 14696, 19491, 19433, 14741, 14705, 14559,
       14624, 20305, 20353, 18132, 18163, 23529, 23663, 21989, 21962,
       23745, 23611]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [28.333984375, 21.01875, 20.927083333333332, 20.75625]}, 'saliency': {'score': [1.1913070678710938, 0.052248333942558074, 0.6830637314740349, 0.05041615091875356, 0.0955638101656143], 'topk_indices': array([14321, 24079, 14337,    24, 24188, 14366, 24045, 24089, 24173,
        6616,  2375, 24179,  9722, 24181,  4895,  2380,  4894,  6612,
       24178,  4897]), 'topk_tokens': [' garden', '.\n\n', 'ance', '\n\n', 'assistant', 'ord', ' context', ' location', 'Question', ' bedroom', ' Sandra', ' before', ' hallway', ' garden', ' grabbed', ' hallway', ' Daniel', ' journey', ' apple', ' apple'], 'evidence_proportions': [3.79345703125, 0.85185546875, 0.6311848958333334, 0.121185302734375]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 04:27:42.499 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:27:42.499 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-3_2-4-6-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (2, 4, 6, 9):   4%|▍         | 4/100 [03:02<1:04:07, 40.08s/it]is_0k: False
your chose emoji: ['🕍', '🦶', '🕴🏿', '👨🏽\u200d🦯\u200d➡️', '👌🏽', '😫', '🤰🏻', '👩🏿\u200d⚕️', '🧎🏽\u200d➡', '👩🏿\u200d🦽\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (1, 4, 8, 9):   4%|▍         | 4/100 [03:19<1:04:07, 40.08s/it]2025-01-22 04:27:59.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel grabbed the apple.
2025-01-22 04:27:59.725 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2978, 2982) -->  Daniel grabbed the apple
2025-01-22 04:27:59.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the hallway.
2025-01-22 04:27:59.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9745, 9750) --> . Daniel travelled to the
2025-01-22 04:27:59.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:27:59.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19309, 19315) --> . Daniel went back to the
2025-01-22 04:27:59.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:27:59.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21431, 21436) --> . Mary travelled to the
2025-01-22 04:27:59.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:27:59.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (625, 631) --> . John went back to the
2025-01-22 04:27:59.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bedroom.
2025-01-22 04:28:00.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6605, 6611) -->  John journeyed to the bedroom
2025-01-22 04:28:00.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:28:00.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7002, 7007) -->  John moved to the garden
2025-01-22 04:28:00.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the office.
2025-01-22 04:28:00.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20516, 20522) --> . Sandra journeyed to the
2025-01-22 04:28:00.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:28:00.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22437, 22442) --> . Sandra moved to the
2025-01-22 04:28:00.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went back to the hallway.
2025-01-22 04:28:00.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2190, 2196) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:28:03.005 | INFO     | test_jbb_embedding:begin_test:693 - The ballroom.<|eot_id|>
2025-01-22 04:28:03.006 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24135])
2025-01-22 04:28:11.336 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [157.2671875, 11.75084281009197, 132.49034926470588, 11.45955107644079, 18.899131944444445], 'topk_indices': array([24118,  3001,    24, 24120,  2982, 24134,  2981,    14,  3002,
       10060, 10025, 24122, 10061, 24125, 24123, 10026, 24130, 24133,
           0, 24131]), 'topk_tokens': [':', ' of', '\n\n', ' was', '.', '\n\n', ' apple', '\n', ' its', '\n', 'po', ' apple', 'po', ' garden', ' before', 'or', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [421.125, 131.3, 119.98958333333333, 16.88125]}, 'weight': {'score': [22.388671875, 23.42939555886983, 21.903492647058822, 23.432413959994186, 29.714583333333334], 'topk_indices': array([18721, 18765, 14615, 14579, 14624, 14660, 19425, 19367, 14543,
       14478, 20245, 20293, 18055, 18086, 23621, 23487, 21941, 21914,
       23569, 23703]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [28.333984375, 21.01875, 20.927083333333332, 20.75625]}, 'saliency': {'score': [1.0186302185058593, 0.06642299433692467, 0.8017156264361214, 0.06459422444925021, 0.1367770724826389], 'topk_indices': array([ 3000, 24132, 24134, 24120,  2979,  2191, 23989,  2978, 24117,
        8705,  9750,  2196,  3002,  2981, 10025, 10061, 10026, 24122,
       24123, 24125]), 'topk_tokens': [' any', 'assistant', '\n\n', ' was', ' grabbed', ' Sandra', ' context', ' Daniel', 'Question', ' Father', ' hallway', ' hallway', ' its', ' apple', 'po', 'po', 'or', ' apple', ' before', ' garden'], 'evidence_proportions': [3.1180419921875, 0.7347900390625, 0.6272379557291666, 0.0926116943359375]}}, 'pred_res': 'The ballroom.<|eot_id|>', 'score': 0}
2025-01-22 04:28:11.344 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:28:11.344 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-16_pid-4_1-4-8-9.pkl | len: 3 |  size: 2.0 KB
Processing depth (1, 4, 8, 9):   5%|▌         | 5/100 [03:31<57:02, 36.03s/it]  Processing depth (1, 4, 8, 9):   5%|▌         | 5/100 [03:31<1:06:58, 42.31s/it]
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:82 - Selected idx: 17
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:86 - Needle: [' John moved to the garden.', ' Sandra journeyed to the office.', ' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' John went back to the office.', ' Sandra went back to the hallway.', ' Daniel went back to the garden.', ' Sandra moved to the kitchen.', ' John journeyed to the bedroom.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-22 04:28:11.617 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel travelled to the hallway.', ' Daniel grabbed the apple.', ' Daniel went back to the garden.', ' Daniel put down the apple.', ' Mary travelled to the bedroom.']
2025-01-22 04:28:11.618 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['☣️', '🙅🏿\u200d♀', '\U0001faf1🏾', '🕳️', '☃️', '🧚\u200d♂️', '\U0001faa9', '🎰', '☕', '🇬🇭']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (1, 2, 3, 4, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:28:28.174 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 04:28:28.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2968, 2973) -->  tragedy. Daniel travelled to
2025-01-22 04:28:28.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 04:28:28.212 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4871, 4875) -->  Daniel grabbed the apple
2025-01-22 04:28:28.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:28:28.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7455, 7461) --> . Daniel went back to the
2025-01-22 04:28:28.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel put down the apple.
2025-01-22 04:28:28.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9706, 9711) --> . Daniel put down the
2025-01-22 04:28:28.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:28:28.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19193, 19198) --> . Mary travelled to the
2025-01-22 04:28:28.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:28:28.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9596, 9601) --> . John moved to the
2025-01-22 04:28:28.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:28:28.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14397, 14403) --> . Sandra journeyed to the
2025-01-22 04:28:28.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:28:28.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15730, 15736) --> . John went back to the
2025-01-22 04:28:28.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:28:28.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3619, 3625) --> . Sandra went back to the
2025-01-22 04:28:28.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:28:28.662 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8231, 8236) -->  St. Sandra moved to
2025-01-22 04:28:28.662 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 04:28:28.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11800, 11806) --> . John journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:28:31.442 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 04:28:31.442 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24222])
2025-01-22 04:28:39.838 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [368.2775, 18.187409700722394, 179.31020220588235, 17.598548192708765, 20.87818287037037], 'topk_indices': array([ 9712,  2972,  2969,  4875, 19637,    23,  4874,    14, 24207,
        2971,  9711,  2975, 24220,  3625, 24221,     0,  2974, 24222,
       24217, 24218]), 'topk_tokens': ['.', ' to', '.', '.', ' hall', '4', ' apple', '\n', ' apple', ' travelled', ' apple', '.', '<|end_header_id|>', ' hallway', '\n\n', '<|begin_of_text|>', ' hallway', 'hall', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [670.15, 517.0625, 222.08333333333334, 468.95, 22.1375]}, 'weight': {'score': [23.479375, 23.443498452012385, 20.97035845588235, 23.44694088543822, 29.07908950617284], 'topk_indices': array([18835, 18879, 14718, 14682, 14763, 19480, 19538, 14727, 14581,
       14646, 20352, 20400, 18204, 18173, 23691, 23557, 22023, 21996,
       23773, 23639]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.8265625, 28.333984375, 20.927083333333332, 22.034375, 20.75625]}, 'saliency': {'score': [2.298436279296875, 0.10287387529024768, 1.0343484317555147, 0.09929201668642476, 0.1529017318913966], 'topk_indices': array([   23,  4871,  9709, 24067, 24219, 22743, 24195,  3620, 24201,
       19637,  2970, 24209,  4874, 24207,  9711,  2971, 24221,  3625,
        2974, 24222]), 'topk_tokens': ['4', ' Daniel', ' down', ' context', 'assistant', ' hall', 'Question', ' Sandra', ' prior', ' hall', ' Daniel', ' discarded', ' apple', ' apple', ' apple', ' travelled', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [4.1705078125, 3.88330078125, 1.1875813802083333, 2.66953125, 0.120404052734375]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 04:28:39.842 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:28:39.842 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-0_1-2-3-4-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 2, 3, 4, 8):   1%|          | 1/100 [00:28<46:22, 28.11s/it]is_0k: False
your chose emoji: ['👨🏼\u200d❤\u200d👨🏽', '💇🏿', '🧑\u200d🧑\u200d🧒\u200d🧒', '🧦', '👩🏾\u200d🦰', '🙎🏿\u200d♀', '🍺', '🇲🇽', '🇬🇭', '👰🏿\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]
Processing depth (1, 4, 6, 7, 9):   1%|          | 1/100 [00:45<46:22, 28.11s/it]2025-01-22 04:28:57.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 04:28:57.022 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel travelled to the
2025-01-22 04:28:57.022 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 04:28:57.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9721, 9725) -->  Daniel grabbed the apple
2025-01-22 04:28:57.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:28:57.150 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14384, 14390) --> . Daniel went back to the
2025-01-22 04:28:57.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel put down the apple.
2025-01-22 04:28:57.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16821, 16826) --> . Daniel put down the
2025-01-22 04:28:57.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:28:57.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21499, 21504) --> . Mary travelled to the
2025-01-22 04:28:57.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:28:57.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9610, 9615) --> . John moved to the
2025-01-22 04:28:57.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:28:57.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14467, 14473) --> . Sandra journeyed to the
2025-01-22 04:28:57.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:28:57.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15770, 15776) --> . John went back to the
2025-01-22 04:28:57.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:28:57.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3741, 3747) --> . Sandra went back to the
2025-01-22 04:28:57.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:28:57.627 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8245, 8250) -->  St. Sandra moved to
2025-01-22 04:28:57.628 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 04:28:57.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11823, 11829) --> . John journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:29:00.367 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 04:29:00.367 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24216])
2025-01-22 04:29:08.765 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [454.8725, 27.192555431685868, 233.99632352941177, 26.45897402731788, 24.26231971153846], 'topk_indices': array([   24,     1, 19611, 24201,  9724,  9725, 24210, 24190, 24095,
       24188, 24208,    14,  3747,  3036, 24214, 24215,     0, 24216,
       24211, 24212]), 'topk_tokens': ['\n\n', '<|start_header_id|>', ' hall', ' apple', ' apple', '.', ':', ':', '.\n\n', '.\n\n', '?\n', '\n', ' hallway', ' hallway', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', 'hall', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [691.4, 917.25, 181.14583333333334, 588.5, 43.2875]}, 'weight': {'score': [22.3178125, 23.442142532722244, 20.97035845588235, 23.446784457781458, 29.08533653846154], 'topk_indices': array([18859, 18815, 14672, 14636, 14681, 14717, 19512, 19454, 14535,
       14600, 20326, 20374, 18147, 18178, 23693, 23559, 22009, 21982,
       23641, 23775]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 28.333984375, 20.927083333333332, 22.034375, 20.75625]}, 'saliency': {'score': [2.826982421875, 0.1476206132710888, 1.3430301441865808, 0.14316581321867886, 0.17674881372696313], 'topk_indices': array([24213,  9722, 24199,  3033, 24095,  3742, 24188, 24203, 24195,
       24209, 16826, 24208, 19611, 24189, 24201,  9724, 24215,  3747,
        3036, 24216]), 'topk_tokens': ['assistant', ' grabbed', ' where', ' travelled', '.\n\n', ' Sandra', '.\n\n', ' discarded', ' prior', 'Answer', ' apple', '?\n', ' hall', 'Question', ' apple', ' apple', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [3.9875, 6.798828125, 1.0046793619791667, 3.252734375, 0.25]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 04:29:08.770 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:29:08.770 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-1_1-4-6-7-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 4, 6, 7, 9):   2%|▏         | 2/100 [00:57<46:41, 28.59s/it]is_0k: False
your chose emoji: ['🚵🏾', '🩰', '🥞', '👩🏻\u200d⚖️', '🧯', '🏋🏿\u200d♀', '🐷', '👨🏽\u200d🤝\u200d👨🏻', '👨🏼\u200d❤\u200d💋\u200d👨🏾', '🧛🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.15s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.77s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.91s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.39s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (2, 3, 5, 6, 7):   2%|▏         | 2/100 [01:14<46:41, 28.59s/it]2025-01-22 04:29:26.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 04:29:26.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4950, 4955) --> . Daniel travelled to the
2025-01-22 04:29:26.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 04:29:26.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7619, 7623) -->  Daniel grabbed the apple
2025-01-22 04:29:26.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:29:26.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11900, 11906) --> . Daniel went back to the
2025-01-22 04:29:26.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel put down the apple.
2025-01-22 04:29:26.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14409, 14414) --> . Daniel put down the
2025-01-22 04:29:26.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:29:26.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16785, 16790) --> . Mary travelled to the
2025-01-22 04:29:26.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:29:26.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9649, 9654) --> . John moved to the
2025-01-22 04:29:26.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:29:26.824 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14544, 14550) -->  bonds. Sandra journeyed to
2025-01-22 04:29:26.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:29:26.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15764, 15770) --> . John went back to the
2025-01-22 04:29:26.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:29:26.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3695, 3701) --> . Sandra went back to the
2025-01-22 04:29:26.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:29:26.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8357, 8362) --> . Sandra moved to the
2025-01-22 04:29:26.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 04:29:27.023 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11873, 11879) --> . John journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:29:29.888 | INFO     | test_jbb_embedding:begin_test:693 - The apple was put down by Daniel.<|eot_id|>
2025-01-22 04:29:29.889 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24162])
2025-01-22 04:29:38.284 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [660.88125, 31.07973308504035, 321.0579044117647, 30.01757861113416, 46.189185049019606], 'topk_indices': array([ 4951,  3696, 22731, 22730, 19533,    14,  7623, 24154,  7622,
       22683, 22684,  3702, 24160,     0, 24157,  4955,  3701, 24161,
       24158, 24162]), 'topk_tokens': [' Daniel', ' Sandra', ' after', ' hall', ' hall', '\n', '.', '?\n', ' apple', ' hall', ' after', '.', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', ' hallway', ' hallway', '\n\n', '<|start_header_id|>', 'hall'], 'evidence_proportions': [1074.6, 1250.25, 375.5, 716.75, 62.25625]}, 'weight': {'score': [22.3178125, 23.42900889716532, 21.08984375, 23.433460549240852, 28.96078431372549], 'topk_indices': array([18737, 18781, 14620, 14584, 14629, 14665, 19434, 19376, 14541,
       14476, 20248, 20296, 18075, 18106, 23497, 23631, 21910, 21937,
       23713, 23579]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 28.333984375, 20.927083333333332, 22.034375, 20.75625]}, 'saliency': {'score': [4.12962890625, 0.1764858379241672, 1.7809807272518383, 0.170123048454936, 0.33179788028492646], 'topk_indices': array([ 7620, 24149, 24135,  7619, 22731, 24147, 24159, 14414,  4952,
        4951, 22730, 19533, 22684,  7622,  3696, 22683, 24161,  4955,
        3701, 24162]), 'topk_tokens': [' grabbed', ' discarded', 'Question', ' Daniel', ' after', ' apple', 'assistant', ' apple', ' travelled', ' Daniel', ' hall', ' hall', ' after', ' apple', ' Sandra', ' hall', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [6.277734375, 9.2802734375, 2.0441080729166665, 4.148828125, 0.34443359375]}}, 'pred_res': 'The apple was put down by Daniel.<|eot_id|>', 'score': 0}
2025-01-22 04:29:38.291 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:29:38.291 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-2_2-3-5-6-7.pkl | len: 3 |  size: 2.12 KB
Processing depth (2, 3, 5, 6, 7):   3%|▎         | 3/100 [01:26<46:54, 29.01s/it]is_0k: False
your chose emoji: ['🙇🏾\u200d♂️', '🗺', '💁🏼\u200d♂', '☀', '🐸', '🇬🇲', '\U0001fab8', '♈', '⚠️', '🧑🏿\u200d⚖']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.61s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.29s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (1, 4, 5, 6, 9):   3%|▎         | 3/100 [01:45<46:54, 29.01s/it]2025-01-22 04:29:57.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 04:29:57.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel travelled to the
2025-01-22 04:29:57.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 04:29:57.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9695, 9699) -->  Daniel grabbed the apple
2025-01-22 04:29:57.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:29:57.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11832, 11838) --> . Daniel went back to the
2025-01-22 04:29:57.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel put down the apple.
2025-01-22 04:29:57.742 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14339, 14344) --> . Daniel put down the
2025-01-22 04:29:57.742 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:29:57.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21473, 21478) --> . Mary travelled to the
2025-01-22 04:29:57.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:29:57.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9488, 9493) --> . John moved to the
2025-01-22 04:29:57.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:29:57.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14405, 14411) --> . Sandra journeyed to the
2025-01-22 04:29:57.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:29:58.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15738, 15744) --> . John went back to the
2025-01-22 04:29:58.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:29:58.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3741, 3747) --> . Sandra went back to the
2025-01-22 04:29:58.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:29:58.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8160, 8165) --> . Sandra moved to the
2025-01-22 04:29:58.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 04:29:58.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11771, 11777) --> . John journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:30:00.768 | INFO     | test_jbb_embedding:begin_test:693 - Daniel<|eot_id|>
2025-01-22 04:30:00.769 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24222])
2025-01-22 04:30:09.180 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [524.626875, 18.705989422084624, 157.40625, 17.98746624906894, 20.265576774691358], 'topk_indices': array([24207,     1, 22745,  9695,    14, 14342, 14343, 14345, 14341,
       24220,  9699, 14344, 14339,  3747,  3036, 24218,     0, 24217,
       24221, 24222]), 'topk_tokens': [' apple', '<|start_header_id|>', ' hall', ' Daniel', '\n', ' down', ' the', '.', ' put', '<|end_header_id|>', '.', ' apple', '.', ' hallway', ' hallway', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '\n\n', 'hall'], 'evidence_proportions': [628.75, 788.5, 195.55208333333334, 1103.8, 25.121875]}, 'weight': {'score': [22.3178125, 23.44660990712074, 20.441636029411764, 23.452005464805925, 29.54417438271605], 'topk_indices': array([18865, 18821, 14726, 14690, 14735, 14771, 19464, 19522, 14654,
       14589, 20396, 20348, 18190, 18159, 23693, 23559, 21998, 22025,
       23641, 23775]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 28.333984375, 20.927083333333332, 22.034375, 20.75625]}, 'saliency': {'score': [3.126103515625, 0.1063335469878741, 0.8850779813878676, 0.10211391775732176, 0.15169063615210263], 'topk_indices': array([24219, 22746, 19621, 14340,  3033,  9696, 14342, 24209,  3032,
        9698, 14338, 22745, 24207,  9695, 14341, 14344, 24221,  3747,
        3036, 24222]), 'topk_tokens': ['assistant', ' after', ' hall', ' Daniel', ' travelled', ' grabbed', ' down', ' discarded', ' Daniel', ' apple', ' Gov', ' hall', ' apple', ' Daniel', ' put', ' apple', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [3.6916015625, 5.861328125, 1.1264851888020833, 5.758203125, 0.1398681640625]}}, 'pred_res': 'Daniel<|eot_id|>', 'score': 0}
2025-01-22 04:30:09.213 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:30:09.213 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-3_1-4-5-6-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 4, 5, 6, 9):   4%|▍         | 4/100 [01:57<47:37, 29.77s/it]is_0k: False
your chose emoji: ['🤷🏽\u200d♂', '🚴🏽\u200d♂', '👨🏻\u200d❤\u200d💋\u200d👨🏽', '🙆🏿\u200d♀️', '🔟', '♀️', '👨🏻\u200d🦳', '🦻🏽', '🇰🇭', '🧖🏻\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.64s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.65s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (2, 4, 5, 6, 9):   4%|▍         | 4/100 [02:14<47:37, 29.77s/it]2025-01-22 04:30:26.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the hallway.
2025-01-22 04:30:26.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4912, 4917) --> . Daniel travelled to the
2025-01-22 04:30:26.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the apple.
2025-01-22 04:30:26.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9793, 9797) -->  Daniel grabbed the apple
2025-01-22 04:30:26.433 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the garden.
2025-01-22 04:30:26.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11908, 11914) --> . Daniel went back to the
2025-01-22 04:30:26.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel put down the apple.
2025-01-22 04:30:26.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14361, 14366) --> . Daniel put down the
2025-01-22 04:30:26.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:30:26.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21433, 21438) --> . Mary travelled to the
2025-01-22 04:30:26.707 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the garden.
2025-01-22 04:30:26.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9641, 9646) -->  war. John moved to
2025-01-22 04:30:26.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the office.
2025-01-22 04:30:26.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14443, 14449) --> . Sandra journeyed to the
2025-01-22 04:30:26.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the office.
2025-01-22 04:30:26.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15697, 15703) -->  engagement. John went back to
2025-01-22 04:30:26.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:30:26.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3711, 3717) --> . Sandra went back to the
2025-01-22 04:30:26.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra moved to the kitchen.
2025-01-22 04:30:26.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8316, 8321) --> . Sandra moved to the
2025-01-22 04:30:26.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John journeyed to the bedroom.
2025-01-22 04:30:27.065 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11844, 11850) -->  John journeyed to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:30:29.769 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 04:30:29.769 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 04:30:38.147 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [461.099375, 27.14524498656192, 297.44025735294116, 26.314652112865787, 38.90131915983606], 'topk_indices': array([ 9796, 22752, 22753, 24174,    14, 22705, 22706,  9256,  9346,
        3718, 19593,  9303,     0, 24177, 24180,  3717,  4917, 24178,
       24181, 24182]), 'topk_tokens': [' apple', ' hall', ' after', '?\n', '\n', ' hall', ' after', ' a', ' a', '.', ' hall', ' cause', '<|begin_of_text|>', '<|eot_id|>', '<|end_header_id|>', ' hallway', ' hallway', '<|start_header_id|>', '\n\n', 'hall'], 'evidence_proportions': [939.8, 775.5625, 158.05208333333334, 525.85, 29.734375]}, 'weight': {'score': [22.3178125, 23.436908207566674, 22.41750919117647, 23.43950445059272, 29.620389344262296], 'topk_indices': array([18797, 18841, 14648, 14612, 14693, 14657, 19436, 19494, 14511,
       14576, 20308, 20356, 18166, 18135, 23653, 23519, 21985, 21958,
       23601, 23735]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 28.333984375, 20.927083333333332, 22.034375, 20.75625]}, 'saliency': {'score': [2.86244873046875, 0.15375412268063882, 1.6579123104319853, 0.14882753959275652, 0.29781754290471313], 'topk_indices': array([24175, 22704, 14366,  9198, 22753, 24167,  3712,  4914, 22752,
        9796,  4913, 24179, 22706, 22705, 19593,  9303, 24181,  3717,
        4917, 24182]), 'topk_tokens': ['Answer', ' their', ' apple', ' few', ' after', ' apple', ' Sandra', ' travelled', ' hall', ' apple', ' Daniel', 'assistant', ' after', ' hall', ' hall', ' cause', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [5.423046875, 5.85009765625, 0.8673095703125, 3.0046875, 0.16365966796875]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 04:30:38.155 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:30:38.155 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-17_pid-4_2-4-5-6-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (2, 4, 5, 6, 9):   5%|▌         | 5/100 [02:26<46:39, 29.47s/it]Processing depth (2, 4, 5, 6, 9):   5%|▌         | 5/100 [02:26<46:26, 29.33s/it]
2025-01-22 04:30:38.392 | INFO     | __main__:<module>:82 - Selected idx: 18
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bathroom? 
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:86 - Needle: [' John went back to the office.', ' Sandra moved to the kitchen.', ' Daniel went to the hallway.', ' John moved to the garden.', ' John journeyed to the bedroom.', ' Sandra went back to the hallway.', ' Daniel travelled to the bathroom.', ' Sandra journeyed to the office.', ' Daniel dropped the apple there.', ' Mary travelled to the bedroom.']
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel went to the hallway.', ' Daniel travelled to the bathroom.', ' Daniel dropped the apple there.', ' Mary travelled to the bedroom.']
2025-01-22 04:30:38.393 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🤟🏿', '👩🏾\u200d🏫', '🏃\u200d♂️', '🇷🇼', '⏺', '👞', '👩🏻\u200d✈️', '🇸🇾', '😅', '🙎🏿\u200d♀️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.47s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (1, 3, 4, 5):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:30:55.388 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:30:55.404 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2958, 2963) -->  tragedy. Daniel went to
2025-01-22 04:30:55.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 04:30:55.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7539, 7544) --> . Daniel travelled to the
2025-01-22 04:30:55.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 04:30:55.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9743, 9748) --> . Daniel dropped the apple
2025-01-22 04:30:55.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:30:55.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11915, 11920) --> . Mary travelled to the
2025-01-22 04:30:55.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:30:55.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4022, 4028) -->  John went back to the office
2025-01-22 04:30:55.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:30:55.587 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3837, 3842) --> . Sandra moved to the
2025-01-22 04:30:55.587 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:30:55.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15882, 15887) -->  John moved to the garden
2025-01-22 04:30:55.665 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 04:30:55.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18801, 18807) --> . John journeyed to the
2025-01-22 04:30:55.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 04:30:55.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6412, 6418) --> . Sandra went back to the
2025-01-22 04:30:55.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 04:30:55.903 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (22218, 22224) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:30:58.618 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:30:58.618 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24169])
2025-01-22 04:31:06.976 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [349.065625, 12.504283106486845, 211.11534926470588, 11.945198477278382, 16.88413558467742], 'topk_indices': array([ 3840, 24163, 24067,  2965,  2964,  3837,  2960, 24022,  6418,
          14,  7544,  3839,  9747,  9748, 24167, 24165, 24023,  3838,
           0, 24164]), 'topk_tokens': [' to', ':', ' location', '.', ' hallway', '.', ' Daniel', ' you', ' hallway', '\n', ' bathroom', ' moved', ' apple', ' there', '<|end_header_id|>', '<|start_header_id|>', ' context', ' Sandra', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [426.6, 373.175, 508.1, 88.3875]}, 'weight': {'score': [22.711328125, 23.43703975674334, 21.69577205882353, 23.440096284414132, 29.446320564516128], 'topk_indices': array([18743, 18787, 14601, 14637, 19401, 19459, 14646, 14682, 14500,
       14565, 20321, 20273, 18069, 18100, 23645, 23511, 21970, 21943,
       23593, 23727]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4734375, 21.01875, 23.596875, 20.75625]}, 'saliency': {'score': [2.138336181640625, 0.07092688151903545, 1.2885239545036764, 0.06749597984875146, 0.12420223605248236], 'topk_indices': array([ 9745, 24022,  7541,  7540, 24021,  9324, 24067,  2961,  3842,
        6413, 24159,  9748,  3839,  2960,  9747,  2964,  7544,  6418,
       24023,  3838]), 'topk_tokens': [' dropped', ' you', ' travelled', ' Daniel', ' provided', ' cause', ' location', ' went', ' kitchen', ' Sandra', ' bathroom', ' there', ' moved', ' Daniel', ' apple', ' hallway', ' bathroom', ' hallway', ' context', ' Sandra'], 'evidence_proportions': [2.66484375, 2.2158203125, 3.198046875, 0.4746337890625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:31:07.014 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:31:07.015 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-0_1-3-4-5.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 3, 4, 5):   1%|          | 1/100 [00:28<47:00, 28.49s/it]is_0k: False
your chose emoji: ['💇🏻\u200d♂️', '🎪', '🚣🏻\u200d♀', '👆🏽', '👨🏽\u200d⚖', '🪡', '🕵\u200d♀', '🦹🏾\u200d♀️', '🎅🏽', '🏁']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.48s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 5, 6, 8):   1%|          | 1/100 [00:45<47:00, 28.49s/it]2025-01-22 04:31:24.026 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:31:24.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:31:24.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 04:31:24.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11815, 11820) --> . Daniel travelled to the
2025-01-22 04:31:24.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 04:31:24.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14321, 14326) --> . Daniel dropped the apple
2025-01-22 04:31:24.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:31:24.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19190, 19195) --> . Mary travelled to the
2025-01-22 04:31:24.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:31:24.269 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4035, 4041) --> . John went back to the
2025-01-22 04:31:24.269 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:31:24.287 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3831, 3836) --> . Sandra moved to the
2025-01-22 04:31:24.287 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:31:24.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15731, 15736) --> . John moved to the
2025-01-22 04:31:24.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 04:31:24.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18527, 18533) --> . John journeyed to the
2025-01-22 04:31:24.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 04:31:24.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6396, 6402) --> . Sandra went back to the
2025-01-22 04:31:24.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 04:31:24.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (22200, 22206) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:31:27.276 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 04:31:27.276 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24203])
2025-01-22 04:31:35.646 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [335.40859375, 18.118199000247873, 98.20542279411765, 17.742711524925472, 11.29933494857595], 'topk_indices': array([  449,    39, 24194, 24186,   122, 24201,    24, 24190, 24193,
       24191,   546,   547,   451,   452,   568,   429,     0,   430,
       24198, 24199]), 'topk_tokens': [',', '\n\n\n', '?', ':', ' Published', '<|end_header_id|>', '\n\n', ' apple', ' bathroom', ' before', ' em', 's', ' em', 's', ' em', ' em', '<|begin_of_text|>', 's', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [441.0, 302.85, 556.35, 41.434375]}, 'weight': {'score': [21.5609375, 23.446919152276294, 20.441636029411764, 23.452711602558793, 29.65565664556962], 'topk_indices': array([18876, 18832, 14665, 14701, 19477, 19535, 14710, 14746, 14564,
       14629, 20349, 20397, 18163, 18194, 23561, 23695, 22020, 21993,
       23777, 23643]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 21.01875, 23.596875, 20.75625]}, 'saliency': {'score': [1.926263427734375, 0.10204684772757787, 0.5418360093060661, 0.0999171208687751, 0.0846961299075356], 'topk_indices': array([  567, 24187,    39, 24185, 14325, 18658,     0,   441, 11820,
         547,   452,   122, 24191, 24190,   546, 24193,   451,   430,
         568,   429]), 'topk_tokens': ['000', ' Where', '\n\n\n', 'Question', ' apple', ' rapid', '<|begin_of_text|>', 'positor', ' bathroom', 's', 's', ' Published', ' before', ' apple', ' em', ' bathroom', ' em', 's', ' em', ' em'], 'evidence_proportions': [2.4021484375, 1.7064453125, 3.36171875, 0.2347412109375]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 04:31:35.653 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:31:35.654 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-1_0-5-6-8.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 5, 6, 8):   2%|▏         | 2/100 [00:57<46:40, 28.58s/it]is_0k: False
your chose emoji: ['🧑🏿\u200d🤝\u200d🧑🏽', '🧝🏽\u200d♂', '🏳️', '🤽\u200d♀️', '👩\u200d🦼\u200d➡️', '💇🏾\u200d♂️', '🧜🏾\u200d♂', '⛹\u200d♀', '🤟🏼', '🧆']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.11s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.91s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.84s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (1, 6, 8, 9):   2%|▏         | 2/100 [01:14<46:40, 28.58s/it]2025-01-22 04:31:53.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:31:53.441 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2960, 2965) -->  tragedy. Daniel went to
2025-01-22 04:31:53.441 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 04:31:53.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14375, 14380) --> . Daniel travelled to the
2025-01-22 04:31:53.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 04:31:53.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19253, 19258) -->  Daniel dropped the apple there
2025-01-22 04:31:53.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:31:53.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21506, 21511) --> . Mary travelled to the
2025-01-22 04:31:53.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:31:53.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4049, 4055) --> . John went back to the
2025-01-22 04:31:53.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:31:53.754 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3845, 3850) --> . Sandra moved to the
2025-01-22 04:31:53.754 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:31:53.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15754, 15759) -->  engagement. John moved to
2025-01-22 04:31:53.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 04:31:53.924 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18665, 18671) --> . John journeyed to the
2025-01-22 04:31:53.924 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 04:31:53.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6440, 6446) --> . Sandra went back to the
2025-01-22 04:31:53.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 04:31:54.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (22208, 22214) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:31:56.856 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the apple there.<|eot_id|>
2025-01-22 04:31:56.856 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24211])
2025-01-22 04:32:05.245 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [806.478125, 27.477812422565457, 285.6631433823529, 26.469604330504968, 21.46081984186747], 'topk_indices': array([24194, 24193, 19254, 19624, 19255, 24199, 24198, 14381, 19257,
       14380,  2966, 24206, 19256, 24201,  6446, 24207,     0, 24209,
       24210, 24211]), 'topk_tokens': [':', 'Question', ' dropped', ' hall', ' the', ' before', ' apple', '.', ' there', ' bathroom', ' hallway', '<|eot_id|>', ' apple', ' bathroom', ' hallway', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', '\n\n', 'hall'], 'evidence_proportions': [530.15, 842.0, 1737.2, 116.5625]}, 'weight': {'score': [23.47265625, 23.445491244734452, 21.110064338235293, 23.448755367860098, 29.15850903614458], 'topk_indices': array([18866, 18822, 14695, 14659, 19525, 14704, 14740, 19467, 14558,
       14623, 20387, 20339, 18153, 18184, 23703, 23569, 22016, 21989,
       23785, 23651]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4734375, 21.01875, 26.6421875, 20.75625]}, 'saliency': {'score': [4.97086181640625, 0.15679131584517222, 1.516165340647978, 0.15089313178662433, 0.1552521119634789], 'topk_indices': array([18671, 19350, 22755, 14377, 24065, 19253, 24208, 19254, 19624,
       24199, 19257, 24193, 24198, 14380,  2966, 19256, 24210, 24201,
        6446, 24211]), 'topk_tokens': [' bedroom', ' hall', ' hall', ' travelled', ' context', ' Daniel', 'assistant', ' dropped', ' hall', ' before', ' there', 'Question', ' apple', ' bathroom', ' hallway', ' apple', '\n\n', ' bathroom', ' hallway', 'hall'], 'evidence_proportions': [3.1087890625, 4.533984375, 11.61328125, 0.627392578125]}}, 'pred_res': 'Daniel dropped the apple there.<|eot_id|>', 'score': 0}
2025-01-22 04:32:05.253 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:32:05.253 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-2_1-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 6, 8, 9):   3%|▎         | 3/100 [01:26<46:57, 29.04s/it]is_0k: False
your chose emoji: ['👩🏻\u200d❤️\u200d👨🏻', '🧑🏻\u200d🔬', '🚶🏼\u200d♂️\u200d➡', '\U0001fac5🏽', '👮🏽\u200d♂', '🇪🇺', '🫓', '👩\u200d👩\u200d👧', '🤲🏽', '🔧']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.75s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.45s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.06s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (0, 3, 4, 5):   3%|▎         | 3/100 [01:43<46:57, 29.04s/it]2025-01-22 04:32:22.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:32:22.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:32:22.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 04:32:22.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7610, 7615) -->  war. Daniel travelled to
2025-01-22 04:32:22.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 04:32:22.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9803, 9808) --> . Daniel dropped the apple
2025-01-22 04:32:22.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:32:22.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11859, 11864) --> . Mary travelled to the
2025-01-22 04:32:22.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:32:22.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4035, 4041) --> . John went back to the
2025-01-22 04:32:22.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:32:22.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3831, 3836) --> . Sandra moved to the
2025-01-22 04:32:22.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:32:22.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15623, 15628) --> . John moved to the
2025-01-22 04:32:22.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 04:32:22.576 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18511, 18517) --> . John journeyed to the
2025-01-22 04:32:22.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 04:32:22.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6426, 6432) --> . Sandra went back to the
2025-01-22 04:32:22.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 04:32:22.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (22178, 22184) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:32:25.434 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:32:25.434 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24181])
2025-01-22 04:32:33.793 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [224.328125, 8.910058044574926, 73.03584558823529, 8.64115468296726, 6.9398947323069855], 'topk_indices': array([   35,    34, 24079,    24, 24069, 24164,  9808, 24175, 24168,
       24171,     9, 24179, 24035, 24169, 10357,  9807,    14,     0,
       24177, 24176]), 'topk_tokens': ['.', ' hallway', ' location', '\n\n', '.\n\n', ':', ' there', ':', ' apple', ' bathroom', ':', '<|end_header_id|>', ' context', ' before', 't', ' apple', '\n', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [303.55, 175.05, 356.65, 62.0625]}, 'weight': {'score': [22.426953125, 23.44215700463116, 20.441636029411764, 23.44722628729797, 29.813419117647058], 'topk_indices': array([18860, 18816, 14695, 14659, 14704, 19455, 19513, 14740, 14623,
       14558, 20375, 20327, 18147, 18178, 23673, 23539, 21971, 21998,
       23621, 23755]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 24.4828125, 23.596875, 20.75625]}, 'saliency': {'score': [1.340362548828125, 0.050073283577533184, 0.41059336942784924, 0.048495850165849695, 0.05182476604686064], 'topk_indices': array([    8,  7612,    31, 24174, 24033, 24165, 10356,  9805, 24163,
       24079,  9808, 10357,  7616,  6432,    34, 24168, 24169, 24035,
       24171,  9807]), 'topk_tokens': [' Date', ' Daniel', ' went', 'Answer', ' provided', ' Where', ' Get', ' dropped', 'Question', ' location', ' there', 't', ' bathroom', ' hallway', ' hallway', ' apple', ' before', ' context', ' bathroom', ' apple'], 'evidence_proportions': [1.7005859375, 1.119091796875, 2.22001953125, 0.3217529296875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:32:33.798 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:32:33.798 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-3_0-3-4-5.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 3, 4, 5):   4%|▍         | 4/100 [01:55<46:09, 28.85s/it]is_0k: False
your chose emoji: ['🖊', '🚱', '🤞🏿', '💆🏾\u200d♂', '👰\u200d♂', '🏄🏿\u200d♀️', '🏃\u200d♂️\u200d➡', '🙍🏿\u200d♂️', '👩🏼\u200d🚀', '🕊']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.98s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.45s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.63s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (1, 2, 3, 7):   4%|▍         | 4/100 [02:11<46:09, 28.85s/it]2025-01-22 04:32:50.629 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:32:50.647 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2915, 2920) --> . Daniel went to the
2025-01-22 04:32:50.647 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the bathroom.
2025-01-22 04:32:50.674 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4870, 4875) --> . Daniel travelled to the
2025-01-22 04:32:50.674 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple there.
2025-01-22 04:32:50.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7387, 7392) --> . Daniel dropped the apple
2025-01-22 04:32:50.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 04:32:50.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16715, 16720) --> . Mary travelled to the
2025-01-22 04:32:50.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 04:32:50.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4001, 4007) --> . John went back to the
2025-01-22 04:32:50.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:32:50.845 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3795, 3800) --> . Sandra moved to the
2025-01-22 04:32:50.845 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the garden.
2025-01-22 04:32:50.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15712, 15717) -->  engagement. John moved to
2025-01-22 04:32:50.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John journeyed to the bedroom.
2025-01-22 04:32:51.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18683, 18689) --> . John journeyed to the
2025-01-22 04:32:51.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the hallway.
2025-01-22 04:32:51.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6348, 6354) --> . Sandra went back to the
2025-01-22 04:32:51.064 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra journeyed to the office.
2025-01-22 04:32:51.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (22164, 22170) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:32:53.856 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 04:32:53.856 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24167])
2025-01-22 04:33:02.266 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [276.75625, 12.761835436491518, 107.23483455882354, 12.40970530456958, 16.21711705942623], 'topk_indices': array([    1, 24020,     9, 24160, 24150,  6354,  7392, 24065, 24157,
       24055,  4876,  4875,  7391, 24021, 24161,    14, 24165, 24163,
           0, 24162]), 'topk_tokens': ['<|start_header_id|>', ' you', ':', 'Answer', ':', ' hallway', ' there', ' location', ' bathroom', '.\n\n', '.', ' bathroom', ' apple', ' context', ':', '\n', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [218.85, 385.7, 462.05, 40.425]}, 'weight': {'score': [21.259375, 23.437282788580884, 21.110064338235293, 23.4423700162755, 29.592469262295083], 'topk_indices': array([18780, 18824, 14631, 14667, 14676, 14712, 19419, 19477, 14530,
       14595, 20291, 20339, 18142, 18111, 23525, 23659, 21984, 21957,
       23607, 23741]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.665625, 21.01875, 23.596875, 20.75625]}, 'saliency': {'score': [1.5779052734375, 0.07256122000413737, 0.6121471629423254, 0.07055206412718572, 0.12094241282978996], 'topk_indices': array([10717, 24020, 24155,  4872, 24055,  4871,  7389, 24019, 24149,
        2920,  7392, 24154, 18780, 24160, 24065,  7391, 24021, 24157,
        6354,  4875]), 'topk_tokens': ['count', ' you', ' before', ' travelled', '.\n\n', ' Daniel', ' dropped', ' provided', 'Question', ' hallway', ' there', ' apple', 'untlet', 'Answer', ' location', ' apple', ' context', ' bathroom', ' hallway', ' bathroom'], 'evidence_proportions': [1.124267578125, 2.1427734375, 2.82099609375, 0.223583984375]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 100}
2025-01-22 04:33:02.272 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:33:02.273 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-18_pid-4_1-2-3-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 3, 7):   5%|▌         | 5/100 [02:23<45:27, 28.71s/it]Processing depth (1, 2, 3, 7):   5%|▌         | 5/100 [02:24<45:36, 28.81s/it]
2025-01-22 04:33:02.574 | INFO     | __main__:<module>:82 - Selected idx: 19
2025-01-22 04:33:02.574 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 04:33:02.574 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 04:33:02.574 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:33:02.574 | INFO     | __main__:<module>:86 - Needle: [' Sandra journeyed to the office.', ' Sandra moved to the kitchen.', ' Daniel went to the hallway.', ' John journeyed to the bedroom.', ' Sandra went back to the hallway.', ' Daniel got the football.', ' John went back to the office.', ' Daniel travelled to the bathroom.', ' John moved to the garden.', ' Daniel dropped the football there.', ' Mary travelled to the bedroom.']
2025-01-22 04:33:02.575 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel went to the hallway.', ' Daniel got the football.', ' Daniel travelled to the bathroom.', ' Daniel dropped the football there.', ' Mary travelled to the bedroom.']
2025-01-22 04:33:02.575 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧑🏻\u200d❤️\u200d🧑🏼', '🙆🏻\u200d♂️', '✊🏾', '🙆🏽\u200d♂️', '🎅🏾', '🤛', '🦐', '👲', '♨', '🎬']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.08s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.59s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 3, 5, 6, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:33:19.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:33:19.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:33:19.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the football.
2025-01-22 04:33:19.664 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7539, 7543) -->  Daniel got the football
2025-01-22 04:33:19.664 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 04:33:19.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11926, 11931) --> . Daniel travelled to the
2025-01-22 04:33:19.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 04:33:19.818 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14364, 14369) -->  Daniel dropped the football there
2025-01-22 04:33:19.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:33:19.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21488, 21493) --> . Mary travelled to the
2025-01-22 04:33:19.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 04:33:19.976 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8478, 8484) -->  Sandra journeyed to the office
2025-01-22 04:33:19.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:33:20.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10370, 10375) --> . Sandra moved to the
2025-01-22 04:33:20.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 04:33:20.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12063, 12069) -->  John journeyed to the bedroom
2025-01-22 04:33:20.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:33:20.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16332, 16338) -->  Sandra went back to the hallway
2025-01-22 04:33:20.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:33:20.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6867, 6873) --> . John went back to the
2025-01-22 04:33:20.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the garden.
2025-01-22 04:33:20.287 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11808, 11813) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:33:23.658 | INFO     | test_jbb_embedding:begin_test:693 - The location prior to the place where the football was discarded, left or dropped was the St. Paul.<|eot_id|>
2025-01-22 04:33:23.658 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 04:33:32.074 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [315.4186197916667, 15.008822876208578, 113.02573529411765, 14.572175669938702, 34.614613560267856], 'topk_indices': array([   38,    39,    24, 24088,    64,   508,   546, 14368,   547,
       14367,   451,   452,   539,   132,   429,   568,   430, 24194,
           0, 24195]), 'topk_tokens': ['***', '\n\n\n', '\n\n', ' location', 'ENCES', ',', ' em', ' there', 's', ' football', ' em', 's', ',', 'UL', ' em', ' em', 's', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [432.65, 371.5, 112.675, 643.7, 27.784375]}, 'weight': {'score': [23.134765625, 23.443189095942483, 22.726332720588236, 23.44450516950381, 29.909151785714286], 'topk_indices': array([18810, 18854, 14647, 14683, 14728, 14692, 19449, 19507, 14546,
       14611, 20369, 20321, 18179, 18148, 23544, 23678, 22010, 21983,
       23626, 23760]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.939453125, 21.01875, 26.8484375, 20.75625]}, 'saliency': {'score': [1.9440587361653645, 0.08456138697215106, 0.6828972311580882, 0.08187039314002055, 0.2642813001360212], 'topk_indices': array([  441,     0,    39,   547, 14365,   131, 24088,   452,  7542,
         122, 14368,    38,    64,   546,   451,   132, 14367,   430,
         568,   429]), 'topk_tokens': ['positor', '<|begin_of_text|>', '\n\n\n', 's', ' dropped', ' PA', ' location', 's', ' football', ' Published', ' there', '***', 'ENCES', ' em', ' em', 'UL', ' football', 's', ' em', ' em'], 'evidence_proportions': [2.2326171875, 2.489013671875, 0.61015625, 4.3494140625, 0.14808349609375]}}, 'pred_res': 'The location prior to the place where the football was discarded, left or dropped was the St. Paul.<|eot_id|>', 'score': 0}
2025-01-22 04:33:32.079 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:33:32.079 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-19_pid-0_0-3-5-6-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (0, 3, 5, 6, 9):   1%|          | 1/100 [00:29<48:26, 29.36s/it]is_0k: False
your chose emoji: ['💆🏽\u200d♂', '👨🏿\u200d🦽', '🧏🏾\u200d♀', '🇿🇦', '🛄', '🧖🏽\u200d♂', '📻', '🏄🏼\u200d♀️', '🏊🏾\u200d♂️', '👐🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.16s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (0, 3, 4, 5, 8):   1%|          | 1/100 [00:46<48:26, 29.36s/it]2025-01-22 04:33:49.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:33:49.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:33:49.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the football.
2025-01-22 04:33:49.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7552, 7556) -->  got the football.
2025-01-22 04:33:49.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 04:33:49.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9722, 9727) --> . Daniel travelled to the
2025-01-22 04:33:49.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 04:33:49.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11944, 11949) --> . Daniel dropped the football
2025-01-22 04:33:49.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:33:49.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19291, 19296) -->  Mary travelled to the bedroom
2025-01-22 04:33:49.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 04:33:49.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8517, 8523) --> . Sandra journeyed to the
2025-01-22 04:33:49.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:33:49.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10388, 10393) --> . Sandra moved to the
2025-01-22 04:33:49.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 04:33:49.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12081, 12087) -->  John journeyed to the bedroom
2025-01-22 04:33:49.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:33:49.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16459, 16465) --> . Sandra went back to the
2025-01-22 04:33:49.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:33:49.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6887, 6893) --> . John went back to the
2025-01-22 04:33:49.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the garden.
2025-01-22 04:33:49.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11826, 11831) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:33:52.444 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 04:33:52.444 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24229])
2025-01-22 04:34:00.839 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [418.4401041666667, 20.098418413667876, 198.3639705882353, 19.452218809464714, 24.36394761029412], 'topk_indices': array([24216,  7555, 11946,   546, 24118, 16465,   547,   451, 11948,
        9341,   452,  8001,  9294, 11949,   429,   568, 24224,   430,
           0, 24225]), 'topk_tokens': [' discarded', '.', ' dropped', ' em', ' location', ' hallway', 's', ' em', ' football', ' cause', 's', 'circ', ' a', ' there', ' em', ' em', '<|eot_id|>', 's', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [596.65, 616.125, 143.525, 744.75, 30.6875]}, 'weight': {'score': [22.6123046875, 23.447940739517993, 21.239659926470587, 23.45187624100273, 29.445404411764706], 'topk_indices': array([18800, 18844, 14637, 14673, 19503, 19445, 14718, 14682, 14601,
       14536, 20365, 20317, 18138, 18169, 23706, 23572, 21973, 22000,
       23654, 23788]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 20.828125, 21.01875, 23.803125, 26.1828125]}, 'saliency': {'score': [2.436028798421224, 0.11383525134313614, 1.1350241268382353, 0.11009350124399213, 0.18098988252527573], 'topk_indices': array([  452,  7988,   122, 24214,  9236,  9388, 11946, 24118,    34,
         546, 24216,   451,  9341, 11949, 11948,  8001,   430, 16465,
         568,   429]), 'topk_tokens': ['s', 'nes', ' Published', ' football', ' few', 'did', ' dropped', ' location', ' hallway', ' em', ' discarded', ' em', ' cause', ' there', ' football', 'circ', 's', ' hallway', ' em', ' em'], 'evidence_proportions': [3.3275390625, 3.16162109375, 0.85185546875, 4.772265625, 0.211981201171875]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 04:34:00.845 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:34:00.846 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-19_pid-1_0-3-4-5-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 3, 4, 5, 8):   2%|▏         | 2/100 [00:58<47:23, 29.01s/it]is_0k: False
your chose emoji: ['👩🏻\u200d❤️\u200d💋\u200d👨🏽', '\U0001faf1🏻\u200d\U0001faf2🏾', '🚣🏿\u200d♂', '🧝\u200d♂', '🏃🏾\u200d♂️', '👩🏽\u200d⚕', '🔸', '🥣', '🏌🏽', '🇵🇲']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.73s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.33s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.53s/it]
Processing depth (0, 1, 5, 6, 7):   2%|▏         | 2/100 [01:14<47:23, 29.01s/it]2025-01-22 04:34:17.259 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:34:17.260 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:34:17.260 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the football.
2025-01-22 04:34:17.275 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2976, 2980) -->  Daniel got the football
2025-01-22 04:34:17.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 04:34:17.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11944, 11949) --> . Daniel travelled to the
2025-01-22 04:34:17.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 04:34:17.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14459, 14464) --> . Daniel dropped the football
2025-01-22 04:34:17.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:34:17.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16845, 16850) -->  affair. Mary travelled to
2025-01-22 04:34:17.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 04:34:17.540 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8557, 8563) --> . Sandra journeyed to the
2025-01-22 04:34:17.540 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:34:17.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10428, 10433) --> . Sandra moved to the
2025-01-22 04:34:17.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 04:34:17.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12081, 12087) -->  John journeyed to the bedroom
2025-01-22 04:34:17.652 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:34:17.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16449, 16455) --> . Sandra went back to the
2025-01-22 04:34:17.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:34:17.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6920, 6926) --> . John went back to the
2025-01-22 04:34:17.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the garden.
2025-01-22 04:34:17.828 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11860, 11865) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:34:20.515 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 04:34:20.515 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24187])
2025-01-22 04:34:28.889 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [629.8216145833334, 27.034329785035137, 276.98713235294116, 26.082676788082214, 20.43780517578125], 'topk_indices': array([ 7424, 24161,  2979,    14,  2980, 24173, 24174, 16399,    24,
       24076, 14465, 24172, 14461, 16446, 16400, 14464, 14463,     0,
       24182, 24183]), 'topk_tokens': ['nes', ':', ' football', '\n', '.', ' was', ' discarded', 'round', '\n\n', ' location', '.', ' football', ' dropped', 'round', 'ing', ' there', ' football', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [720.1, 626.75, 447.85, 1304.1, 49.69375]}, 'weight': {'score': [23.538411458333332, 23.43535035138487, 21.239659926470587, 23.43834140094066, 29.034423828125], 'topk_indices': array([18822, 18778, 14663, 14627, 14708, 19475, 14672, 19417, 14526,
       14591, 20289, 20337, 18102, 18133, 23670, 23536, 21969, 21996,
       23618, 23752]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.939453125, 21.01875, 23.803125, 25.7390625]}, 'saliency': {'score': [3.855626424153646, 0.15291747542243694, 1.55615234375, 0.1472579776480011, 0.14885878562927246], 'topk_indices': array([24160,  7490,  7379, 24166, 16445,  7423, 16400,  7424,    34,
       16398, 16399,  2979, 24076, 16455, 24174, 14461, 16446, 24172,
       14464, 14463]), 'topk_tokens': ['Question', ' tele', 'nes', ' prior', 'sur', ' Min', 'ing', 'nes', ' hallway', 'sur', 'round', ' football', ' location', ' hallway', ' discarded', ' dropped', 'round', ' football', ' there', ' football'], 'evidence_proportions': [3.9177734375, 4.322998046875, 2.38310546875, 8.42734375, 0.3203857421875]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 04:34:28.896 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:34:28.897 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-19_pid-2_0-1-5-6-7.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 5, 6, 7):   3%|▎         | 3/100 [01:26<46:11, 28.57s/it]is_0k: False
your chose emoji: ['✉', '👨\u200d🏫', '👨🏼\u200d🦲', '👰🏼\u200d♀', '🧑🏼\u200d🤝\u200d🧑🏼', '💂🏿\u200d♀', '🛥️', '🈯', '🕸', '🔆']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.44s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]
Processing depth (0, 1, 2, 4, 9):   3%|▎         | 3/100 [01:44<46:11, 28.57s/it]2025-01-22 04:34:47.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:34:47.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:34:47.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the football.
2025-01-22 04:34:47.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2920, 2924) -->  Daniel got the football
2025-01-22 04:34:47.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 04:34:47.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4866, 4871) --> . Daniel travelled to the
2025-01-22 04:34:47.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 04:34:47.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9700, 9705) --> . Daniel dropped the football
2025-01-22 04:34:47.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:34:47.529 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21482, 21487) --> . Mary travelled to the
2025-01-22 04:34:47.529 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 04:34:47.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8453, 8459) --> . Sandra journeyed to the
2025-01-22 04:34:47.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:34:47.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10318, 10323) --> . Sandra moved to the
2025-01-22 04:34:47.626 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 04:34:47.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12054, 12060) --> . John journeyed to the
2025-01-22 04:34:47.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:34:47.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16361, 16367) --> . Sandra went back to the
2025-01-22 04:34:47.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:34:47.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6812, 6818) --> . John went back to the
2025-01-22 04:34:47.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the garden.
2025-01-22 04:34:47.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11782, 11787) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:34:50.583 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 04:34:50.583 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 04:34:58.973 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [670.3125, 28.5195122202956, 189.40992647058823, 27.65569390415494, 17.96585693359375], 'topk_indices': array([  449, 24199, 24206,    39,  2924,   122, 24204,  2923,  9704,
        9705,   546,   547,   451,   452, 24214,   429,   568,     0,
         430, 24215]), 'topk_tokens': [',', ' to', ' discarded', '\n\n\n', '.', ' Published', ' football', ' football', ' football', ' there', ' em', 's', ' em', 's', '<|eot_id|>', ' em', ' em', '<|begin_of_text|>', 's', '<|start_header_id|>'], 'evidence_proportions': [858.1, 878.125, 592.6, 1002.7, 61.6]}, 'weight': {'score': [22.500325520833332, 23.444508091817355, 20.441636029411764, 23.449671062841418, 29.3009765625], 'topk_indices': array([18796, 18840, 14633, 14669, 14714, 19493, 14678, 19435, 14597,
       14532, 20363, 20315, 18134, 18165, 23538, 23672, 21965, 21992,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.939453125, 21.01875, 23.803125, 20.75625]}, 'saliency': {'score': [4.1301015218098955, 0.16118300737707456, 1.0579905790441178, 0.15597917101790112, 0.132830810546875], 'topk_indices': array([24198,  9702, 16367,    38,    39,    34,   441,   547, 24204,
        2923,  9705, 24206,   452,   122,  9704,   546,   451,   430,
         568,   429]), 'topk_tokens': [' prior', ' dropped', ' hallway', '***', '\n\n\n', ' hallway', 'positor', 's', ' football', ' football', ' there', ' discarded', 's', ' Published', ' football', ' em', ' em', 's', ' em', ' em'], 'evidence_proportions': [4.81484375, 6.19140625, 3.36328125, 6.3560546875, 0.3371826171875]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 04:34:58.984 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:34:58.984 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-19_pid-3_0-1-2-4-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 2, 4, 9):   4%|▍         | 4/100 [01:56<46:40, 29.17s/it]is_0k: False
your chose emoji: ['🧧', '👨🏼\u200d❤️\u200d👨🏿', '🗜️', '🧖🏻', '👨🏿\u200d🦽\u200d➡', '🤜🏿', '👨🏻', '🩲', '🙂\u200d↕', '🧑🏿\u200d❤\u200d💋\u200d🧑🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.11s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.42s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (0, 3, 5, 7, 9):   4%|▍         | 4/100 [02:13<46:40, 29.17s/it]2025-01-22 04:35:16.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 04:35:16.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 04:35:16.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the football.
2025-01-22 04:35:16.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7595, 7599) -->  Daniel got the football
2025-01-22 04:35:16.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the bathroom.
2025-01-22 04:35:16.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11924, 11929) --> . Daniel travelled to the
2025-01-22 04:35:16.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football there.
2025-01-22 04:35:16.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16832, 16837) --> . Daniel dropped the football
2025-01-22 04:35:16.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 04:35:16.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21526, 21531) --> . Mary travelled to the
2025-01-22 04:35:16.361 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the office.
2025-01-22 04:35:16.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8501, 8507) --> . Sandra journeyed to the
2025-01-22 04:35:16.407 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the kitchen.
2025-01-22 04:35:16.468 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10408, 10413) --> . Sandra moved to the
2025-01-22 04:35:16.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John journeyed to the bedroom.
2025-01-22 04:35:16.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12061, 12067) -->  John journeyed to the bedroom
2025-01-22 04:35:16.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the hallway.
2025-01-22 04:35:16.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16365, 16371) --> . Sandra went back to the
2025-01-22 04:35:16.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 04:35:16.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6867, 6873) --> . John went back to the
2025-01-22 04:35:16.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John moved to the garden.
2025-01-22 04:35:16.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (11840, 11845) --> . John moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:35:19.436 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:35:19.437 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24231])
2025-01-22 04:35:27.843 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [378.7669270833333, 20.752224405793513, 185.39154411764707, 20.165274962772997, 15.479424055232558], 'topk_indices': array([   35,   563,   567,    38,   441, 24205,   449,    24,   122,
          39,   546,   547,   451,   452,   429,   568, 24226,     0,
         430, 24227]), 'topk_tokens': ['.', ' to', '000', '***', 'positor', ':', ',', '\n\n', ' Published', '\n\n\n', ' em', 's', ' em', 's', ' em', ' em', '<|eot_id|>', '<|begin_of_text|>', 's', '<|start_header_id|>'], 'evidence_proportions': [667.5, 341.5625, 331.0, 504.0, 42.33125]}, 'weight': {'score': [22.500325520833332, 23.448177147808863, 21.239659926470587, 23.452224054847782, 29.4093386627907], 'topk_indices': array([18856, 18812, 14645, 14609, 14690, 19451, 14654, 19509, 14573,
       14508, 20371, 20323, 18181, 18150, 23566, 23700, 21985, 22012,
       23782, 23648]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.939453125, 21.01875, 23.803125, 20.75625]}, 'saliency': {'score': [2.2304102579752603, 0.11681198040782062, 1.0660831227022058, 0.11337875830740585, 0.11693679454714753], 'topk_indices': array([  431,   198, 24210, 16836, 24204,     0,   567,    34,    38,
       16371,    39,   441,   547,   452,   122,   546,   451,   430,
         568,   429]), 'topk_tokens': [' per', ' INCIDENT', ' prior', ' football', 'Question', '<|begin_of_text|>', '000', ' hallway', '***', ' hallway', '\n\n\n', 'positor', 's', 's', ' Published', ' em', ' em', 's', ' em', ' em'], 'evidence_proportions': [3.6337890625, 2.3946533203125, 1.76083984375, 3.1732421875, 0.22237548828125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 04:35:27.856 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:35:27.856 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-19_pid-4_0-3-5-7-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 3, 5, 7, 9):   5%|▌         | 5/100 [02:25<46:01, 29.06s/it]Processing depth (0, 3, 5, 7, 9):   5%|▌         | 5/100 [02:25<46:03, 29.09s/it]
2025-01-22 04:35:28.184 | INFO     | __main__:<module>:82 - Selected idx: 20
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:86 - Needle: [' John grabbed the football.', ' Sandra moved to the kitchen.', ' John went to the kitchen.', ' Sandra went back to the garden.', ' Daniel went back to the bathroom.', ' John went back to the bedroom.', ' Sandra discarded the apple.', ' Daniel journeyed to the hallway.']
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra moved to the kitchen.', ' Sandra went back to the garden.', ' Sandra discarded the apple.', ' Daniel journeyed to the hallway.']
2025-01-22 04:35:28.185 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🤳🏿', '🦈', '👨', '👨🏼\u200d🦼\u200d➡️', '🦸🏼', '🧑🏼\u200d🎓', '👩🏾\u200d❤️\u200d💋\u200d👨🏾', '🧑🏾\u200d🚀', '🏃🏻\u200d♀', '👮\u200d♂️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.55s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (1, 2, 5, 7):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 04:35:45.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:35:45.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3045, 3050) --> . Sandra moved to the
2025-01-22 04:35:45.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:35:45.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4943, 4949) --> . Sandra went back to the
2025-01-22 04:35:45.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:35:45.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11870, 11874) -->  Sandra discarded the apple
2025-01-22 04:35:45.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the hallway.
2025-01-22 04:35:45.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16816, 16822) --> . Daniel journeyed to the
2025-01-22 04:35:45.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John grabbed the football.
2025-01-22 04:35:45.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9329, 9333) -->  John grabbed the football
2025-01-22 04:35:45.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:35:45.877 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2899, 2904) -->  John went back to the
2025-01-22 04:35:45.877 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:35:45.878 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (251, 257) --> . Daniel went back to the
2025-01-22 04:35:45.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:35:45.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2898, 2904) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:35:48.809 | INFO     | test_jbb_embedding:begin_test:693 - The apple was discarded in 1880.<|eot_id|>
2025-01-22 04:35:48.809 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 04:35:57.203 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [203.421875, 8.913919139527195, 140.48214285714286, 8.630421591661836, 11.873866705246913], 'topk_indices': array([   24, 11871, 24187,    14, 24174, 24184,  4950,     1,  4949,
       24176, 24182,    23, 24191, 24091, 24181, 24180, 24183,     0,
       24189, 24188]), 'topk_tokens': ['\n\n', ' discarded', ':', '\n', '.\n\n', '?', '.', '<|start_header_id|>', ' garden', ':', ' the', '4', '<|end_header_id|>', ' location', ' before', ' apple', ' garden', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [182.25, 249.5625, 418.21875, 31.7265625]}, 'weight': {'score': [22.881324404761905, 23.44638060009919, 22.189732142857142, 23.447964428769563, 29.320216049382715], 'topk_indices': array([18841, 18797, 14666, 14702, 19436, 14747, 14711, 19494, 14565,
       14630, 20362, 20314, 18166, 18135, 23551, 23685, 21990, 22017,
       23633, 23767]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 21.869791666666668, 30.615234375, 20.270833333333332]}, 'saliency': {'score': [1.3594592866443453, 0.04979411305071086, 0.7808372860863095, 0.048019877922814286, 0.08589859950689623], 'topk_indices': array([24178, 24186, 24090,    20, 24174,  4944, 24184,   257,    23,
       24177,     0, 11870, 24175, 11873,  4949, 11871, 24091, 24181,
       24180, 24183]), 'topk_tokens': [' was', 'Answer', ' first', ' Jul', '.\n\n', ' Sandra', '?', ' bathroom', '4', ' Where', '<|begin_of_text|>', ' Sandra', 'Question', ' apple', ' garden', ' discarded', ' location', ' before', ' apple', ' garden'], 'evidence_proportions': [1.04560546875, 1.4241536458333333, 3.4130859375, 0.187225341796875]}}, 'pred_res': 'The apple was discarded in 1880.<|eot_id|>', 'score': 0}
2025-01-22 04:35:57.217 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:35:57.230 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-20_pid-0_1-2-5-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 5, 7):   1%|          | 1/100 [00:28<47:44, 28.93s/it]is_0k: False
your chose emoji: ['👨🏿\u200d🌾', '🤾🏽\u200d♂', '🚣', '🧚🏾', '🚶🏿\u200d♀️\u200d➡', '🧏🏼', '👩🏼\u200d❤️\u200d👨🏻', '🤸\u200d♀️', '🧑🏼\u200d🦲', '🤼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.57s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.73s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (0, 1, 4, 5):   1%|          | 1/100 [00:45<47:44, 28.93s/it]2025-01-22 04:36:13.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:36:13.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:36:13.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:36:14.014 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2975, 2981) -->  tragedy. Sandra went back to
2025-01-22 04:36:14.014 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:36:14.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9713, 9717) -->  Sandra discarded the apple
2025-01-22 04:36:14.064 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the hallway.
2025-01-22 04:36:14.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11921, 11927) --> . Daniel journeyed to the
2025-01-22 04:36:14.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John grabbed the football.
2025-01-22 04:36:14.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9302, 9306) -->  John grabbed the football
2025-01-22 04:36:14.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:36:14.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2856, 2861) -->  John went back to the
2025-01-22 04:36:14.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:36:14.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (258, 264) --> . Daniel went back to the
2025-01-22 04:36:14.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:36:14.205 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2855, 2861) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:36:16.955 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:36:16.955 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24152])
2025-01-22 04:36:25.290 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [81.25855654761905, 5.8455444007451876, 53.607886904761905, 5.738271044146311, 7.5568359375], 'topk_indices': array([24040, 24141, 17609, 17296, 24135, 24146, 24134,    14, 17298,
       17326, 17648, 17651, 24142, 24140, 24143, 24148, 24150, 17652,
           0, 24147]), 'topk_tokens': ['.\n\n', ' the', 'ed', ' the', ':', ':', 'Question', '\n', ' of', ' service', 'om', 'ar', ' garden', ' before', '?', '<|start_header_id|>', '<|end_header_id|>', 'ison', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [113.6, 90.17708333333333, 125.65625, 15.790364583333334]}, 'weight': {'score': [23.970982142857142, 23.43402504657421, 22.189732142857142, 23.43464106498569, 28.94921875], 'topk_indices': array([18782, 18738, 14650, 14614, 19377, 14695, 14659, 19435, 14513,
       14578, 20335, 20287, 18076, 18107, 23638, 23504, 21958, 21931,
       23720, 23586]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.709635416666668, 30.615234375, 20.270833333333332]}, 'saliency': {'score': [0.5085471017020089, 0.03267911967469209, 0.2961396716889881, 0.03203523881312888, 0.05479876200358073], 'topk_indices': array([24149, 24121, 24040, 24136, 17647, 17605, 17297, 24145, 17608,
       24006, 17650, 17648, 17651, 24139, 17326, 24143, 24134, 24140,
       24142, 17652]), 'topk_tokens': ['assistant', ' return', '.\n\n', ' Where', ' hands', 'ely', ' service', 'Answer', 'ison', ' context', ' cap', 'om', 'ar', ' apple', ' service', '?', 'Question', ' before', ' garden', 'ison'], 'evidence_proportions': [0.5802490234375, 0.5361328125, 1.020263671875, 0.08006540934244792]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 04:36:25.297 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:36:25.297 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-20_pid-1_0-1-4-5.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 1, 4, 5):   2%|▏         | 2/100 [00:56<46:25, 28.42s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d🔬', '🚅', '🇦🇸', '👩\u200d🍼', '🧑🏻\u200d🏫', '🪧', '👾', '📒', '🙋🏾\u200d♂️', '🧚🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.58s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.13s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.37s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.45s/it]
Processing depth (5, 6, 7, 8):   2%|▏         | 2/100 [01:12<46:25, 28.42s/it]2025-01-22 04:36:41.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:36:41.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11863, 11868) -->  Sandra moved to the kitchen
2025-01-22 04:36:41.359 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:36:41.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14368, 14374) --> . Sandra went back to the
2025-01-22 04:36:41.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:36:41.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16712, 16716) -->  Sandra discarded the apple
2025-01-22 04:36:41.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the hallway.
2025-01-22 04:36:41.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19215, 19221) -->  Daniel journeyed to the hallway
2025-01-22 04:36:41.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John grabbed the football.
2025-01-22 04:36:41.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9406, 9410) -->  John grabbed the football
2025-01-22 04:36:41.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:36:41.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2869, 2874) -->  John went back to the
2025-01-22 04:36:41.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:36:41.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (258, 264) -->  Daniel went back to the bathroom
2025-01-22 04:36:41.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:36:41.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2868, 2874) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:36:44.382 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:36:44.382 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24155])
2025-01-22 04:36:52.724 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [145.89285714285714, 5.292817079228413, 66.09672619047619, 5.117436297478852, 17.716481854838708], 'topk_indices': array([24154,    24, 24043,     9, 24008, 16716, 24053,     1, 24142,
          23, 24145, 24143,    14, 16713, 16715, 24009, 24151, 24153,
           0, 24150]), 'topk_tokens': ['\n\n', '\n\n', '.\n\n', ':', ' you', '.', ' location', '<|start_header_id|>', ' apple', '4', ' garden', ' before', '\n', ' discarded', ' apple', ' context', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [111.625, 121.88541666666667, 346.46875, 64.73958333333333]}, 'weight': {'score': [25.472842261904763, 23.438038124016888, 23.544270833333332, 23.43617372802289, 29.495211693548388], 'topk_indices': array([18828, 18784, 14649, 14613, 14694, 19488, 19430, 14658, 14512,
       14577, 20302, 20350, 18122, 18153, 23647, 23513, 21979, 21952,
       23729, 23595]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.8296875, 21.869791666666668, 30.615234375, 25.350260416666668]}, 'saliency': {'score': [1.0557454427083333, 0.030068402022298515, 0.42002650669642855, 0.028835679429307825, 0.13264329971805697], 'topk_indices': array([24152, 14374, 24043, 24139,    20, 24124, 24148,     0, 24008,
       24007,    23, 24137, 16712, 24053, 24143, 24142, 24145, 24009,
       16715, 16713]), 'topk_tokens': ['assistant', ' garden', '.\n\n', ' Where', ' Jul', ' return', 'Answer', '<|begin_of_text|>', ' you', ' provided', '4', 'Question', ' Sandra', ' location', ' before', ' apple', ' garden', ' context', ' apple', ' discarded'], 'evidence_proportions': [0.8149169921875, 0.6988932291666666, 2.7628173828125, 0.4752400716145833]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:36:52.728 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:36:52.728 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-20_pid-2_5-6-7-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (5, 6, 7, 8):   3%|▎         | 3/100 [01:24<45:13, 27.97s/it]is_0k: False
your chose emoji: ['🏳\u200d⚧', '🧑🏿\u200d🦼\u200d➡', '🌇', '👨\u200d🚀', '🏌️\u200d♀', '👱🏼\u200d♀', '👰🏼\u200d♀', '⏹️', '🦵', '👻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.53s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.82s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (3, 6, 7, 9):   3%|▎         | 3/100 [01:41<45:13, 27.97s/it]2025-01-22 04:37:10.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:37:10.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7517, 7522) --> . Sandra moved to the
2025-01-22 04:37:10.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:37:10.125 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14336, 14342) --> . Sandra went back to the
2025-01-22 04:37:10.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:37:10.214 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16722, 16726) -->  Sandra discarded the apple
2025-01-22 04:37:10.214 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the hallway.
2025-01-22 04:37:10.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21446, 21452) --> . Daniel journeyed to the
2025-01-22 04:37:10.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John grabbed the football.
2025-01-22 04:37:10.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9354, 9358) -->  John grabbed the football
2025-01-22 04:37:10.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:37:10.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2849, 2854) -->  John went back to the
2025-01-22 04:37:10.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:37:10.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (238, 244) -->  Daniel went back to the bathroom
2025-01-22 04:37:10.397 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:37:10.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2848, 2854) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:37:13.097 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:37:13.097 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 04:37:21.468 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [211.25892857142858, 9.403688863185725, 139.4717261904762, 9.115251024166184, 16.660908315373565], 'topk_indices': array([16725, 24059,  1633, 24132,   234, 16723,  1634, 24192, 24133,
       24093, 24103,    14,   243, 24193, 24201, 24195,  1712, 24203,
       24200,     0]), 'topk_tokens': [' apple', ' context', "'s", ' to', '�', ' discarded', ' work', ' apple', ' the', '.\n\n', ' location', '\n', ' bathroom', ' before', '<|start_header_id|>', ' garden', "'s", '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [205.425, 182.1875, 355.4375, 149.07291666666666]}, 'weight': {'score': [22.881324404761905, 23.45043477362855, 23.544270833333332, 23.45084778200778, 29.479166666666668], 'topk_indices': array([18798, 18842, 14657, 14621, 19507, 19449, 14702, 14666, 14585,
       14520, 20369, 20321, 18167, 18136, 23667, 23533, 21972, 21999,
       23615, 23749]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 21.869791666666668, 30.615234375, 20.270833333333332]}, 'saliency': {'score': [1.3715413411458333, 0.05355884378033605, 0.9240780784970238, 0.05165705877769904, 0.12154550661985901], 'topk_indices': array([ 1633,     0,   232,   234, 16759, 24093, 16798,  7518,  1711,
        1634, 24059, 16725,  1712, 24193, 21452, 24103, 24192, 16723,
       24195,   243]), 'topk_tokens': ["'s", '<|begin_of_text|>', '�', '�', 'present', '.\n\n', 'present', ' Sandra', ' summer', ' work', ' context', ' apple', "'s", ' before', ' hallway', ' location', ' apple', ' discarded', ' garden', ' bathroom'], 'evidence_proportions': [1.3021484375, 1.001220703125, 2.83251953125, 0.8257039388020834]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:37:21.475 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:37:21.475 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-20_pid-3_3-6-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (3, 6, 7, 9):   4%|▍         | 4/100 [01:53<45:14, 28.28s/it]is_0k: False
your chose emoji: ['🕓', '🤝🏾', '🧑🏾\u200d❤️\u200d🧑🏽', '🧜🏻\u200d♀', '🇮🇲', '⏹', '🏃🏾\u200d➡️', '👨🏻\u200d🦼\u200d➡️', '🧹', '🧑🏻\u200d❤\u200d💋\u200d🧑🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.99s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (1, 3, 5, 8):   4%|▍         | 4/100 [02:10<45:14, 28.28s/it]2025-01-22 04:37:38.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:37:38.627 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2978, 2983) -->  tragedy. Sandra moved to
2025-01-22 04:37:38.628 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:37:38.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7547, 7553) --> . Sandra went back to the
2025-01-22 04:37:38.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:37:38.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11938, 11942) -->  Sandra discarded the apple
2025-01-22 04:37:38.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the hallway.
2025-01-22 04:37:38.826 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19269, 19275) -->  Daniel journeyed to the hallway
2025-01-22 04:37:38.826 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John grabbed the football.
2025-01-22 04:37:38.874 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9385, 9389) -->  John grabbed the football
2025-01-22 04:37:38.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:37:38.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2859, 2864) -->  John went back to the
2025-01-22 04:37:38.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:37:38.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (251, 257) --> . Daniel went back to the
2025-01-22 04:37:38.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:37:38.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2858, 2864) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:37:41.699 | INFO     | test_jbb_embedding:begin_test:693 - The apple was discarded.<|eot_id|>
2025-01-22 04:37:41.700 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 04:37:50.060 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [104.85193452380952, 5.017146663840873, 74.89732142857143, 4.8695073011599, 8.593670819256756], 'topk_indices': array([24148, 24162, 24173, 24168, 11939, 11941,    14, 24033,  7261,
          23,     1,    24, 24077, 24166, 24175, 24167, 24169,     0,
       24177, 24174]), 'topk_tokens': [' return', ':', ':', ' the', ' discarded', ' apple', '\n', ' context', 'nes', '4', '<|start_header_id|>', '\n\n', ' location', ' apple', '<|start_header_id|>', ' before', ' garden', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [93.684375, 114.125, 221.875, 26.869791666666668]}, 'weight': {'score': [25.715401785714285, 23.447476428748654, 22.189732142857142, 23.44659764654101, 30.0551097972973], 'topk_indices': array([18822, 18778, 14672, 14636, 14717, 19424, 19482, 14681, 14535,
       14600, 20306, 20354, 18147, 18116, 23535, 23669, 21977, 21950,
       23617, 23751]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.8484375, 21.869791666666668, 30.615234375, 25.350260416666668]}, 'saliency': {'score': [0.7354198637462798, 0.028498455801965306, 0.4329833984375, 0.027531615149016805, 0.06533519641773121], 'topk_indices': array([ 7548, 24161,  7454, 24163,  7553,    23, 24150,    24,  7409,
           0, 24148, 11938,  7261, 24033, 11941, 24077, 11939, 24166,
       24167, 24169]), 'topk_tokens': [' Sandra', 'Question', 'nes', ' Where', ' garden', '4', ' location', '\n\n', 'nes', '<|begin_of_text|>', ' return', ' Sandra', 'nes', ' context', ' apple', ' location', ' discarded', ' apple', ' before', ' garden'], 'evidence_proportions': [0.6162353515625, 0.6603190104166666, 1.8070068359375, 0.1954498291015625]}}, 'pred_res': 'The apple was discarded.<|eot_id|>', 'score': 0}
2025-01-22 04:37:50.067 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:37:50.067 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-20_pid-4_1-3-5-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 3, 5, 8):   5%|▌         | 5/100 [02:21<44:57, 28.39s/it]Processing depth (1, 3, 5, 8):   5%|▌         | 5/100 [02:22<44:59, 28.41s/it]
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:82 - Selected idx: 21
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:86 - Needle: [' John went to the kitchen.', ' John grabbed the football.', ' Sandra moved to the kitchen.', ' Sandra grabbed the milk.', ' Daniel went back to the bathroom.', ' John went back to the bedroom.', ' Sandra went back to the garden.', ' Sandra discarded the milk.', ' Daniel journeyed to the hallway.']
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra moved to the kitchen.', ' Sandra grabbed the milk.', ' Sandra went back to the garden.', ' Sandra discarded the milk.', ' Daniel journeyed to the hallway.']
2025-01-22 04:37:50.376 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🇿🇼', '🏃🏽\u200d♀️\u200d➡', '🌇', '🏃🏼\u200d♂️\u200d➡', '💆🏼\u200d♀', '👨🏽\u200d✈', '💇🏻\u200d♀', '🇨🇩', '🙍🏼\u200d♀', '⚰️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.54s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.35s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.55s/it]
Processing depth (3, 4, 5, 8, 9):   0%|          | 0/100 [00:15<?, ?it/s]2025-01-22 04:38:06.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:38:06.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7391, 7396) --> . Sandra moved to the
2025-01-22 04:38:06.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the milk.
2025-01-22 04:38:06.845 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9670, 9674) -->  Sandra grabbed the milk
2025-01-22 04:38:06.845 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:38:06.912 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11812, 11818) --> . Sandra went back to the
2025-01-22 04:38:06.913 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the milk.
2025-01-22 04:38:07.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19256, 19260) -->  discarded the milk.
2025-01-22 04:38:07.016 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:38:07.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21507, 21513) --> . Daniel journeyed to the
2025-01-22 04:38:07.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:38:07.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14141, 14146) --> . John went to the
2025-01-22 04:38:07.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:38:07.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13862, 13866) -->  grabbed the football.
2025-01-22 04:38:07.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:38:07.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11977, 11983) --> . Daniel went back to the
2025-01-22 04:38:07.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:38:07.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16765, 16771) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:38:10.084 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 04:38:10.085 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24253])
2025-01-22 04:38:18.465 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [254.328125, 8.34350006184037, 66.48065476190476, 8.03905991842214, 15.071946364182692], 'topk_indices': array([ 7397,     1, 24233, 19256, 19259, 24232, 24132,  9673,  7395,
        9670, 24249, 24251, 24142,  7396,  7391,  7393, 24248, 11818,
           0,  7392]), 'topk_tokens': ['.', '<|start_header_id|>', ' to', ' discarded', '.', ' prior', '.\n\n', ' milk', ' the', ' Sandra', '<|start_header_id|>', '<|end_header_id|>', ' location', ' kitchen', '.', ' moved', '<|eot_id|>', ' garden', '<|begin_of_text|>', ' Sandra'], 'evidence_proportions': [523.75, 283.375, 233.66666666666666, 266.09375, 23.263020833333332]}, 'weight': {'score': [22.7534375, 23.462519582783642, 20.433035714285715, 23.46587960811648, 29.902193509615383], 'topk_indices': array([18824, 18868, 14693, 14729, 19526, 14774, 14738, 19468, 14592,
       14657, 20340, 20388, 18162, 18193, 23716, 23582, 21991, 22018,
       23664, 23798]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 29.677734375, 21.869791666666668, 23.01953125, 20.270833333333332]}, 'saliency': {'score': [1.626163330078125, 0.04799993811936995, 0.3454677036830357, 0.04611225088782076, 0.1110219221848708], 'topk_indices': array([24226, 24132, 24246, 19258, 11815,  9671, 19255,  2529, 11813,
       24238, 24240, 24232,  9673, 19256, 24142,  9670,  7393,  7396,
       11818,  7392]), 'topk_tokens': ['Question', '.\n\n', 'Answer', ' milk', ' back', ' grabbed', ' Sandra', '♀', ' Sandra', ' milk', ' discarded', ' prior', ' milk', ' discarded', ' location', ' Sandra', ' moved', ' kitchen', ' garden', ' Sandra'], 'evidence_proportions': [3.25859375, 2.29736328125, 1.3492838541666667, 1.5921630859375, 0.11788431803385417]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:38:18.472 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:38:18.473 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-21_pid-0_3-4-5-8-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (3, 4, 5, 8, 9):   1%|          | 1/100 [00:27<46:05, 27.94s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d❤\u200d🧑🏿', '🇬🇳', '🧚🏿\u200d♀️', '🧏🏻\u200d♀️', '👩🏼\u200d❤️\u200d💋\u200d👩🏿', '🤸🏻', '🛐', '🚶\u200d♂\u200d➡', '👨\u200d👩\u200d👦\u200d👦', '👩\u200d❤️\u200d💋\u200d👩']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.59s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (0, 1, 4, 5, 9):   1%|          | 1/100 [00:44<46:05, 27.94s/it]2025-01-22 04:38:35.448 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:38:35.448 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:38:35.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the milk.
2025-01-22 04:38:35.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3039, 3043) -->  Sandra grabbed the milk
2025-01-22 04:38:35.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:38:35.516 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9707, 9713) --> . Sandra went back to the
2025-01-22 04:38:35.516 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the milk.
2025-01-22 04:38:35.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11919, 11923) -->  Sandra discarded the milk
2025-01-22 04:38:35.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:38:35.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21494, 21500) --> . Daniel journeyed to the
2025-01-22 04:38:35.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:38:35.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14237, 14242) --> . John went to the
2025-01-22 04:38:35.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:38:35.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13961, 13965) -->  John grabbed the football
2025-01-22 04:38:35.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:38:35.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12054, 12060) -->  Daniel went back to the bathroom
2025-01-22 04:38:35.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:38:36.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16825, 16831) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:38:38.644 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:38:38.644 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24200])
2025-01-22 04:38:46.976 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [167.19375, 6.043080919720696, 54.318824404761905, 5.834339878192656, 10.00093851461039], 'topk_indices': array([24160, 24179, 11969,  3042,  3043, 24187, 11968, 24172, 24198,
       11920, 24079, 24180, 24173, 24174,    14,    32, 24045, 24195,
           0, 24196]), 'topk_tokens': [' return', ' prior', 'IC', ' milk', '.', ' discarded', 'ANT', '.\n\n', '<|end_header_id|>', ' discarded', '.\n\n', ' to', 'Question', ':', '\n', ' moved', ' context', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [286.275, 229.1875, 120.171875, 244.0, 22.447916666666668]}, 'weight': {'score': [23.71125, 23.44711399413296, 22.706845238095237, 23.447484166080226, 29.78612012987013], 'topk_indices': array([18828, 18784, 14641, 14677, 19481, 14722, 19423, 14686, 14605,
       14540, 20343, 20295, 18122, 18153, 23551, 23685, 22005, 21978,
       23767, 23633]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 29.677734375, 21.869791666666668, 30.544921875, 20.270833333333332]}, 'saliency': {'score': [1.126107177734375, 0.03420925309814383, 0.3307407924107143, 0.03282147272634808, 0.07491436252346287], 'topk_indices': array([12012, 11919,  9708, 24079, 24185, 11969,  3039, 24160,  3040,
          31,    35, 11922, 11968, 24179,  3042, 24187,    32, 11920,
       24045, 24173]), 'topk_tokens': ['ANT', ' Sandra', ' Sandra', '.\n\n', ' milk', 'IC', ' Sandra', ' return', ' grabbed', 'andra', ' kitchen', ' milk', 'ANT', ' prior', ' milk', ' discarded', ' moved', ' discarded', ' context', 'Question'], 'evidence_proportions': [1.59716796875, 1.78253173828125, 0.7256673177083334, 1.99951171875, 0.11411031087239583]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 100}
2025-01-22 04:38:46.984 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:38:46.984 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-21_pid-1_0-1-4-5-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 1, 4, 5, 9):   2%|▏         | 2/100 [00:56<46:11, 28.28s/it]is_0k: False
your chose emoji: ['👨\u200d👩\u200d👧\u200d👦', '👩🏻\u200d❤️\u200d👨🏼', '🧵', '🇹🇬', '🚶🏼\u200d♂️', '🅰', '💁\u200d♂️', '👩🏻\u200d🦰', '🏄\u200d♀️', '🍨']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (3, 4, 6, 7, 8):   2%|▏         | 2/100 [01:13<46:11, 28.28s/it]2025-01-22 04:39:04.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:39:04.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7621, 7626) --> . Sandra moved to the
2025-01-22 04:39:04.415 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the milk.
2025-01-22 04:39:04.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9850, 9854) -->  Sandra grabbed the milk
2025-01-22 04:39:04.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:39:04.544 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14608, 14614) --> . Sandra went back to the
2025-01-22 04:39:04.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the milk.
2025-01-22 04:39:04.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16908, 16912) -->  Sandra discarded the milk
2025-01-22 04:39:04.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:39:04.732 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19471, 19477) --> . Daniel journeyed to the
2025-01-22 04:39:04.733 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:39:04.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14324, 14329) --> . John went to the
2025-01-22 04:39:04.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:39:04.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14012, 14016) -->  John grabbed the football
2025-01-22 04:39:04.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:39:04.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12150, 12156) --> . Daniel went back to the
2025-01-22 04:39:04.943 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:39:05.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16950, 16956) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:39:07.687 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 04:39:07.687 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 04:39:16.052 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [374.53, 12.457628038194445, 125.32440476190476, 11.98458854468649, 16.76321072048611], 'topk_indices': array([24174, 24168, 24169,  7624,  7625, 24172, 24161,  7626, 24187,
       24068, 24163, 16909, 24181,    14,  7623, 24176,  7622,     0,
       24184, 24185]), 'topk_tokens': [' milk', ' prior', ' to', ' to', ' the', ' where', '.\n\n', ' kitchen', '<|end_header_id|>', '.\n\n', ':', ' discarded', '?\n', '\n', ' moved', ' discarded', ' Sandra', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [794.9, 406.1875, 266.6458333333333, 494.75, 30.854166666666668]}, 'weight': {'score': [23.9575, 23.444715711805557, 21.352306547619047, 23.446004578915762, 29.7734375], 'topk_indices': array([18811, 18767, 14568, 14604, 14656, 19406, 19464, 14620, 14532,
       14467, 20285, 20333, 18105, 18136, 23538, 23672, 21962, 21935,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 29.677734375, 21.869791666666668, 30.544921875, 20.270833333333332]}, 'saliency': {'score': [2.52435546875, 0.06956877027239118, 0.6839715866815477, 0.06649281041955707, 0.1264489491780599], 'topk_indices': array([24171, 24167, 24161,  9853, 14614, 24068, 24181, 24182, 16908,
       24172, 16911, 24174, 24162, 24168,  9850,  7626,  7623, 16909,
       24176,  7622]), 'topk_tokens': [' place', ' location', '.\n\n', ' milk', ' garden', '.\n\n', '?\n', 'Answer', ' Sandra', ' where', ' milk', ' milk', 'Question', ' prior', ' Sandra', ' kitchen', ' moved', ' discarded', ' discarded', ' Sandra'], 'evidence_proportions': [4.99609375, 3.152099609375, 1.4510091145833333, 3.9554443359375, 0.16536458333333334]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:39:16.059 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:39:16.060 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-21_pid-2_3-4-6-7-8.pkl | len: 3 |  size: 2.09 KB
Processing depth (3, 4, 6, 7, 8):   3%|▎         | 3/100 [01:25<46:18, 28.64s/it]is_0k: False
your chose emoji: ['🧑🏻\u200d🍳', '🇼🇫', '🤹🏿\u200d♂️', '🧏\u200d♂️', '👩🏾\u200d💼', '🏌🏼\u200d♂', '🧺', '💂🏼\u200d♀', '🤦🏼\u200d♂', '⏳']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.53s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.98s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 1, 4, 6, 7):   3%|▎         | 3/100 [01:42<46:18, 28.64s/it]2025-01-22 04:39:33.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:39:33.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:39:33.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the milk.
2025-01-22 04:39:33.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2921, 2925) -->  Sandra grabbed the milk
2025-01-22 04:39:33.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:39:33.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9677, 9683) --> . Sandra went back to the
2025-01-22 04:39:33.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the milk.
2025-01-22 04:39:33.473 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14361, 14365) -->  Sandra discarded the milk
2025-01-22 04:39:33.473 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:39:33.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16773, 16779) --> . Daniel journeyed to the
2025-01-22 04:39:33.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:39:33.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14194, 14199) --> . John went to the
2025-01-22 04:39:33.644 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:39:33.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13918, 13922) -->  John grabbed the football
2025-01-22 04:39:33.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:39:33.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12008, 12014) --> . Daniel went back to the
2025-01-22 04:39:33.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:39:33.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16808, 16814) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:39:36.504 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 04:39:36.505 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 04:39:47.133 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [101.31046875, 4.614979752902773, 23.450892857142858, 4.498525958652453, 6.7718762849506575], 'topk_indices': array([24043,    35, 24077, 24192, 24171, 24190, 24197, 23343,     1,
       24170,     9,    23,    24, 24172,    28,    14, 24196,     0,
       24193, 24194]), 'topk_tokens': [' context', ' kitchen', '.\n\n', ':', 'Question', '?\n', '\n\n', ' ', '<|start_header_id|>', '.\n\n', ':', '4', '\n\n', ':', '<|end_header_id|>', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [200.775, 160.515625, 51.135416666666664, 130.515625, 9.658203125]}, 'weight': {'score': [23.71125, 23.441655303499854, 21.352306547619047, 23.443192726661145, 29.000205592105264], 'topk_indices': array([18805, 18761, 14623, 14659, 19488, 19430, 14704, 14668, 14522,
       14587, 20350, 20302, 18130, 18099, 23539, 23673, 21985, 21958,
       23621, 23755]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 29.677734375, 21.869791666666668, 30.544921875, 20.270833333333332]}, 'saliency': {'score': [0.680020751953125, 0.025171177371770537, 0.12927972702752977, 0.024402909149567893, 0.049770104257684], 'topk_indices': array([    0, 24087, 24177, 24197, 23342, 24077, 24190, 24195, 24158,
          24, 24170,    23,     8, 24043,  2921, 24191, 14362,    35,
       24185, 24171]), 'topk_tokens': ['<|begin_of_text|>', ' location', ' prior', '\n\n', ' IN', '.\n\n', '?\n', 'assistant', ' return', '\n\n', '.\n\n', '4', ' Date', ' context', ' Sandra', 'Answer', ' discarded', ' kitchen', ' discarded', 'Question'], 'evidence_proportions': [1.10693359375, 1.2733154296875, 0.2968851725260417, 1.070465087890625, 0.051569620768229164]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 100}
2025-01-22 04:39:47.140 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:39:47.141 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-21_pid-3_0-1-4-6-7.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 1, 4, 6, 7):   4%|▍         | 4/100 [01:56<47:22, 29.60s/it]is_0k: False
your chose emoji: ['🚚', '🇱🇺', '👨\u200d👩\u200d👧\u200d👧', '🚣🏿\u200d♀', '🤽🏽\u200d♀', '👩🏿\u200d🎓', '👯', '\U0001faf7🏼', '🧎🏿\u200d➡️', '🐕\u200d🦺']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.23s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.45s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.57s/it]
Processing depth (2, 3, 5, 6, 7):   4%|▍         | 4/100 [02:13<47:22, 29.60s/it]2025-01-22 04:40:03.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:40:03.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4987, 4992) --> . Sandra moved to the
2025-01-22 04:40:03.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the milk.
2025-01-22 04:40:03.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7620, 7624) -->  Sandra grabbed the milk
2025-01-22 04:40:03.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:40:04.008 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11882, 11888) --> . Sandra went back to the
2025-01-22 04:40:04.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the milk.
2025-01-22 04:40:04.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14384, 14388) -->  Sandra discarded the milk
2025-01-22 04:40:04.083 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:40:04.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16788, 16794) --> . Daniel journeyed to the
2025-01-22 04:40:04.177 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:40:04.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14217, 14222) --> . John went to the
2025-01-22 04:40:04.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:40:04.327 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13957, 13961) -->  John grabbed the football
2025-01-22 04:40:04.327 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:40:04.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12020, 12026) -->  Daniel went back to the bathroom
2025-01-22 04:40:04.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:40:04.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16823, 16829) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:40:07.126 | INFO     | test_jbb_embedding:begin_test:693 - the hallway<|eot_id|>
2025-01-22 04:40:07.126 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24231])
2025-01-22 04:40:15.532 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [328.491875, 11.268946779318313, 90.13392857142857, 10.872604054696543, 23.235477570564516], 'topk_indices': array([ 4986, 11977,  7623,    23,  4992,  7627, 24227,  7624,  4989,
        4987, 24229, 11978,  7620, 11935,  4963,  4964, 11934, 24226,
           0,  4988]), 'topk_tokens': [' St', 'L', ' milk', '4', ' kitchen', '�', '<|start_header_id|>', '.', ' moved', '.', '<|end_header_id|>', 'ANT', ' Sandra', 'IC', '.', ' *', 'ANT', '<|eot_id|>', '<|begin_of_text|>', ' Sandra'], 'evidence_proportions': [769.15, 530.875, 170.46875, 247.265625, 38.528645833333336]}, 'weight': {'score': [23.9575, 23.453301662952875, 22.706845238095237, 23.453428611294857, 29.463205645161292], 'topk_indices': array([18782, 18826, 14662, 14626, 19421, 19479, 14707, 14671, 14525,
       14590, 20313, 20361, 18114, 18145, 23688, 23554, 22020, 21993,
       23636, 23770]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 29.677734375, 21.869791666666668, 30.544921875, 20.270833333333332]}, 'saliency': {'score': [2.36760009765625, 0.06475196908269373, 0.5432332356770834, 0.061956396492449765, 0.16805111464633737], 'topk_indices': array([24204,  4962, 24120, 24076,  7627, 24216,  4948, 14385,  4986,
        4972,  7621,  7623,  4992,  4989, 11978, 11935,  4964, 11934,
        7620,  4988]), 'topk_tokens': ['Question', ' St', ' location', ' context', '�', ' milk', ' *\n\n', ' discarded', ' St', ' *\n\n', ' grabbed', ' milk', ' kitchen', ' moved', 'ANT', 'IC', ' *', 'ANT', ' Sandra', ' Sandra'], 'evidence_proportions': [5.321875, 4.3115234375, 0.9633382161458334, 2.08575439453125, 0.20191446940104166]}}, 'pred_res': 'the hallway<|eot_id|>', 'score': 0}
2025-01-22 04:40:15.556 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:40:15.556 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-21_pid-4_2-3-5-6-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 3, 5, 6, 7):   5%|▌         | 5/100 [02:25<46:11, 29.18s/it]Processing depth (2, 3, 5, 6, 7):   5%|▌         | 5/100 [02:25<46:01, 29.07s/it]
2025-01-22 04:40:15.864 | INFO     | __main__:<module>:82 - Selected idx: 22
2025-01-22 04:40:15.864 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 04:40:15.864 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:40:15.864 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:40:15.864 | INFO     | __main__:<module>:86 - Needle: [' John went back to the bedroom.', ' Daniel journeyed to the hallway.', ' John grabbed the apple.', ' Daniel went back to the kitchen.', ' Sandra moved to the kitchen.', ' Sandra went back to the garden.', ' Daniel went back to the bathroom.', ' John went to the kitchen.', ' Mary took the milk.', ' Sandra discarded the apple.', ' Mary journeyed to the garden.']
2025-01-22 04:40:15.865 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra moved to the kitchen.', ' Sandra went back to the garden.', ' Sandra discarded the apple.', ' Mary journeyed to the garden.']
2025-01-22 04:40:15.865 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🕵🏿\u200d♂️', '👨🏾\u200d🤝\u200d👨🏼', '\U0001fac3🏻', '🧍🏽\u200d♀️', '🤟🏻', '👩🏻\u200d❤\u200d💋\u200d👩🏻', '☢', '🇲🇫', '🚷', '👩🏿\u200d❤️\u200d👩🏼']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.16s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.84s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.12s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (0, 1, 3, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:40:32.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:40:32.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:40:32.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:40:32.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3045, 3051) --> . Sandra went back to the
2025-01-22 04:40:32.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:40:32.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7620, 7624) -->  Sandra discarded the apple
2025-01-22 04:40:32.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the garden.
2025-01-22 04:40:32.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19238, 19244) -->  Mary journeyed to the garden
2025-01-22 04:40:32.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:40:32.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21616, 21622) --> . John went back to the
2025-01-22 04:40:32.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:40:32.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18838, 18844) --> . Daniel journeyed to the
2025-01-22 04:40:32.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the apple.
2025-01-22 04:40:32.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18104, 18108) -->  John grabbed the apple
2025-01-22 04:40:32.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 04:40:32.961 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2842, 2848) --> . Daniel went back to the
2025-01-22 04:40:32.961 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:40:32.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2842, 2848) --> . Daniel went back to the
2025-01-22 04:40:32.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went to the kitchen.
2025-01-22 04:40:32.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2845, 2850) -->  back to the kitchen.
2025-01-22 04:40:32.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary took the milk.
2025-01-22 04:40:33.017 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (5475, 5479) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:40:35.698 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:40:35.698 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 04:40:44.080 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [246.38392857142858, 12.941546047193983, 178.48902027027026, 12.484729701739852, 22.75259060329861], 'topk_indices': array([24122,    24,     1,  1706, 24185, 24123, 24083,  1628, 24183,
          14,  7624,  7623, 18108, 24093, 24182, 24191, 18107, 24193,
           0, 24190]), 'topk_tokens': [' to', '\n\n', '<|start_header_id|>', "'s", ' garden', ' the', '.\n\n', ' work', ' before', '\n', '.', ' apple', '.', ' location', ' apple', '<|start_header_id|>', ' apple', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [305.325, 152.53125, 503.3125, 119.83333333333333]}, 'weight': {'score': [23.600446428571427, 23.442484916108768, 21.666385135135137, 23.445069775269264, 29.59223090277778], 'topk_indices': array([18780, 18824, 14623, 14659, 19511, 14722, 19453, 14686, 14522,
       14587, 20373, 20325, 18118, 18149, 23679, 23545, 22003, 21976,
       23627, 23761]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 21.869791666666668, 30.615234375, 23.813802083333332]}, 'saliency': {'score': [1.6051781063988095, 0.07292445559808662, 1.0662635597022805, 0.07006899273484427, 0.16478390163845485], 'topk_indices': array([   32,    24,  6898, 18780, 24177,  2848,  1706,  1705, 24083,
        7620, 24049, 18105,  1628, 24183, 24185,  7621, 24093,  7623,
       24182, 18107]), 'topk_tokens': [' moved', '\n\n', ' kitchen', 'untlet', 'Question', ' kitchen', "'s", ' summer', '.\n\n', ' Sandra', ' context', ' grabbed', ' work', ' before', ' garden', ' discarded', ' location', ' apple', ' apple', ' apple'], 'evidence_proportions': [1.52626953125, 0.8852132161458334, 4.062255859375, 0.7528483072916666]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 04:40:44.087 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:40:44.087 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-22_pid-0_0-1-3-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 1, 3, 8):   1%|          | 1/100 [00:28<46:22, 28.10s/it]is_0k: False
your chose emoji: ['🙋🏻\u200d♀️', '👮🏿\u200d♂️', '💆🏽', '⚽', '⚰️', '🚣🏻\u200d♂', '🧑🏼\u200d🎄', '🚴🏽\u200d♂', '🧓🏽', '🧍\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (0, 1, 6, 9):   1%|          | 1/100 [00:45<46:22, 28.10s/it]2025-01-22 04:41:01.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:41:01.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:41:01.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:41:01.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3045, 3051) --> . Sandra went back to the
2025-01-22 04:41:01.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:41:01.687 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14333, 14337) -->  Sandra discarded the apple
2025-01-22 04:41:01.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the garden.
2025-01-22 04:41:01.800 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21445, 21451) --> . Mary journeyed to the
2025-01-22 04:41:01.800 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:41:01.909 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21592, 21598) --> . John went back to the
2025-01-22 04:41:01.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:41:02.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18735, 18741) -->  order. Daniel journeyed to
2025-01-22 04:41:02.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the apple.
2025-01-22 04:41:02.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18056, 18060) -->  John grabbed the apple
2025-01-22 04:41:02.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 04:41:02.106 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2842, 2848) --> . Daniel went back to the
2025-01-22 04:41:02.106 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2842, 2848) --> . Daniel went back to the
2025-01-22 04:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went to the kitchen.
2025-01-22 04:41:02.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2845, 2850) -->  back to the kitchen.
2025-01-22 04:41:02.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary took the milk.
2025-01-22 04:41:02.163 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (5461, 5465) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:41:04.861 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom.<|eot_id|>
2025-01-22 04:41:04.861 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 04:41:13.235 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [276.7767857142857, 15.211707054834173, 191.9423564189189, 14.712954746828885, 21.277084350585938], 'topk_indices': array([   24, 18059, 24033, 24107, 24169, 18060, 24077,    14, 24167,
       18380, 24166, 17537, 17527, 18381, 24177, 17528, 17538,     0,
       24175, 24174]), 'topk_tokens': ['\n\n', ' apple', ' context', ' the', ' garden', '.', ' location', '\n', ' before', ' in', ' apple', 'arp', 'arp', ' ', '<|end_header_id|>', 'ente', 'ente', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [365.25, 191.79166666666666, 525.125, 122.46875]}, 'weight': {'score': [22.525669642857142, 23.439309196923332, 22.09353885135135, 23.442168587298955, 29.760986328125], 'topk_indices': array([18817, 18773, 14651, 14615, 19470, 14660, 19412, 14696, 14579,
       14514, 20368, 20320, 18104, 18135, 23539, 23673, 22005, 21978,
       23755, 23621]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 21.869791666666668, 30.615234375, 20.052083333333332]}, 'saliency': {'score': [1.7180059523809523, 0.08469147619484534, 1.1406546927787162, 0.08165009653087951, 0.1542508602142334], 'topk_indices': array([   24, 18379, 14333, 24161,    40, 18057,    39, 18341, 24033,
       14334, 14336, 18059, 24077, 24167, 24169, 17537, 24166, 17527,
       17538, 17528]), 'topk_tokens': ['\n\n', ' one', ' Sandra', 'Question', '\n\n\n', ' grabbed', '***', '186', ' context', ' discarded', ' apple', ' apple', ' location', ' before', ' garden', 'arp', ' apple', 'arp', 'ente', 'ente'], 'evidence_proportions': [1.784765625, 1.0855305989583333, 4.220703125, 0.6263834635416666]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 04:41:13.240 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:41:13.240 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-22_pid-1_0-1-6-9.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 1, 6, 9):   2%|▏         | 2/100 [00:57<46:54, 28.72s/it]is_0k: False
your chose emoji: ['🇵🇹', '🥱', '👩🏽\u200d❤\u200d💋\u200d👨🏿', '✒️', '🤢', '🎚️', '🚶🏻\u200d♀', '🧎🏻\u200d♂', '🚵🏼\u200d♀', '📨']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.95s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.69s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]
Processing depth (3, 4, 6, 7):   2%|▏         | 2/100 [01:14<46:54, 28.72s/it]2025-01-22 04:41:30.529 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:41:30.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7628, 7633) -->  war. Sandra moved to
2025-01-22 04:41:30.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:41:30.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9763, 9769) --> . Sandra went back to the
2025-01-22 04:41:30.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:41:30.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14446, 14450) -->  Sandra discarded the apple
2025-01-22 04:41:30.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the garden.
2025-01-22 04:41:30.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16866, 16872) --> . Mary journeyed to the
2025-01-22 04:41:30.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:41:30.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21671, 21677) --> . John went back to the
2025-01-22 04:41:30.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:41:31.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19155, 19161) -->  order. Daniel journeyed to
2025-01-22 04:41:31.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the apple.
2025-01-22 04:41:31.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18228, 18232) -->  John grabbed the apple
2025-01-22 04:41:31.106 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 04:41:31.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2715, 2721) --> . Daniel went back to the
2025-01-22 04:41:31.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:41:31.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2715, 2721) --> . Daniel went back to the
2025-01-22 04:41:31.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went to the kitchen.
2025-01-22 04:41:31.150 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2718, 2723) -->  back to the kitchen.
2025-01-22 04:41:31.150 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary took the milk.
2025-01-22 04:41:31.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (5375, 5379) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:41:33.839 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 04:41:33.839 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24270])
2025-01-22 04:41:42.241 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [240.13392857142858, 10.874214662382071, 143.78673986486487, 10.47230603448276, 15.97333096590909], 'topk_indices': array([    1, 24264, 24255, 24168, 24256,    14, 24253, 24158, 24261,
       18232,  7630, 18231, 24257, 24124, 24260, 24258, 24268,     0,
       24266, 24265]), 'topk_tokens': ['<|start_header_id|>', ':', ' was', ' location', ' the', '\n', ':', '.\n\n', '?', '.', ' Sandra', ' apple', ' apple', ' context', ' garden', ' before', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [345.9125, 186.66666666666666, 331.1875, 144.75]}, 'weight': {'score': [23.64360119047619, 23.46387961933012, 22.09353885135135, 23.46581761046872, 29.795738636363637], 'topk_indices': array([18842, 18886, 14724, 14688, 14733, 19488, 19546, 14769, 14587,
       14652, 20360, 20408, 18206, 18175, 23742, 23608, 22058, 22031,
       23690, 23824]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.5046875, 21.869791666666668, 30.615234375, 20.052083333333332]}, 'saliency': {'score': [1.5848330543154763, 0.06156737134624892, 0.8551421294341216, 0.059033782851860936, 0.12014756636186079], 'topk_indices': array([19162, 24239, 24158, 24255, 18229,  9769, 24261, 24263, 24254,
       24252,  7631, 24168, 14449, 14447, 18231, 24124, 24258, 24257,
        7630, 24260]), 'topk_tokens': [' hallway', ' return', '.\n\n', ' was', ' grabbed', ' garden', '?', 'Answer', ' Where', 'Question', ' moved', ' location', ' apple', ' discarded', ' apple', ' context', ' before', ' apple', ' Sandra', ' garden'], 'evidence_proportions': [2.305810546875, 1.05322265625, 2.7205810546875, 0.7584635416666666]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 04:41:42.266 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:41:42.267 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-22_pid-2_3-4-6-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (3, 4, 6, 7):   3%|▎         | 3/100 [01:26<46:39, 28.86s/it]is_0k: False
your chose emoji: ['👩🏻\u200d❤\u200d💋\u200d👨🏻', '🙍\u200d♀', '👩🏾\u200d🔧', '🧑🏾\u200d⚕️', '🦹🏻\u200d♂️', '🤽🏾\u200d♀', '🚴\u200d♀', '🏴\U000e0067\U000e0062\U000e0077\U000e006c\U000e0073\U000e007f', '🧎🏿\u200d♂', '🧑🏽\u200d🦽\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.29s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.41s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.11s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.94s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]
Processing depth (1, 3, 4, 8):   3%|▎         | 3/100 [01:42<46:39, 28.86s/it]2025-01-22 04:41:59.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:41:59.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2989, 2994) -->  tragedy. Sandra moved to
2025-01-22 04:41:59.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:41:59.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7632, 7638) -->  war. Sandra went back to
2025-01-22 04:41:59.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:41:59.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9729, 9733) -->  Sandra discarded the apple
2025-01-22 04:41:59.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the garden.
2025-01-22 04:41:59.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19243, 19249) -->  Mary journeyed to the garden
2025-01-22 04:41:59.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:41:59.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21631, 21637) --> . John went back to the
2025-01-22 04:41:59.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:41:59.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18774, 18780) -->  order. Daniel journeyed to
2025-01-22 04:41:59.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the apple.
2025-01-22 04:41:59.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18123, 18127) -->  John grabbed the apple
2025-01-22 04:41:59.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 04:41:59.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2807, 2813) --> . Daniel went back to the
2025-01-22 04:41:59.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:41:59.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2807, 2813) --> . Daniel went back to the
2025-01-22 04:41:59.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went to the kitchen.
2025-01-22 04:41:59.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2810, 2815) -->  back to the kitchen.
2025-01-22 04:41:59.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary took the milk.
2025-01-22 04:41:59.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (5379, 5383) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:42:02.289 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:42:02.289 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24228])
2025-01-22 04:42:10.677 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [171.66369047619048, 10.627794715447154, 133.90371621621622, 10.299206501054895, 11.29565704002809], 'topk_indices': array([   23,     1, 24209, 24212, 24081,    14, 24222, 18126, 24213,
       24211, 18127, 24219, 24218, 24082, 24215, 24216, 24226,     0,
       24224, 24223]), 'topk_tokens': ['4', '<|start_header_id|>', '.\n\n', ' Where', ' you', '\n', ':', ' apple', ' was', ':', '.', '?', ' garden', ' context', ' apple', ' before', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [122.4, 114.14583333333333, 269.65625, 204.90625]}, 'weight': {'score': [26.101190476190474, 23.448542156741365, 22.09353885135135, 23.44831171399909, 29.201544943820224], 'topk_indices': array([18812, 18856, 14664, 14700, 19516, 19458, 14745, 14709, 14628,
       14563, 20366, 20414, 18168, 18137, 23712, 23578, 22017, 22044,
       23794, 23660]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.8484375, 24.756510416666668, 30.615234375, 23.813802083333332]}, 'saliency': {'score': [1.1301037016369047, 0.05887542708236608, 0.8170710383234797, 0.05678429104705534, 0.0859966492385007], 'topk_indices': array([24116, 24126, 24209, 24080,     0, 24081,  9732, 24220, 24221,
       18124, 24213, 18812, 24219, 24212, 24210, 18126, 24082, 24218,
       24216, 24215]), 'topk_tokens': ['.\n\n', ' location', '.\n\n', ' provided', '<|begin_of_text|>', ' you', ' apple', ' \n', 'Answer', ' grabbed', ' was', 'untlet', '?', ' Where', 'Question', ' apple', ' context', ' garden', ' before', ' apple'], 'evidence_proportions': [0.770849609375, 0.703125, 2.0706787109375, 1.2294108072916667]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:42:10.682 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:42:10.682 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-22_pid-3_1-3-4-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 3, 4, 8):   4%|▍         | 4/100 [01:54<45:53, 28.68s/it]is_0k: False
your chose emoji: ['👦🏻', '🧑🏾\u200d⚕️', '👩🏼\u200d⚕', '🧝🏿', '🧑🏿\u200d🤝\u200d🧑🏼', '👨🏾\u200d❤\u200d💋\u200d👨🏿', '🧛🏼\u200d♀', '🎦', '🎂', '🧎\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.69s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (1, 2, 6, 8):   4%|▍         | 4/100 [02:11<45:53, 28.68s/it]2025-01-22 04:42:27.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:42:27.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3038, 3043) --> . Sandra moved to the
2025-01-22 04:42:27.777 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 04:42:27.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4964, 4970) --> . Sandra went back to the
2025-01-22 04:42:27.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra discarded the apple.
2025-01-22 04:42:27.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14354, 14358) -->  Sandra discarded the apple
2025-01-22 04:42:27.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary journeyed to the garden.
2025-01-22 04:42:27.983 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19247, 19253) -->  Mary journeyed to the garden
2025-01-22 04:42:27.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:42:28.103 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21585, 21591) --> . John went back to the
2025-01-22 04:42:28.103 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:42:28.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18807, 18813) --> . Daniel journeyed to the
2025-01-22 04:42:28.212 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the apple.
2025-01-22 04:42:28.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18073, 18077) -->  John grabbed the apple
2025-01-22 04:42:28.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the kitchen.
2025-01-22 04:42:28.320 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2835, 2841) --> . Daniel went back to the
2025-01-22 04:42:28.320 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:42:28.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2835, 2841) --> . Daniel went back to the
2025-01-22 04:42:28.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went to the kitchen.
2025-01-22 04:42:28.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2838, 2843) -->  back to the kitchen.
2025-01-22 04:42:28.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary took the milk.
2025-01-22 04:42:28.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (5466, 5470) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:42:31.032 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 04:42:31.032 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24174])
2025-01-22 04:42:39.410 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [167.2827380952381, 10.76327449642222, 145.5472972972973, 10.420228865210001, 16.579952116935484], 'topk_indices': array([18740,    24,    23, 24164, 24168,     1,    14, 24028, 18077,
       24157, 18739, 18076, 18792, 24161, 24162, 18749, 24172,     0,
       24170, 24169]), 'topk_tokens': [' soon', '\n\n', '4', ' garden', ':', '<|start_header_id|>', '\n', ' context', '.', ':', ' as', ' apple', ' ga', ' apple', ' before', 'untlet', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [140.3375, 101.4375, 347.34375, 135.54166666666666]}, 'weight': {'score': [23.89360119047619, 23.434028208628035, 21.666385135135137, 23.436339737344003, 28.877520161290324], 'topk_indices': array([18793, 18749, 14632, 14596, 14641, 19460, 19402, 14677, 14495,
       14560, 20342, 20294, 18087, 18118, 23526, 23660, 21965, 21992,
       23608, 23742]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 21.869791666666668, 30.615234375, 23.813802083333332]}, 'saliency': {'score': [1.1321788969494047, 0.0606765642934142, 0.8435652449324325, 0.058542627141400454, 0.11946167484406502], 'topk_indices': array([24072, 18074, 24167, 24158, 21591,     0, 18805, 18748, 18739,
       14355, 14357, 18740, 24156, 24164, 24028, 24162, 18076, 18792,
       24161, 18749]), 'topk_tokens': [' location', ' grabbed', 'Answer', ' Where', ' bedroom', '<|begin_of_text|>', ' proc', ' ga', ' as', ' discarded', ' apple', ' soon', 'Question', ' garden', ' context', ' before', ' apple', ' ga', ' apple', 'untlet'], 'evidence_proportions': [0.814990234375, 0.5650431315104166, 2.7926025390625, 0.856689453125]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 04:42:39.417 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:42:39.417 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-22_pid-4_1-2-6-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (1, 2, 6, 8):   5%|▌         | 5/100 [02:23<45:26, 28.70s/it]Processing depth (1, 2, 6, 8):   5%|▌         | 5/100 [02:23<45:30, 28.74s/it]
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:82 - Selected idx: 23
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:86 - Needle: [' Sandra moved to the kitchen.', ' John went to the kitchen.', ' John grabbed the apple.', ' Mary took the milk.', ' Sandra took the football.', ' Daniel went back to the bathroom.', ' John went back to the bedroom.', ' Daniel journeyed to the hallway.', ' Daniel went back to the kitchen.', ' Sandra went back to the garden.', ' Sandra discarded the football.', ' Mary journeyed to the garden.']
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra moved to the kitchen.', ' Sandra took the football.', ' Sandra went back to the garden.', ' Sandra discarded the football.', ' Mary journeyed to the garden.']
2025-01-22 04:42:39.690 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧑🏻\u200d🎄', '\U0001faf4', '🥷🏾', '🍁', '🤦🏿\u200d♂️', '😭', '🤲🏽', '🧑🏿\u200d🏭', '🤝🏽', '👩🏼']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (0, 1, 3, 6, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:42:56.757 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:42:56.757 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:42:56.757 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the football.
2025-01-22 04:42:56.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3039, 3043) -->  Sandra took the football
2025-01-22 04:42:56.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:42:56.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7654, 7660) --> . Sandra went back to the
2025-01-22 04:42:56.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the football.
2025-01-22 04:42:56.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14449, 14453) -->  Sandra discarded the football
2025-01-22 04:42:56.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the garden.
2025-01-22 04:42:57.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19458, 19464) --> . Mary journeyed to the
2025-01-22 04:42:57.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:42:57.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4798, 4803) -->  John went back to the
2025-01-22 04:42:57.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the apple.
2025-01-22 04:42:57.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19241, 19245) -->  grabbed the apple.
2025-01-22 04:42:57.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary took the milk.
2025-01-22 04:42:57.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10810, 10814) -->  Mary took the milk
2025-01-22 04:42:57.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the bathroom.
2025-01-22 04:42:57.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3840, 3846) --> . Daniel went back to the
2025-01-22 04:42:57.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:42:57.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4797, 4803) --> . John went back to the
2025-01-22 04:42:57.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the hallway.
2025-01-22 04:42:57.257 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3177, 3183) --> . Daniel journeyed to the
2025-01-22 04:42:57.257 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel went back to the kitchen.
2025-01-22 04:42:57.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (3840, 3846) --> . Daniel went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:42:59.918 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:42:59.918 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 04:43:08.264 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [353.575, 18.54430484377583, 146.4535472972973, 18.00115371467639, 28.856048583984375], 'topk_indices': array([20682, 14449, 18893,    33,    36, 14452, 24166, 24180, 14450,
       24072,    32, 24178,    23,    24, 24167,    14, 24082, 24188,
           0, 24189]), 'topk_tokens': ['nes', ' Sandra', ' in', ' to', '.', ' football', 'Question', ' discarded', ' discarded', '.\n\n', ' moved', ' football', '4', '\n\n', ':', '\n', ' location', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [622.6, 383.1875, 228.85416666666666, 623.875, 54.166666666666664]}, 'weight': {'score': [23.48125, 23.43659592494627, 21.55447635135135, 23.439435158904452, 29.27001953125], 'topk_indices': array([18793, 18749, 14615, 14651, 19451, 19393, 14696, 14660, 14579,
       14514, 20340, 20292, 18118, 18087, 23542, 23676, 21960, 21987,
       23624, 23758]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 28.240234375, 21.869791666666668, 30.873046875, 20.052083333333332]}, 'saliency': {'score': [2.329013671875, 0.10516649026440321, 0.8243441195101351, 0.10176026867567539, 0.20715951919555664], 'topk_indices': array([24172,  1628,    23, 17526,    20, 24038, 17516,  3042, 18892,
       20682, 11705,    35, 14452,    32, 14449, 24166, 24178, 24082,
       24180, 14450]), 'topk_tokens': [' prior', ' work', '4', 'ente', ' Jul', ' context', 'ente', ' football', ' manner', 'nes', ' kitchen', ' kitchen', ' football', ' moved', ' Sandra', 'Question', ' football', ' location', ' discarded', ' discarded'], 'evidence_proportions': [3.262109375, 2.77294921875, 1.4019368489583333, 5.1435546875, 0.3061930338541667]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:43:08.272 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:43:08.272 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-23_pid-0_0-1-3-6-8.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 1, 3, 6, 8):   1%|          | 1/100 [00:28<46:58, 28.47s/it]is_0k: False
your chose emoji: ['🇦🇷', '💪🏽', '🤽🏻\u200d♂️', '👮\u200d♂', '🧙🏻', '👮\u200d♂️', '🧑🏾\u200d🦲', '✊🏿', '🤌🏻', '👫']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.38s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.72s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (0, 5, 7, 8, 9):   1%|          | 1/100 [00:45<46:58, 28.47s/it]2025-01-22 04:43:25.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:43:25.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  moved to the kitchen.
2025-01-22 04:43:25.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the football.
2025-01-22 04:43:25.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11847, 11851) -->  Sandra took the football
2025-01-22 04:43:25.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:43:25.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16751, 16757) --> . Sandra went back to the
2025-01-22 04:43:25.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the football.
2025-01-22 04:43:25.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19326, 19330) -->  Sandra discarded the football
2025-01-22 04:43:25.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the garden.
2025-01-22 04:43:26.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21438, 21444) --> . Mary journeyed to the
2025-01-22 04:43:26.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:43:26.150 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11534, 11539) --> . John went to the
2025-01-22 04:43:26.150 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the apple.
2025-01-22 04:43:26.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19164, 19168) -->  grabbed the apple.
2025-01-22 04:43:26.254 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary took the milk.
2025-01-22 04:43:26.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10668, 10672) -->  Mary took the milk
2025-01-22 04:43:26.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the bathroom.
2025-01-22 04:43:26.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3835, 3841) --> . Daniel went back to the
2025-01-22 04:43:26.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:43:26.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4663, 4669) -->  John went back to the bedroom
2025-01-22 04:43:26.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the hallway.
2025-01-22 04:43:26.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3172, 3178) --> . Daniel journeyed to the
2025-01-22 04:43:26.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel went back to the kitchen.
2025-01-22 04:43:26.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (3835, 3841) --> . Daniel went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:43:29.033 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:43:29.034 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24145])
2025-01-22 04:43:37.384 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [233.505, 9.715381449809508, 111.13935810810811, 9.327295939549947, 28.1822021484375], 'topk_indices': array([   35, 19327,    14,     1, 23990,     3, 24130, 24141, 11998,
          36,    23,    24,    32, 11539, 11955, 11850, 11954, 24143,
           0, 24140]), 'topk_tokens': [' kitchen', ' discarded', '\n', '<|start_header_id|>', ' context', '<|end_header_id|>', ' football', '<|start_header_id|>', 'ANT', '.', '4', '\n\n', ' moved', ' kitchen', 'IC', ' football', 'ANT', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [408.85, 328.3125, 177.63541666666666, 309.59375, 29.322916666666668]}, 'weight': {'score': [23.48125, 23.42416297415935, 21.8125, 23.426579496387944, 29.0171875], 'topk_indices': array([18776, 18732, 14601, 14637, 19439, 14682, 14646, 19381, 14565,
       14500, 20307, 20259, 18101, 18070, 23496, 23630, 21949, 21922,
       23712, 23578]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 28.240234375, 21.869791666666668, 30.873046875, 20.052083333333332]}, 'saliency': {'score': [1.50541259765625, 0.05540784020998116, 0.6171083192567568, 0.05303994862728219, 0.19695549011230468], 'topk_indices': array([24138, 24118,    23, 24105, 19326, 19329, 11847,  3841, 16757,
       11998,    31, 23990,    35, 11955,    32, 24130, 11954, 19327,
       11539, 11850]), 'topk_tokens': ['Answer', 'Question', '4', ' return', ' Sandra', ' football', ' Sandra', ' bathroom', ' garden', 'ANT', 'andra', ' context', ' kitchen', 'IC', ' moved', ' football', 'ANT', ' discarded', ' kitchen', ' football'], 'evidence_proportions': [2.13828125, 2.4378662109375, 1.0304361979166667, 2.515380859375, 0.158050537109375]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:43:37.392 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:43:37.392 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-23_pid-1_0-5-7-8-9.pkl | len: 3 |  size: 2.13 KB
Processing depth (0, 5, 7, 8, 9):   2%|▏         | 2/100 [00:57<47:07, 28.85s/it]is_0k: False
your chose emoji: ['🐗', '👌🏻', '🇬🇵', '🧘\u200d♂', '🤑', '🏈', '🏋🏾', '🏵', '🚷', '😜']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.68s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.88s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (2, 5, 7, 8, 9):   2%|▏         | 2/100 [01:14<47:07, 28.85s/it]2025-01-22 04:43:54.549 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:43:54.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4932, 4937) --> . Sandra moved to the
2025-01-22 04:43:54.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the football.
2025-01-22 04:43:54.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11865, 11869) -->  took the football.
2025-01-22 04:43:54.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:43:54.719 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16720, 16726) --> . Sandra went back to the
2025-01-22 04:43:54.719 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the football.
2025-01-22 04:43:54.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19199, 19203) -->  Sandra discarded the football
2025-01-22 04:43:54.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the garden.
2025-01-22 04:43:54.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21481, 21487) --> . Mary journeyed to the
2025-01-22 04:43:54.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:43:54.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4664, 4669) -->  John went back to the
2025-01-22 04:43:54.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the apple.
2025-01-22 04:43:55.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18894, 18898) -->  John grabbed the apple
2025-01-22 04:43:55.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary took the milk.
2025-01-22 04:43:55.090 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10679, 10683) -->  Mary took the milk
2025-01-22 04:43:55.090 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the bathroom.
2025-01-22 04:43:55.109 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3798, 3804) --> . Daniel went back to the
2025-01-22 04:43:55.109 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:43:55.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4663, 4669) --> . John went back to the
2025-01-22 04:43:55.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the hallway.
2025-01-22 04:43:55.147 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3165, 3171) --> . Daniel journeyed to the
2025-01-22 04:43:55.147 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel went back to the kitchen.
2025-01-22 04:43:55.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (3798, 3804) --> . Daniel went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:43:57.856 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 04:43:57.857 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24238])
2025-01-22 04:44:06.280 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [339.84375, 12.83042417804546, 131.13175675675674, 12.311276055668142, 32.89028825431034], 'topk_indices': array([    1,    14, 19200,    24, 16835, 24223,  4935, 24117, 16796,
       11868,  4933,     3,  4934, 16797,    23, 11867, 24233, 24236,
           0, 24234]), 'topk_tokens': ['<|start_header_id|>', '\n', ' discarded', '\n\n', 'present', ' football', ' to', '.\n\n', 'present', '.', ' Sandra', '<|end_header_id|>', ' moved', 'ed', '4', ' football', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [536.85, 490.75, 304.5416666666667, 391.28125, 76.078125]}, 'weight': {'score': [22.5234375, 23.451582546099583, 22.07622466216216, 23.45464685264072, 29.774425287356323], 'topk_indices': array([18835, 18879, 14684, 14720, 19542, 14765, 19484, 14729, 14583,
       14648, 20404, 20356, 18153, 18184, 23709, 23575, 22034, 22007,
       23791, 23657]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 20.71484375, 21.869791666666668, 30.873046875, 20.052083333333332]}, 'saliency': {'score': [2.0681298828125, 0.0729223905747236, 0.7401683910472973, 0.06983838009772168, 0.2397148746183549], 'topk_indices': array([24235,    20,  3804, 24117, 16797,    23, 16721, 19202, 24127,
        4937, 11864, 16726, 16835, 24225,  4934, 24223, 16796, 19200,
        4933, 11867]), 'topk_tokens': ['assistant', ' Jul', ' bathroom', '.\n\n', 'ed', '4', ' Sandra', ' football', ' location', ' kitchen', ' Sandra', ' garden', 'present', ' discarded', ' moved', ' football', 'present', ' discarded', ' Sandra', ' football'], 'evidence_proportions': [2.9701171875, 2.7332763671875, 1.7753092447916667, 3.182861328125, 0.4227091471354167]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 04:44:06.297 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:44:06.298 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-23_pid-2_2-5-7-8-9.pkl | len: 3 |  size: 2.13 KB
Processing depth (2, 5, 7, 8, 9):   3%|▎         | 3/100 [01:26<46:40, 28.88s/it]is_0k: False
your chose emoji: ['👨', '👨🏽\u200d🚀', '👩🏾\u200d❤\u200d💋\u200d👨🏼', '💇🏾\u200d♂', '🙎🏿', '🙆🏾\u200d♀', '🇫🇲', '👱🏽\u200d♀️', '🧔🏾\u200d♂️', '🏌🏿\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.99s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.09s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.32s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.48s/it]
Processing depth (1, 2, 6, 7, 8):   3%|▎         | 3/100 [01:42<46:40, 28.88s/it]2025-01-22 04:44:22.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:44:22.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2939, 2944) --> . Sandra moved to the
2025-01-22 04:44:22.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the football.
2025-01-22 04:44:22.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4911, 4915) -->  Sandra took the football
2025-01-22 04:44:22.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:44:22.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14284, 14290) --> . Sandra went back to the
2025-01-22 04:44:22.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the football.
2025-01-22 04:44:22.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16726, 16730) -->  Sandra discarded the football
2025-01-22 04:44:22.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the garden.
2025-01-22 04:44:22.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19207, 19213) --> . Mary journeyed to the
2025-01-22 04:44:22.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:44:22.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4600, 4605) -->  John went back to the
2025-01-22 04:44:22.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the apple.
2025-01-22 04:44:22.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (18903, 18907) -->  John grabbed the apple
2025-01-22 04:44:22.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary took the milk.
2025-01-22 04:44:23.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10664, 10668) -->  Mary took the milk
2025-01-22 04:44:23.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the bathroom.
2025-01-22 04:44:23.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3820, 3826) --> . Daniel went back to the
2025-01-22 04:44:23.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:44:23.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4599, 4605) --> . John went back to the
2025-01-22 04:44:23.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the hallway.
2025-01-22 04:44:23.089 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3085, 3091) --> . Daniel journeyed to the
2025-01-22 04:44:23.089 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel went back to the kitchen.
2025-01-22 04:44:23.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (3820, 3826) --> . Daniel went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:44:25.757 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:44:25.757 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24230])
2025-01-22 04:44:34.164 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [223.68, 9.081209636652499, 88.81440033783784, 8.737198308406768, 12.308734939759036], 'topk_indices': array([24203, 24217,     9, 16727, 24222,    24,     3, 24224, 16729,
       24204, 16730,    14, 24215,    23,  4914,  4915, 24228,     0,
       24225, 24226]), 'topk_tokens': ['Question', ' discarded', ':', ' discarded', '?\n', '\n\n', '<|end_header_id|>', ':', ' football', ':', '.', '\n', ' football', '4', ' football', '.', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [206.95, 371.125, 190.85416666666666, 394.75, 58.104166666666664]}, 'weight': {'score': [23.7275, 23.447909359138364, 22.07622466216216, 23.449719899362872, 29.543109939759034], 'topk_indices': array([18844, 18888, 14689, 14725, 19553, 19495, 14770, 14734, 14653,
       14588, 20367, 20415, 18182, 18213, 23579, 23713, 22011, 22038,
       23661, 23795]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 28.240234375, 21.869791666666668, 30.873046875, 20.052083333333332]}, 'saliency': {'score': [1.5719970703125, 0.05084078654204752, 0.5091552734375, 0.04856588922255751, 0.09194514263107116], 'topk_indices': array([24109, 24202, 24119, 24227,    20, 24190, 14290, 24075, 16726,
          23, 14285,  4911,  2940, 24223, 24203, 16729, 24217, 16727,
       24215,  4914]), 'topk_tokens': ['.\n\n', '.\n\n', ' location', 'assistant', ' Jul', ' return', ' garden', ' context', ' Sandra', '4', ' Sandra', ' Sandra', ' Sandra', 'Answer', 'Question', ' football', ' discarded', ' discarded', ' football', ' football'], 'evidence_proportions': [1.3248046875, 2.8011474609375, 1.1680908203125, 3.1378173828125, 0.3185831705729167]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:44:34.171 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:44:34.171 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-23_pid-3_1-2-6-7-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 6, 7, 8):   4%|▍         | 4/100 [01:54<45:34, 28.48s/it]is_0k: False
your chose emoji: ['🧛🏼\u200d♂', '🚶🏿\u200d♀\u200d➡', '\U0001faf1🏽\u200d\U0001faf2🏻', '👩\u200d👧', '👩🏼\u200d💼', '👩🏽\u200d❤\u200d👨🏻', '🚣🏾', '✈️', '⛹\u200d♂️', '🌃']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.82s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.74s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
Processing depth (1, 3, 5, 6, 9):   4%|▍         | 4/100 [02:12<45:34, 28.48s/it]2025-01-22 04:44:52.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the kitchen.
2025-01-22 04:44:52.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Sandra moved to the
2025-01-22 04:44:52.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the football.
2025-01-22 04:44:52.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7531, 7535) -->  Sandra took the football
2025-01-22 04:44:52.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 04:44:52.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11912, 11918) --> . Sandra went back to the
2025-01-22 04:44:52.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra discarded the football.
2025-01-22 04:44:52.254 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14356, 14360) -->  Sandra discarded the football
2025-01-22 04:44:52.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary journeyed to the garden.
2025-01-22 04:44:52.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21475, 21481) --> . Mary journeyed to the
2025-01-22 04:44:52.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went to the kitchen.
2025-01-22 04:44:52.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4792, 4797) -->  John went back to the
2025-01-22 04:44:52.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the apple.
2025-01-22 04:44:52.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19107, 19111) -->  John grabbed the apple
2025-01-22 04:44:52.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary took the milk.
2025-01-22 04:44:52.592 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10784, 10788) -->  Mary took the milk
2025-01-22 04:44:52.592 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went back to the bathroom.
2025-01-22 04:44:52.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3834, 3840) --> . Daniel went back to the
2025-01-22 04:44:52.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the bedroom.
2025-01-22 04:44:52.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4791, 4797) --> . John went back to the
2025-01-22 04:44:52.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the hallway.
2025-01-22 04:44:52.652 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (3171, 3177) --> . Daniel journeyed to the
2025-01-22 04:44:52.652 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel went back to the kitchen.
2025-01-22 04:44:52.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (3834, 3840) --> . Daniel went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:44:55.322 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 04:44:55.322 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24200])
2025-01-22 04:45:03.731 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [204.526875, 9.672269450068173, 78.15582770270271, 9.365519241125057, 10.353106330422793], 'topk_indices': array([    9, 24173, 24185, 24160, 24192, 24199,    14,  7535, 24174,
       24194,    24, 24079,  7534,     3, 24045,    23, 24198,     0,
       24195, 24196]), 'topk_tokens': [':', 'Question', ' football', ' return', '?\n', '\n\n', '\n', '.', ':', ':', '\n\n', '.\n\n', ' football', '<|end_header_id|>', ' context', '4', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [239.0, 349.25, 190.95833333333334, 287.96875, 37.2578125]}, 'weight': {'score': [23.7275, 23.439098458868735, 22.07622466216216, 23.440888620500395, 29.319623161764707], 'topk_indices': array([18792, 18836, 14634, 14598, 19494, 14643, 14679, 19436, 14497,
       14562, 20308, 20356, 18161, 18130, 23679, 23545, 22004, 21977,
       23627, 23761]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.040625, 28.240234375, 21.869791666666668, 30.873046875, 20.052083333333332]}, 'saliency': {'score': [1.39293701171875, 0.054517202759923875, 0.4447268924197635, 0.05253309879814165, 0.07376351076013901], 'topk_indices': array([24043, 11913, 24197,    22,    19, 11918,  7531,    20, 24079,
       24160, 14359, 24193,  3032, 24173, 24185, 14357, 24187,    23,
        7534, 24045]), 'topk_tokens': [' provided', ' Sandra', 'assistant', '202', '26', ' garden', ' Sandra', ' Jul', '.\n\n', ' return', ' football', 'Answer', ' Sandra', 'Question', ' football', ' discarded', ' discarded', '4', ' football', ' context'], 'evidence_proportions': [1.421875, 2.551025390625, 1.1533610026041667, 2.338134765625, 0.206207275390625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 04:45:03.739 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:45:03.739 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-23_pid-4_1-3-5-6-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 3, 5, 6, 9):   5%|▌         | 5/100 [02:23<45:42, 28.87s/it]Processing depth (1, 3, 5, 6, 9):   5%|▌         | 5/100 [02:24<45:43, 28.88s/it]
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:82 - Selected idx: 24
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the office? 
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:86 - Needle: [' Daniel went back to the bathroom.', ' John went to the kitchen.', ' Mary went to the kitchen.', ' John went back to the bedroom.', ' Mary travelled to the office.', ' John grabbed the football.', ' Daniel journeyed to the hallway.', ' Mary discarded the football.', ' John left the football.']
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:87 - Real Needle: [' Mary went to the kitchen.', ' Mary travelled to the office.', ' Mary discarded the football.', ' John left the football.']
2025-01-22 04:45:04.199 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🎅🏻', '🧑🏾\u200d❤\u200d🧑🏿', '🚇', '6⃣', '🪕', '👩🏻\u200d✈️', '👩🏼\u200d🍳', '🏃🏿\u200d♀️', '🙋', '💃🏼']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.54s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.75s/it]
Processing depth (2, 5, 6, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:45:21.260 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:45:21.283 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4738, 4743) -->  the senate. Mary went
2025-01-22 04:45:21.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:45:21.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11786, 11791) --> . Mary travelled to the
2025-01-22 04:45:21.351 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:45:21.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14293, 14297) -->  Mary discarded the football
2025-01-22 04:45:21.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:45:21.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21421, 21425) -->  John left the football
2025-01-22 04:45:21.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:45:21.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4019, 4025) --> . Daniel went back to the
2025-01-22 04:45:21.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:45:21.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4741, 4746) -->  Mary went to the kitchen
2025-01-22 04:45:21.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:45:21.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6242, 6248) --> . John went back to the
2025-01-22 04:45:21.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John grabbed the football.
2025-01-22 04:45:21.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11156, 11160) -->  John grabbed the football
2025-01-22 04:45:21.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:45:21.738 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15453, 15459) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:45:24.525 | INFO     | test_jbb_embedding:begin_test:693 - John grabbed the football.<|eot_id|>
2025-01-22 04:45:24.525 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24155])
2025-01-22 04:45:32.916 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [355.9947916666667, 19.53320069956122, 397.4166666666667, 18.85891039066064, 27.444401041666666], 'topk_indices': array([24146, 24148, 14294, 24008, 11156, 24149, 24138, 11160, 14296,
       11155, 24145, 24143, 24142, 11157, 24009, 24153, 24150,     0,
       24151, 11159]), 'topk_tokens': ['?', 'Answer', ' discarded', ' you', ' John', ':', ':', '.', ' football', '.', ' office', ' before', ' football', ' grabbed', ' context', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', ' football'], 'evidence_proportions': [197.26875, 209.1625, 699.75, 394.1875]}, 'weight': {'score': [24.610243055555557, 23.435114661809752, 22.140625, 23.435686921992286, 29.3390625], 'topk_indices': array([18784, 18828, 14635, 14671, 19423, 14716, 14680, 19481, 14599,
       14534, 20295, 20343, 18122, 18153, 23511, 23645, 21944, 21971,
       23593, 23727]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.13125, 20.75625, 29.130859375, 25.505859375]}, 'saliency': {'score': [2.486667209201389, 0.11046717508175345, 2.5588921440972223, 0.10595180682489735, 0.20513890584309896], 'topk_indices': array([14293, 24146, 24008, 24139, 24053, 24007, 21424, 11156, 24137,
       11154,  4025, 24148, 24143, 24145, 14296, 14294, 24009, 24142,
       11157, 11159]), 'topk_tokens': [' Mary', '?', ' you', ' Where', ' location', ' provided', ' football', ' John', 'Question', ' Geo', ' bathroom', 'Answer', ' before', ' office', ' football', ' discarded', ' context', ' football', ' grabbed', ' football'], 'evidence_proportions': [1.19345703125, 1.232373046875, 5.489501953125, 2.668212890625]}}, 'pred_res': 'John grabbed the football.<|eot_id|>', 'score': 0}
2025-01-22 04:45:32.923 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:45:32.923 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-24_pid-0_2-5-6-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 5, 6, 9):   1%|          | 1/100 [00:28<47:10, 28.59s/it]is_0k: False
your chose emoji: ['🙏🏻', '🧑🏻\u200d❤️\u200d🧑🏽', '🤹🏾\u200d♀', '🏃🏻\u200d➡', '😃', '🍍', '🌡', '🤲🏼', '🕟', '🖨️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.43s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.98s/it]
Processing depth (1, 3, 4, 8):   1%|          | 1/100 [00:46<47:10, 28.59s/it]2025-01-22 04:45:51.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:45:51.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2877, 2882) --> . Mary went to the
2025-01-22 04:45:51.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:45:51.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7457, 7462) --> . Mary travelled to the
2025-01-22 04:45:51.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:45:51.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9652, 9656) -->  Mary discarded the football
2025-01-22 04:45:51.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:45:51.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19260, 19264) -->  left the football.
2025-01-22 04:45:51.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:45:51.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4047, 4053) --> . Daniel went back to the
2025-01-22 04:45:51.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:45:51.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2878, 2883) -->  Mary went to the kitchen
2025-01-22 04:45:51.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:45:51.418 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6328, 6334) --> . John went back to the
2025-01-22 04:45:51.418 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John grabbed the football.
2025-01-22 04:45:51.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11165, 11169) -->  John grabbed the football
2025-01-22 04:45:51.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:45:51.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15455, 15461) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:45:54.270 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom.<|eot_id|>
2025-01-22 04:45:54.270 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 04:46:02.694 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [235.41145833333334, 10.813944918841896, 167.11574074074073, 10.472036377084454, 15.406631645114942], 'topk_indices': array([   23,    24,     1, 24190,  9350, 24097, 24191, 24200, 24203,
       11168,    14,  9652, 24199, 24196, 24192, 24197, 24207, 24205,
       24204,     0]), 'topk_tokens': ['4', '\n\n', '<|start_header_id|>', '.\n\n', ' cause', '.\n\n', 'Question', '?', ':', ' football', '\n', ' Mary', ' office', ' football', ':', ' before', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [174.025, 81.23125, 444.1875, 296.09375]}, 'weight': {'score': [22.22439236111111, 23.449167768048902, 22.140625, 23.45154193993876, 29.461925287356323], 'topk_indices': array([18768, 18812, 14637, 14673, 14718, 14682, 19412, 19470, 14536,
       14601, 20320, 20368, 18137, 18106, 23691, 23557, 22017, 21990,
       23773, 23639]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 29.130859375, 20.6796875]}, 'saliency': {'score': [1.5105726453993056, 0.06084713005495725, 1.0498408564814814, 0.058662419917591656, 0.11471820699757543], 'topk_indices': array([24190,     0,  4053, 24178, 24193, 11166, 24097,  9653, 19262,
       24200, 24202, 22700,  9655,  9350, 24197, 24199, 24191, 11168,
        9652, 24196]), 'topk_tokens': ['.\n\n', '<|begin_of_text|>', ' bathroom', ' return', ' Where', ' grabbed', '.\n\n', ' discarded', ' football', '?', 'Answer', ' kitchen', ' football', ' cause', ' before', ' office', 'Question', ' football', ' Mary', ' football'], 'evidence_proportions': [0.87587890625, 0.4586669921875, 3.4296875, 1.69970703125]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 04:46:02.702 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:46:02.702 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-24_pid-1_1-3-4-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 3, 4, 8):   2%|▏         | 2/100 [00:58<47:50, 29.29s/it]is_0k: False
your chose emoji: ['👨🏼\u200d❤\u200d💋\u200d👨🏽', '🧜🏽\u200d♀️', '🤓', '\U0001faba', '👭🏽', '🧑🏻\u200d❤\u200d🧑🏿', '👷🏿\u200d♀️', '🚯', '👩🏾\u200d🦼\u200d➡', '👷🏿\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.20s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.32s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.91s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.98s/it]
Processing depth (1, 2, 7, 9):   2%|▏         | 2/100 [01:16<47:50, 29.29s/it]2025-01-22 04:46:20.797 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:46:20.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2945, 2950) --> . Mary went to the
2025-01-22 04:46:20.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:46:20.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4946, 4951) --> . Mary travelled to the
2025-01-22 04:46:20.837 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:46:20.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16708, 16712) -->  Mary discarded the football
2025-01-22 04:46:20.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:46:21.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21457, 21461) -->  John left the football
2025-01-22 04:46:21.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:46:21.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4127, 4133) --> . Daniel went back to the
2025-01-22 04:46:21.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:46:21.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2946, 2951) -->  Mary went to the kitchen
2025-01-22 04:46:21.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:46:21.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6396, 6402) --> . John went back to the
2025-01-22 04:46:21.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John grabbed the football.
2025-01-22 04:46:21.144 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11204, 11208) -->  John grabbed the football
2025-01-22 04:46:21.144 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:46:21.223 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15476, 15482) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:46:23.921 | INFO     | test_jbb_embedding:begin_test:693 - John's bedroom<|eot_id|>
2025-01-22 04:46:23.921 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 04:46:32.301 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [222.61458333333334, 11.403575229376756, 202.85185185185185, 11.032124704981161, 20.684323081487342], 'topk_indices': array([    9, 11208, 16709, 24175,    23,    24, 16711, 24081, 24187,
       24184, 24176,    14, 24180, 24181, 11207, 24183, 24191, 24189,
           0, 24188]), 'topk_tokens': [':', '.', ' discarded', 'Question', '4', '\n\n', ' football', '.\n\n', ':', '?', ':', '\n', ' football', ' before', ' football', ' office', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [157.15, 170.4125, 419.28125, 173.03125]}, 'weight': {'score': [23.296875, 23.441715572821956, 22.140625, 23.44327809925055, 28.926819620253166], 'topk_indices': array([18864, 18820, 14658, 14694, 19517, 19459, 14703, 14739, 14622,
       14557, 20331, 20379, 18189, 18158, 23681, 23547, 22007, 21980,
       23629, 23763]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 29.130859375, 25.505859375]}, 'saliency': {'score': [1.4958902994791667, 0.06424813405366589, 1.2684009693287037, 0.06183491366817109, 0.15235012392454508], 'topk_indices': array([   24,    23, 24045, 24162, 24081,  6402,  4133, 24184, 24177,
       24047, 24186, 22690, 11205, 24175, 16711, 24181, 16709, 24183,
       24180, 11207]), 'topk_tokens': ['\n\n', '4', ' provided', ' return', '.\n\n', ' bedroom', ' bathroom', '?', ' Where', ' context', 'Answer', ' kitchen', ' grabbed', 'Question', ' football', ' before', ' discarded', ' office', ' football', ' football'], 'evidence_proportions': [0.811767578125, 0.970751953125, 3.306640625, 1.19671630859375]}}, 'pred_res': "John's bedroom<|eot_id|>", 'score': 0}
2025-01-22 04:46:32.335 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:46:32.335 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-24_pid-2_1-2-7-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 2, 7, 9):   3%|▎         | 3/100 [01:28<47:36, 29.45s/it]is_0k: False
your chose emoji: ['🙏🏾', '👷🏿\u200d♀', '👨🏽\u200d🤝\u200d👨🏼', '⛵', '🙍', '👩🏼\u200d🦯\u200d➡️', '👨🏻\u200d🚀', '👩🏼\u200d⚕', '👗', '🧖\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.11s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 1, 3, 8):   3%|▎         | 3/100 [01:45<47:36, 29.45s/it]2025-01-22 04:46:49.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:46:49.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:46:49.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:46:49.649 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2972, 2977) -->  tragedy. Mary travelled to
2025-01-22 04:46:49.649 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:46:49.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7646, 7650) -->  Mary discarded the football
2025-01-22 04:46:49.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:46:49.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19270, 19274) -->  left the football.
2025-01-22 04:46:49.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:46:49.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4164, 4170) -->  Daniel went back to the bathroom
2025-01-22 04:46:49.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:46:49.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:46:49.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:46:49.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6414, 6420) --> . John went back to the
2025-01-22 04:46:49.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John grabbed the football.
2025-01-22 04:46:49.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11211, 11215) -->  John grabbed the football
2025-01-22 04:46:49.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:46:50.008 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15493, 15499) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:46:52.722 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:46:52.722 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24173])
2025-01-22 04:47:01.102 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [300.13194444444446, 13.457258748345467, 188.0648148148148, 13.048052815879988, 16.966627038043477], 'topk_indices': array([22674, 24146,  7647, 24156, 24160,     9,  7650,    24, 24027,
       24061, 18778, 24167, 24163, 24169, 24161,    14,  7649, 24171,
       24168,     0]), 'topk_tokens': [' kitchen', '.', ' discarded', ':', ' football', ':', '.', '\n\n', ' context', '.\n\n', 'untlet', ':', ' office', '<|start_header_id|>', ' before', '\n', ' football', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [258.375, 188.325, 561.375, 230.84375]}, 'weight': {'score': [23.8828125, 23.436926083719392, 22.33738425925926, 23.437823753677844, 28.879302536231883], 'topk_indices': array([18778, 18822, 14663, 14627, 14672, 14708, 19422, 19480, 14526,
       14591, 20300, 20348, 18116, 18147, 23665, 23531, 21971, 21944,
       23747, 23613]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.565625, 26.5640625, 29.130859375, 20.6796875]}, 'saliency': {'score': [1.9273003472222223, 0.07662701811875672, 1.1641710069444444, 0.07402970314539647, 0.12271505162335825], 'topk_indices': array([ 3057,    24, 24025, 24157, 24142, 24061,  4169, 24071, 11214,
       18821, 24155, 24166, 22674, 24160, 24027, 24161,  7647, 24163,
       18778,  7649]), 'topk_tokens': [' Fourth', '\n\n', ' provided', ' Where', ' return', '.\n\n', ' bathroom', ' location', ' football', ' ga', 'Question', 'Answer', ' kitchen', ' football', ' context', ' before', ' discarded', ' office', 'untlet', ' football'], 'evidence_proportions': [1.31572265625, 1.151416015625, 4.266845703125, 1.32208251953125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:47:01.117 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:47:01.117 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-24_pid-3_0-1-3-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 3, 8):   4%|▍         | 4/100 [01:56<46:41, 29.18s/it]is_0k: False
your chose emoji: ['🚵🏽\u200d♂', '🤵🏽', '🪴', '👩🏼\u200d🏭', '\U0001faf8', '🧑🏽\u200d🦱', '👱🏾\u200d♀️', '👩🏾\u200d🦼', '👫🏾', '🧀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.85s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.99s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (2, 5, 7, 8):   4%|▍         | 4/100 [02:14<46:41, 29.18s/it]2025-01-22 04:47:18.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:47:18.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4994, 4999) --> . Mary went to the
2025-01-22 04:47:18.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:47:18.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11932, 11937) --> . Mary travelled to the
2025-01-22 04:47:18.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:47:18.990 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16794, 16798) -->  Mary discarded the football
2025-01-22 04:47:18.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:47:19.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19363, 19367) -->  John left the football
2025-01-22 04:47:19.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:47:19.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4158, 4164) -->  Daniel went back to the bathroom
2025-01-22 04:47:19.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:47:19.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4995, 5000) -->  Mary went to the kitchen
2025-01-22 04:47:19.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the bedroom.
2025-01-22 04:47:19.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6409, 6415) -->  John went back to the bedroom
2025-01-22 04:47:19.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John grabbed the football.
2025-01-22 04:47:19.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11310, 11314) -->  John grabbed the football
2025-01-22 04:47:19.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:47:19.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15652, 15658) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:47:21.983 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:47:21.983 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24217])
2025-01-22 04:47:30.362 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [295.0798611111111, 15.035069157720892, 249.02546296296296, 14.56522233712513, 28.105318509615383], 'topk_indices': array([   14, 24200, 22764, 19366, 11247, 24204,  7516, 11964, 11963,
       16797, 24213, 24208, 24212, 24205, 11313, 24207,  7473, 24215,
        7517,     0]), 'topk_tokens': ['\n', ':', ' kitchen', ' football', '�', ' football', ' two', 'IC', 'ANT', ' football', '<|start_header_id|>', '?', '<|eot_id|>', ' before', ' football', ' office', ' three', '<|end_header_id|>', ' or', '<|begin_of_text|>'], 'evidence_proportions': [257.925, 143.2375, 441.09375, 385.3125]}, 'weight': {'score': [23.296875, 23.451906998348473, 24.199074074074073, 23.451187952430196, 29.562328296703296], 'topk_indices': array([18818, 18774, 14598, 14634, 14679, 19418, 14643, 19476, 14497,
       14562, 20358, 20310, 18143, 18112, 23679, 23545, 21987, 21960,
       23627, 23761]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 29.130859375, 25.505859375]}, 'saliency': {'score': [1.9476589626736112, 0.08617986652301816, 1.7051233362268519, 0.08298573633014478, 0.21136441073574863], 'topk_indices': array([11247, 12007,  7464, 24199, 24208,  7442, 11964, 11311,  7516,
       11963, 22764, 19366, 16795, 24204, 24205, 16797,  7473, 24207,
        7517, 11313]), 'topk_tokens': ['�', 'ANT', 'graph', 'Question', '?', 'nes', 'IC', ' grabbed', ' two', 'ANT', ' kitchen', ' football', ' discarded', ' football', ' before', ' football', ' three', ' office', ' or', ' football'], 'evidence_proportions': [1.398974609375, 0.7966796875, 3.41845703125, 2.6014404296875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:47:30.369 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:47:30.370 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-24_pid-4_2-5-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (2, 5, 7, 8):   5%|▌         | 5/100 [02:26<46:14, 29.21s/it]Processing depth (2, 5, 7, 8):   5%|▌         | 5/100 [02:26<46:20, 29.27s/it]
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:82 - Selected idx: 25
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:86 - Needle: [' Mary went to the kitchen.', ' Daniel went back to the bathroom.', ' John grabbed the football.', ' Mary took the milk.', ' John went to the kitchen.', ' John went back to the bedroom.', ' Mary travelled to the office.', ' Daniel journeyed to the hallway.', ' Mary discarded the milk.', ' John left the football.']
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:87 - Real Needle: [' Mary went to the kitchen.', ' Mary took the milk.', ' Mary travelled to the office.', ' Mary discarded the milk.', ' John left the football.']
2025-01-22 04:47:30.696 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👞', '👩🏿\u200d🎤', '🤚', '👩🏻\u200d🦽\u200d➡', '💤', '🧔🏾\u200d♂', '⛹🏻\u200d♂', '🧙🏻\u200d♀️', '🧑🏿\u200d❤\u200d🧑🏻', '👨🏾\u200d❤\u200d💋\u200d👨🏻']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.89s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.53s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.21s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (1, 3, 4, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:47:47.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:47:47.387 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2966, 2971) -->  Mary went to the kitchen
2025-01-22 04:47:47.388 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary took the milk.
2025-01-22 04:47:47.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7458, 7462) -->  Mary took the milk
2025-01-22 04:47:47.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:47:47.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9708, 9713) --> . Mary travelled to the
2025-01-22 04:47:47.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the milk.
2025-01-22 04:47:47.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19331, 19335) -->  Mary discarded the milk
2025-01-22 04:47:47.578 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:47:47.691 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21452, 21456) -->  John left the football
2025-01-22 04:47:47.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:47:47.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7128, 7134) --> . Daniel went back to the
2025-01-22 04:47:47.737 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:47:47.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6226, 6230) -->  John grabbed the football
2025-01-22 04:47:47.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 04:47:47.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2967, 2972) -->  went to the kitchen.
2025-01-22 04:47:47.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:47:47.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9521, 9527) --> . John went back to the
2025-01-22 04:47:47.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:47:47.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8692, 8698) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:47:50.553 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 04:47:50.554 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24171])
2025-01-22 04:47:58.918 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [345.0568181818182, 13.59269773310168, 200.0601851851852, 13.081740932642488, 31.606525358606557], 'topk_indices': array([   24, 24170,  9711,    23, 24050, 24060, 24151, 19332, 24145,
       24144,    14,  9710, 24156, 24158,  7462, 24169,  7461, 24167,
           0, 24166]), 'topk_tokens': ['\n\n', '\n\n', ' to', '4', '.\n\n', ' location', ' to', ' discarded', ':', 'Question', '\n', ' travelled', ' milk', ' discarded', '.', '<|end_header_id|>', ' milk', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [249.125, 583.4375, 426.7, 416.8125, 52.78125]}, 'weight': {'score': [24.848011363636363, 23.434904235955987, 21.283854166666668, 23.43602299222798, 29.30840163934426], 'topk_indices': array([18742, 18786, 14602, 14638, 19444, 19386, 14683, 14647, 14501,
       14566, 20258, 20306, 18111, 18080, 23640, 23506, 21933, 21960,
       23722, 23588]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.1921875, 26.169921875, 20.75625, 28.802734375, 25.505859375]}, 'saliency': {'score': [2.3080333362926138, 0.07742915781159511, 1.1267722800925926, 0.07422063734617876, 0.23682165927574284], 'topk_indices': array([ 9709,  8694,  8698,  2957,  7459, 24168,  9338,     0, 19334,
       24016, 24164, 24060, 24150,  7134, 24156, 24144, 19332,  9710,
       24158,  7461]), 'topk_tokens': [' Mary', ' journey', ' hallway', '♀', ' took', 'assistant', ' cause', '<|begin_of_text|>', ' milk', ' context', 'Answer', ' location', ' prior', ' bathroom', ' milk', 'Question', ' discarded', ' travelled', ' discarded', ' milk'], 'evidence_proportions': [1.659375, 4.0224609375, 2.441015625, 3.1944580078125, 0.351776123046875]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 04:47:58.923 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:47:58.924 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-25_pid-0_1-3-4-8-9.pkl | len: 3 |  size: 2.12 KB
Processing depth (1, 3, 4, 8, 9):   1%|          | 1/100 [00:28<46:20, 28.09s/it]is_0k: False
your chose emoji: ['⛹', '🙍🏽\u200d♀', '👚', '💾', '👮', '🙇🏼\u200d♀️', '🤼\u200d♀', '👩🏿\u200d🚀', '🙎🏼\u200d♀', '👨🏼\u200d🎤']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.64s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (1, 3, 4, 5, 8):   1%|          | 1/100 [00:45<46:20, 28.09s/it]2025-01-22 04:48:16.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:48:16.708 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2905, 2910) --> . Mary went to the
2025-01-22 04:48:16.708 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary took the milk.
2025-01-22 04:48:16.744 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7538, 7542) -->  Mary took the milk
2025-01-22 04:48:16.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:48:16.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9693, 9698) -->  war. Mary travelled to
2025-01-22 04:48:16.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the milk.
2025-01-22 04:48:16.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11836, 11840) -->  discarded the milk.
2025-01-22 04:48:16.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:48:16.949 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19265, 19269) -->  left the football.
2025-01-22 04:48:16.949 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:48:16.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7175, 7181) -->  cold. Daniel went back to
2025-01-22 04:48:16.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:48:17.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6316, 6320) -->  John grabbed the football
2025-01-22 04:48:17.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 04:48:17.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2906, 2911) -->  Mary went to the kitchen
2025-01-22 04:48:17.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:48:17.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9517, 9523) --> . John went back to the
2025-01-22 04:48:17.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:48:17.134 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8798, 8804) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:48:19.970 | INFO     | test_jbb_embedding:begin_test:693 - The top of the car.<|eot_id|>
2025-01-22 04:48:19.970 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24167])
2025-01-22 04:48:28.368 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [271.6832386363636, 12.949674182871329, 145.27199074074073, 12.565575639484267, 50.75086069915254], 'topk_indices': array([24046,    23, 24159, 24139, 11839, 24141, 24152,    14, 11838,
       11836, 24010,  7541, 24154, 24011, 24165,  7542, 24012, 24162,
           0, 24163]), 'topk_tokens': ['.\n\n', '4', '?\n', '.\n\n', '.', ':', ' milk', '\n', ' milk', ' discarded', ' provided', ' milk', ' discarded', ' you', '<|end_header_id|>', '.', ' context', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [241.5875, 402.03125, 102.46875, 583.875, 78.28125]}, 'weight': {'score': [22.617897727272727, 23.436341539098056, 22.885127314814813, 23.437705021039758, 29.802436440677965], 'topk_indices': array([18817, 18773, 14685, 14649, 14730, 14694, 19475, 19417, 14613,
       14548, 20337, 20289, 18142, 18111, 23512, 23646, 21939, 21966,
       23728, 23594]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 26.169921875, 24.2203125, 23.01953125, 20.6796875]}, 'saliency': {'score': [1.6517555930397727, 0.07338440926251552, 0.9132758246527778, 0.07100468892510675, 0.3797131554555085], 'topk_indices': array([24017, 11819,  7182, 24150, 11997,  6319,  2878, 24056, 11817,
       11996, 24146, 24011, 24140, 24152, 24010, 11838,  7541, 11836,
       24154, 24012]), 'topk_tokens': [' locations', '�', ' bathroom', ' where', 'IC', ' football', ' During', ' location', '�', 'ANT', ' prior', ' you', 'Question', ' milk', ' provided', ' milk', ' milk', ' discarded', ' discarded', ' context'], 'evidence_proportions': [1.156982421875, 2.7388916015625, 0.6492431640625, 3.6800537109375, 0.407928466796875]}}, 'pred_res': 'The top of the car.<|eot_id|>', 'score': 0}
2025-01-22 04:48:28.376 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:48:28.376 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-25_pid-1_1-3-4-5-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 3, 4, 5, 8):   2%|▏         | 2/100 [00:57<47:11, 28.89s/it]is_0k: False
your chose emoji: ['\U0001facf', '🏃🏽\u200d♂️\u200d➡', '🌐', '🦯', '✋🏾', '🙆🏻\u200d♂', '↕', '🤹🏻\u200d♀️', '👩\u200d🦰', '✊🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.54s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.45s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (0, 1, 6, 7, 9):   2%|▏         | 2/100 [01:14<47:11, 28.89s/it]2025-01-22 04:48:45.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:48:45.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:48:45.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary took the milk.
2025-01-22 04:48:45.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2973, 2977) -->  took the milk.
2025-01-22 04:48:45.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:48:45.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14362, 14367) --> . Mary travelled to the
2025-01-22 04:48:45.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the milk.
2025-01-22 04:48:45.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16725, 16729) -->  Mary discarded the milk
2025-01-22 04:48:45.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:48:45.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21472, 21476) -->  John left the football
2025-01-22 04:48:45.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:48:45.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7131, 7137) --> . Daniel went back to the
2025-01-22 04:48:45.447 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:48:45.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6229, 6233) -->  John grabbed the football
2025-01-22 04:48:45.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 04:48:45.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:48:45.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:48:45.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9519, 9525) --> . John went back to the
2025-01-22 04:48:45.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:48:45.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8690, 8696) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:48:49.457 | INFO     | test_jbb_embedding:begin_test:693 - Mary took the milk. The Pioneer Guards was the crack military company of the state, and the only service any of its members ever expected to do was in the
2025-01-22 04:48:49.457 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24191])
2025-01-22 04:48:57.885 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [812.5028409090909, 33.53524117549806, 420.9571759259259, 32.39224347691033, 47.919619278169016], 'topk_indices': array([   24,  7546, 24177, 24174,    29,    31, 24185,  2091,    14,
       24164,    34,  2976, 24186, 24165,    35, 24178, 16726, 24187,
        7547,     0]), 'topk_tokens': ['\n\n', ' Min', ' was', ' where', '\n\n', ' went', ':', ' the', '\n', 'Question', ' kitchen', '.', '<|eot_id|>', ':', '.', ' discarded', ' discarded', '<|start_header_id|>', 'nes', '<|begin_of_text|>'], 'evidence_proportions': [1404.9, 1108.375, 345.175, 1112.625, 60.171875]}, 'weight': {'score': [22.74502840909091, 23.441028767463006, 21.283854166666668, 23.44407518378546, 29.518705985915492], 'topk_indices': array([18837, 18793, 14682, 14646, 14727, 19432, 19490, 14691, 14610,
       14545, 20352, 20304, 18150, 18119, 23660, 23526, 21959, 21986,
       23608, 23742]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.565625, 20.38671875, 20.75625, 28.802734375, 25.505859375]}, 'saliency': {'score': [4.680109197443182, 0.1927473382476027, 2.271140769675926, 0.1863344749818803, 0.3707937321192782], 'topk_indices': array([24177,     0,  7457,    39,    30, 20606,  7502,  2975, 24170,
       24184, 24176, 24174,  8244,    31,  7546,    34, 24164,  7547,
       24178, 16726]), 'topk_tokens': [' was', '<|begin_of_text|>', 'nes', '\n\n\n', 'Mary', 'Mr', 'nes', ' milk', ' prior', 'Answer', ' milk', ' where', ' Minneapolis', ' went', ' Min', ' kitchen', 'Question', 'nes', ' discarded', ' discarded'], 'evidence_proportions': [7.196875, 5.310546875, 1.947412109375, 8.63525390625, 0.36444091796875]}}, 'pred_res': 'Mary took the milk. The Pioneer Guards was the crack military company of the state, and the only service any of its members ever expected to do was in the', 'score': 0}
2025-01-22 04:48:57.892 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:48:57.892 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-25_pid-2_0-1-6-7-9.pkl | len: 3 |  size: 2.2 KB
Processing depth (0, 1, 6, 7, 9):   3%|▎         | 3/100 [01:27<47:10, 29.18s/it]is_0k: False
your chose emoji: ['🗒️', '🖕🏽', '🚴🏿', '👶🏾', '🔺', '👨🏾\u200d❤️\u200d💋\u200d👨🏿', '🚴🏼\u200d♀', '🕖', '🕵\u200d♀️', '👳🏼\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.12s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (2, 6, 7, 8, 9):   3%|▎         | 3/100 [01:44<47:10, 29.18s/it]2025-01-22 04:49:15.644 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:49:15.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4885, 4890) --> . Mary went to the
2025-01-22 04:49:15.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary took the milk.
2025-01-22 04:49:15.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14304, 14308) -->  Mary took the milk
2025-01-22 04:49:15.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:49:15.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16686, 16691) --> . Mary travelled to the
2025-01-22 04:49:15.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the milk.
2025-01-22 04:49:15.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19212, 19216) -->  discarded the milk.
2025-01-22 04:49:15.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:49:16.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21464, 21468) -->  John left the football
2025-01-22 04:49:16.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:49:16.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7032, 7038) --> . Daniel went back to the
2025-01-22 04:49:16.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:49:16.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6338, 6342) -->  John grabbed the football
2025-01-22 04:49:16.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 04:49:16.103 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4886, 4891) -->  Mary went to the kitchen
2025-01-22 04:49:16.103 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:49:16.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9432, 9438) --> . John went back to the
2025-01-22 04:49:16.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:49:16.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8601, 8607) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:49:19.051 | INFO     | test_jbb_embedding:begin_test:693 - Mary travelled to the office.<|eot_id|>
2025-01-22 04:49:19.051 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 04:49:27.493 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [335.33309659090907, 11.809397199735669, 128.45138888888889, 11.384497346149072, 13.0172607421875], 'topk_indices': array([24098, 14305, 19215, 24192, 24188, 24196, 19214,    24, 14303,
       19212,    23, 24189, 24195, 24194, 24207, 14307, 14308, 24204,
           0, 24205]), 'topk_tokens': [' location', ' took', '.', ' where', ' prior', ' discarded', ' milk', '\n\n', '.', ' discarded', '4', ' to', ' was', ' milk', '<|end_header_id|>', ' milk', '.', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [183.025, 638.6875, 294.425, 575.0, 33.83203125]}, 'weight': {'score': [22.70809659090909, 23.447817714356518, 22.140625, 23.449951889252162, 29.867578125], 'topk_indices': array([18824, 18780, 14686, 14650, 19424, 19482, 14695, 14731, 14614,
       14549, 20296, 20344, 18149, 18118, 23680, 23546, 21972, 21945,
       23762, 23628]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 26.169921875, 20.75625, 23.01953125, 25.505859375]}, 'saliency': {'score': [2.1089422052556817, 0.06666370078628366, 0.7926341869212963, 0.06399303364131109, 0.09833755493164062], 'topk_indices': array([14302,    24, 16688, 16687, 24054,    23, 14304, 24191,     0,
       24195, 24182, 24192, 14305, 24098, 19214, 24188, 24196, 19212,
       24194, 14307]), 'topk_tokens': [' Gov', '\n\n', ' travelled', ' Mary', ' context', '4', ' Mary', ' place', '<|begin_of_text|>', ' was', 'Question', ' where', ' took', ' location', ' milk', ' prior', ' discarded', ' discarded', ' milk', ' milk'], 'evidence_proportions': [1.005078125, 4.30517578125, 1.75849609375, 3.619873046875, 0.21966552734375]}}, 'pred_res': 'Mary travelled to the office.<|eot_id|>', 'score': 0}
2025-01-22 04:49:27.500 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:49:27.501 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-25_pid-3_2-6-7-8-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 6, 7, 8, 9):   4%|▍         | 4/100 [01:56<46:57, 29.35s/it]is_0k: False
your chose emoji: ['🤷\u200d♂️', '🇾🇪', '🌩', '🔮', '🧏🏾\u200d♂️', '🖍️', '👩🏼\u200d❤️\u200d💋\u200d👩🏾', '👨🏽\u200d🤝\u200d👨🏼', '🌨', '🙋🏻\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.44s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.62s/it]
Processing depth (0, 1, 2, 6, 8):   4%|▍         | 4/100 [02:17<46:57, 29.35s/it]2025-01-22 04:49:48.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:49:48.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:49:48.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary took the milk.
2025-01-22 04:49:48.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3038, 3042) -->  Mary took the milk
2025-01-22 04:49:48.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:49:48.235 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4956, 4961) --> . Mary travelled to the
2025-01-22 04:49:48.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the milk.
2025-01-22 04:49:48.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14369, 14373) -->  Mary discarded the milk
2025-01-22 04:49:48.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:49:48.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19237, 19241) -->  left the football.
2025-01-22 04:49:48.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:49:48.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7142, 7148) -->  Daniel went back to the bathroom
2025-01-22 04:49:48.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John grabbed the football.
2025-01-22 04:49:48.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6320, 6324) -->  grabbed the football.
2025-01-22 04:49:48.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 04:49:48.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:49:48.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the bedroom.
2025-01-22 04:49:48.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9453, 9459) --> . John went back to the
2025-01-22 04:49:48.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:49:48.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8652, 8658) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:49:51.301 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:49:51.301 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 04:49:59.701 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [644.5852272727273, 24.978873203073363, 293.2453703703704, 24.11482522041475, 24.031224959935898], 'topk_indices': array([24177,    29,    14, 24203,    39,  3041, 24190, 14372,    24,
        1214,  2091,    35, 24178, 24179,  3042, 24200, 24192, 14370,
       24201,     0]), 'topk_tokens': ['.\n\n', '\n\n', '\n', '<|end_header_id|>', '\n\n\n', ' milk', ' milk', ' milk', '\n\n', 'nes', ' the', '.', 'Question', ':', '.', '<|eot_id|>', ' discarded', ' discarded', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [881.4, 825.375, 259.475, 1220.875, 72.875]}, 'weight': {'score': [22.91903409090909, 23.444491490416393, 21.622395833333332, 23.447006356326835, 29.510416666666668], 'topk_indices': array([18805, 18849, 14647, 14611, 14656, 19449, 14692, 19507, 14510,
       14575, 20369, 20321, 18174, 18143, 23544, 23678, 21977, 22004,
       23626, 23760]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.565625, 26.169921875, 20.75625, 28.802734375, 20.6796875]}, 'saliency': {'score': [4.140286532315341, 0.14166987911408419, 1.6283365885416667, 0.13636711130395246, 0.18286553407326722], 'topk_indices': array([24187,    24,  2044,  1154, 24177,    31, 24050,     0,    38,
       24188,    34,    39, 24184,  1214,  3041, 14372, 24190, 24178,
       24192, 14370]), 'topk_tokens': [' place', '\n\n', ' river', 'nes', '.\n\n', ' went', ' context', '<|begin_of_text|>', '***', ' where', ' kitchen', '\n\n\n', ' prior', 'nes', ' milk', ' milk', ' milk', 'Question', ' discarded', ' discarded'], 'evidence_proportions': [4.353515625, 5.659912109375, 1.420703125, 9.486328125, 0.407562255859375]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 04:49:59.709 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:49:59.709 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-25_pid-4_0-1-2-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 2, 6, 8):   5%|▌         | 5/100 [02:28<48:05, 30.38s/it]Processing depth (0, 1, 2, 6, 8):   5%|▌         | 5/100 [02:29<47:16, 29.86s/it]
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:82 - Selected idx: 26
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the office? 
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:86 - Needle: [' Daniel journeyed to the hallway.', ' Mary went to the kitchen.', ' John went to the kitchen.', ' John grabbed the football.', ' John left the football.', ' Mary travelled to the office.', ' Daniel went back to the bathroom.', ' John went back to the bedroom.', ' Mary discarded the football.', ' John got the football there.']
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:87 - Real Needle: [' Mary went to the kitchen.', ' Mary travelled to the office.', ' Mary discarded the football.', ' John got the football there.']
2025-01-22 04:50:00.121 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🔛', '🦵🏽', '🤽🏼\u200d♀️', '👩🏾\u200d💻', '📇', '👸🏻', '\U0001fab8', '🧑🏼\u200d🦯\u200d➡️', '👩🏽\u200d❤\u200d💋\u200d👨🏿', '🚵🏿\u200d♀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.92s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (3, 4, 8, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 04:50:17.516 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:50:17.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7358, 7363) --> . Mary went to the
2025-01-22 04:50:17.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:50:17.605 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9656, 9661) --> . Mary travelled to the
2025-01-22 04:50:17.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:50:17.703 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19194, 19198) -->  discarded the football.
2025-01-22 04:50:17.704 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John got the football there.
2025-01-22 04:50:17.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21445, 21450) --> . John got the football
2025-01-22 04:50:17.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the hallway.
2025-01-22 04:50:17.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7896, 7902) --> . Daniel journeyed to the
2025-01-22 04:50:17.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:50:17.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7359, 7364) -->  Mary went to the kitchen
2025-01-22 04:50:17.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the football.
2025-01-22 04:50:17.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11145, 11149) -->  John grabbed the football
2025-01-22 04:50:17.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:50:17.955 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (1502, 1506) -->  John left the football
2025-01-22 04:50:17.955 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:50:17.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8145, 8151) --> . Daniel went back to the
2025-01-22 04:50:17.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the bedroom.
2025-01-22 04:50:18.065 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (14233, 14239) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:50:20.724 | INFO     | test_jbb_embedding:begin_test:693 - Bridge Square<|eot_id|>
2025-01-22 04:50:20.725 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24151])
2025-01-22 04:50:29.103 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [242.73026315789474, 13.806046617537469, 291.5, 13.26845648025224, 26.562926136363636], 'topk_indices': array([ 7803,  1505, 24144, 24145,  1506, 24134, 24039, 24049,    23,
           9, 24142, 11148, 24138, 24149,    14, 24139, 24141, 24147,
       24146,     0]), 'topk_tokens': [' Square', ' football', 'Answer', ':', '.', ':', '.\n\n', ' location', '4', ':', '?', ' football', ' football', '<|end_header_id|>', '\n', ' before', ' office', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [357.6, 56.8, 402.625, 185.875]}, 'weight': {'score': [21.392269736842106, 23.433181357125115, 22.57484879032258, 23.435894003588615, 29.482386363636362], 'topk_indices': array([18762, 18806, 14638, 14674, 19464, 14719, 19406, 14683, 14602,
       14537, 20278, 20326, 18100, 18131, 23495, 23629, 21934, 21961,
       23711, 23577]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 23.34765625, 22.453125]}, 'saliency': {'score': [1.475598787006579, 0.0780192232491254, 1.8159888482862903, 0.07468238828034247, 0.1969476873224432], 'topk_indices': array([21452, 24005, 11146, 24142,  7363,  8151, 19196, 24135,  7360,
        7803,  7902, 24133, 24049,  1505, 19194, 24144, 24139, 11148,
       24138, 24141]), 'topk_tokens': ['�', ' context', ' grabbed', '?', ' kitchen', ' bathroom', ' football', ' Where', ' went', ' Square', ' hallway', 'Question', ' location', ' football', ' discarded', 'Answer', ' before', ' football', ' football', ' office'], 'evidence_proportions': [2.002734375, 0.340673828125, 2.6710205078125, 1.12705078125]}}, 'pred_res': 'Bridge Square<|eot_id|>', 'score': 0}
2025-01-22 04:50:29.111 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:50:29.111 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-26_pid-0_3-4-8-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (3, 4, 8, 9):   1%|          | 1/100 [00:28<47:32, 28.82s/it]is_0k: False
your chose emoji: ['⚽', '🙍🏾\u200d♂️', '🔯', '💩', '🕵🏿', '🧛🏿', '✌🏻', '🤞🏽', '🗃️', '🧎\u200d♂️\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.45s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.53s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (0, 1, 6, 8):   1%|          | 1/100 [00:45<47:32, 28.82s/it]2025-01-22 04:50:46.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:50:46.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:50:46.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:50:46.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3042, 3047) --> . Mary travelled to the
2025-01-22 04:50:46.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:50:46.171 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14378, 14382) -->  Mary discarded the football
2025-01-22 04:50:46.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John got the football there.
2025-01-22 04:50:46.274 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19266, 19271) -->  John got the football there
2025-01-22 04:50:46.274 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the hallway.
2025-01-22 04:50:46.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8181, 8187) -->  St. Daniel journeyed to
2025-01-22 04:50:46.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:50:46.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:50:46.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the football.
2025-01-22 04:50:46.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11247, 11251) -->  John grabbed the football
2025-01-22 04:50:46.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:50:46.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (1508, 1512) -->  John left the football
2025-01-22 04:50:46.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:50:46.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8365, 8371) --> . Daniel went back to the
2025-01-22 04:50:46.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the bedroom.
2025-01-22 04:50:46.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (14381, 14387) -->  football. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:50:49.170 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:50:49.170 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 04:50:57.538 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [467.17763157894734, 25.594899371848914, 396.8508064516129, 24.77085576445254, 22.778738839285715], 'topk_indices': array([ 1630,  7469, 24193,  7431,  7439,  7438, 24190,    34,    14,
        7483, 24186,  1709,  1631, 24183,  1632, 24185,  7440,  1710,
        7484,     0]), 'topk_tokens': [' summer', ' or', '<|end_header_id|>', 'graph', ' or', ' two', '<|eot_id|>', ' kitchen', '\n', ' two', '?', ' summer', "'s", ' before', ' work', ' office', ' three', "'s", ' or', '<|begin_of_text|>'], 'evidence_proportions': [859.7, 287.425, 393.9375, 313.0]}, 'weight': {'score': [23.45394736842105, 23.444654516902222, 23.13155241935484, 23.445049150033128, 29.551136363636363], 'topk_indices': array([18757, 18801, 14663, 14627, 14672, 19478, 19420, 14708, 14591,
       14526, 20292, 20340, 18126, 18095, 23523, 23657, 21942, 21969,
       23739, 23605]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.565625, 20.75625, 29.130859375, 25.4984375]}, 'saliency': {'score': [2.8035310444078947, 0.1467102338325998, 2.418850806451613, 0.14170294738435896, 0.17221406218293425], 'topk_indices': array([ 7439,  7470,  7430, 24186,  1631,  7475,  1630, 14581, 24182,
        7438,  7483,  7431,    34, 24183,  1709,  1632, 24185,  1710,
        7484,  7440]), 'topk_tokens': [' or', ' five', ' tele', '?', "'s", ' tele', ' summer', ' accordance', ' football', ' two', ' two', 'graph', ' kitchen', ' before', ' summer', ' work', ' office', "'s", ' or', ' three'], 'evidence_proportions': [4.646875, 1.49833984375, 3.05078125, 2.067578125]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 04:50:57.544 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:50:57.544 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-26_pid-1_0-1-6-8.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 1, 6, 8):   2%|▏         | 2/100 [00:57<46:41, 28.59s/it]is_0k: False
your chose emoji: ['👩🏼\u200d❤\u200d👩🏻', '🥔', '🤫', '🙍🏽\u200d♂️', '🇨🇼', '👳🏼\u200d♂', '♥', '⚗', '👩🏿\u200d🦰', '👨🏻\u200d❤\u200d💋\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.80s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (3, 4, 7, 8):   2%|▏         | 2/100 [01:14<46:41, 28.59s/it]2025-01-22 04:51:15.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:51:15.217 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7358, 7363) --> . Mary went to the
2025-01-22 04:51:15.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:51:15.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9681, 9686) -->  war. Mary travelled to
2025-01-22 04:51:15.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:51:15.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16693, 16697) -->  Mary discarded the football
2025-01-22 04:51:15.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John got the football there.
2025-01-22 04:51:15.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19129, 19134) --> . John got the football
2025-01-22 04:51:15.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the hallway.
2025-01-22 04:51:15.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7896, 7902) --> . Daniel journeyed to the
2025-01-22 04:51:15.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:51:15.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7359, 7364) -->  Mary went to the kitchen
2025-01-22 04:51:15.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the football.
2025-01-22 04:51:15.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11159, 11163) -->  John grabbed the football
2025-01-22 04:51:15.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:51:15.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (1288, 1292) -->  John left the football
2025-01-22 04:51:15.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:51:15.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8145, 8151) --> . Daniel went back to the
2025-01-22 04:51:15.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the bedroom.
2025-01-22 04:51:15.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (14275, 14281) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:51:18.411 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom<|eot_id|>
2025-01-22 04:51:18.411 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24133])
2025-01-22 04:51:26.751 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [215.8844572368421, 14.168008887139543, 237.84879032258064, 13.720997480382795, 19.714164402173914], 'topk_indices': array([24031, 24061, 24115, 24126, 24114, 24021,    14,  1292,    23,
       24116, 11162, 24131, 24127, 24120, 24124, 24121, 24123, 24129,
           0, 24128]), 'topk_tokens': [' location', ' the', 'Question', 'Answer', '.\n\n', '.\n\n', '\n', '.', '4', ':', ' football', '<|end_header_id|>', ':', ' football', '?', ' before', ' office', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [224.925, 64.7109375, 280.875, 306.025]}, 'weight': {'score': [23.52138157894737, 23.428229615512098, 22.57484879032258, 23.429254481337708, 29.366847826086957], 'topk_indices': array([18815, 18771, 14620, 14656, 19474, 14665, 19416, 14701, 14584,
       14519, 20336, 20288, 18109, 18140, 23493, 23627, 21959, 21932,
       23575, 23709]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 24.2203125, 29.130859375, 22.453125]}, 'saliency': {'score': [1.3566958778782894, 0.07988493972793494, 1.5076392389112903, 0.07704014228960807, 0.14772829802139945], 'topk_indices': array([   20, 24114, 24021,    23,  7902, 24117, 11160,  1291, 16696,
        8569, 19133, 23987, 24031, 24124, 24115, 24126, 11162, 24121,
       24120, 24123]), 'topk_tokens': [' Jul', '.\n\n', '.\n\n', '4', ' hallway', ' Where', ' grabbed', ' football', ' football', ' kitchen', ' football', ' context', ' location', '?', 'Question', 'Answer', ' football', ' before', ' football', ' office'], 'evidence_proportions': [1.187109375, 0.3831298828125, 2.14453125, 1.869580078125]}}, 'pred_res': 'The bedroom<|eot_id|>', 'score': 0}
2025-01-22 04:51:26.759 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:51:26.759 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-26_pid-2_3-4-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (3, 4, 7, 8):   3%|▎         | 3/100 [01:26<46:40, 28.88s/it]is_0k: False
your chose emoji: ['🔵', '🤯', '🧱', '♣️', '🇹🇭', '🧚\u200d♀', '🦵🏽', '⛹️\u200d♂', '🧗🏾\u200d♀', '💿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.48s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.77s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (2, 4, 7, 8):   3%|▎         | 3/100 [01:43<46:40, 28.88s/it]2025-01-22 04:51:44.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:51:44.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4916, 4921) --> . Mary went to the
2025-01-22 04:51:44.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:51:44.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9734, 9739) --> . Mary travelled to the
2025-01-22 04:51:44.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:51:44.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16825, 16829) -->  Mary discarded the football
2025-01-22 04:51:44.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John got the football there.
2025-01-22 04:51:44.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19274, 19279) -->  John got the football there
2025-01-22 04:51:44.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the hallway.
2025-01-22 04:51:44.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8116, 8122) --> . Daniel journeyed to the
2025-01-22 04:51:44.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:51:44.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4917, 4922) -->  Mary went to the kitchen
2025-01-22 04:51:44.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the football.
2025-01-22 04:51:44.473 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11225, 11229) -->  John grabbed the football
2025-01-22 04:51:44.473 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:51:44.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (1502, 1506) -->  John left the football
2025-01-22 04:51:44.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:51:44.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8327, 8333) --> . Daniel went back to the
2025-01-22 04:51:44.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the bedroom.
2025-01-22 04:51:44.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (14411, 14417) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:51:47.316 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom.<|eot_id|>
2025-01-22 04:51:47.316 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24237])
2025-01-22 04:51:55.729 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [298.7845394736842, 21.436453176567657, 343.80040322580646, 20.805494264158742, 28.097257653061224], 'topk_indices': array([23815, 24228, 24224, 23821, 24225, 23684, 24227,  1505, 24233,
       23685, 23822, 11228, 23818, 24232, 23689, 23823, 23690,     0,
       23824, 23691]), 'topk_tokens': ['\n', '?', ' football', ' returned', ' before', ' disposed', ' office', ' football', '<|start_header_id|>', ' of', ' with', ' football', ' disposed', '<|eot_id|>', ' arms', ' arms', ' and', '<|begin_of_text|>', ' and', ' ammunition'], 'evidence_proportions': [251.1, 122.30625, 381.21875, 457.0]}, 'weight': {'score': [23.411184210526315, 23.457018358085808, 22.57484879032258, 23.458184877532037, 29.776147959183675], 'topk_indices': array([18887, 18843, 14692, 14656, 14701, 14737, 19546, 19488, 14555,
       14620, 20360, 20408, 18184, 18153, 23583, 23717, 22049, 22022,
       23799, 23665]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 29.130859375, 25.4984375]}, 'saliency': {'score': [1.9445286800986843, 0.12312015747473185, 2.231500441028226, 0.11898760060329681, 0.21365496577048787], 'topk_indices': array([24230, 19277, 24228, 11226, 24219,  8701, 23822, 24225, 23821,
       24224, 24227, 23690,  1505, 23684, 23824, 11228, 23818, 23823,
       23689, 23691]), 'topk_tokens': ['Answer', ' football', '?', ' grabbed', 'Question', ' kitchen', ' with', ' before', ' returned', ' football', ' office', ' and', ' football', ' disposed', ' and', ' football', ' disposed', ' arms', ' arms', ' ammunition'], 'evidence_proportions': [1.392626953125, 0.71865234375, 2.89111328125, 2.9650390625]}}, 'pred_res': 'The bedroom.<|eot_id|>', 'score': 0}
2025-01-22 04:51:55.760 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:51:55.761 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-26_pid-3_2-4-7-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 4, 7, 8):   4%|▍         | 4/100 [01:55<46:16, 28.93s/it]is_0k: False
your chose emoji: ['🏄🏼\u200d♂', '🧎🏽\u200d➡', '🚶\u200d♂️', '🤦🏼\u200d♂️', '👩🏿\u200d🦼\u200d➡️', '🧎🏿\u200d➡️', '👶🏾', '👩🏿\u200d❤\u200d👨🏼', '🧗🏿\u200d♂', '🤸🏻\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (3, 4, 5, 8):   4%|▍         | 4/100 [02:12<46:16, 28.93s/it]2025-01-22 04:52:12.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:52:12.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7654, 7659) --> . Mary went to the
2025-01-22 04:52:12.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 04:52:12.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9896, 9901) --> . Mary travelled to the
2025-01-22 04:52:12.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 04:52:13.041 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12041, 12045) -->  discarded the football.
2025-01-22 04:52:13.041 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John got the football there.
2025-01-22 04:52:13.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19473, 19478) --> . John got the football
2025-01-22 04:52:13.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel journeyed to the hallway.
2025-01-22 04:52:13.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8200, 8206) --> . Daniel journeyed to the
2025-01-22 04:52:13.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the kitchen.
2025-01-22 04:52:13.229 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7655, 7660) -->  Mary went to the kitchen
2025-01-22 04:52:13.229 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John grabbed the football.
2025-01-22 04:52:13.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11343, 11347) -->  John grabbed the football
2025-01-22 04:52:13.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the football.
2025-01-22 04:52:13.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (1502, 1506) -->  John left the football
2025-01-22 04:52:13.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the bathroom.
2025-01-22 04:52:13.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (8375, 8381) --> . Daniel went back to the
2025-01-22 04:52:13.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John went back to the bedroom.
2025-01-22 04:52:13.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (14706, 14712) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:52:16.116 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:52:16.117 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24177])
2025-01-22 04:52:24.509 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [351.7368421052632, 19.357943031430935, 318.14516129032256, 18.712373083298797, 21.402113970588236], 'topk_indices': array([ 8381, 19477, 12043, 24175,  1506, 19478, 18907, 24168,    14,
       24104, 11346, 24065, 24164, 24105, 24075, 24165, 24173, 24167,
       24172,     0]), 'topk_tokens': [' bathroom', ' football', ' football', '<|end_header_id|>', '.', ' there', ' in', '?', '\n', ' to', ' football', '.\n\n', ' football', ' the', ' location', ' before', '<|start_header_id|>', ' office', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [339.2, 148.35, 545.8125, 412.4]}, 'weight': {'score': [21.392269736842106, 23.439112903225805, 22.57484879032258, 23.441834917633653, 29.380284926470587], 'topk_indices': array([18763, 18807, 14594, 14630, 19466, 14675, 14639, 19408, 14558,
       14493, 20334, 20286, 18132, 18101, 23665, 23531, 21979, 21952,
       23747, 23613]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 20.75625, 23.34765625, 22.453125]}, 'saliency': {'score': [2.1324398643092106, 0.10988669572929591, 1.9964402721774193, 0.10587046402300042, 0.16196172377642462], 'topk_indices': array([24170, 24071, 24031, 24065, 19478,  8855, 11344, 24159, 18906,
        1505, 19477, 12043, 12041,  8381,  8206, 24165, 24075, 11346,
       24164, 24167]), 'topk_tokens': ['Answer', ' item', ' context', '.\n\n', ' there', ' kitchen', ' grabbed', 'Question', ' manner', ' football', ' football', ' football', ' discarded', ' bathroom', ' hallway', ' before', ' location', ' football', ' football', ' office'], 'evidence_proportions': [1.7484375, 0.910302734375, 3.704345703125, 2.4810546875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:52:24.521 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:52:24.521 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-26_pid-4_3-4-5-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (3, 4, 5, 8):   5%|▌         | 5/100 [02:24<45:42, 28.87s/it]Processing depth (3, 4, 5, 8):   5%|▌         | 5/100 [02:24<45:46, 28.91s/it]
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:82 - Selected idx: 27
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:86 - Needle: [' John went back to the bedroom.', ' Daniel journeyed to the hallway.', ' Mary went to the kitchen.', ' Daniel went back to the bathroom.', ' John went to the kitchen.', ' Mary picked up the apple.', ' John left the football.', ' Mary travelled to the office.', ' John grabbed the football.', ' Mary discarded the apple.', ' John got the football there.']
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:87 - Real Needle: [' Mary went to the kitchen.', ' Mary picked up the apple.', ' Mary travelled to the office.', ' Mary discarded the apple.', ' John got the football there.']
2025-01-22 04:52:24.828 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🇮🇲', '👬🏽', '🍳', '🏴\U000e0067\U000e0062\U000e0077\U000e006c\U000e0073\U000e007f', '🧗🏼\u200d♀️', '\U0001faf6', '🐕\u200d🦺', '👨\u200d⚖️', '⛺', '🌀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.54s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (1, 5, 7, 8, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 04:52:42.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:52:42.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2966, 2971) -->  tragedy. Mary went to
2025-01-22 04:52:42.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 04:52:42.252 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11927, 11932) --> . Mary picked up the
2025-01-22 04:52:42.252 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:52:42.341 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16745, 16750) --> . Mary travelled to the
2025-01-22 04:52:42.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 04:52:42.441 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19256, 19260) -->  discarded the apple.
2025-01-22 04:52:42.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John got the football there.
2025-01-22 04:52:42.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21485, 21490) --> . John got the football
2025-01-22 04:52:42.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:52:42.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18144, 18150) --> . John went back to the
2025-01-22 04:52:42.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:52:42.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10376, 10382) --> . Daniel journeyed to the
2025-01-22 04:52:42.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:52:42.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17403, 17409) --> . Daniel went back to the
2025-01-22 04:52:42.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 04:52:42.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2968, 2973) -->  Mary went to the kitchen
2025-01-22 04:52:42.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:52:42.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21852, 21856) -->  John left the football
2025-01-22 04:52:42.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John grabbed the football.
2025-01-22 04:52:42.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6292, 6296) -->  John grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:52:45.747 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the apple.<|eot_id|>
2025-01-22 04:52:45.747 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24233])
2025-01-22 04:52:54.173 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [476.8033854166667, 26.92007241293943, 205.36491935483872, 26.2447905897192, 37.16002457865169], 'topk_indices': array([11933, 24121, 19256, 18966, 24206, 24218, 24216, 24212, 19138,
       24078, 18967, 24112, 11932, 24213, 24207, 24220, 24228, 24122,
           0, 24229]), 'topk_tokens': ['.', ' first', ' discarded', ' manner', 'Question', ' apple', ' where', ' prior', '♀♀', ' context', ' in', '.\n\n', ' apple', ' to', ':', ' discarded', '<|eot_id|>', ' location', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [327.0875, 724.2, 277.75, 1109.5, 72.01875]}, 'weight': {'score': [22.706705729166668, 23.447526406997856, 22.57484879032258, 23.449380453661966, 29.057408707865168], 'topk_indices': array([18867, 18823, 14625, 14661, 19526, 14670, 19468, 14706, 14524,
       14589, 20408, 20360, 18161, 18192, 23710, 23576, 22042, 22015,
       23792, 23658]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.2109375, 22.1, 20.75625, 23.08984375, 22.453125]}, 'saliency': {'score': [2.8111572265625, 0.1523303825049771, 1.2603090347782258, 0.14827103084549853, 0.23052052701457162], 'topk_indices': array([24118, 24083, 24209, 19258, 24208, 24215, 24112, 24193, 24226,
       24121, 24216, 18966, 24218, 24078, 24206, 24212, 19256, 11932,
       24122, 24220]), 'topk_tokens': [' item', ' locations', ' was', ' apple', ' Where', ' place', '.\n\n', ' return', 'Answer', ' first', ' where', ' manner', ' apple', ' context', 'Question', ' prior', ' discarded', ' apple', ' location', ' discarded'], 'evidence_proportions': [1.92861328125, 3.940625, 1.70029296875, 6.82177734375, 0.4666015625]}}, 'pred_res': 'Mary discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 04:52:54.177 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:52:54.178 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-27_pid-0_1-5-7-8-9.pkl | len: 3 |  size: 2.12 KB
Processing depth (1, 5, 7, 8, 9):   1%|          | 1/100 [00:29<48:13, 29.23s/it]is_0k: False
your chose emoji: ['👩🏼\u200d🦽', '🇮🇷', '🤷\u200d♀️', '💁🏾\u200d♀', '🧑🏿\u200d🤝\u200d🧑🏽', '👩🏻\u200d🤝\u200d👩🏽', '♀', '👩🏽\u200d💻', '🤾🏾\u200d♂️', '🤾🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.95s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.91s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.12s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (2, 3, 6, 7, 8):   1%|          | 1/100 [00:46<48:13, 29.23s/it]2025-01-22 04:53:11.243 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:53:11.267 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4955, 4960) --> . Mary went to the
2025-01-22 04:53:11.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 04:53:11.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7601, 7606) -->  war. Mary picked up
2025-01-22 04:53:11.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:53:11.393 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14276, 14281) -->  Mary travelled to the office
2025-01-22 04:53:11.393 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 04:53:11.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16692, 16696) -->  Mary discarded the apple
2025-01-22 04:53:11.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John got the football there.
2025-01-22 04:53:11.564 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19227, 19232) -->  John got the football there
2025-01-22 04:53:11.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:53:11.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18083, 18089) --> . John went back to the
2025-01-22 04:53:11.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:53:11.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10404, 10410) --> . Daniel journeyed to the
2025-01-22 04:53:11.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:53:11.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17317, 17323) -->  business. Daniel went back to
2025-01-22 04:53:11.851 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 04:53:11.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4956, 4961) -->  Mary went to the kitchen
2025-01-22 04:53:11.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:53:12.014 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21825, 21829) -->  John left the football
2025-01-22 04:53:12.014 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John grabbed the football.
2025-01-22 04:53:12.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6212, 6216) -->  John grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:53:14.713 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:53:14.713 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24210])
2025-01-22 04:53:23.062 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [153.72005208333334, 8.382507588898525, 72.7555443548387, 8.155515900529846, 31.037718141233768], 'topk_indices': array([ 7608, 16751,     9, 16801, 24204, 24089,    14, 24209,  7607,
       16802, 24202, 24197, 24183,    23, 24182, 24184, 24208,     0,
       24205, 24206]), 'topk_tokens': ['.', 'um', ':', ',\n', ':', '.\n\n', '\n', '\n\n', ' apple', 'present', '?\n', ' discarded', 'Question', '4', '.\n\n', ':', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [152.1625, 189.125, 100.05625, 320.53125, 40.0875]}, 'weight': {'score': [24.652994791666668, 23.44899434188246, 23.144153225806452, 23.4481893937826, 30.286931818181817], 'topk_indices': array([18840, 18796, 14655, 14619, 14700, 14664, 19441, 19499, 14583,
       14518, 20313, 20361, 18134, 18165, 23549, 23683, 22015, 21988,
       23765, 23631]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 25.5640625, 24.7703125, 28.873046875, 25.4984375]}, 'saliency': {'score': [1.0213114420572917, 0.04708365841051295, 0.47866919732862906, 0.045561983664508136, 0.23367012321174918], 'topk_indices': array([16789, 24202, 24193, 24170, 24195, 24207,    20, 16751, 16695,
       16750, 24055,    23, 24203, 24182, 16729, 16693,  7607, 16802,
       24197, 24183]), 'topk_tokens': ['dr', '?\n', ' where', ' return', ' apple', 'assistant', ' Jul', 'um', ' apple', 'dr', ' context', '4', 'Answer', '.\n\n', ' kitchen', ' discarded', ' apple', 'present', ' discarded', 'Question'], 'evidence_proportions': [0.8009521484375, 1.149560546875, 0.7325439453125, 2.4468994140625, 0.26171875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 04:53:23.069 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:53:23.069 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-27_pid-1_2-3-6-7-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 3, 6, 7, 8):   2%|▏         | 2/100 [00:58<47:24, 29.03s/it]is_0k: False
your chose emoji: ['🙎🏼', '🏊🏼\u200d♂️', '👨🏻\u200d🦽\u200d➡️', '🦹🏾\u200d♀️', '🙇🏻\u200d♀', '🚶🏾\u200d♀️\u200d➡️', '💑🏾', '😄', '❌', '🙋🏻\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.73s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.72s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.85s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.40s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (1, 2, 4, 6, 8):   2%|▏         | 2/100 [01:15<47:24, 29.03s/it]2025-01-22 04:53:40.803 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:53:40.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary went to the
2025-01-22 04:53:40.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 04:53:40.857 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4993, 4998) --> . Mary picked up the
2025-01-22 04:53:40.857 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:53:40.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9732, 9737) --> . Mary travelled to the
2025-01-22 04:53:40.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 04:53:40.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14330, 14334) -->  Mary discarded the apple
2025-01-22 04:53:40.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John got the football there.
2025-01-22 04:53:41.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19192, 19197) --> . John got the football
2025-01-22 04:53:41.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:53:41.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17941, 17947) --> . John went back to the
2025-01-22 04:53:41.181 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:53:41.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10402, 10408) --> . Daniel journeyed to the
2025-01-22 04:53:41.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:53:41.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17323, 17329) -->  business. Daniel went back to
2025-01-22 04:53:41.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 04:53:41.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3032, 3037) -->  Mary went to the kitchen
2025-01-22 04:53:41.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:53:41.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21837, 21841) -->  John left the football
2025-01-22 04:53:41.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John grabbed the football.
2025-01-22 04:53:41.495 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6300, 6304) -->  John grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:53:44.273 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the apple.<|eot_id|>
2025-01-22 04:53:44.274 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24226])
2025-01-22 04:53:52.696 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [106.6474609375, 7.641251186594577, 44.47221522177419, 7.495726700820096, 9.139119944852942], 'topk_indices': array([   21, 14331,     9,     4, 24199,    19, 24198, 24200, 24115,
           3, 24218, 24105,     1,    14, 24224,    24,    23,     0,
       24221, 24222]), 'topk_tokens': [' ', ' discarded', ':', '\n\n', 'Question', '26', '.\n\n', ':', ' location', '<|end_header_id|>', '?\n', '.\n\n', '<|start_header_id|>', '\n', '<|end_header_id|>', '\n\n', '4', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [81.24375, 149.375, 84.984375, 220.34375, 20.0296875]}, 'weight': {'score': [22.460611979166668, 23.445721036774113, 23.144153225806452, 23.447085776557458, 29.176654411764705], 'topk_indices': array([18834, 18878, 14648, 14612, 14657, 14693, 19479, 19537, 14511,
       14576, 20399, 20351, 18172, 18203, 23695, 23561, 22000, 22027,
       23777, 23643]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 22.1, 20.75625, 28.873046875, 22.453125]}, 'saliency': {'score': [0.6819432576497396, 0.04178598696696753, 0.2833818004977319, 0.040840622330753404, 0.06622493968290441], 'topk_indices': array([14234, 24198,  4998, 14333, 24219,    16, 24218, 24071,    22,
       14264, 24105,    19, 24205,    20, 24213, 24199,    24,    23,
       24115, 14331]), 'topk_tokens': ['rail', '.\n\n', ' apple', ' apple', 'Answer', ' Date', '?\n', ' context', '202', 'rail', '.\n\n', '26', ' prior', ' Jul', ' discarded', 'Question', '\n\n', '4', ' location', ' discarded'], 'evidence_proportions': [0.4319091796875, 0.819970703125, 0.51983642578125, 1.72412109375, 0.122314453125]}}, 'pred_res': 'Mary discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 04:53:52.730 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:53:52.730 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-27_pid-2_1-2-4-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 2, 4, 6, 8):   3%|▎         | 3/100 [01:27<47:23, 29.32s/it]is_0k: False
your chose emoji: ['🦰', '👩\u200d🔬', '👨🏿\u200d🌾', '🧏\u200d♂️', '🧑🏾\u200d🎨', '\U0001f6de', '🇮🇪', '🧑🏽\u200d🦼\u200d➡', '👩🏽\u200d❤️\u200d👩🏿', '👨🏽\u200d⚖']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.52s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.99s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.79s/it]
Processing depth (0, 2, 3, 7, 8):   3%|▎         | 3/100 [01:45<47:23, 29.32s/it]2025-01-22 04:54:10.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:54:10.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:54:10.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 04:54:10.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4993, 4998) --> . Mary picked up the
2025-01-22 04:54:10.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:54:10.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7623, 7628) -->  war. Mary travelled to
2025-01-22 04:54:10.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 04:54:10.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16732, 16736) -->  Mary discarded the apple
2025-01-22 04:54:10.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John got the football there.
2025-01-22 04:54:10.404 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19240, 19245) --> . John got the football
2025-01-22 04:54:10.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:54:10.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18131, 18137) --> . John went back to the
2025-01-22 04:54:10.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:54:10.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10268, 10274) --> . Daniel journeyed to the
2025-01-22 04:54:10.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:54:10.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17365, 17371) -->  business. Daniel went back to
2025-01-22 04:54:10.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 04:54:10.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (31, 36) -->  went to the kitchen.
2025-01-22 04:54:10.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:54:10.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21885, 21889) -->  John left the football
2025-01-22 04:54:10.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John grabbed the football.
2025-01-22 04:54:10.764 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6308, 6312) -->  John grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:54:13.578 | INFO     | test_jbb_embedding:begin_test:693 - Mary travelled to the office.<|eot_id|>
2025-01-22 04:54:13.578 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24274])
2025-01-22 04:54:22.008 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [361.9114583333333, 19.95753954360094, 156.6517137096774, 19.443774641854514, 11.199353139334862], 'topk_indices': array([   23, 22923,    24, 16736, 24261, 24272, 24259, 16733,  4998,
       24246,  4999, 24269, 24247, 17396, 17395, 24248, 17425, 17397,
           0, 24270]), 'topk_tokens': ['4', 'nes', '\n\n', '.', ' discarded', '<|end_header_id|>', ' apple', ' discarded', ' apple', '.\n\n', '.', '<|eot_id|>', 'Question', ' service', ' the', ':', ' service', ' of', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [438.15, 512.6, 246.4625, 586.0, 71.1625]}, 'weight': {'score': [23.216145833333332, 23.461416670099272, 22.397933467741936, 23.46302076882586, 29.658686926605505], 'topk_indices': array([18882, 18926, 14731, 14695, 19585, 14740, 19527, 14776, 14659,
       14594, 20399, 20447, 18182, 18213, 23743, 23609, 22048, 22075,
       23691, 23825]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.565625, 22.1, 24.2203125, 28.873046875, 22.453125]}, 'saliency': {'score': [2.248255411783854, 0.11297977750931952, 0.9017983713457661, 0.10985451982492027, 0.08413003344054616], 'topk_indices': array([24163, 16467, 24267, 24253, 24246, 17395, 24248, 16777, 16735,
       17397,  4994, 16514, 22923, 24259,  4998, 17396, 24261, 16733,
       17425, 24247]), 'topk_tokens': [' location', 'round', 'Answer', ' prior', '.\n\n', ' the', ':', ' kitchen', ' apple', ' of', ' Mary', 'round', 'nes', ' apple', ' apple', ' service', ' discarded', ' discarded', ' service', 'Question'], 'evidence_proportions': [2.2736328125, 2.9384765625, 1.495751953125, 4.560791015625, 0.4351318359375]}}, 'pred_res': 'Mary travelled to the office.<|eot_id|>', 'score': 0}
2025-01-22 04:54:22.015 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:54:22.015 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-27_pid-3_0-2-3-7-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (0, 2, 3, 7, 8):   4%|▍         | 4/100 [01:57<46:53, 29.30s/it]is_0k: False
your chose emoji: ['👩🏾\u200d❤️\u200d💋\u200d👩🏿', '👩🏾\u200d❤️\u200d💋\u200d👨🏻', '🤹🏻', '👨🏼\u200d🔬', '👨🏽\u200d⚕', '🧎🏻', '🚮', '👩🏾\u200d❤\u200d👨🏼', '🧎🏿\u200d♀\u200d➡', '🏃🏾\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.44s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.42s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (2, 3, 4, 5, 8):   4%|▍         | 4/100 [02:14<46:53, 29.30s/it]2025-01-22 04:54:39.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went to the kitchen.
2025-01-22 04:54:39.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4957, 4962) --> . Mary went to the
2025-01-22 04:54:39.810 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 04:54:39.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7589, 7594) -->  war. Mary picked up
2025-01-22 04:54:39.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the office.
2025-01-22 04:54:39.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9734, 9739) --> . Mary travelled to the
2025-01-22 04:54:39.900 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 04:54:39.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11916, 11920) -->  Mary discarded the apple
2025-01-22 04:54:39.961 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John got the football there.
2025-01-22 04:54:40.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19245, 19250) -->  John got the football there
2025-01-22 04:54:40.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the bedroom.
2025-01-22 04:54:40.152 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (18101, 18107) --> . John went back to the
2025-01-22 04:54:40.153 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:54:40.205 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10404, 10410) --> . Daniel journeyed to the
2025-01-22 04:54:40.205 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bathroom.
2025-01-22 04:54:40.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17382, 17388) --> . Daniel went back to the
2025-01-22 04:54:40.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 04:54:40.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4958, 4963) -->  Mary went to the kitchen
2025-01-22 04:54:40.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John left the football.
2025-01-22 04:54:40.434 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21943, 21947) -->  John left the football
2025-01-22 04:54:40.434 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John grabbed the football.
2025-01-22 04:54:40.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6200, 6204) -->  John grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:54:43.514 | INFO     | test_jbb_embedding:begin_test:693 - THE FIVE MILLION LOAN ELECTION.<|eot_id|>
2025-01-22 04:54:43.515 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24194])
2025-01-22 04:54:51.917 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [541.6458333333334, 24.4464783857503, 229.52217741935485, 23.668989727445943, 62.95629528985507], 'topk_indices': array([11916, 24166, 24188, 24179, 24073, 24186, 11919, 24167,  6203,
        6204, 24168, 24181,    14, 11920, 11917,  7596,  7595,     0,
       24189, 24190]), 'topk_tokens': [' Mary', '.\n\n', ':', ' apple', '.\n\n', '?\n', ' apple', 'Question', ' football', '.', ':', ' discarded', '\n', '.', ' discarded', '.', ' apple', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [344.1, 729.625, 375.575, 1291.75, 117.2]}, 'weight': {'score': [23.816731770833332, 23.441798053477704, 22.57484879032258, 23.4425385479662, 29.817934782608695], 'topk_indices': array([18858, 18814, 14668, 14632, 14713, 19459, 19517, 14677, 14531,
       14596, 20379, 20331, 18183, 18152, 23545, 23679, 21980, 22007,
       23627, 23761]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.403125, 25.5640625, 20.75625, 28.873046875, 25.4984375]}, 'saliency': {'score': [3.639312744140625, 0.13562167604713393, 1.5328605405745968, 0.13034444174862533, 0.4639824024145154], 'topk_indices': array([24172,     8, 24166, 24177, 24186, 24073, 24187, 24039, 11904,
        6201, 24173,  7592, 11916, 24179, 11919,  6203, 24167, 24181,
        7595, 11917]), 'topk_tokens': [' location', ' Date', '.\n\n', ' where', '?\n', '.\n\n', 'Answer', ' context', 'LECTION', ' grabbed', ' prior', ' picked', ' Mary', ' apple', ' apple', ' football', 'Question', ' discarded', ' apple', ' discarded'], 'evidence_proportions': [1.9583984375, 4.4560546875, 2.326953125, 9.95654296875, 0.762060546875]}}, 'pred_res': 'THE FIVE MILLION LOAN ELECTION.<|eot_id|>', 'score': 0}
2025-01-22 04:54:51.921 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:54:51.922 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-27_pid-4_2-3-4-5-8.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 3, 4, 5, 8):   5%|▌         | 5/100 [02:26<46:44, 29.52s/it]Processing depth (2, 3, 4, 5, 8):   5%|▌         | 5/100 [02:27<46:36, 29.44s/it]
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:82 - Selected idx: 28
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the bathroom? 
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:86 - Needle: [' Daniel went back to the bathroom.', ' Daniel journeyed to the hallway.', ' Mary journeyed to the garden.', ' Mary dropped the milk.', ' John travelled to the office.', ' John went to the bathroom.', ' Daniel went back to the kitchen.', ' Mary took the milk.', ' John left the milk.', ' Sandra travelled to the garden.']
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:87 - Real Needle: [' John travelled to the office.', ' John went to the bathroom.', ' John left the milk.', ' Sandra travelled to the garden.']
2025-01-22 04:54:52.153 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['⛹️\u200d♀️', '🧖🏻\u200d♂️', '👩🏼\u200d❤️\u200d👨🏾', '🟣', '♑', '👎🏻', '\U0001fac4🏻', '🧏🏽\u200d♀', '👨🏽\u200d💼', '☠']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (2, 4, 6, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:55:09.042 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:55:09.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4992, 4997) --> . John travelled to the
2025-01-22 04:55:09.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the bathroom.
2025-01-22 04:55:09.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9868, 9873) --> . John went to the
2025-01-22 04:55:09.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John left the milk.
2025-01-22 04:55:09.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14374, 14378) -->  John left the milk
2025-01-22 04:55:09.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra travelled to the garden.
2025-01-22 04:55:09.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19352, 19357) --> . Sandra travelled to the
2025-01-22 04:55:09.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:55:09.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14059, 14065) -->  Daniel went back to the bathroom
2025-01-22 04:55:09.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:55:09.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17598, 17604) -->  city. Daniel journeyed to
2025-01-22 04:55:09.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:55:09.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (20891, 20897) -->  Mary journeyed to the garden
2025-01-22 04:55:09.571 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:55:09.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (232, 236) -->  Mary dropped the milk
2025-01-22 04:55:09.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 04:55:09.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14060, 14066) -->  went back to the bathroom.
2025-01-22 04:55:09.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:55:09.732 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16807, 16811) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:55:12.329 | INFO     | test_jbb_embedding:begin_test:693 - Mary<|eot_id|>
2025-01-22 04:55:12.329 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24202])
2025-01-22 04:55:20.749 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [223.45559210526315, 14.65686327673112, 334.7509765625, 14.068522802550408, 24.869140625], 'topk_indices': array([   23,   234, 24198,     9,   207,     4,   235, 24192, 24201,
           1,   233,   231, 24190,    14,   232,    24,   236, 24197,
       24200,     0]), 'topk_tokens': ['4', ' the', '<|start_header_id|>', ':', '.', '\n\n', ' milk', ' bathroom', '\n\n', '<|start_header_id|>', ' dropped', '.', ' before', '\n', ' Mary', '\n\n', '.', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [162.70625, 309.05, 305.96875, 132.6]}, 'weight': {'score': [21.29481907894737, 23.444784436456786, 24.264404296875, 23.445389806649278, 29.35625], 'topk_indices': array([18764, 18808, 14616, 14652, 19409, 19467, 14661, 14697, 14580,
       14515, 20281, 20329, 18133, 18102, 23662, 23528, 21984, 21957,
       23610, 23744]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.990625, 18.6375, 25.177734375, 22.15]}, 'saliency': {'score': [1.2920500102796053, 0.08334316795196559, 2.178760528564453, 0.0796161449923405, 0.1851329803466797], 'topk_indices': array([16807,   206,     4,    20, 24171, 24201, 24199, 16810,   230,
       24189, 24100,  9873, 14064, 24184,    24, 24190,   235,   233,
       24192,   232]), 'topk_tokens': [' Mary', ' ST', '\n\n', ' Jul', ' return', '\n\n', 'assistant', ' milk', ' ST', ' milk', ' location', ' bathroom', ' bathroom', 'Question', '\n\n', ' before', ' milk', ' dropped', ' bathroom', ' Mary'], 'evidence_proportions': [0.9775634765625, 1.51689453125, 1.961181640625, 0.84638671875]}}, 'pred_res': 'Mary<|eot_id|>', 'score': 0}
2025-01-22 04:55:20.758 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:55:20.759 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-28_pid-0_2-4-6-8.pkl | len: 3 |  size: 2.01 KB
Processing depth (2, 4, 6, 8):   1%|          | 1/100 [00:28<46:59, 28.48s/it]is_0k: False
your chose emoji: ['⛹🏽\u200d♀', '🧏🏼\u200d♀️', '🙅', '💏', '🇧🇦', '🧆', '🔐', '👩🏽\u200d🤝\u200d👨🏻', '🧑🏼\u200d⚖', '🧑🏾\u200d❤️\u200d💋\u200d🧑🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.50s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]
Processing depth (0, 2, 4, 7):   1%|          | 1/100 [00:44<46:59, 28.48s/it]2025-01-22 04:55:37.436 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:55:37.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 04:55:37.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the bathroom.
2025-01-22 04:55:37.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4872, 4877) --> . John went to the
2025-01-22 04:55:37.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John left the milk.
2025-01-22 04:55:37.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9698, 9702) -->  left the milk.
2025-01-22 04:55:37.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra travelled to the garden.
2025-01-22 04:55:37.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16694, 16699) --> . Sandra travelled to the
2025-01-22 04:55:37.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:55:37.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14059, 14065) --> . Daniel went back to the
2025-01-22 04:55:37.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:55:37.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17567, 17573) --> . Daniel journeyed to the
2025-01-22 04:55:37.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:55:37.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (20842, 20848) --> . Mary journeyed to the
2025-01-22 04:55:37.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:55:37.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (238, 242) -->  Mary dropped the milk
2025-01-22 04:55:37.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 04:55:37.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14059, 14065) --> . Daniel went back to the
2025-01-22 04:55:38.000 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:55:38.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16737, 16741) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:55:40.733 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 04:55:40.733 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 04:55:49.138 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [444.6381578947368, 23.75904700413223, 322.0546875, 23.032633338854613, 37.31911057692308], 'topk_indices': array([ 9700, 24182, 23487, 24181,   241, 10056, 24188,     1, 24185,
       24189,    23, 23488,    24, 24196,    14,   242, 24186,     0,
       24193, 24194]), 'topk_tokens': [' milk', ' Where', '\n', ':', ' milk', 'or', ' bathroom', '<|start_header_id|>', ' milk', '?', '4', 're', '\n\n', '<|end_header_id|>', '\n', '.', ' before', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [427.675, 277.9, 856.75, 298.65]}, 'weight': {'score': [20.31907894736842, 23.447195247933884, 22.01513671875, 23.451554023976147, 29.8671875], 'topk_indices': array([18800, 18844, 14690, 14654, 19439, 14699, 14735, 19497, 14618,
       14553, 20359, 20311, 18169, 18138, 23678, 23544, 21969, 21996,
       23760, 23626]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 18.6375, 20.3515625, 22.15]}, 'saliency': {'score': [2.4388106496710527, 0.13405974585162705, 2.0095748901367188, 0.12976115163283158, 0.28192745110927486], 'topk_indices': array([10091, 24190, 24096,    20, 10056,   238, 10055, 23437,    23,
         239,    24, 24189, 24180, 23488, 24182,  9700,   241, 24185,
       24188, 24186]), 'topk_tokens': ['po', ' \n', ' location', ' Jul', 'or', ' Mary', 'po', 'bellion', '4', ' dropped', '\n\n', '?', 'Question', 're', ' Where', ' milk', ' milk', ' milk', ' bathroom', ' before'], 'evidence_proportions': [2.43876953125, 1.3703125, 4.447509765625, 1.900390625]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 04:55:49.146 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:55:49.146 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-28_pid-1_0-2-4-7.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 2, 4, 7):   2%|▏         | 2/100 [00:56<46:25, 28.43s/it]is_0k: False
your chose emoji: ['🤛🏿', '🧖\u200d♂️', '🤽🏼\u200d♂', '✋', '👮🏽\u200d♀', '🧍🏼\u200d♀️', '🏈', '⛹🏽\u200d♂️', '👨🏿\u200d❤\u200d👨🏾', '🙆🏼\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.00s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.50s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.36s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (0, 1, 4, 6):   2%|▏         | 2/100 [01:14<46:25, 28.43s/it]2025-01-22 04:56:06.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:56:06.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 04:56:06.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the bathroom.
2025-01-22 04:56:06.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2977, 2982) -->  tragedy. John went to
2025-01-22 04:56:06.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John left the milk.
2025-01-22 04:56:06.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9725, 9729) -->  John left the milk
2025-01-22 04:56:06.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra travelled to the garden.
2025-01-22 04:56:06.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14320, 14325) --> . Sandra travelled to the
2025-01-22 04:56:06.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:56:06.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14077, 14083) --> . Daniel went back to the
2025-01-22 04:56:06.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:56:06.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17599, 17605) --> . Daniel journeyed to the
2025-01-22 04:56:06.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:56:07.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (20769, 20775) -->  Mary journeyed to the garden
2025-01-22 04:56:07.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:56:07.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (208, 212) -->  Mary dropped the milk
2025-01-22 04:56:07.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 04:56:07.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14077, 14083) --> . Daniel went back to the
2025-01-22 04:56:07.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:56:07.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16815, 16819) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:56:09.838 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 04:56:09.838 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24190])
2025-01-22 04:56:18.204 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [386.0296052631579, 21.232092478918652, 263.9296875, 20.623274460461456, 27.746911951013512], 'topk_indices': array([24159, 10052, 24180,     9, 10059, 10088,     4, 10094, 10053,
       24078, 10095,    23, 15596, 10060,    24,     1,    14, 24188,
       24185,     0]), 'topk_tokens': [' return', ' as', ' bathroom', ':', 'po', ' as', '\n\n', '\n', ' if', '.\n\n', 'po', '4', 'ope', 'or', '\n\n', '<|start_header_id|>', '\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [521.3, 256.6875, 368.71875, 393.95]}, 'weight': {'score': [22.863486842105264, 23.445167824074073, 22.720458984375, 23.4465862666211, 29.8828125], 'topk_indices': array([18850, 18806, 14664, 14700, 19503, 14709, 19445, 14745, 14628,
       14563, 20365, 20317, 18144, 18175, 23550, 23684, 22016, 21989,
       23632, 23766]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 24.4453125, 25.177734375, 22.15]}, 'saliency': {'score': [2.228541324013158, 0.12149810791015625, 1.6696395874023438, 0.11778764237642911, 0.20942007528769002], 'topk_indices': array([14078,     8,    31,  2983, 24078, 24178, 15581,    20, 15582,
       24159,    23, 24172, 10053, 24044,    24, 10059, 10060, 10095,
       24180, 15596]), 'topk_tokens': [' Daniel', ' Date', ' travelled', ' bathroom', '.\n\n', ' before', ' latter', ' Jul', ' part', ' return', '4', 'Question', ' if', ' context', '\n\n', 'po', 'or', 'po', ' bathroom', 'ope'], 'evidence_proportions': [2.81328125, 1.401953125, 2.476806640625, 2.27177734375]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 04:56:18.210 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:56:18.210 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-28_pid-2_0-1-4-6.pkl | len: 3 |  size: 1.98 KB
Processing depth (0, 1, 4, 6):   3%|▎         | 3/100 [01:25<46:25, 28.72s/it]is_0k: False
your chose emoji: ['🧘\u200d♂️', '🌖', '🧎\u200d♀\u200d➡', '🙅🏾\u200d♀️', '📊', '🏊🏽\u200d♂', '🤦🏽\u200d♀', '🙍🏽\u200d♀', '👩🏾\u200d🤝\u200d👩🏽', '◼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.21s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (1, 2, 7, 8):   3%|▎         | 3/100 [01:43<46:25, 28.72s/it]2025-01-22 04:56:35.698 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:56:35.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3036, 3041) --> . John travelled to the
2025-01-22 04:56:35.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the bathroom.
2025-01-22 04:56:35.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4976, 4981) --> . John went to the
2025-01-22 04:56:35.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John left the milk.
2025-01-22 04:56:35.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16734, 16738) -->  John left the milk
2025-01-22 04:56:35.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra travelled to the garden.
2025-01-22 04:56:35.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19186, 19191) --> . Sandra travelled to the
2025-01-22 04:56:35.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:56:36.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14048, 14054) --> . Daniel went back to the
2025-01-22 04:56:36.012 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:56:36.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17589, 17595) --> . Daniel journeyed to the
2025-01-22 04:56:36.106 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:56:36.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (20782, 20788) --> . Mary journeyed to the
2025-01-22 04:56:36.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:56:36.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (232, 236) -->  Mary dropped the milk
2025-01-22 04:56:36.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 04:56:36.295 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14048, 14054) --> . Daniel went back to the
2025-01-22 04:56:36.295 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:56:36.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16817, 16821) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:56:39.042 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:56:39.042 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24206])
2025-01-22 04:56:47.429 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [203.875, 13.41665590920357, 204.98828125, 13.01308756261125, 13.134372618140244], 'topk_indices': array([18972, 24189,     9, 24094, 24060,   235,     4,   233, 24196,
       24194, 24104,     1,    23,    24,    14,   236, 24202, 24201,
       24204,     0]), 'topk_tokens': [' in', ':', ':', '.\n\n', ' context', ' milk', '\n\n', ' dropped', ' bathroom', ' before', ' location', '<|start_header_id|>', '4', '\n\n', '\n', '.', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [240.8875, 168.875, 349.8125, 85.1125]}, 'weight': {'score': [21.29481907894737, 23.448643010575015, 22.01513671875, 23.452235959038788, 29.78125], 'topk_indices': array([18822, 18866, 14661, 14697, 14706, 19473, 14742, 19531, 14625,
       14560, 20345, 20393, 18191, 18160, 23698, 23564, 22003, 22030,
       23646, 23780]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.990625, 18.6375, 25.177734375, 22.15]}, 'saliency': {'score': [1.1759482935855263, 0.07571655192756527, 1.278280258178711, 0.07325819858519916, 0.1004989437940644], 'topk_indices': array([    8, 24203,  7807, 16737,    22, 24199, 24175,    20, 18971,
       24193,   232,    23, 24060,    24, 24188, 24194,   233,   235,
       24104, 24196]), 'topk_tokens': [' Date', 'assistant', ' Square', ' milk', '202', 'Answer', ' return', ' Jul', ' manner', ' milk', ' Mary', '4', ' context', '\n\n', 'Question', ' before', ' dropped', ' milk', ' location', ' bathroom'], 'evidence_proportions': [1.329052734375, 0.808349609375, 2.2935791015625, 0.496337890625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:56:47.436 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:56:47.437 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-28_pid-3_1-2-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 7, 8):   4%|▍         | 4/100 [01:55<46:16, 28.92s/it]is_0k: False
your chose emoji: ['💂🏻\u200d♂', '👨\u200d🦯\u200d➡', '💂🏿', '🧑🏻\u200d❤️\u200d💋\u200d🧑🏾', '🏋🏼\u200d♂️', '👨🏿\u200d🦽\u200d➡', '🧚🏽\u200d♀', '🌭', '🍦', '🐻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.48s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (3, 5, 6, 8):   4%|▍         | 4/100 [02:11<46:16, 28.92s/it]2025-01-22 04:57:04.329 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:57:04.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7472, 7477) --> . John travelled to the
2025-01-22 04:57:04.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went to the bathroom.
2025-01-22 04:57:04.435 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11888, 11893) --> . John went to the
2025-01-22 04:57:04.435 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John left the milk.
2025-01-22 04:57:04.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14402, 14406) -->  John left the milk
2025-01-22 04:57:04.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra travelled to the garden.
2025-01-22 04:57:04.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19374, 19379) --> . Sandra travelled to the
2025-01-22 04:57:04.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:57:04.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14086, 14092) --> . Daniel went back to the
2025-01-22 04:57:04.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the hallway.
2025-01-22 04:57:04.784 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17620, 17626) -->  city. Daniel journeyed to
2025-01-22 04:57:04.784 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:57:04.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (20906, 20912) --> . Mary journeyed to the
2025-01-22 04:57:04.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:57:04.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (232, 236) -->  Mary dropped the milk
2025-01-22 04:57:04.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went back to the kitchen.
2025-01-22 04:57:04.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14086, 14092) --> . Daniel went back to the
2025-01-22 04:57:04.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:57:05.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (16829, 16833) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:57:07.812 | INFO     | test_jbb_embedding:begin_test:693 - Mary took the milk.<|eot_id|>
2025-01-22 04:57:07.812 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24206])
2025-01-22 04:57:16.209 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [208.66118421052633, 13.009761752313285, 201.446533203125, 12.606261577907025, 14.936237614329269], 'topk_indices': array([  235,    13, 24196, 24194,     3,   233, 24094, 24202, 24205,
           4, 24104,     9,     1,    23,   236,    14,    24, 24201,
       24204,     0]), 'topk_tokens': [' milk', '3', ' bathroom', ' before', '<|end_header_id|>', ' dropped', '.\n\n', '<|start_header_id|>', '\n\n', '\n\n', ' location', ':', '<|start_header_id|>', '4', '.', '\n', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [204.15, 237.3, 356.8125, 66.0125]}, 'weight': {'score': [21.29481907894737, 23.445844348975545, 22.545166015625, 23.448729276193237, 29.3687118902439], 'topk_indices': array([18786, 18830, 14680, 14644, 19489, 19431, 14695, 14731, 14543,
       14608, 20363, 20315, 18124, 18155, 23692, 23558, 21985, 22012,
       23774, 23640]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.990625, 18.6375, 25.177734375, 22.15]}, 'saliency': {'score': [1.2007863898026316, 0.07335691584393589, 1.2515392303466797, 0.07090946814474712, 0.10910592428067835], 'topk_indices': array([ 7827,    16, 24060, 14405,     8,    19,    22, 24194, 24188,
         232, 16832, 24193,   233,    20,   235,    23, 24104,    24,
       11893, 24196]), 'topk_tokens': [' Square', ' Date', ' context', ' milk', ' Date', '26', '202', ' before', 'Question', ' Mary', ' milk', ' milk', ' dropped', ' Jul', ' milk', '4', ' location', '\n\n', ' bathroom', ' bathroom'], 'evidence_proportions': [1.14736328125, 1.182373046875, 2.265869140625, 0.420556640625]}}, 'pred_res': 'Mary took the milk.<|eot_id|>', 'score': 0}
2025-01-22 04:57:16.216 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:57:16.216 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-28_pid-4_3-5-6-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (3, 5, 6, 8):   5%|▌         | 5/100 [02:23<45:42, 28.87s/it]Processing depth (3, 5, 6, 8):   5%|▌         | 5/100 [02:24<45:39, 28.84s/it]
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:82 - Selected idx: 29
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:86 - Needle: [' Daniel went back to the bathroom.', ' Daniel went back to the kitchen.', ' Mary journeyed to the garden.', ' Mary dropped the milk.', ' John travelled to the office.', ' John got the apple.', ' John went to the bathroom.', ' Daniel journeyed to the hallway.', ' Mary took the milk.', ' John left the apple.', ' Sandra travelled to the garden.']
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:87 - Real Needle: [' John travelled to the office.', ' John got the apple.', ' John went to the bathroom.', ' John left the apple.', ' Sandra travelled to the garden.']
2025-01-22 04:57:16.462 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👨🏽\u200d❤️\u200d👨🏿', '🧚🏿\u200d♂️', '🅱', '🇩🇪', '☄️', '🏇🏻', '👭🏾', '🧑🏽\u200d⚕', '👩🏻\u200d❤️\u200d👩🏾', '🤴🏾']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.79s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.62s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (1, 3, 4, 7, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 04:57:33.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:57:33.309 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2984, 2989) -->  tragedy. John travelled to
2025-01-22 04:57:33.309 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the apple.
2025-01-22 04:57:33.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7491, 7495) -->  John got the apple
2025-01-22 04:57:33.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the bathroom.
2025-01-22 04:57:33.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9727, 9732) --> . John went to the
2025-01-22 04:57:33.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the apple.
2025-01-22 04:57:33.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16762, 16766) -->  John left the apple
2025-01-22 04:57:33.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra travelled to the garden.
2025-01-22 04:57:33.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21517, 21522) --> . Sandra travelled to the
2025-01-22 04:57:33.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:57:33.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4549, 4555) -->  went back to the kitchen.
2025-01-22 04:57:33.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:57:33.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4548, 4554) -->  Daniel went back to the kitchen
2025-01-22 04:57:33.623 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:57:33.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11142, 11148) --> . Mary journeyed to the
2025-01-22 04:57:33.685 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:57:33.704 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4036, 4040) -->  Mary dropped the milk
2025-01-22 04:57:33.704 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:57:33.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21468, 21474) --> . Daniel journeyed to the
2025-01-22 04:57:33.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:57:33.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4200, 4204) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:57:36.501 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 04:57:36.501 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24242])
2025-01-22 04:57:44.915 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [265.1807065217391, 15.865791536050157, 112.3251953125, 15.501123961304724, 20.169564852150536], 'topk_indices': array([ 7494,    22,     1, 17228,     3, 16765,     9,    19, 24161,
        7495, 17181, 24121, 24131,    14, 24240,    24,    23,     0,
       24237, 24238]), 'topk_tokens': [' apple', '202', '<|start_header_id|>', ' a', '<|end_header_id|>', ' apple', ':', '26', ' the', '.', ' spear', '.\n\n', ' location', '\n', '<|end_header_id|>', '\n\n', '4', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [241.7, 475.75, 132.35, 516.3125, 52.13125]}, 'weight': {'score': [23.28294836956522, 23.453686994720343, 22.02490234375, 23.455739501943032, 29.806619623655912], 'topk_indices': array([18862, 18818, 14725, 14689, 19515, 19457, 14734, 14770, 14588,
       14653, 20391, 20343, 18187, 18156, 23703, 23569, 22027, 22000,
       23785, 23651]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.7984375, 25.396484375, 18.6375, 25.248046875, 22.15]}, 'saliency': {'score': [1.6969816788383152, 0.08903299245754104, 0.6681995391845703, 0.08673789347527544, 0.1497930711315524], 'topk_indices': array([24235, 24127, 24215,    16, 24221, 24239, 24087, 24121,     0,
       24227, 24229,    22,    19,    24,  7494,    20, 16765, 24131,
          23, 17181]), 'topk_tokens': ['Answer', ' item', 'Question', ' Date', ' prior', 'assistant', ' context', '.\n\n', '<|begin_of_text|>', ' apple', ' discarded', '202', '26', '\n\n', ' apple', ' Jul', ' apple', ' location', '4', ' spear'], 'evidence_proportions': [1.537890625, 3.16650390625, 0.688818359375, 3.40576171875, 0.32159423828125]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 04:57:44.924 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:57:44.924 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-29_pid-0_1-3-4-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 3, 4, 7, 9):   1%|          | 1/100 [00:28<46:45, 28.34s/it]is_0k: False
your chose emoji: ['🏳\u200d⚧', '🕝', '👩🏽\u200d❤️\u200d👩🏼', '3⃣', '👩🏾\u200d❤\u200d💋\u200d👩🏽', '🚴🏿\u200d♀️', '⏏️', '🙅🏽\u200d♂', '🤵🏾\u200d♀️', '👩🏻\u200d🦯\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.66s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.29s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.00s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.39s/it]
Processing depth (0, 4, 5, 7, 9):   1%|          | 1/100 [00:43<46:45, 28.34s/it]2025-01-22 04:58:00.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:58:00.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 04:58:00.663 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the apple.
2025-01-22 04:58:00.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9679, 9683) -->  John got the apple
2025-01-22 04:58:00.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the bathroom.
2025-01-22 04:58:00.778 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11010, 11015) -->  back to the bathroom.
2025-01-22 04:58:00.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the apple.
2025-01-22 04:58:00.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16652, 16656) -->  John left the apple
2025-01-22 04:58:00.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra travelled to the garden.
2025-01-22 04:58:00.990 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21425, 21430) --> . Sandra travelled to the
2025-01-22 04:58:00.991 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:58:01.023 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4497, 4503) -->  went back to the kitchen.
2025-01-22 04:58:01.023 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:58:01.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4496, 4502) -->  Daniel went back to the kitchen
2025-01-22 04:58:01.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:58:01.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11116, 11122) --> . Mary journeyed to the
2025-01-22 04:58:01.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:58:01.129 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3918, 3922) -->  Mary dropped the milk
2025-01-22 04:58:01.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:58:01.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21367, 21373) -->  St. Daniel journeyed to
2025-01-22 04:58:01.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:58:01.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4107, 4111) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:58:03.964 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:58:03.964 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24168])
2025-01-22 04:58:12.371 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [288.9816576086956, 20.095901427389325, 118.1787109375, 19.70929465581588, 34.790945870535715], 'topk_indices': array([ 9683,     1,    22, 16655,     9,    20,  9682, 18825,    19,
       24013,    24, 17511,    14, 24166, 17521, 18782,    23, 24163,
           0, 24164]), 'topk_tokens': ['.', '<|start_header_id|>', '202', ' apple', ':', ' Jul', ' apple', ' ga', '26', ' context', '\n\n', 'ente', '\n', '<|end_header_id|>', 'ente', 'untlet', '4', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [219.3875, 525.4375, 181.45, 606.125, 23.228125]}, 'weight': {'score': [22.35461956521739, 23.43080264791063, 23.3095703125, 23.431989944018245, 29.045200892857142], 'topk_indices': array([18826, 18782, 14611, 14647, 14656, 19479, 19421, 14692, 14575,
       14510, 20293, 20341, 18120, 18151, 23645, 23511, 21950, 21977,
       23727, 23593]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 25.396484375, 20.021875, 25.248046875, 22.15]}, 'saliency': {'score': [1.8485903532608696, 0.11425991350330988, 0.7238693237304688, 0.11179683652874248, 0.245685304914202], 'topk_indices': array([   16, 24011,    24,  3594, 24147, 24153, 17520,    22, 17510,
       24155,    19, 16655,  9682,    20, 18825, 24013,    23, 17511,
       17521, 18782]), 'topk_tokens': [' Date', ' provided', '\n\n', '�', ' prior', ' apple', 'arp', '202', 'arp', ' discarded', '26', ' apple', ' apple', ' Jul', ' ga', ' context', '4', 'ente', 'ente', 'untlet'], 'evidence_proportions': [1.317431640625, 3.47412109375, 1.037451171875, 4.032470703125, 0.143359375]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:58:12.375 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:58:12.375 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-29_pid-1_0-4-5-7-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 4, 5, 7, 9):   2%|▏         | 2/100 [00:55<45:25, 27.82s/it]is_0k: False
your chose emoji: ['↩', '🇨🇺', '⏺', '🍄', '🇦🇹', '👩🏽\u200d❤️\u200d💋\u200d👨🏻', '🔷', '😥', '🧕🏼', '🇺🇾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (0, 1, 5, 6, 8):   2%|▏         | 2/100 [01:13<45:25, 27.82s/it]2025-01-22 04:58:29.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:58:29.978 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 04:58:29.978 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the apple.
2025-01-22 04:58:29.992 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2974, 2978) -->  John got the apple
2025-01-22 04:58:29.993 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the bathroom.
2025-01-22 04:58:30.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11066, 11071) -->  back to the bathroom.
2025-01-22 04:58:30.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the apple.
2025-01-22 04:58:30.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14400, 14404) -->  John left the apple
2025-01-22 04:58:30.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra travelled to the garden.
2025-01-22 04:58:30.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19303, 19308) -->  Sandra travelled to the garden
2025-01-22 04:58:30.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:58:30.259 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4554, 4560) -->  went back to the kitchen.
2025-01-22 04:58:30.259 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:58:30.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4553, 4559) -->  Daniel went back to the kitchen
2025-01-22 04:58:30.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:58:30.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11196, 11202) --> . Mary journeyed to the
2025-01-22 04:58:30.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:58:30.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4025, 4029) -->  Mary dropped the milk
2025-01-22 04:58:30.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:58:30.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21496, 21502) --> . Daniel journeyed to the
2025-01-22 04:58:30.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:58:30.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4205, 4209) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:58:33.173 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 04:58:33.173 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24216])
2025-01-22 04:58:41.570 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [336.8152173913044, 24.71545957552234, 90.623046875, 24.331097649298513, 26.082373046875], 'topk_indices': array([   23,    24, 12038, 10098, 10092, 24214,  9320,  9262,  9263,
       10128,    14, 24211, 10093, 10134, 10135, 10099,  9321, 10100,
           0, 24212]), 'topk_tokens': ['4', '\n\n', 'ANT', '\n', ' as', '<|end_header_id|>', ' in', ' a', ' few', ' as', '\n', '<|eot_id|>', ' if', '\n', 'po', 'po', ' a', 'or', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [358.925, 618.125, 113.5875, 566.1875, 129.3875]}, 'weight': {'score': [23.3359375, 23.448522276818895, 22.02490234375, 23.450514797934858, 30.04296875], 'topk_indices': array([18856, 18812, 14688, 14724, 19457, 14733, 19515, 14769, 14652,
       14587, 20329, 20377, 18150, 18181, 23681, 23547, 21980, 22007,
       23763, 23629]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 25.396484375, 20.021875, 25.248046875, 26.6640625]}, 'saliency': {'score': [2.0552076256793477, 0.13922493648190395, 0.5426177978515625, 0.1368669439555053, 0.19665069580078126], 'topk_indices': array([   24,    23,  9262,  9319, 12082, 14403, 24203,  9260, 12039,
       10092, 10128, 24061,  2977, 12038,  9321, 10093,  9263, 10135,
       10099, 10100]), 'topk_tokens': ['\n\n', '4', ' a', ' draft', 'ANT', ' apple', ' discarded', ' draft', 'IC', ' as', ' as', ' context', ' apple', 'ANT', ' a', ' if', ' few', 'po', 'po', 'or'], 'evidence_proportions': [1.861328125, 4.050048828125, 0.623486328125, 3.687255859375, 0.779296875]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 04:58:41.579 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:58:41.579 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-29_pid-2_0-1-5-6-8.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 1, 5, 6, 8):   3%|▎         | 3/100 [01:24<45:59, 28.45s/it]is_0k: False
your chose emoji: ['🇼🇸', '👩🏻\u200d❤️\u200d💋\u200d👩🏿', '🚵\u200d♂️', '👩🏿\u200d❤\u200d💋\u200d👩🏾', '☯', '🏄🏿\u200d♂', '\U0001fae2', '🤽🏼\u200d♀️', '🙏', '🤸\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.54s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.90s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (0, 3, 5, 6, 8):   3%|▎         | 3/100 [01:42<45:59, 28.45s/it]2025-01-22 04:58:59.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:58:59.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 04:58:59.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the apple.
2025-01-22 04:58:59.100 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7627, 7631) -->  John got the apple
2025-01-22 04:58:59.100 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the bathroom.
2025-01-22 04:58:59.156 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11044, 11049) -->  back to the bathroom.
2025-01-22 04:58:59.156 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the apple.
2025-01-22 04:58:59.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14414, 14418) -->  John left the apple
2025-01-22 04:58:59.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra travelled to the garden.
2025-01-22 04:58:59.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19295, 19300) -->  Sandra travelled to the garden
2025-01-22 04:58:59.319 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:58:59.341 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4563, 4569) --> . Daniel went back to the
2025-01-22 04:58:59.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:58:59.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4563, 4569) --> . Daniel went back to the
2025-01-22 04:58:59.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:58:59.435 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11174, 11180) --> . Mary journeyed to the
2025-01-22 04:58:59.436 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:58:59.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4030, 4034) -->  Mary dropped the milk
2025-01-22 04:58:59.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:58:59.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21433, 21439) -->  St. Daniel journeyed to
2025-01-22 04:58:59.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:58:59.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4200, 4204) -->  took the milk.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:59:02.267 | INFO     | test_jbb_embedding:begin_test:693 - The table.<|eot_id|>
2025-01-22 04:59:02.267 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24236])
2025-01-22 04:59:10.670 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [402.0978260869565, 22.69835588744946, 194.243896484375, 22.110519592792457, 25.230555555555554], 'topk_indices': array([24125,    24, 24223, 24221,  7631, 24230, 14417,  7630, 24208,
       24081, 24209, 24228,    23, 24115, 24210, 24234,    14,     0,
       24231, 24232]), 'topk_tokens': [' location', '\n\n', ' discarded', ' apple', '.', ':', ' apple', ' apple', '.\n\n', ' context', 'Question', '?\n', '4', '.\n\n', ':', '<|end_header_id|>', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [398.775, 775.75, 138.4875, 711.0625, 122.9375]}, 'weight': {'score': [23.3359375, 23.448758148362074, 21.85400390625, 23.450975698321134, 29.34201388888889], 'topk_indices': array([18804, 18848, 14656, 14692, 14701, 14737, 19449, 19507, 14620,
       14555, 20375, 20327, 18142, 18173, 23705, 23571, 22010, 22037,
       23653, 23787]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 25.396484375, 20.021875, 25.248046875, 26.6640625]}, 'saliency': {'score': [2.541864809782609, 0.12358945784705318, 1.161482810974121, 0.1199160955514495, 0.18282843695746528], 'topk_indices': array([24210, 24211,    31, 24219,    19, 18947, 24229,    20, 24208,
       24228, 24125,    23, 24215, 24115, 24221, 14417,  7630, 24223,
       24081, 24209]), 'topk_tokens': [':', ' Where', ' travelled', ' where', '26', ' manner', 'Answer', ' Jul', '.\n\n', '?\n', ' location', '4', ' prior', '.\n\n', ' apple', ' apple', ' apple', ' discarded', ' context', 'Question'], 'evidence_proportions': [2.22490234375, 5.200927734375, 0.78837890625, 4.7412109375, 0.7255859375]}}, 'pred_res': 'The table.<|eot_id|>', 'score': 0}
2025-01-22 04:59:10.678 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:59:10.678 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-29_pid-3_0-3-5-6-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 3, 5, 6, 8):   4%|▍         | 4/100 [01:54<45:55, 28.71s/it]is_0k: False
your chose emoji: ['🤹🏻\u200d♂', '🚴🏻\u200d♀️', '🇾🇹', '👩🏻\u200d❤️\u200d👩🏾', '👨🏿\u200d🔧', '🖥', '🧘🏾\u200d♂', '\U0001f9cc', '👩🏽\u200d❤\u200d👨🏽', '👨🏽\u200d⚕']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (1, 2, 5, 8, 9):   4%|▍         | 4/100 [02:11<45:55, 28.71s/it]2025-01-22 04:59:28.295 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the office.
2025-01-22 04:59:28.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2958, 2963) -->  tragedy. John travelled to
2025-01-22 04:59:28.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the apple.
2025-01-22 04:59:28.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4887, 4891) -->  John got the apple
2025-01-22 04:59:28.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the bathroom.
2025-01-22 04:59:28.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11012, 11017) -->  back to the bathroom.
2025-01-22 04:59:28.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John left the apple.
2025-01-22 04:59:28.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19207, 19211) -->  left the apple.
2025-01-22 04:59:28.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra travelled to the garden.
2025-01-22 04:59:28.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21465, 21470) --> . Sandra travelled to the
2025-01-22 04:59:28.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went back to the bathroom.
2025-01-22 04:59:28.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4511, 4517) -->  went back to the kitchen.
2025-01-22 04:59:28.614 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the kitchen.
2025-01-22 04:59:28.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4510, 4516) -->  Daniel went back to the kitchen
2025-01-22 04:59:28.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the garden.
2025-01-22 04:59:28.691 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11142, 11148) --> . Mary journeyed to the
2025-01-22 04:59:28.691 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 04:59:28.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3932, 3936) -->  Mary dropped the milk
2025-01-22 04:59:28.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the hallway.
2025-01-22 04:59:28.822 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21416, 21422) --> . Daniel journeyed to the
2025-01-22 04:59:28.822 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary took the milk.
2025-01-22 04:59:28.845 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4121, 4125) -->  Mary took the milk
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 04:59:31.531 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 04:59:31.531 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 04:59:39.911 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [317.0095108695652, 18.365547456575683, 130.6201171875, 17.931932642487048, 28.14203381147541], 'topk_indices': array([24150, 24158,     1, 24165, 24170, 19209, 24151, 24161, 24160,
        4891, 24057,    23, 24152,    24, 24067, 24176,    14,     0,
       24173, 24174]), 'topk_tokens': ['.\n\n', ' to', '<|start_header_id|>', ' discarded', '?\n', ' apple', 'Question', ' where', ' place', '.', '.\n\n', '4', ':', '\n\n', ' location', '<|end_header_id|>', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [197.2125, 527.125, 140.4875, 762.875, 88.54375]}, 'weight': {'score': [22.744565217391305, 23.43375206782465, 22.747802734375, 23.435318976683938, 29.18801229508197], 'topk_indices': array([18775, 18819, 14631, 14667, 14712, 14676, 19477, 19419, 14595,
       14530, 20339, 20291, 18124, 18093, 23661, 23527, 21975, 21948,
       23609, 23743]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.7984375, 25.396484375, 20.021875, 20.421875, 22.15]}, 'saliency': {'score': [1.8982066278872283, 0.10145593837236352, 0.81866455078125, 0.09879165064362047, 0.20790375256147542], 'topk_indices': array([   19, 18918, 19207, 24156,     0,    20, 24063, 24057,    23,
       24023,    24,  4890, 24161, 24163, 24160, 24157, 19209, 24151,
       24067, 24165]), 'topk_tokens': ['26', ' manner', ' left', ' location', '<|begin_of_text|>', ' Jul', ' item', '.\n\n', '4', ' context', '\n\n', ' apple', ' where', ' apple', ' place', ' prior', ' apple', 'Question', ' location', ' discarded'], 'evidence_proportions': [1.31396484375, 3.474609375, 0.75087890625, 4.182861328125, 0.54093017578125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 04:59:39.916 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 04:59:39.916 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-29_pid-4_1-2-5-8-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 5, 8, 9):   5%|▌         | 5/100 [02:23<45:45, 28.90s/it]Processing depth (1, 2, 5, 8, 9):   5%|▌         | 5/100 [02:23<45:29, 28.74s/it]
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:82 - Selected idx: 30
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the kitchen? 
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:84 - Answer: garden
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:86 - Needle: [' Sandra went to the garden.', ' Mary got the football.', ' Sandra picked up the milk.', ' Mary journeyed to the garden.', ' Daniel travelled to the garden.', ' John moved to the office.', ' Daniel journeyed to the office.', ' John went to the kitchen.', ' Mary went to the kitchen.', ' Daniel travelled to the garden.']
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:87 - Real Needle: [' Mary got the football.', ' Mary journeyed to the garden.', ' Mary went to the kitchen.', ' Daniel travelled to the garden.']
2025-01-22 04:59:40.270 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['⛹🏼\u200d♀', '🇬🇬', '\U0001fadb', '🐈', '🤷\u200d♀', '👩🏿\u200d✈', '⛹🏾\u200d♂️', '🧘🏾\u200d♀', '🇾🇪', '🥬']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.59s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (2, 4, 6, 9):   0%|          | 0/100 [00:18<?, ?it/s]2025-01-22 04:59:58.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 04:59:58.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4897, 4901) -->  Mary got the football
2025-01-22 04:59:58.861 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 04:59:58.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9731, 9737) --> . Mary journeyed to the
2025-01-22 04:59:58.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 04:59:58.925 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1526, 1531) -->  John went to the kitchen
2025-01-22 04:59:58.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 04:59:59.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21478, 21483) --> . Daniel travelled to the
2025-01-22 04:59:59.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 04:59:59.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9101, 9106) --> . Sandra went to the
2025-01-22 04:59:59.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 04:59:59.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16960, 16965) --> . Sandra picked up the
2025-01-22 04:59:59.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 04:59:59.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8493, 8498) --> . John moved to the
2025-01-22 04:59:59.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 04:59:59.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4585, 4591) --> . Daniel journeyed to the
2025-01-22 04:59:59.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 04:59:59.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1525, 1530) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:00:02.029 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:00:02.029 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24180])
2025-01-22 05:00:10.434 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [216.565625, 9.303933806806434, 124.68810096153847, 9.00790604155446, 13.83447265625], 'topk_indices': array([    1, 24169,    23, 24033,     3, 24068,  4901, 24163, 24170,
       24179, 24168,    24, 24167,    14, 24034, 24176,  4900,     0,
       24178, 24175]), 'topk_tokens': ['<|start_header_id|>', ' the', '4', ' you', '<|end_header_id|>', '.\n\n', '.', ':', ' kitchen', '\n\n', ' before', '\n\n', ' football', '\n', ' context', '<|start_header_id|>', ' football', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [556.5, 112.01041666666667, 230.05, 56.6]}, 'weight': {'score': [22.44921875, 23.44165839639416, 20.41045673076923, 23.445745898413225, 29.63628472222222], 'topk_indices': array([18776, 18820, 14682, 14646, 14727, 19415, 19473, 14691, 14610,
       14545, 20335, 20287, 18145, 18114, 23538, 23672, 21988, 21961,
       23620, 23754]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 23.4265625, 21.01875]}, 'saliency': {'score': [1.4180755615234375, 0.051778553802322394, 0.6706496018629807, 0.04997979722926032, 0.10237969292534722], 'topk_indices': array([24172, 24164,  4928,  4920, 24149, 24033, 24068, 24078, 24179,
       24032,    24,  1530, 24177, 24162,  4897, 24168, 24170, 24167,
       24034,  4900]), 'topk_tokens': [' \n', ' Where', ' o', ' veto', ' return', ' you', '.\n\n', ' location', '\n\n', ' provided', '\n\n', ' kitchen', 'assistant', 'Question', ' Mary', ' before', ' kitchen', ' football', ' context', ' football'], 'evidence_proportions': [3.98193359375, 0.5767008463541666, 1.4609375, 0.33377685546875]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:00:10.439 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:00:10.439 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-30_pid-0_2-4-6-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (2, 4, 6, 9):   1%|          | 1/100 [00:30<49:33, 30.04s/it]is_0k: False
your chose emoji: ['🕊', '🤝', '👍', '🚶🏼\u200d♂', '🤹🏽\u200d♀', '🏄🏿\u200d♂', '🤵🏿\u200d♀️', '🤼\u200d♂', '👩🏿\u200d❤️\u200d💋\u200d👨🏿', '🚱']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.27s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.39s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (3, 4, 6, 7):   1%|          | 1/100 [00:46<49:33, 30.04s/it]2025-01-22 05:00:27.185 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:00:27.226 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7523, 7527) -->  Mary got the football
2025-01-22 05:00:27.227 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:00:27.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9719, 9725) --> . Mary journeyed to the
2025-01-22 05:00:27.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:00:27.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1550, 1555) -->  John went to the kitchen
2025-01-22 05:00:27.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:00:27.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16704, 16709) --> . Daniel travelled to the
2025-01-22 05:00:27.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:00:27.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9112, 9117) -->  Press. Sandra went to
2025-01-22 05:00:27.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 05:00:27.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16849, 16854) -->  affair. Sandra picked up
2025-01-22 05:00:27.516 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:00:27.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8489, 8494) --> . John moved to the
2025-01-22 05:00:27.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:00:27.586 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4554, 4560) -->  Daniel journeyed to the office
2025-01-22 05:00:27.586 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:00:27.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1549, 1554) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:00:30.374 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:00:30.374 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 05:00:38.807 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [393.371875, 18.98637876845457, 245.95072115384616, 18.43163326082453, 45.320793904049296], 'topk_indices': array([ 4610, 24172, 24066, 24032,  4620,  4572,  4617,  4568,  7525,
       24165,  4569,    14,  4570,  4571,  4621,  7527,  7526,  4618,
           0, 24173]), 'topk_tokens': ['.', ':', '.\n\n', ' context', ' A', ' Grow', ' Gal', 'ush', ' the', ' football', 'a', '\n', ' A', '.', '.', '.', ' football', 'ush', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [1081.625, 193.72916666666666, 348.925, 126.7875]}, 'weight': {'score': [22.44921875, 23.440518899135686, 22.837439903846153, 23.441990042987364, 29.529269366197184], 'topk_indices': array([18768, 18812, 14652, 14616, 19407, 19465, 14661, 14697, 14515,
       14580, 20327, 20279, 18106, 18137, 23522, 23656, 21961, 21988,
       23738, 23604]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 23.4265625, 21.01875]}, 'saliency': {'score': [2.5505859375, 0.10797848856669492, 1.4909855769230769, 0.10446449091697742, 0.3345455115949604], 'topk_indices': array([24171,  9114, 24076,  4569, 24030,  4570,  4554, 24168, 24160,
       14453,  7523,  7786,  7524, 24032,  4568,  4572,  4617, 24165,
        7526,  4618]), 'topk_tokens': ['Answer', ' Sandra', ' location', 'a', ' provided', ' A', ' Daniel', ' kitchen', 'Question', ' kitchen', ' Mary', ' Square', ' got', ' context', 'ush', ' Grow', ' Gal', ' football', ' football', 'ush'], 'evidence_proportions': [7.46484375, 1.0145263671875, 2.26884765625, 0.744189453125]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:00:38.813 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:00:38.814 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-30_pid-1_3-4-6-7.pkl | len: 3 |  size: 1.98 KB
Processing depth (3, 4, 6, 7):   2%|▏         | 2/100 [00:58<47:27, 29.06s/it]is_0k: False
your chose emoji: ['🥣', '🔪', '💑🏿', '😹', '👩🏼\u200d🦼\u200d➡', '🙇🏾', '👨\u200d🏭', '👩🏻\u200d❤️\u200d💋\u200d👨🏿', '🛳', '🤾🏽\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.51s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.16s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (0, 1, 3, 9):   2%|▏         | 2/100 [01:14<47:27, 29.06s/it]2025-01-22 05:00:55.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:00:55.678 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:00:55.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2995, 3001) -->  tragedy. Mary journeyed to
2025-01-22 05:00:55.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:00:55.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1707, 1712) -->  John went to the kitchen
2025-01-22 05:00:55.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:00:55.820 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21482, 21487) --> . Daniel travelled to the
2025-01-22 05:00:55.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:00:55.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9110, 9115) --> . Sandra went to the
2025-01-22 05:00:55.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 05:00:55.956 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16865, 16870) -->  affair. Sandra picked up
2025-01-22 05:00:55.956 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:00:56.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8502, 8507) --> . John moved to the
2025-01-22 05:00:56.002 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:00:56.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4601, 4607) --> . Daniel journeyed to the
2025-01-22 05:00:56.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:00:56.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1706, 1711) --> . John went to the
2025-01-22 05:00:56.038 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:00:56.038 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:00:56.038 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:00:56.038 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🛐', '☠', '🚣\u200d♀', '🤹🏽\u200d♀', '👩\u200d❤\u200d👩', '🏌🏻\u200d♂️', '👨🏽\u200d❤\u200d💋\u200d👨🏼', '🏌', '🏌️\u200d♀️', '🍟']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.64s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (5, 7, 8, 9):   2%|▏         | 2/100 [01:31<47:27, 29.06s/it]2025-01-22 05:01:12.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:01:12.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11925, 11929) -->  Mary got the football
2025-01-22 05:01:12.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:01:12.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16799, 16805) --> . Mary journeyed to the
2025-01-22 05:01:12.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:01:12.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1528, 1533) -->  John went to the kitchen
2025-01-22 05:01:12.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:01:12.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21448, 21453) --> . Daniel travelled to the
2025-01-22 05:01:12.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:01:12.899 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9090, 9095) --> . Sandra went to the
2025-01-22 05:01:12.899 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 05:01:12.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16980, 16985) --> . Sandra picked up the
2025-01-22 05:01:12.988 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:01:13.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8482, 8487) --> . John moved to the
2025-01-22 05:01:13.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:01:13.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4585, 4591) --> . Daniel journeyed to the
2025-01-22 05:01:13.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:01:13.065 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1527, 1532) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:01:15.893 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:01:15.893 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24184])
2025-01-22 05:01:24.304 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [330.31328125, 12.871552900318353, 210.67067307692307, 12.39553236713475, 17.97284100506757], 'topk_indices': array([24180, 24167, 24082, 24171, 24038, 11926, 24072,     9,    24,
       12018, 11927, 11929, 11975, 24178,    14, 11974, 24182, 24179,
           0, 11928]), 'topk_tokens': ['<|start_header_id|>', ':', ' location', ' football', ' context', ' got', '.\n\n', ':', '\n\n', 'ANT', ' the', '.', 'IC', ':', '\n', 'ANT', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1132.875, 152.5, 182.5, 49.453125]}, 'weight': {'score': [22.44921875, 23.442231363955845, 20.41045673076923, 23.446319280477198, 29.56228885135135], 'topk_indices': array([18762, 18806, 14645, 14609, 19419, 14690, 19477, 14654, 14573,
       14508, 20291, 20339, 18096, 18127, 23668, 23534, 21973, 22000,
       23750, 23616]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 23.4265625, 21.01875]}, 'saliency': {'score': [2.1973968505859376, 0.07295376503261049, 1.1552922175480769, 0.07002805104079289, 0.13594292305611275], 'topk_indices': array([24168,     8,    24, 24072, 24153, 24174, 24172, 24166, 11925,
        9095, 24082, 24177, 12018, 24038, 11926, 11975,  9091, 24171,
       11974, 11928]), 'topk_tokens': [' Where', ' Date', '\n\n', '.\n\n', ' return', ' kitchen', ' before', 'Question', ' Mary', ' garden', ' location', 'Answer', 'ANT', ' context', ' got', 'IC', ' Sandra', ' football', 'ANT', ' football'], 'evidence_proportions': [7.91455078125, 0.8507486979166666, 1.156494140625, 0.28055419921875]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:01:24.317 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:01:24.317 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-30_pid-2_5-7-8-9.pkl | len: 3 |  size: 2.02 KB
Processing depth (5, 7, 8, 9):   3%|▎         | 3/100 [01:43<59:07, 36.57s/it]is_0k: False
your chose emoji: ['▶️', '😭', '👮🏿\u200d♂️', '👨🏾\u200d❤\u200d💋\u200d👨🏻', '👶🏻', '🏃🏿', '👩🏿\u200d🦼', '🧏🏻', '🚶🏽', '💂🏿\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.68s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.30s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (2, 4, 5, 9):   3%|▎         | 3/100 [02:00<59:07, 36.57s/it]2025-01-22 05:01:41.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:01:41.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4971, 4975) -->  Mary got the football
2025-01-22 05:01:41.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:01:41.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9703, 9709) --> . Mary journeyed to the
2025-01-22 05:01:41.433 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:01:41.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1702, 1707) -->  John went to the kitchen
2025-01-22 05:01:41.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:01:41.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21436, 21441) --> . Daniel travelled to the
2025-01-22 05:01:41.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:01:41.605 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9073, 9078) --> . Sandra went to the
2025-01-22 05:01:41.605 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 05:01:41.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16857, 16862) -->  affair. Sandra picked up
2025-01-22 05:01:41.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:01:41.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8465, 8470) --> . John moved to the
2025-01-22 05:01:41.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:01:41.759 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4695, 4701) -->  the senate. Daniel journeyed
2025-01-22 05:01:41.759 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:01:41.768 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1701, 1706) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:01:44.546 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:01:44.546 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 05:01:52.915 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [377.259375, 15.677174250258531, 227.17487980769232, 15.149569112893198, 27.273983226102942], 'topk_indices': array([24165, 12009, 24025, 24060, 24166, 24155, 24070, 24162,  4975,
        4973, 11966, 11965, 24160,    14, 24170, 24159, 24026, 24167,
        4974,     0]), 'topk_tokens': ['Answer', 'ANT', ' you', '.\n\n', ':', ':', ' location', ' kitchen', '.', ' the', 'IC', 'ANT', ' before', '\n', '<|end_header_id|>', ' football', ' context', '<|eot_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [1057.25, 209.77083333333334, 304.6, 106.9125]}, 'weight': {'score': [22.44921875, 23.439738883143743, 22.27794471153846, 23.44181178509263, 29.659007352941178], 'topk_indices': array([18776, 18820, 14682, 14646, 19473, 14727, 14691, 19415, 14610,
       14545, 20287, 20335, 18145, 18114, 23656, 23522, 21961, 21988,
       23604, 23738]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 23.4265625, 21.01875]}, 'saliency': {'score': [2.4269287109375, 0.08914801738624613, 1.3276508037860577, 0.08587574392620136, 0.20269853928509882], 'topk_indices': array([24156, 24060, 24141, 24025,  9074,  4699, 12009,  4971,  4972,
       24154, 11966, 24024, 24070, 24165, 24160, 11965, 24162, 24026,
       24159,  4974]), 'topk_tokens': [' Where', '.\n\n', ' return', ' you', ' Sandra', ' journey', 'ANT', ' Mary', ' got', 'Question', 'IC', ' provided', ' location', 'Answer', ' before', 'ANT', ' kitchen', ' context', ' football', ' football'], 'evidence_proportions': [7.28076171875, 1.1131998697916667, 1.91796875, 0.629296875]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:01:52.922 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:01:52.922 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-30_pid-3_2-4-5-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (2, 4, 5, 9):   4%|▍         | 4/100 [02:12<53:28, 33.42s/it]is_0k: False
your chose emoji: ['🏊🏼\u200d♀', '🚣🏼\u200d♂️', '🎭', '🏃🏽\u200d♂', '🍆', '🥗', '🏌\u200d♂', '👨🏻\u200d🚀', '🧍🏽\u200d♀️', '👨\u200d🦰']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (3, 5, 7, 8):   4%|▍         | 4/100 [02:29<53:28, 33.42s/it]2025-01-22 05:02:10.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:02:10.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7625, 7629) -->  Mary got the football
2025-01-22 05:02:10.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:02:10.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11985, 11991) --> . Mary journeyed to the
2025-01-22 05:02:10.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:02:10.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1702, 1707) -->  John went to the kitchen
2025-01-22 05:02:10.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:02:10.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19448, 19453) --> . Daniel travelled to the
2025-01-22 05:02:10.361 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:02:10.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9149, 9154) --> . Sandra went to the
2025-01-22 05:02:10.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the milk.
2025-01-22 05:02:10.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17114, 17119) --> . Sandra picked up the
2025-01-22 05:02:10.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:02:10.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8593, 8598) --> . John moved to the
2025-01-22 05:02:10.547 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:02:10.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4687, 4693) -->  the senate. Daniel journeyed
2025-01-22 05:02:10.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:02:10.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1701, 1706) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:02:13.395 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:02:13.395 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24192])
2025-01-22 05:02:21.810 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [491.0234375, 19.504926121099402, 265.99759615384613, 18.849032309826494, 35.29041466346154], 'topk_indices': array([24175,  4696, 24186,  4691,  1707, 24080,    14,  7627,  7625,
        7626, 24090, 24046, 24180, 24182, 24188,  7629, 24179, 24187,
           0,  7628]), 'topk_tokens': [':', '.', ':', ' journey', '.', '.\n\n', '\n', ' the', ' Mary', ' got', ' location', ' context', ' before', ' kitchen', '<|start_header_id|>', '.', ' football', '<|eot_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1472.5, 250.83333333333334, 443.675, 41.41875]}, 'weight': {'score': [22.44921875, 23.449212130605495, 21.31971153846154, 23.452333041740857, 30.331129807692307], 'topk_indices': array([18788, 18744, 14600, 14564, 14645, 19383, 14609, 19441, 14463,
       14528, 20267, 20315, 18095, 18064, 23670, 23536, 21938, 21911,
       23618, 23752]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 23.4265625, 21.01875]}, 'saliency': {'score': [3.2667083740234375, 0.1111894607938107, 1.5450721153846154, 0.10703229787675603, 0.27140534229767627], 'topk_indices': array([ 7629, 24080, 24086,  4695, 24185,  1706, 16891, 24161, 24176,
       24174, 24044,  4691,  7626, 24090,  7625, 24180, 24046, 24182,
       24179,  7628]), 'topk_tokens': ['.', '.\n\n', ' item', ' office', 'Answer', ' kitchen', ' kitchen', ' return', ' Where', 'Question', ' provided', ' journey', ' got', ' location', ' Mary', ' before', ' context', ' kitchen', ' football', ' football'], 'evidence_proportions': [10.43408203125, 1.358642578125, 2.8494140625, 0.23978271484375]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:02:21.819 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:02:21.819 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-30_pid-4_3-5-7-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (3, 5, 7, 8):   5%|▌         | 5/100 [02:41<50:20, 31.79s/it]Processing depth (3, 5, 7, 8):   5%|▌         | 5/100 [02:41<51:12, 32.34s/it]
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:82 - Selected idx: 31
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:84 - Answer: garden
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:86 - Needle: [' Mary journeyed to the garden.', ' Mary got the football.', ' Sandra went to the garden.', ' Daniel journeyed to the office.', ' Mary went to the kitchen.', ' Daniel travelled to the garden.', ' John moved to the office.', ' Sandra picked up the milk.', ' John went to the kitchen.', ' Mary put down the football.', ' Daniel travelled to the garden.']
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the garden.', ' Mary got the football.', ' Mary went to the kitchen.', ' Mary put down the football.', ' Daniel travelled to the garden.']
2025-01-22 05:02:22.104 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👉🏾', '👩🏽\u200d❤\u200d💋\u200d👨🏾', '🇬🇭', '👱🏼\u200d♀️', '🛋️', '⌨️', '💇🏿\u200d♀️', '🏄🏼\u200d♂', '🤽\u200d♀', '🏊🏻\u200d♂️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.57s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.02s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.84s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]
Processing depth (1, 2, 4, 5, 7):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:02:39.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:02:39.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Mary journeyed to the
2025-01-22 05:02:40.000 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:02:40.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5001, 5005) -->  Mary got the football
2025-01-22 05:02:40.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:02:40.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7965, 7970) -->  went to the kitchen.
2025-01-22 05:02:40.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:02:40.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11916, 11921) --> . Mary put down the
2025-01-22 05:02:40.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel travelled to the garden.
2025-01-22 05:02:40.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16786, 16791) --> . Daniel travelled to the
2025-01-22 05:02:40.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:02:40.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (23107, 23112) -->  Sandra went to the garden
2025-01-22 05:02:40.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:02:40.418 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10269, 10275) --> . Daniel journeyed to the
2025-01-22 05:02:40.418 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:02:40.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6583, 6588) --> . John moved to the
2025-01-22 05:02:40.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:02:40.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (809, 814) --> . Sandra picked up the
2025-01-22 05:02:40.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:02:40.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7964, 7969) -->  John went to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:02:43.166 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:02:43.167 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 05:02:51.551 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [460.605, 17.010890777300514, 275.72836538461536, 16.273483092587227, 21.958292128164558], 'topk_indices': array([ 4999, 24183,  5003, 24088, 11918, 11919,    24,     9,  5000,
       24204,  4965,    14, 11921,  5002,  5001, 24194, 24207,  5005,
        5004,     0]), 'topk_tokens': [' St', ':', ' the', '.\n\n', ' put', ' down', '\n\n', ':', '.', '<|eot_id|>', '4', '\n', ' football', ' got', ' Mary', ' football', '<|end_header_id|>', '.', ' football', '<|begin_of_text|>'], 'evidence_proportions': [366.2916666666667, 986.375, 359.775, 538.95, 175.65]}, 'weight': {'score': [21.5415625, 23.443625578225674, 22.19951923076923, 23.446932491929143, 29.4501582278481], 'topk_indices': array([18782, 18738, 14602, 14638, 14683, 19377, 19435, 14647, 14501,
       14566, 20285, 20333, 18076, 18107, 23654, 23520, 21968, 21941,
       23602, 23736]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.611328125, 19.565625, 21.771875, 21.01875]}, 'saliency': {'score': [2.7616796875, 0.09719914679446143, 1.7234262319711537, 0.09269213476134058, 0.15817280057110364], 'topk_indices': array([ 4961, 24188,     0,  3039,  4964, 11174,  4965,  7968,  4999,
       23111,  3043, 24182, 11919, 11918, 24196,  5002, 11921,  5001,
       24194,  5004]), 'topk_tokens': [' *\n\n', ' prior', '<|begin_of_text|>', ' journey', '185', 'coop', '4', ' kitchen', ' St', ' garden', ' garden', 'Question', ' down', ' put', ' discarded', ' got', ' football', ' Mary', ' football', ' football'], 'evidence_proportions': [1.9410807291666667, 6.90283203125, 1.94716796875, 3.0298828125, 0.97978515625]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 05:02:51.564 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:02:51.564 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-31_pid-0_1-2-4-5-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 4, 5, 7):   1%|          | 1/100 [00:29<48:24, 29.33s/it]is_0k: False
your chose emoji: ['🧘🏿', '👨🏾\u200d❤️\u200d👨🏻', '👨🏽\u200d❤\u200d💋\u200d👨🏾', '🦖', '🤚🏽', '🗃', '🧎🏿', '🚴🏾\u200d♀️', '🧙\u200d♀️', '👧🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.75s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (0, 1, 4, 6, 8):   1%|          | 1/100 [00:46<48:24, 29.33s/it]2025-01-22 05:03:09.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:03:09.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the garden.
2025-01-22 05:03:09.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:03:09.217 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2969, 2973) -->  Mary got the football
2025-01-22 05:03:09.217 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:03:09.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7718, 7723) -->  John went to the kitchen
2025-01-22 05:03:09.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:03:09.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14354, 14359) --> . Mary put down the
2025-01-22 05:03:09.325 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel travelled to the garden.
2025-01-22 05:03:09.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19189, 19194) -->  Daniel travelled to the garden
2025-01-22 05:03:09.419 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:03:09.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22918, 22923) --> . Sandra went to the
2025-01-22 05:03:09.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:03:09.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10077, 10083) --> . Daniel journeyed to the
2025-01-22 05:03:09.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:03:09.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6395, 6400) --> . John moved to the
2025-01-22 05:03:09.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:03:09.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (816, 821) --> . Sandra picked up the
2025-01-22 05:03:09.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:03:09.685 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7717, 7722) --> . John went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:03:12.368 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:03:12.368 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 05:03:20.768 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [262.5975, 12.569898048911694, 134.59615384615384, 12.17912995023844, 15.232840401785714], 'topk_indices': array([14360, 24135, 24155,  2971, 24008, 24042, 14359, 24137, 24142,
       10358, 24143,    14, 24161,  2973, 24148,  2972,    23,     0,
       24158, 24159]), 'topk_tokens': ['.', '.\n\n', '?\n', ' the', ' context', '.\n\n', ' football', ':', ' prior', 't', ' to', '\n', '<|end_header_id|>', '.', ' football', ' football', '4', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [263.1458333333333, 547.375, 112.5625, 301.375, 145.375]}, 'weight': {'score': [23.1940625, 23.432260200281387, 20.41045673076923, 23.43576514876633, 29.45703125], 'topk_indices': array([18758, 18802, 14634, 14598, 19403, 14679, 19461, 14643, 14562,
       14497, 20323, 20275, 18096, 18127, 23498, 23632, 21946, 21919,
       23580, 23714]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.958333333333332, 26.611328125, 23.4265625, 21.771875, 25.5328125]}, 'saliency': {'score': [1.599619140625, 0.07044953233623687, 0.7881188025841346, 0.06809047609598538, 0.11040714808872767], 'topk_indices': array([24052,  2970,    22, 24156, 10358, 24146, 24150,    19,   817,
       24123,     0, 22923,    20, 24008,    23, 24136, 14359, 24142,
        2972, 24148]), 'topk_tokens': [' location', ' got', '202', 'Answer', 't', ' where', ' discarded', '26', ' Sandra', ' return', '<|begin_of_text|>', ' garden', ' Jul', ' context', '4', 'Question', ' football', ' prior', ' football', ' football'], 'evidence_proportions': [1.4349365234375, 3.6904296875, 0.69560546875, 1.638818359375, 0.989404296875]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 100}
2025-01-22 05:03:20.775 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:03:20.775 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-31_pid-1_0-1-4-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 4, 6, 8):   2%|▏         | 2/100 [00:58<47:47, 29.26s/it]is_0k: False
your chose emoji: ['🙍🏼', '🥷🏼', '🧛\u200d♀️', '🔄', '💆🏾', '🪀', '🏃🏽\u200d♀️', '🎖', '🦶🏽', '🧑🏽\u200d🎨']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.20s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.52s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]
Processing depth (1, 4, 6, 7, 8):   2%|▏         | 2/100 [01:14<47:47, 29.26s/it]2025-01-22 05:03:37.381 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:03:37.398 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Mary journeyed to the
2025-01-22 05:03:37.399 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:03:37.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9707, 9711) -->  Mary got the football
2025-01-22 05:03:37.447 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:03:37.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7791, 7796) -->  John went to the kitchen
2025-01-22 05:03:37.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:03:37.574 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16768, 16773) --> . Mary put down the
2025-01-22 05:03:37.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel travelled to the garden.
2025-01-22 05:03:37.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19251, 19256) -->  Daniel travelled to the garden
2025-01-22 05:03:37.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:03:37.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22930, 22935) --> . Sandra went to the
2025-01-22 05:03:37.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:03:37.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10115, 10121) --> . Daniel journeyed to the
2025-01-22 05:03:37.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:03:37.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6450, 6455) --> . John moved to the
2025-01-22 05:03:37.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:03:37.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (809, 814) --> . Sandra picked up the
2025-01-22 05:03:37.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:03:37.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7789, 7794) -->  city. John went to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:03:40.551 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:03:40.551 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 05:03:48.904 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [331.4775, 12.952671670937345, 164.1953125, 12.460005306481673, 17.233632262323944], 'topk_indices': array([14373, 24167, 24173,  3043,    23,    14, 24072,    24,  9711,
       10116, 22806, 16774,  3039, 24191, 16773, 24178,  9710,     0,
       24188, 24189]), 'topk_tokens': [' kitchen', ':', ' to', ' garden', '4', '\n', '.\n\n', '\n\n', '.', ' Daniel', 'nes', '.', ' journey', '<|end_header_id|>', ' football', ' football', ' football', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [383.5, 507.5625, 151.1625, 324.125, 315.85]}, 'weight': {'score': [23.2165625, 23.44017089601587, 21.06280048076923, 23.442962440463866, 29.538292253521128], 'topk_indices': array([18760, 18804, 14648, 14612, 14693, 19463, 14657, 19405, 14576,
       14511, 20335, 20287, 18098, 18129, 23644, 23510, 21958, 21931,
       23726, 23592]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.611328125, 23.4265625, 21.771875, 25.5328125]}, 'saliency': {'score': [2.075537109375, 0.07320267457689288, 0.9869103064903846, 0.07014552985543072, 0.12853144256161972], 'topk_indices': array([24072, 16770, 16771, 24082, 10117,     0, 24153, 19255, 24180,
       19252, 24172, 24166,  3043, 14373, 22806, 10116,  3039, 16773,
       24178,  9710]), 'topk_tokens': ['.\n\n', ' put', ' down', ' location', ' journey', '<|begin_of_text|>', ' return', ' garden', ' discarded', ' travelled', ' prior', 'Question', ' garden', ' kitchen', 'nes', ' Daniel', ' journey', ' football', ' football', ' football'], 'evidence_proportions': [2.1145833333333335, 3.600341796875, 0.981884765625, 1.775390625, 2.20263671875]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:03:48.909 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:03:48.909 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-31_pid-2_1-4-6-7-8.pkl | len: 3 |  size: 2.12 KB
Processing depth (1, 4, 6, 7, 8):   3%|▎         | 3/100 [01:26<46:28, 28.75s/it]is_0k: False
your chose emoji: ['🐓', '☯️', '⛹\u200d♀', '🤲🏾', '🧎🏽\u200d➡', '👳🏽\u200d♀', '💟', '🤱🏽', '🧑🏽\u200d🎄', '👩🏼\u200d❤️\u200d💋\u200d👨🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.13s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.38s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (1, 2, 3, 4, 5):   3%|▎         | 3/100 [01:43<46:28, 28.75s/it]2025-01-22 05:04:06.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:04:06.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2960, 2966) -->  tragedy. Mary journeyed to
2025-01-22 05:04:06.041 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:04:06.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4959, 4963) -->  Mary got the football
2025-01-22 05:04:06.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:04:06.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7631, 7636) --> . Mary went to the
2025-01-22 05:04:06.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:04:06.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9865, 9870) --> . Mary put down the
2025-01-22 05:04:06.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel travelled to the garden.
2025-01-22 05:04:06.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (12000, 12005) --> . Daniel travelled to the
2025-01-22 05:04:06.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:04:06.367 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22980, 22985) --> . Sandra went to the
2025-01-22 05:04:06.367 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:04:06.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10329, 10335) --> . Daniel journeyed to the
2025-01-22 05:04:06.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:04:06.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6557, 6562) --> . John moved to the
2025-01-22 05:04:06.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:04:06.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (809, 814) --> . Sandra picked up the
2025-01-22 05:04:06.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:04:06.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7632, 7637) -->  Mary went to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:04:09.131 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:04:09.132 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24223])
2025-01-22 05:04:17.527 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [190.2175, 9.46766903327004, 141.91706730769232, 9.138302740434334, 8.965173055959303], 'topk_indices': array([   23, 24196, 24112,  2967, 24222, 24215, 24208, 24102, 24217,
           9, 24197,  7636,    24,  4962,     1,    14, 24221,     0,
       24218, 24219]), 'topk_tokens': ['4', 'Question', ' location', ' garden', '\n\n', '?\n', ' football', '.\n\n', ':', ':', ':', ' kitchen', '\n\n', ' football', '<|start_header_id|>', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [180.34375, 294.78125, 162.025, 191.15, 145.675]}, 'weight': {'score': [22.670625, 23.448562494840253, 21.478665865384617, 23.451485586866596, 29.656613372093023], 'topk_indices': array([18852, 18808, 14634, 14670, 19505, 19447, 14715, 14679, 14533,
       14598, 20379, 20331, 18177, 18146, 23560, 23694, 21993, 22020,
       23776, 23642]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.891927083333332, 26.611328125, 19.403125, 21.771875, 21.01875]}, 'saliency': {'score': [1.1135546875, 0.05316081488348004, 0.8816903921274039, 0.05117315755881593, 0.06805885669796966], 'topk_indices': array([22985, 24215, 24198, 24201,  2963, 24102,     0,   810,    24,
       24220, 24216, 24112,  9870, 24210, 24202,  2967, 24196, 24208,
        7636,  4962]), 'topk_tokens': [' garden', '?\n', ' Where', ' location', ' journey', '.\n\n', '<|begin_of_text|>', ' Sandra', '\n\n', 'assistant', 'Answer', ' location', ' football', ' discarded', ' prior', ' garden', 'Question', ' football', ' kitchen', ' football'], 'evidence_proportions': [1.0052490234375, 2.0675048828125, 0.831787109375, 1.042041015625, 0.833642578125]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:04:17.545 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:04:17.545 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-31_pid-3_1-2-3-4-5.pkl | len: 3 |  size: 2.12 KB
Processing depth (1, 2, 3, 4, 5):   4%|▍         | 4/100 [01:55<45:55, 28.70s/it]is_0k: False
your chose emoji: ['🏃\u200d➡', '🧑🏽\u200d🔬', '🧚🏾\u200d♂', '👩🏽\u200d❤️\u200d💋\u200d👨🏿', '👩🏻\u200d❤\u200d👩🏿', '🌂', '👋🏾', '👖', '\U0001faf1🏻', '🙍🏼\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.43s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (1, 2, 3, 6, 9):   4%|▍         | 4/100 [02:11<45:55, 28.70s/it]2025-01-22 05:04:34.447 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:04:34.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Mary journeyed to the
2025-01-22 05:04:34.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:04:34.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4961, 4965) -->  Mary got the football
2025-01-22 05:04:34.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:04:34.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7592, 7597) -->  war. Mary went to
2025-01-22 05:04:34.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:04:34.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14336, 14341) --> . Mary put down the
2025-01-22 05:04:34.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel travelled to the garden.
2025-01-22 05:04:34.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21458, 21463) --> . Daniel travelled to the
2025-01-22 05:04:34.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:04:34.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22952, 22957) --> . Sandra went to the
2025-01-22 05:04:34.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:04:34.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10185, 10191) --> . Daniel journeyed to the
2025-01-22 05:04:34.896 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John moved to the office.
2025-01-22 05:04:34.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6539, 6544) --> . John moved to the
2025-01-22 05:04:34.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:04:34.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (809, 814) --> . Sandra picked up the
2025-01-22 05:04:34.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:04:34.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7594, 7599) -->  Mary went to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:04:37.832 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:04:37.832 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24183])
2025-01-22 05:04:47.533 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [347.30375, 13.834811461175887, 170.81730769230768, 13.320277864097783, 12.623416785037879], 'topk_indices': array([24167, 24143, 24163, 24170, 24155,  3039, 24062, 24156,     9,
          23, 24157, 14342, 14341,    14, 24181, 24168, 24178,  4965,
        4964,     0]), 'topk_tokens': [' the', ' return', ' to', ' discarded', '.\n\n', ' journey', '.\n\n', 'Question', ':', '4', ':', '.', ' football', '\n', '<|end_header_id|>', ' football', '<|eot_id|>', '.', ' football', '<|begin_of_text|>'], 'evidence_proportions': [361.4583333333333, 685.5625, 215.28125, 379.6, 159.4375]}, 'weight': {'score': [22.201875, 23.438750723559085, 21.478665865384617, 23.44214347679718, 29.740293560606062], 'topk_indices': array([18824, 18780, 14620, 14656, 14701, 19419, 19477, 14665, 14584,
       14519, 20339, 20291, 18098, 18129, 23532, 23666, 21965, 21992,
       23614, 23748]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.611328125, 22.8671875, 21.771875, 21.01875]}, 'saliency': {'score': [2.082138671875, 0.0788904705976598, 1.0631314791165865, 0.075755128925672, 0.09520576939438329], 'topk_indices': array([ 4965, 24155, 24062,  3043,    20, 24176, 24028, 14339,  4962,
          23,  4961, 24143, 24162,  7598,  3039, 24170, 24156, 14341,
       24168,  4964]), 'topk_tokens': ['.', '.\n\n', '.\n\n', ' garden', ' Jul', 'Answer', ' context', ' down', ' got', '4', ' Mary', ' return', ' prior', ' kitchen', ' journey', ' discarded', 'Question', ' football', ' football', ' football'], 'evidence_proportions': [1.994873046875, 4.787109375, 1.199755859375, 2.0701171875, 0.91728515625]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:04:47.538 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:04:47.538 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-31_pid-4_1-2-3-6-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 3, 6, 9):   5%|▌         | 5/100 [02:25<46:10, 29.17s/it]Processing depth (1, 2, 3, 6, 9):   5%|▌         | 5/100 [02:25<46:05, 29.11s/it]
2025-01-22 05:04:47.798 | INFO     | __main__:<module>:82 - Selected idx: 32
2025-01-22 05:04:47.798 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the kitchen? 
2025-01-22 05:04:47.798 | INFO     | __main__:<module>:84 - Answer: garden
2025-01-22 05:04:47.798 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:04:47.799 | INFO     | __main__:<module>:86 - Needle: [' Mary got the football.', ' Sandra went to the garden.', ' John moved to the office.', ' Mary journeyed to the garden.', ' Daniel travelled to the garden.', ' Sandra picked up the milk.', ' Mary went to the kitchen.', ' Daniel journeyed to the office.']
2025-01-22 05:04:47.799 | INFO     | __main__:<module>:87 - Real Needle: [' Mary got the football.', ' Mary journeyed to the garden.', ' Mary went to the kitchen.', ' Daniel journeyed to the office.']
2025-01-22 05:04:47.799 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['〽️', '🕺🏿', '🐭', '🙇🏾\u200d♀️', '🏃🏽\u200d♀️\u200d➡', '🤵🏽\u200d♀', '👩🏽\u200d🦳', '🗂️', '🧜🏿\u200d♀', '🍝']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.96s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]
Processing depth (0, 7, 8, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:05:05.586 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:05:05.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:05:05.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16788, 16794) --> . Mary journeyed to the
2025-01-22 05:05:05.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:05:05.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19242, 19247) -->  Mary went to the kitchen
2025-01-22 05:05:05.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:05:06.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21473, 21479) --> . Daniel journeyed to the
2025-01-22 05:05:06.025 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:05:06.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13146, 13151) --> . Sandra went to the
2025-01-22 05:05:06.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:05:06.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17543, 17548) --> . John moved to the
2025-01-22 05:05:06.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:05:06.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16668, 16673) --> . Daniel travelled to the
2025-01-22 05:05:06.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:05:06.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3125, 3130) -->  ranks. Sandra picked up
2025-01-22 05:05:06.328 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:05:06.328 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:05:06.328 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:05:06.328 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['👷\u200d♀', '👩\u200d👧\u200d👧', '👩🏻\u200d🦼\u200d➡️', '👩🏽\u200d❤\u200d👨🏻', '🧑🏼\u200d🚒', '💁🏽\u200d♀', '⛽', '🧎\u200d➡', '🚶🏾\u200d♀\u200d➡️', '🧘🏾\u200d♂️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.38s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (3, 6, 7, 8):   0%|          | 0/100 [00:35<?, ?it/s]2025-01-22 05:05:23.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:05:23.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7574, 7578) -->  Mary got the football
2025-01-22 05:05:23.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:05:23.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14376, 14382) --> . Mary journeyed to the
2025-01-22 05:05:23.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:05:23.742 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16805, 16810) --> . Mary went to the
2025-01-22 05:05:23.742 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:05:23.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19358, 19364) -->  Daniel journeyed to the office
2025-01-22 05:05:23.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:05:23.903 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13020, 13025) --> . Sandra went to the
2025-01-22 05:05:23.903 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:05:23.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17563, 17568) --> . John moved to the
2025-01-22 05:05:23.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:05:24.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16693, 16698) --> . Daniel travelled to the
2025-01-22 05:05:24.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:05:24.086 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3080, 3085) -->  ranks. Sandra picked up
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:05:27.295 | INFO     | test_jbb_embedding:begin_test:693 - The football was in Mary's possession before she went to the kitchen.<|eot_id|>
2025-01-22 05:05:27.295 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 05:05:35.705 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [371.2872023809524, 15.570444060573461, 212.746875, 15.0974011003813, 26.418658088235293], 'topk_indices': array([ 7575, 16811,  7576, 24018, 24019,  7574, 24156, 24064, 24164,
       24154, 24162, 24160, 24054,  7578,    14, 24153, 24020,  7577,
           0, 24161]), 'topk_tokens': [' got', '.', ' the', ' provided', ' you', ' Mary', ' kitchen', ' location', '<|end_header_id|>', ' before', '<|start_header_id|>', ':', '.\n\n', '.', '\n', ' football', ' context', ' football', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [1019.625, 186.72916666666666, 376.8, 119.02604166666667]}, 'weight': {'score': [22.165178571428573, 23.43609323927345, 22.251171875, 23.438181586227618, 28.909466911764707], 'topk_indices': array([18763, 18807, 14657, 14621, 19473, 14702, 19415, 14666, 14585,
       14520, 20335, 20287, 18101, 18132, 23658, 23524, 21984, 21957,
       23606, 23740]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 19.403125, 23.615885416666668]}, 'saliency': {'score': [2.3008045014880953, 0.08888120823679094, 1.31275634765625, 0.08594155754271489, 0.19089508056640625], 'topk_indices': array([13021, 24150, 24019, 24060, 24135,  3082, 13025, 24054, 24148,
        7575, 24159, 16810, 24154, 24018, 24064,  7574, 24156, 24153,
       24020,  7577]), 'topk_tokens': [' Sandra', ' Where', ' you', ' item', ' return', ' Sandra', ' garden', '.\n\n', 'Question', ' got', 'Answer', ' kitchen', ' before', ' provided', ' location', ' Mary', ' kitchen', ' football', ' context', ' football'], 'evidence_proportions': [7.1982421875, 1.0174560546875, 1.8642578125, 0.6829833984375]}}, 'pred_res': "The football was in Mary's possession before she went to the kitchen.<|eot_id|>", 'score': 0}
2025-01-22 05:05:35.714 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:05:35.714 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-32_pid-0_3-6-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (3, 6, 7, 8):   1%|          | 1/100 [00:47<1:18:50, 47.78s/it]is_0k: False
your chose emoji: ['👩🏻\u200d🤝\u200d👩🏾', '🥷🏾', '👩🏻\u200d❤\u200d👨🏾', '🕕', '🤸🏾\u200d♀', '🏴', '👩\u200d🎨', '🍵', '🔮', '🚶']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.63s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]
Processing depth (0, 1, 2, 5):   1%|          | 1/100 [01:04<1:18:50, 47.78s/it]2025-01-22 05:05:53.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:05:53.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:05:53.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2973, 2979) -->  tragedy. Mary journeyed to
2025-01-22 05:05:53.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:05:53.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4924, 4929) -->  Mary went to the kitchen
2025-01-22 05:05:53.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:05:53.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11907, 11913) --> . Daniel journeyed to the
2025-01-22 05:05:53.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:05:53.368 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13026, 13031) --> . Sandra went to the
2025-01-22 05:05:53.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:05:53.457 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17518, 17523) --> . John moved to the
2025-01-22 05:05:53.458 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:05:53.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16692, 16697) --> . Daniel travelled to the
2025-01-22 05:05:53.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:05:53.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3197, 3202) --> . Sandra picked up the
2025-01-22 05:05:53.557 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:05:53.557 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:05:53.557 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:05:53.557 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🧑🏼\u200d🦽', '👨🏼\u200d🚀', '👌🏼', '🙇🏼\u200d♂️', '🌶', '🧎🏿', '🦸\u200d♂️', '👩🏾\u200d🏭', '💡', '👱🏼\u200d♀️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.51s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.54s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.78s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (1, 2, 5, 9):   1%|          | 1/100 [01:22<1:18:50, 47.78s/it]2025-01-22 05:06:10.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:06:10.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2956, 2960) -->  Mary got the football
2025-01-22 05:06:10.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:06:10.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4856, 4862) --> . Mary journeyed to the
2025-01-22 05:06:10.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:06:10.949 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11817, 11822) --> . Mary went to the
2025-01-22 05:06:10.949 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:06:11.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21465, 21471) --> . Daniel journeyed to the
2025-01-22 05:06:11.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:06:11.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12961, 12966) --> . Sandra went to the
2025-01-22 05:06:11.144 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:06:11.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17505, 17510) --> . John moved to the
2025-01-22 05:06:11.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:06:11.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16621, 16626) --> . Daniel travelled to the
2025-01-22 05:06:11.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:06:11.340 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3176, 3181) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:06:14.144 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:06:14.144 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24190])
2025-01-22 05:06:22.540 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [324.203125, 13.370441605009713, 123.90625, 13.00864123571547, 15.583251953125], 'topk_indices': array([   23,  2956,     9, 24173, 24078, 24189, 24044,  7523,  2955,
       24178, 24177,    14,  2958,  2957,  2960, 24186, 24185, 24188,
           0,  2959]), 'topk_tokens': ['4', ' Mary', ':', ':', '.\n\n', '\n\n', ' context', ' or', '.', ' before', ' football', '\n', ' the', ' got', '.', '<|start_header_id|>', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1192.75, 181.71875, 143.2, 38.4921875]}, 'weight': {'score': [21.209449404761905, 23.44656253875088, 21.04765625, 23.450494200790825, 29.6712890625], 'topk_indices': array([18819, 18775, 14639, 14675, 19414, 14684, 19472, 14720, 14603,
       14538, 20286, 20334, 18144, 18113, 23538, 23672, 21949, 21976,
       23754, 23620]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 19.403125, 20.270833333333332]}, 'saliency': {'score': [2.068150111607143, 0.07596781731479664, 0.75797119140625, 0.07367086900153197, 0.11722149848937988], 'topk_indices': array([   23,     8, 24159, 24189, 24078, 24183, 24187,  3177, 24174,
        7523, 24042, 24180, 24172, 24088, 24178,  2956, 24044, 24177,
        2957,  2959]), 'topk_tokens': ['4', ' Date', ' return', '\n\n', '.\n\n', 'Answer', 'assistant', ' Sandra', ' Where', ' or', ' provided', ' kitchen', 'Question', ' location', ' before', ' Mary', ' context', ' football', ' got', ' football'], 'evidence_proportions': [8.1416015625, 1.0126546223958333, 0.726708984375, 0.19254557291666666]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:06:22.547 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:06:22.547 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-32_pid-1_1-2-5-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 5, 9):   2%|▏         | 2/100 [01:34<1:17:07, 47.22s/it]is_0k: False
your chose emoji: ['🤲🏿', '🛃', '👷\u200d♀️', '\U0001faf0', '🧑🏽\u200d🎄', '👩🏻\u200d❤\u200d💋\u200d👨🏾', '👩🏾\u200d🦼\u200d➡️', '☀️', '👨🏽\u200d💻', '🤷🏾\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (0, 1, 3, 9):   2%|▏         | 2/100 [01:51<1:17:07, 47.22s/it]2025-01-22 05:06:39.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:06:39.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:06:39.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3036, 3042) --> . Mary journeyed to the
2025-01-22 05:06:39.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:06:39.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7625, 7630) --> . Mary went to the
2025-01-22 05:06:39.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:06:39.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21425, 21431) --> . Daniel journeyed to the
2025-01-22 05:06:39.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:06:39.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13143, 13148) --> . Sandra went to the
2025-01-22 05:06:39.946 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:06:40.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17537, 17542) --> . John moved to the
2025-01-22 05:06:40.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:06:40.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16673, 16678) --> . Daniel travelled to the
2025-01-22 05:06:40.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:06:40.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3227, 3232) --> . Sandra picked up the
2025-01-22 05:06:40.142 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:06:40.142 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:06:40.142 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:06:40.142 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['💂🏼\u200d♂', '👨🏽\u200d🎤', '💇\u200d♀', '👨🏽\u200d✈️', '🦑', '🌑', '🧍🏾\u200d♀️', '🙍🏻\u200d♂', '\U0001f6dc', '🧑\u200d🎓']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.99s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.21s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.41s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.53s/it]
Processing depth (2, 5, 6, 9):   2%|▏         | 2/100 [02:08<1:17:07, 47.22s/it]2025-01-22 05:06:56.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:06:56.746 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4876, 4880) -->  Mary got the football
2025-01-22 05:06:56.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:06:56.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11770, 11776) --> . Mary journeyed to the
2025-01-22 05:06:56.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:06:56.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14283, 14288) --> . Mary went to the
2025-01-22 05:06:56.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:06:57.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21435, 21441) --> . Daniel journeyed to the
2025-01-22 05:06:57.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:06:57.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12869, 12874) --> . Sandra went to the
2025-01-22 05:06:57.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:06:57.175 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17463, 17468) --> . John moved to the
2025-01-22 05:06:57.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:06:57.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16605, 16610) --> . Daniel travelled to the
2025-01-22 05:06:57.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:06:57.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3110, 3115) -->  ranks. Sandra picked up
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:07:00.064 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:07:00.064 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24174])
2025-01-22 05:07:08.449 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [535.7604166666666, 17.579107984861647, 167.5796875, 17.003958039857473, 31.85007052951389], 'topk_indices': array([24159,  3117, 24062, 24168,    24,  4875, 24072, 24162,    14,
       24157,  4878,  4877, 24172, 24028,  4876,  4880, 24169, 24161,
           0,  4879]), 'topk_tokens': [' was', '.', '.\n\n', ':', '\n\n', '.', ' location', ' before', '\n', ':', ' the', ' got', '<|end_header_id|>', ' context', ' Mary', '.', '<|eot_id|>', ' football', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1743.25, 403.5, 291.625, 66.47395833333333]}, 'weight': {'score': [21.209449404761905, 23.44299851098151, 22.251171875, 23.445929446677162, 29.764539930555557], 'topk_indices': array([18781, 18825, 14663, 14627, 14672, 19420, 14708, 19478, 14591,
       14526, 20292, 20340, 18150, 18119, 23656, 23522, 21988, 21961,
       23604, 23738]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.052083333333332, 19.403125, 20.270833333333332]}, 'saliency': {'score': [3.4643496558779763, 0.10001486231426046, 1.069818115234375, 0.09628404131147456, 0.23755115932888454], 'topk_indices': array([24062, 14288,    24, 11776, 24159, 11772, 24143, 24158,  3112,
       24026, 24164, 24156, 24162, 24167, 24072,  4877, 24028,  4876,
       24161,  4879]), 'topk_tokens': ['.\n\n', ' kitchen', '\n\n', ' garden', ' was', ' journey', ' return', ' Where', ' Sandra', ' provided', ' kitchen', 'Question', ' before', 'Answer', ' location', ' got', ' context', ' Mary', ' football', ' football'], 'evidence_proportions': [12.37890625, 2.2466634114583335, 1.51328125, 0.3648885091145833]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:07:08.457 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:07:08.457 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-32_pid-2_2-5-6-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (2, 5, 6, 9):   3%|▎         | 3/100 [02:20<1:15:22, 46.62s/it]is_0k: False
your chose emoji: ['🇲🇶', '🏌🏾\u200d♀️', '👤', '💿', '🧎🏿\u200d➡', '🚁', '🚕', '🧎\u200d♂\u200d➡️', '👨🏽\u200d❤️\u200d💋\u200d👨🏼', '🧝🏽\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.79s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.97s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.56s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]
Processing depth (4, 5, 8, 9):   3%|▎         | 3/100 [02:38<1:15:22, 46.62s/it]2025-01-22 05:07:26.705 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:07:26.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9737, 9741) -->  got the football.
2025-01-22 05:07:26.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:07:26.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11848, 11854) --> . Mary journeyed to the
2025-01-22 05:07:26.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:07:26.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19262, 19267) -->  Mary went to the kitchen
2025-01-22 05:07:26.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:07:27.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21479, 21485) --> . Daniel journeyed to the
2025-01-22 05:07:27.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:07:27.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13005, 13010) --> . Sandra went to the
2025-01-22 05:07:27.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:07:27.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17501, 17506) --> . John moved to the
2025-01-22 05:07:27.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:07:27.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16617, 16622) --> . Daniel travelled to the
2025-01-22 05:07:27.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:07:27.306 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3191, 3196) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:07:30.149 | INFO     | test_jbb_embedding:begin_test:693 - The football was obtained by Mary.<|eot_id|>
2025-01-22 05:07:30.149 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24186])
2025-01-22 05:07:38.567 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [319.70089285714283, 11.97879330480797, 160.840625, 11.587895477886367, 64.84878305288461], 'topk_indices': array([24167, 24182, 24074, 24184, 24177, 24038,    14, 24168, 24180,
       24169, 24084, 24039,  9740, 24176, 24173, 24174,  9739, 24040,
       24181,     0]), 'topk_tokens': ['.\n\n', '<|start_header_id|>', '.\n\n', '<|end_header_id|>', '?', ' provided', '\n', 'Question', ':', ':', ' location', ' you', '.', ' kitchen', ' football', ' before', ' football', ' context', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [793.875, 245.64583333333334, 335.475, 64.49479166666667]}, 'weight': {'score': [21.24813988095238, 23.44201393195254, 21.04765625, 23.44590487566258, 29.12560096153846], 'topk_indices': array([18771, 18815, 14635, 14599, 14644, 19474, 14680, 19416, 14563,
       14498, 20336, 20288, 18109, 18140, 23540, 23674, 21984, 21957,
       23622, 23756]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.828125, 20.052083333333332, 24.1921875, 20.270833333333332]}, 'saliency': {'score': [1.7937883649553572, 0.06841338287483463, 0.959466552734375, 0.06617493919333382, 0.4604239830603966], 'topk_indices': array([24171, 24026,  1544, 24177,  9716, 24170,  9737, 24155, 24179,
       19266,  9736, 24039, 24038, 24084, 24174, 24168, 24176, 24173,
        9739, 24040]), 'topk_tokens': [' was', '️', '�', '?', '�', ' Where', ' got', ' return', 'Answer', ' kitchen', ' Mary', ' you', ' provided', ' location', ' before', 'Question', ' kitchen', ' football', ' football', ' context'], 'evidence_proportions': [4.36279296875, 1.3096516927083333, 2.04677734375, 0.35443115234375]}}, 'pred_res': 'The football was obtained by Mary.<|eot_id|>', 'score': 0}
2025-01-22 05:07:38.585 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:07:38.585 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-32_pid-3_4-5-8-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (4, 5, 8, 9):   4%|▍         | 4/100 [02:50<1:04:10, 40.11s/it]is_0k: False
your chose emoji: ['🍅', '🧑🏾\u200d🦼\u200d➡', '🧙🏻\u200d♂', '🤍', '🤴🏾', '👨\u200d👨\u200d👦', '🏋🏽\u200d♀️', '👨🏼\u200d🦼', '🧑🏽\u200d💻', '6️⃣']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.68s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.73s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.69s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (4, 7, 8, 9):   4%|▍         | 4/100 [03:07<1:04:10, 40.11s/it]2025-01-22 05:07:55.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:07:55.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9794, 9798) -->  Mary got the football
2025-01-22 05:07:55.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary journeyed to the garden.
2025-01-22 05:07:56.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16817, 16823) -->  affair. Mary journeyed to
2025-01-22 05:07:56.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:07:56.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19467, 19472) --> . Mary went to the
2025-01-22 05:07:56.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the office.
2025-01-22 05:07:56.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21477, 21483) --> . Daniel journeyed to the
2025-01-22 05:07:56.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:07:56.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13148, 13153) --> . Sandra went to the
2025-01-22 05:07:56.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:07:56.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (17563, 17568) --> . John moved to the
2025-01-22 05:07:56.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:07:56.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16750, 16755) --> . Daniel travelled to the
2025-01-22 05:07:56.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:07:56.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3215, 3220) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:07:59.296 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:07:59.296 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24174])
2025-01-22 05:08:07.700 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [545.7053571428571, 17.9639197377673, 482.846875, 17.11952840155784, 32.25916883680556], 'topk_indices': array([24072, 24164, 24027,    14, 16753, 24162,  9388, 13154, 16755,
       16823, 16821,  9798, 16820, 24028, 24161, 16752, 24170,  9797,
       24169,     0]), 'topk_tokens': [' location', ' kitchen', ' you', '\n', ' to', ' before', '�', '.', ' garden', ' the', 'ed', '.', ' journey', ' context', ' football', ' travelled', '<|start_header_id|>', ' football', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1053.5, 818.2291666666666, 342.575, 103.92708333333333]}, 'weight': {'score': [22.395833333333332, 23.439917069942506, 21.04765625, 23.4428078130179, 29.247395833333332], 'topk_indices': array([18763, 18807, 14588, 14624, 19402, 14633, 14669, 19460, 14552,
       14487, 20340, 20292, 18101, 18132, 23532, 23666, 21971, 21998,
       23748, 23614]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 24.204427083333332, 19.403125, 20.270833333333332]}, 'saliency': {'score': [3.31103515625, 0.10202097179783472, 2.831561279296875, 0.09696710603617263, 0.2375269995795356], 'topk_indices': array([13153,  9794,  9387, 24156, 24027, 24167,  9386, 16821, 24072,
       24026,  9388,  3216, 24162, 24164, 16755, 24028, 16820, 24161,
       16752,  9797]), 'topk_tokens': [' garden', ' Mary', '�', 'Question', ' you', 'Answer', '�', 'ed', ' location', ' provided', '�', ' Sandra', ' before', ' kitchen', ' garden', ' context', ' journey', ' football', ' travelled', ' football'], 'evidence_proportions': [7.4140625, 4.796712239583333, 1.58818359375, 0.5257161458333334]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:08:07.742 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:08:07.742 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-32_pid-4_4-7-8-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (4, 7, 8, 9):   5%|▌         | 5/100 [03:19<57:15, 36.16s/it]  Processing depth (4, 7, 8, 9):   5%|▌         | 5/100 [03:20<1:03:22, 40.03s/it]
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:82 - Selected idx: 33
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:84 - Answer: garden
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:86 - Needle: [' Daniel travelled to the garden.', ' Mary journeyed to the garden.', ' John moved to the office.', ' Sandra picked up the milk.', ' Mary got the football.', ' Mary went to the kitchen.', ' Sandra went to the garden.', ' Mary put down the football.', ' Daniel journeyed to the office.']
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:87 - Real Needle: [' Mary journeyed to the garden.', ' Mary got the football.', ' Mary went to the kitchen.', ' Mary put down the football.', ' Daniel journeyed to the office.']
2025-01-22 05:08:08.078 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['💏🏾', '\U0001faf3🏽', '🤟🏼', '\U0001faf1🏿\u200d\U0001faf2🏽', '🧚🏼\u200d♂️', '🧚🏿', '📠', '🙋🏼\u200d♂️', '🦹🏽\u200d♂️', '🍂']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.45s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.71s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.42s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.05s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (3, 5, 6, 7, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:08:24.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:08:24.589 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7537, 7543) --> . Mary journeyed to the
2025-01-22 05:08:24.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:08:24.682 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11914, 11918) -->  got the football.
2025-01-22 05:08:24.683 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:08:24.777 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14357, 14362) --> . Mary went to the
2025-01-22 05:08:24.777 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:08:24.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16791, 16796) --> . Mary put down the
2025-01-22 05:08:24.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:08:24.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19359, 19365) --> . Daniel journeyed to the
2025-01-22 05:08:24.962 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:08:25.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12305, 12310) --> . Daniel travelled to the
2025-01-22 05:08:25.029 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:08:25.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4657, 4662) --> . John moved to the
2025-01-22 05:08:25.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra picked up the milk.
2025-01-22 05:08:25.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11832, 11837) --> . Sandra picked up the
2025-01-22 05:08:25.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the garden.
2025-01-22 05:08:25.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3711, 3716) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:08:27.789 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:08:27.789 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24175])
2025-01-22 05:08:36.139 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [272.21875, 11.179589916452974, 178.9296875, 10.759317244737279, 36.95530348557692], 'topk_indices': array([    1, 24155, 24169, 24064,     9, 16797, 24171, 24174, 24015,
       24149, 16796, 24020, 24054, 11917,    14, 24160, 11916, 24173,
       24170,     0]), 'topk_tokens': ['<|start_header_id|>', ' to', ':', ' location', ':', '.', '<|start_header_id|>', '\n\n', '\n', ':', ' football', ' context', '.\n\n', '.', '\n', ' football', ' football', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [243.45833333333334, 522.0, 252.375, 328.5, 104.09375]}, 'weight': {'score': [20.427884615384617, 23.436665046736703, 21.04765625, 23.44188668054865, 29.34278846153846], 'topk_indices': array([18771, 18815, 14641, 14677, 19475, 14722, 19417, 14686, 14605,
       14540, 20337, 20289, 18140, 18109, 23524, 23658, 21963, 21990,
       23740, 23606]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 20.828125, 19.403125, 21.771875, 20.270833333333332]}, 'saliency': {'score': [1.4901076096754808, 0.06327276192781144, 1.079168701171875, 0.06089353000230503, 0.2744598388671875], 'topk_indices': array([11914, 11913, 24135,    20,  7543, 24018, 24172, 24009, 14362,
       24054, 24064,  3712, 24154, 24168, 24148, 24162, 16796, 24020,
       24160, 11916]), 'topk_tokens': [' got', ' Mary', ' return', ' Jul', ' garden', ' provided', 'assistant', '�', ' kitchen', '.\n\n', ' location', ' Sandra', ' prior', 'Answer', 'Question', ' discarded', ' football', ' context', ' football', ' football'], 'evidence_proportions': [1.33203125, 2.9114990234375, 1.32265625, 1.823828125, 0.5620320638020834]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:08:36.150 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:08:36.150 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-33_pid-0_3-5-6-7-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (3, 5, 6, 7, 8):   1%|          | 1/100 [00:27<46:03, 27.91s/it]is_0k: False
your chose emoji: ['💋', '👨🏿\u200d🏭', '🚵🏽', '🧖🏿\u200d♂️', '🧎\u200d➡️', '🤸🏽\u200d♂️', '🚬', '🦸🏽', '🤵🏼\u200d♀', '👢']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.37s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.27s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.98s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.47s/it]
Processing depth (0, 1, 2, 7, 9):   1%|          | 1/100 [00:43<46:03, 27.91s/it]2025-01-22 05:08:52.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:08:52.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the garden.
2025-01-22 05:08:52.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:08:52.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3039, 3043) -->  Mary got the football
2025-01-22 05:08:52.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:08:52.511 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (5011, 5016) --> . Mary went to the
2025-01-22 05:08:52.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:08:52.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16812, 16817) -->  affair. Mary put down
2025-01-22 05:08:52.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:08:52.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21471, 21477) --> . Daniel journeyed to the
2025-01-22 05:08:52.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:08:52.785 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12303, 12308) --> . Daniel travelled to the
2025-01-22 05:08:52.785 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:08:52.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4789, 4794) --> . John moved to the
2025-01-22 05:08:52.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra picked up the milk.
2025-01-22 05:08:52.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11797, 11802) --> . Sandra picked up the
2025-01-22 05:08:52.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the garden.
2025-01-22 05:08:52.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3747, 3752) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:08:55.565 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:08:55.565 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24209])
2025-01-22 05:09:03.952 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [246.60697115384616, 11.700055757475631, 131.09375, 11.348510045104693, 7.956709698932927], 'topk_indices': array([24202,    20,     4,    35, 24201,  1627,    19, 24182,    24,
       24203,     9, 24183, 24207,    14,  1706,  1628,    23, 24205,
       24204,     0]), 'topk_tokens': ['Answer', ' Jul', '\n\n', ' garden', '?\n', "'s", '26', 'Question', '\n\n', ':', ':', ':', '<|end_header_id|>', '\n', "'s", ' work', '4', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [439.2916666666667, 276.75, 144.85, 240.88125, 123.39583333333333]}, 'weight': {'score': [22.25420673076923, 23.445972038658518, 21.04765625, 23.449239116941158, 29.486661585365855], 'topk_indices': array([18731, 18775, 14601, 14637, 14682, 19448, 19390, 14646, 14500,
       14565, 20310, 20262, 18100, 18069, 23692, 23558, 21961, 21988,
       23640, 23774]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.958333333333332, 26.611328125, 19.403125, 26.7546875, 20.270833333333332]}, 'saliency': {'score': [1.4402278019831731, 0.06584214620623297, 0.76123046875, 0.06378794635929612, 0.058046108338891005], 'topk_indices': array([ 1626,  3042, 24054, 24169,  1627, 24188,    30, 24194,    22,
       16818,    31,  1706, 24202,  1705,    19,    20,    35,    23,
        1628, 24182]), 'topk_tokens': [' summer', ' football', ' context', ' return', "'s", ' prior', 'Mary', ' football', '202', ' football', ' journey', "'s", 'Answer', ' summer', '26', ' Jul', ' garden', '4', ' work', 'Question'], 'evidence_proportions': [2.4640299479166665, 1.9010009765625, 0.740869140625, 1.52412109375, 0.62213134765625]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 100}
2025-01-22 05:09:03.960 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:09:03.961 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-33_pid-1_0-1-2-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 2, 7, 9):   2%|▏         | 2/100 [00:55<45:29, 27.85s/it]is_0k: False
your chose emoji: ['👨🏽\u200d❤\u200d💋\u200d👨🏼', '🚵🏿\u200d♀️', '🧑🏻\u200d❤\u200d🧑🏼', '🙄', '🤶', '👁️\u200d🗨️', '👷🏿', '👨🏿\u200d✈️', '🦻🏻', '🛩']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.95s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.42s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (0, 2, 3, 8, 9):   2%|▏         | 2/100 [01:12<45:29, 27.85s/it]2025-01-22 05:09:21.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:09:21.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the garden.
2025-01-22 05:09:21.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:09:21.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5007, 5011) -->  Mary got the football
2025-01-22 05:09:21.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:09:21.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7598, 7603) -->  war. Mary went to
2025-01-22 05:09:21.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:09:21.244 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19228, 19233) -->  Mary put down the football
2025-01-22 05:09:21.245 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:09:21.359 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21481, 21487) --> . Daniel journeyed to the
2025-01-22 05:09:21.359 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:09:21.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12293, 12298) --> . Daniel travelled to the
2025-01-22 05:09:21.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:09:21.448 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4784, 4789) --> . John moved to the
2025-01-22 05:09:21.448 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra picked up the milk.
2025-01-22 05:09:21.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11837, 11842) --> . Sandra picked up the
2025-01-22 05:09:21.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the garden.
2025-01-22 05:09:21.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3742, 3747) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:09:24.207 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:09:24.207 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 05:09:32.581 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [200.83052884615384, 10.021404481034626, 118.090625, 9.726553444278855, 9.021002435064934], 'topk_indices': array([ 1628,    22, 24078, 24044, 24191, 24171, 24193, 12157, 24173,
          20,    24, 24195,    19,     9, 12153,    14, 24197,    23,
       24194,     0]), 'topk_tokens': [' work', '202', '.\n\n', ' context', '?\n', '.\n\n', ':', 'u', ':', ' Jul', '\n\n', '<|start_header_id|>', '26', ':', 'le', '\n', '<|end_header_id|>', '4', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [306.3958333333333, 239.5, 90.4875, 271.8, 102.296875]}, 'weight': {'score': [22.887920673076923, 23.443297558053054, 21.04765625, 23.44587880599023, 29.458603896103895], 'topk_indices': array([18797, 18841, 14663, 14627, 19442, 19500, 14672, 14708, 14526,
       14591, 20362, 20314, 18166, 18135, 23676, 23542, 21965, 21992,
       23624, 23758]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.958333333333332, 26.611328125, 22.8671875, 26.5859375, 20.270833333333332]}, 'saliency': {'score': [1.2166372445913463, 0.056787243540371145, 0.684283447265625, 0.05501931738944258, 0.06748070654931007], 'topk_indices': array([   24, 24088,    30,  3743,     8,    35,    16,  1628, 12298,
       12153,    22, 24192, 24184, 19232, 24044,    31, 24172,    19,
          20,    23]), 'topk_tokens': ['\n\n', ' location', 'Mary', ' Sandra', ' Date', ' garden', ' Date', ' work', ' garden', 'le', '202', 'Answer', ' football', ' football', ' context', ' journey', 'Question', '26', ' Jul', '4'], 'evidence_proportions': [1.7259928385416667, 1.65673828125, 0.5025390625, 1.81337890625, 0.5116780598958334]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 100}
2025-01-22 05:09:32.586 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:09:32.586 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-33_pid-2_0-2-3-8-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 2, 3, 8, 9):   3%|▎         | 3/100 [01:24<45:35, 28.20s/it]is_0k: False
your chose emoji: ['🤾🏼\u200d♀', '🚄', '😑', '👩🏼\u200d🦽', '🧏🏽\u200d♀', '😩', '👮🏾\u200d♂️', '👩🏾\u200d❤\u200d💋\u200d👩🏼', '🧑🏽\u200d🔧', '🤚🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.82s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  3.96s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.65s/it]
Processing depth (0, 2, 4, 5, 8):   3%|▎         | 3/100 [01:44<45:35, 28.20s/it]2025-01-22 05:09:53.399 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:09:53.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the garden.
2025-01-22 05:09:53.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:09:53.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4925, 4929) -->  Mary got the football
2025-01-22 05:09:53.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:09:53.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9745, 9750) --> . Mary went to the
2025-01-22 05:09:53.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:09:53.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11875, 11880) --> . Mary put down the
2025-01-22 05:09:53.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:09:53.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19349, 19355) --> . Daniel journeyed to the
2025-01-22 05:09:53.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:09:53.693 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12219, 12224) --> . Daniel travelled to the
2025-01-22 05:09:53.693 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:09:53.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4656, 4661) --> . John moved to the
2025-01-22 05:09:53.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra picked up the milk.
2025-01-22 05:09:53.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11769, 11774) --> . Sandra picked up the
2025-01-22 05:09:53.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the garden.
2025-01-22 05:09:53.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3718, 3723) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:09:56.449 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen<|eot_id|>
2025-01-22 05:09:56.449 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 05:10:04.811 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [188.10456730769232, 11.31697074446098, 134.403125, 11.024657293133439, 19.450276692708332], 'topk_indices': array([24169, 24174, 24185, 24181, 24183,     1,    22, 24068, 24162,
          20, 24163,     9,    24,    19,    14, 24034, 24184, 24187,
          23,     0]), 'topk_tokens': [' to', ' football', '<|start_header_id|>', '?\n', ':', '<|start_header_id|>', '202', '.\n\n', 'Question', ' Jul', ':', ':', '\n\n', '26', '\n', ' context', '<|eot_id|>', '<|end_header_id|>', '4', '<|begin_of_text|>'], 'evidence_proportions': [253.01041666666666, 270.40625, 131.4625, 238.9, 73.203125]}, 'weight': {'score': [21.295973557692307, 23.438773664847883, 21.04765625, 23.443061544872858, 29.116319444444443], 'topk_indices': array([18805, 18761, 14661, 14625, 14670, 14706, 19465, 19407, 14589,
       14524, 20339, 20291, 18099, 18130, 23672, 23538, 21935, 21962,
       23754, 23620]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.958333333333332, 26.611328125, 19.403125, 21.771875, 20.270833333333332]}, 'saliency': {'score': [1.0770592322716346, 0.06451167757548983, 0.748199462890625, 0.06285508881841195, 0.14079136318630642], 'topk_indices': array([    0, 24068, 24149,    24, 24032,     8,  7502, 24182,    16,
       11880,  4928, 24176, 24168,    22, 24174,    19,    20, 24162,
          23, 24034]), 'topk_tokens': ['<|begin_of_text|>', '.\n\n', ' return', '\n\n', ' provided', ' Date', ' tele', 'Answer', ' Date', ' football', ' football', ' discarded', ' prior', '202', ' football', '26', ' Jul', 'Question', '4', ' context'], 'evidence_proportions': [1.3648681640625, 1.9036865234375, 0.687646484375, 1.2908203125, 0.3845418294270833]}}, 'pred_res': 'The kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:10:04.816 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:10:04.816 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-33_pid-3_0-2-4-5-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 2, 4, 5, 8):   4%|▍         | 4/100 [01:56<47:40, 29.79s/it]is_0k: False
your chose emoji: ['🇵🇷', '🧏🏽', '👩🏻\u200d🌾', '🤽🏽\u200d♀', '🅱', '🕴🏽', '🚶🏾\u200d♀️', '🧏🏼', '👩🏽\u200d❤\u200d👩🏽', '🥮']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.58s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.78s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]
Processing depth (4, 5, 7, 8, 9):   4%|▍         | 4/100 [02:14<47:40, 29.79s/it]2025-01-22 05:10:22.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary journeyed to the garden.
2025-01-22 05:10:22.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9681, 9687) --> . Mary journeyed to the
2025-01-22 05:10:22.544 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:10:22.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11859, 11863) -->  Mary got the football
2025-01-22 05:10:22.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:10:22.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16785, 16790) --> . Mary went to the
2025-01-22 05:10:22.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the football.
2025-01-22 05:10:22.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19337, 19342) --> . Mary put down the
2025-01-22 05:10:22.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:10:22.913 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21531, 21537) --> . Daniel journeyed to the
2025-01-22 05:10:22.914 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:10:22.976 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12201, 12206) --> . Daniel travelled to the
2025-01-22 05:10:22.976 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:10:23.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4689, 4694) -->  the senate. John moved
2025-01-22 05:10:23.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra picked up the milk.
2025-01-22 05:10:23.064 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11752, 11757) --> . Sandra picked up the
2025-01-22 05:10:23.064 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the garden.
2025-01-22 05:10:23.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3703, 3708) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:10:25.911 | INFO     | test_jbb_embedding:begin_test:693 - Mary put down the football.<|eot_id|>
2025-01-22 05:10:25.911 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 05:10:34.306 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [326.8798076923077, 15.388950107606986, 235.005859375, 14.870991056248963, 39.36622807017544], 'topk_indices': array([24148,    24, 24147, 18571, 18498, 24144, 24153, 18489, 24149,
       24154, 11862, 24038, 24048, 18490, 24133, 24157, 24138, 24139,
       24155,     0]), 'topk_tokens': [' left', '\n\n', ',', 'G', 'oth', ' football', ':', ' rapid', ' or', '<|eot_id|>', ' football', '.\n\n', ' location', 'ity', ':', '<|end_header_id|>', ' prior', ' to', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [403.4375, 548.9375, 236.675, 346.05, 161.47916666666666]}, 'weight': {'score': [21.317608173076923, 23.432052396324806, 22.2296875, 23.435329177206004, 29.19435307017544], 'topk_indices': array([18793, 18749, 14643, 14607, 19394, 19452, 14688, 14652, 14506,
       14571, 20266, 20314, 18106, 18075, 23630, 23496, 21944, 21917,
       23712, 23578]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.052083333333332, 26.611328125, 19.403125, 21.771875, 20.270833333333332]}, 'saliency': {'score': [1.9486271784855769, 0.08721042226041925, 1.400146484375, 0.084114736537076, 0.28925631339089913], 'topk_indices': array([24139,  9687, 24149, 18571, 24038,  3704, 18563,  9683, 19342,
       24148, 24152, 18490, 18498, 24132, 24146, 24048, 24144, 18489,
       11862, 24138]), 'topk_tokens': [' to', ' garden', ' or', 'G', '.\n\n', ' Sandra', ' rapid', ' journey', ' football', ' left', 'Answer', 'ity', 'oth', 'Question', ' discarded', ' location', ' football', ' rapid', ' football', ' prior'], 'evidence_proportions': [2.2682291666666665, 3.734130859375, 1.27998046875, 1.95546875, 0.9901936848958334]}}, 'pred_res': 'Mary put down the football.<|eot_id|>', 'score': 0}
2025-01-22 05:10:34.312 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:10:34.313 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-33_pid-4_4-5-7-8-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (4, 5, 7, 8, 9):   5%|▌         | 5/100 [02:26<47:00, 29.69s/it]Processing depth (4, 5, 7, 8, 9):   5%|▌         | 5/100 [02:26<46:21, 29.28s/it]
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:82 - Selected idx: 34
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the hallway? 
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:86 - Needle: [' Sandra went to the garden.', ' Mary picked up the milk there.', ' John got the football.', ' John dropped the football.', ' Mary travelled to the bathroom.', ' Daniel travelled to the garden.', ' Mary went back to the hallway.', ' John moved to the office.']
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:87 - Real Needle: [' Mary picked up the milk there.', ' Mary travelled to the bathroom.', ' Mary went back to the hallway.', ' John moved to the office.']
2025-01-22 05:10:34.641 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🥬', '🚴🏻\u200d♀', '👓', '🙍🏻\u200d♀', '🧏🏼\u200d♂', '🧎🏼', '🙌🏾', '🆓', '◀', '👩🏻\u200d✈']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.97s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.82s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (1, 6, 7, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:10:52.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary picked up the milk there.
2025-01-22 05:10:52.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2902, 2908) -->  Mary picked up the milk there
2025-01-22 05:10:52.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bathroom.
2025-01-22 05:10:52.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14284, 14289) --> . Mary travelled to the
2025-01-22 05:10:52.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:10:52.219 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16704, 16710) --> . Mary went back to the
2025-01-22 05:10:52.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the office.
2025-01-22 05:10:52.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21441, 21446) --> . John moved to the
2025-01-22 05:10:52.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:10:52.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10283, 10288) --> . Sandra went to the
2025-01-22 05:10:52.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:10:52.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4809, 4813) -->  John got the football
2025-01-22 05:10:52.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John dropped the football.
2025-01-22 05:10:52.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12069, 12073) -->  dropped the football.
2025-01-22 05:10:52.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:10:52.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15480, 15485) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:10:55.223 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:10:55.223 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 05:11:03.615 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [312.12863991477275, 9.790427960499132, 167.79340277777777, 9.397435070436016, 24.444140625], 'topk_indices': array([ 2901, 24187, 24192,     9, 24193,  2905,  2867,  2907, 10284,
        2906,  2896,    14, 14286, 24197,  2895,  2903, 14289,  2902,
           0, 24194]), 'topk_tokens': ['�', ' before', 'Answer', ':', ':', ' the', '.', ' there', ' Sandra', ' milk', '�', '\n', ' travelled', '<|end_header_id|>', '.', ' picked', ' bathroom', ' Mary', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [661.2916666666666, 441.15, 110.86458333333333, 5.628515625]}, 'weight': {'score': [21.844105113636363, 23.449330117345674, 22.034288194444443, 23.451845873685954, 29.516360294117646], 'topk_indices': array([18843, 18799, 14628, 14664, 14709, 14673, 19444, 19502, 14592,
       14527, 20364, 20316, 18168, 18137, 23527, 23661, 21993, 21966,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.35546875, 20.75625, 20.708333333333332, 18.88125]}, 'saliency': {'score': [1.9978141784667969, 0.05605925013138119, 1.0629340277777777, 0.05354114921171335, 0.1732072718003217], 'topk_indices': array([ 2866,  2901,  2910,  4812, 24186, 24181, 24187,  2899,  2894,
        2907, 14285, 24192, 24189,  2906,  2896, 10284, 14286,  2903,
       14289,  2902]), 'topk_tokens': [' Gov', '�', 'ible', ' football', ' milk', 'Question', ' before', '�', ' Gov', ' there', ' Mary', 'Answer', ' hallway', ' milk', '�', ' Sandra', ' travelled', ' picked', ' bathroom', ' Mary'], 'evidence_proportions': [4.503743489583333, 2.6736328125, 0.5711669921875, 0.02685699462890625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 05:11:03.622 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:11:03.623 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-34_pid-0_1-6-7-9.pkl | len: 3 |  size: 2.01 KB
Processing depth (1, 6, 7, 9):   1%|          | 1/100 [00:28<47:37, 28.86s/it]is_0k: False
your chose emoji: ['👱🏼', '♦', '👰🏼\u200d♂️', '👷🏽\u200d♀️', '🟣', '👩🏾\u200d🎨', '👯\u200d♂️', '🚑', '👩🏾\u200d🤝\u200d👨🏽', '👩🏾\u200d❤\u200d💋\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.28s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.25s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.31s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.02s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.48s/it]
Processing depth (0, 2, 3, 5):   1%|          | 1/100 [00:44<47:37, 28.86s/it]2025-01-22 05:11:19.853 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary picked up the milk there.
2025-01-22 05:11:19.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  picked up the milk there.
2025-01-22 05:11:19.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bathroom.
2025-01-22 05:11:19.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4965, 4970) --> . Mary travelled to the
2025-01-22 05:11:19.879 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:11:19.923 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7602, 7608) -->  war. Mary went back to
2025-01-22 05:11:19.923 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the office.
2025-01-22 05:11:19.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11820, 11825) --> . John moved to the
2025-01-22 05:11:19.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:11:20.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10306, 10311) --> . Sandra went to the
2025-01-22 05:11:20.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:11:20.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4901, 4905) -->  John got the football
2025-01-22 05:11:20.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John dropped the football.
2025-01-22 05:11:20.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12090, 12094) -->  dropped the football.
2025-01-22 05:11:20.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:11:20.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15469, 15474) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:11:23.438 | INFO     | test_jbb_embedding:begin_test:693 - There is no information about the milk before the hallway in the given text.<|eot_id|>
2025-01-22 05:11:23.438 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 05:11:31.824 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [144.1818181818182, 4.701087800082782, 14.422526041666666, 4.566611765391791, 3.5277175903320312], 'topk_indices': array([   23,     1,    31,     3,  1556,    26,    14,    32,    34,
          33, 24155,    28,  1563,    25,  7793, 24153,    24,    35,
       24152,     0]), 'topk_tokens': ['4', '<|start_header_id|>', ' picked', '<|end_header_id|>', ' part', '<|start_header_id|>', '\n', ' up', ' milk', ' the', '<|end_header_id|>', '<|end_header_id|>', 'ern', '<|eot_id|>', ' Square', '<|start_header_id|>', '\n\n', ' there', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [403.0416666666667, 68.39375, 61.744791666666664, 8.2625]}, 'weight': {'score': [21.579900568181817, 23.441297599337748, 22.034288194444443, 23.44404539800995, 29.983642578125], 'topk_indices': array([18787, 18831, 14635, 14671, 19426, 19484, 14732, 14696, 14599,
       14534, 20346, 20298, 18125, 18156, 23637, 23503, 21969, 21942,
       23585, 23719]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.5, 20.75625, 23.595052083333332, 18.88125]}, 'saliency': {'score': [0.8249737132679332, 0.026307471066910698, 0.09086439344618055, 0.025530825879047956, 0.027550309896469116], 'topk_indices': array([ 1492,     0, 24144,  7609,    30, 24145,  1562,    39,  7792,
        1642,  1460,    32,  1556,    24,    31, 24147,  1563,    34,
        7793,    35]), 'topk_tokens': ['nes', '<|begin_of_text|>', ' milk', ' hallway', 'Mary', ' before', ' const', '***', 'Bridge', 'ern', 'nes', ' up', ' part', '\n\n', ' picked', ' hallway', 'ern', ' milk', ' Square', ' there'], 'evidence_proportions': [2.3314208984375, 0.3822998046875, 0.3403422037760417, 0.04146881103515625]}}, 'pred_res': 'There is no information about the milk before the hallway in the given text.<|eot_id|>', 'score': 0}
2025-01-22 05:11:31.831 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:11:31.832 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-34_pid-1_0-2-3-5.pkl | len: 3 |  size: 2.13 KB
Processing depth (0, 2, 3, 5):   2%|▏         | 2/100 [00:57<46:30, 28.48s/it]is_0k: False
your chose emoji: ['☪', '🧟\u200d♂️', '🏋🏾\u200d♀', '\U0001faf6🏿', '👩🏻\u200d✈', '🛡️', '⛴', '💆🏽\u200d♀️', '🧔🏽\u200d♀', '🙍🏽\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.16s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (4, 6, 7, 8):   2%|▏         | 2/100 [01:14<46:30, 28.48s/it]2025-01-22 05:11:49.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary picked up the milk there.
2025-01-22 05:11:49.468 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9654, 9660) --> . Mary picked up the milk
2025-01-22 05:11:49.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bathroom.
2025-01-22 05:11:49.544 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14286, 14291) --> . Mary travelled to the
2025-01-22 05:11:49.544 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:11:49.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16660, 16666) --> . Mary went back to the
2025-01-22 05:11:49.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the office.
2025-01-22 05:11:49.733 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19167, 19172) --> . John moved to the
2025-01-22 05:11:49.733 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:11:49.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10231, 10236) --> . Sandra went to the
2025-01-22 05:11:49.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:11:49.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4900, 4904) -->  John got the football
2025-01-22 05:11:49.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John dropped the football.
2025-01-22 05:11:49.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12071, 12075) -->  dropped the football.
2025-01-22 05:11:49.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:11:49.942 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15478, 15483) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:11:52.632 | INFO     | test_jbb_embedding:begin_test:693 - St. Mary<|eot_id|>
2025-01-22 05:11:52.632 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24185])
2025-01-22 05:12:01.004 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [390.25284090909093, 12.66152896064164, 166.91493055555554, 12.202543947738944, 19.886793870192307], 'topk_indices': array([24172,    14, 24184,  9659,  9654,    24, 24073,  9660, 24179,
       24038, 14291,  9634, 24175, 24173,  9655, 24039, 24183, 24181,
           0, 24180]), 'topk_tokens': [' milk', '\n', '\n\n', ' milk', '.', '\n\n', '.\n\n', ' there', ':', ' you', ' bathroom', ' The', ' hallway', ' before', ' Mary', ' context', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [683.2083333333334, 479.6, 299.9583333333333, 57.7125]}, 'weight': {'score': [21.151988636363637, 23.446479142550025, 22.034288194444443, 23.44962218662001, 29.618790064102566], 'topk_indices': array([18853, 18809, 14630, 14666, 19454, 14675, 14711, 19512, 14594,
       14529, 20326, 20374, 18147, 18178, 23665, 23531, 21970, 21997,
       23747, 23613]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.817708333333332, 20.75625, 20.708333333333332, 18.88125]}, 'saliency': {'score': [2.279812899502841, 0.07108601739983411, 1.0557793511284723, 0.06833976630602172, 0.14713932917668268], 'topk_indices': array([ 9632,  9652, 24169, 14287, 24038, 24037,  9660,  9653, 24167,
        9656, 24178, 14288, 16666, 24172,  9659, 24173,  9655, 14291,
       24175, 24039]), 'topk_tokens': [' St', 'old', ' Where', ' Mary', ' you', ' provided', ' there', ' St', 'Question', ' picked', 'Answer', ' travelled', ' hallway', ' milk', ' milk', ' before', ' Mary', ' bathroom', ' hallway', ' context'], 'evidence_proportions': [4.236979166666667, 2.715234375, 1.6370849609375, 0.2670654296875]}}, 'pred_res': 'St. Mary<|eot_id|>', 'score': 0}
2025-01-22 05:12:01.011 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:12:01.011 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-34_pid-2_4-6-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (4, 6, 7, 8):   3%|▎         | 3/100 [01:26<46:33, 28.80s/it]is_0k: False
your chose emoji: ['🏃🏻\u200d♂\u200d➡️', '🧑🏽\u200d🦯', '👨🏿\u200d❤\u200d👨🏼', '🎡', '📎', '👇🏼', '👨🏾\u200d🦳', '🙂\u200d↕️', '🩱', '🙍🏻\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.48s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.92s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (1, 3, 4, 5):   3%|▎         | 3/100 [01:43<46:33, 28.80s/it]2025-01-22 05:12:18.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary picked up the milk there.
2025-01-22 05:12:18.441 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Mary picked up the milk
2025-01-22 05:12:18.441 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bathroom.
2025-01-22 05:12:18.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7590, 7595) -->  war. Mary travelled to
2025-01-22 05:12:18.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:12:18.534 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9677, 9683) --> . Mary went back to the
2025-01-22 05:12:18.534 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the office.
2025-01-22 05:12:18.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11894, 11899) --> . John moved to the
2025-01-22 05:12:18.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:12:18.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10344, 10349) --> . Sandra went to the
2025-01-22 05:12:18.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:12:18.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4853, 4857) -->  John got the football
2025-01-22 05:12:18.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John dropped the football.
2025-01-22 05:12:18.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12289, 12293) -->  John dropped the football
2025-01-22 05:12:18.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:12:18.834 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15641, 15646) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:12:21.495 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:12:21.495 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 05:12:29.872 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [241.1427556818182, 9.162203612513448, 75.1328125, 8.901445794992954, 10.628920825559701], 'topk_indices': array([24136,  3031,  3038, 24162, 24061, 24153, 24156, 24150, 24051,
        3037, 24017,    24, 24157,    14,  3036, 24161,  7596,  9683,
           0, 24158]), 'topk_tokens': ['.', '.', '.', '\n\n', ' location', ' hallway', 'Answer', ' milk', '.\n\n', ' there', ' context', '\n\n', ':', '\n', ' milk', '<|end_header_id|>', ' bathroom', ' hallway', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [438.0833333333333, 256.40625, 212.5, 23.921875]}, 'weight': {'score': [21.939275568181817, 23.440774228254572, 23.106770833333332, 23.44239260807842, 29.603078358208954], 'topk_indices': array([18777, 18821, 14629, 14593, 19474, 14674, 14638, 19416, 14492,
       14557, 20308, 20356, 18146, 18115, 23513, 23647, 21952, 21979,
       23595, 23729]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.817708333333332, 24.2203125, 20.708333333333332, 18.88125]}, 'saliency': {'score': [1.4149058948863635, 0.05228471617859286, 0.4772203233506944, 0.0507251328717592, 0.08216584618411847], 'topk_indices': array([11945,  3699,  7780,    24, 24051, 24015, 24151, 24132, 24061,
        3037,  7593, 24145,  3033, 24017, 24156, 24150, 24153,  3036,
        7596,  9683]), 'topk_tokens': ['ANT', '�', ' Square', '\n\n', '.\n\n', ' provided', ' before', ' return', ' location', ' there', ' travelled', 'Question', ' picked', ' context', 'Answer', ' milk', ' hallway', ' milk', ' bathroom', ' hallway'], 'evidence_proportions': [2.7154947916666665, 1.520556640625, 1.1069742838541667, 0.11806640625]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:12:29.878 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:12:29.878 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-34_pid-3_1-3-4-5.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 3, 4, 5):   4%|▍         | 4/100 [01:55<46:07, 28.83s/it]is_0k: False
your chose emoji: ['👩🏿\u200d❤\u200d💋\u200d👨🏼', '🤓', '🌡️', '🆘', '🙋🏿\u200d♂️', '👏🏻', '👩🏼\u200d💼', '🐡', '🗯️', '🕵\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.48s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (1, 5, 7, 8):   4%|▍         | 4/100 [02:11<46:07, 28.83s/it]2025-01-22 05:12:46.914 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary picked up the milk there.
2025-01-22 05:12:46.933 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Mary picked up the milk
2025-01-22 05:12:46.933 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bathroom.
2025-01-22 05:12:46.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11817, 11822) --> . Mary travelled to the
2025-01-22 05:12:46.995 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:12:47.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16710, 16716) --> . Mary went back to the
2025-01-22 05:12:47.083 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John moved to the office.
2025-01-22 05:12:47.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19204, 19209) -->  John moved to the office
2025-01-22 05:12:47.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:12:47.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10227, 10232) --> . Sandra went to the
2025-01-22 05:12:47.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:12:47.273 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4847, 4851) -->  John got the football
2025-01-22 05:12:47.273 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John dropped the football.
2025-01-22 05:12:47.338 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12087, 12091) -->  dropped the football.
2025-01-22 05:12:47.338 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:12:47.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15498, 15503) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:12:50.082 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:12:50.082 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24153])
2025-01-22 05:12:58.476 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [130.3224431818182, 5.909768147665177, 45.490451388888886, 5.766729038812406, 8.24710181451613], 'topk_indices': array([24026, 24006,     1, 24002,  3036,     3, 24152, 24147,    23,
       24141,     9, 24143,    24, 24007, 24041, 24051,    14, 24151,
           0, 24148]), 'topk_tokens': ['.', ' you', '<|start_header_id|>', '\n', ' milk', '<|end_header_id|>', '\n\n', ':', '4', ' before', ':', ' hallway', '\n\n', ' context', '.\n\n', ' location', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [216.35416666666666, 98.8125, 151.0625, 33.70625]}, 'weight': {'score': [22.064275568181817, 23.435223132969035, 22.034288194444443, 23.437519437303035, 29.018649193548388], 'topk_indices': array([18773, 18817, 14632, 14668, 19476, 14677, 14713, 19418, 14596,
       14531, 20290, 20338, 18142, 18111, 23641, 23507, 21946, 21973,
       23723, 23589]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.817708333333332, 20.75625, 20.708333333333332, 22.8953125]}, 'saliency': {'score': [0.7414301091974432, 0.0333439333783532, 0.263916015625, 0.03252587925045224, 0.06015765282415574], 'topk_indices': array([24152, 24150, 24006,    23,  3037,    24, 24135,  7768, 24122,
       24140, 24005, 24041, 24141, 24146, 11822,  3036, 16716, 24051,
       24007, 24143]), 'topk_tokens': ['\n\n', 'assistant', ' you', '4', ' there', '\n\n', 'Question', ' Square', ' return', ' milk', ' provided', '.\n\n', ' before', 'Answer', ' bathroom', ' milk', ' hallway', ' location', ' context', ' hallway'], 'evidence_proportions': [1.3553059895833333, 0.5153564453125, 0.7724609375, 0.19361572265625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 05:12:58.505 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:12:58.505 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-34_pid-4_1-5-7-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 5, 7, 8):   5%|▌         | 5/100 [02:23<45:31, 28.75s/it]Processing depth (1, 5, 7, 8):   5%|▌         | 5/100 [02:24<45:36, 28.80s/it]
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:82 - Selected idx: 35
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:86 - Needle: [' John got the football.', ' Daniel travelled to the garden.', ' Mary travelled to the bathroom.', ' Mary picked up the milk there.', ' Mary went back to the hallway.', ' Sandra went to the garden.', ' John dropped the football.', ' Mary dropped the milk.', ' John moved to the office.']
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the bathroom.', ' Mary picked up the milk there.', ' Mary went back to the hallway.', ' Mary dropped the milk.', ' John moved to the office.']
2025-01-22 05:12:58.769 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🇻🇮', '🤸🏻\u200d♂️', '🚵🏼', '🤦', '👩🏽\u200d❤\u200d👨🏾', '🌒', '🏋🏻', '🤏🏿', '⚾', '💁🏿']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.02s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.70s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (0, 1, 4, 5, 6):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:13:15.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bathroom.
2025-01-22 05:13:15.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:13:15.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk there.
2025-01-22 05:13:15.508 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2967, 2973) --> . Mary picked up the milk
2025-01-22 05:13:15.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:13:15.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9662, 9668) -->  war. Mary went back to
2025-01-22 05:13:15.562 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 05:13:15.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11805, 11809) -->  Mary dropped the milk
2025-01-22 05:13:15.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the office.
2025-01-22 05:13:15.699 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14280, 14285) --> . John moved to the
2025-01-22 05:13:15.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John got the football.
2025-01-22 05:13:15.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6507, 6511) -->  John got the football
2025-01-22 05:13:15.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:13:15.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13485, 13490) --> . Daniel travelled to the
2025-01-22 05:13:15.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:13:15.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10082, 10087) --> . Sandra went to the
2025-01-22 05:13:15.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:13:15.931 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (13822, 13826) -->  John dropped the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:13:18.592 | INFO     | test_jbb_embedding:begin_test:693 - the bathroom<|eot_id|>
2025-01-22 05:13:18.592 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24167])
2025-01-22 05:13:26.905 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [156.49549278846155, 7.5156624948283, 67.15277777777777, 7.31061633455608, 7.354271673387097], 'topk_indices': array([   31,  1207,    23,    14,  1212,  1194,    24,  1155,    29,
          35,  1213,    38, 24165,    34,  1154, 24163, 24162,    39,
        1214,     0]), 'topk_tokens': [' travelled', ' the', '4', '\n', '\n', 'nes', '\n\n', 'ot', '\n\n', '.', 'Min', '***', '<|end_header_id|>', ' bathroom', 'nes', '<|start_header_id|>', '<|eot_id|>', '\n\n\n', 'nes', '<|begin_of_text|>'], 'evidence_proportions': [505.5, 117.65625, 61.829427083333336, 82.546875, 26.85625]}, 'weight': {'score': [22.873798076923077, 23.436222589987587, 23.106770833333332, 23.43707449950261, 29.359375], 'topk_indices': array([18822, 18778, 14660, 14624, 14669, 19433, 14705, 19491, 14588,
       14523, 20353, 20305, 18086, 18117, 23644, 23510, 21949, 21976,
       23592, 23726]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 23.817708333333332, 23.595052083333332, 26.685546875, 18.88125]}, 'saliency': {'score': [0.9343238243689904, 0.04275842744750724, 0.388519287109375, 0.04153964290826028, 0.05365531675277218], 'topk_indices': array([ 1141,    23,  1004,  1153,    36,    29,    37,  1042,    24,
        1134,  1155, 24012,  1194,    31,  1213,    38,  1154,    39,
          34,  1214]), 'topk_tokens': ['ioneer', '4', ' states', 'Min', ' PA', '\n\n', 'UL', ' galaxy', '\n\n', 'nes', 'ot', ' context', 'nes', ' travelled', 'Min', '***', 'nes', '\n\n\n', ' bathroom', 'nes'], 'evidence_proportions': [2.9888671875, 0.7351277669270834, 0.350921630859375, 0.54339599609375, 0.131640625]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-22 05:13:26.913 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:13:26.913 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-35_pid-0_0-1-4-5-6.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 1, 4, 5, 6):   1%|          | 1/100 [00:27<46:11, 28.00s/it]is_0k: False
your chose emoji: ['🧑🏻\u200d❤\u200d🧑🏼', '🌭', '❄️', '🧖🏼', '🤏🏾', '🍒', '🚵🏽\u200d♀', '🍉', '🧗🏿\u200d♀️', '🇭🇺']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.82s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.36s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.10s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]
Processing depth (2, 5, 6, 7, 9):   1%|          | 1/100 [00:44<46:11, 28.00s/it]2025-01-22 05:13:43.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bathroom.
2025-01-22 05:13:43.513 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4881, 4886) --> . Mary travelled to the
2025-01-22 05:13:43.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk there.
2025-01-22 05:13:43.581 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11842, 11848) --> . Mary picked up the milk
2025-01-22 05:13:43.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:13:43.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14374, 14380) --> . Mary went back to the
2025-01-22 05:13:43.660 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 05:13:43.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16810, 16814) -->  Mary dropped the milk
2025-01-22 05:13:43.757 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the office.
2025-01-22 05:13:43.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21506, 21511) --> . John moved to the
2025-01-22 05:13:43.874 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John got the football.
2025-01-22 05:13:43.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6378, 6382) -->  John got the football
2025-01-22 05:13:43.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:13:43.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13603, 13608) --> . Daniel travelled to the
2025-01-22 05:13:43.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:13:44.043 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10120, 10125) --> . Sandra went to the
2025-01-22 05:13:44.043 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:13:44.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (13924, 13928) -->  John dropped the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:13:46.764 | INFO     | test_jbb_embedding:begin_test:693 - the hallway<|eot_id|>
2025-01-22 05:13:46.764 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24213])
2025-01-22 05:13:55.108 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [189.6466346153846, 8.757204689048564, 100.36631944444444, 8.494417611285785, 7.106548713235294], 'topk_indices': array([24058, 24212,    14, 24187,  4887,    23,  4886, 17953,    24,
           1, 11847, 24209, 17962, 24198, 16813, 16814, 11848, 24211,
       24208,     0]), 'topk_tokens': [' context', '\n\n', '\n', ':', '.', '4', ' bathroom', ' Eighth', '\n\n', '<|start_header_id|>', ' milk', '<|start_header_id|>', ' Broadway', ' milk', ' milk', '.', ' there', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [200.7125, 270.7291666666667, 131.08333333333334, 341.5, 30.075]}, 'weight': {'score': [22.00330528846154, 23.44976461843409, 23.106770833333332, 23.451575880667715, 29.690992647058824], 'topk_indices': array([18832, 18788, 14695, 14659, 14704, 14740, 19427, 19485, 14558,
       14623, 20353, 20305, 18126, 18157, 23558, 23692, 22016, 21989,
       23640, 23774]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 23.817708333333332, 20.708333333333332, 26.685546875, 18.88125]}, 'saliency': {'score': [1.1481792743389423, 0.05004351502170569, 0.6189778645833334, 0.04843866858639387, 0.05212469661937041], 'topk_indices': array([11844, 24102, 24192,    23, 24196,    24, 17938, 24200, 16811,
        4883, 24058, 24186, 14380,  4886, 17953, 11847, 11848, 24198,
       16813, 17962]), 'topk_tokens': [' picked', ' location', ' prior', '4', ' where', '\n\n', ' Seventh', ' discarded', ' dropped', ' travelled', ' context', 'Question', ' hallway', ' bathroom', ' Eighth', ' milk', ' there', ' milk', ' milk', ' Broadway'], 'evidence_proportions': [1.121142578125, 1.6815592447916667, 0.6783854166666666, 2.3392333984375, 0.1460693359375]}}, 'pred_res': 'the hallway<|eot_id|>', 'score': 0}
2025-01-22 05:13:55.115 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:13:55.115 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-35_pid-1_2-5-6-7-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 5, 6, 7, 9):   2%|▏         | 2/100 [00:56<45:55, 28.12s/it]is_0k: False
your chose emoji: ['🇺🇿', '🥧', '👩🏼\u200d🤝\u200d👨🏽', '🧑🏾\u200d❤\u200d💋\u200d🧑🏿', '👅', '😏', '🖋️', '👩🏼\u200d❤️\u200d💋\u200d👩🏽', '🙋🏻\u200d♀', '💏🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.89s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (1, 3, 4, 6, 8):   2%|▏         | 2/100 [01:13<45:55, 28.12s/it]2025-01-22 05:14:12.214 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bathroom.
2025-01-22 05:14:12.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary travelled to the
2025-01-22 05:14:12.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk there.
2025-01-22 05:14:12.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7616, 7622) --> . Mary picked up the milk
2025-01-22 05:14:12.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:14:12.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9775, 9781) --> . Mary went back to the
2025-01-22 05:14:12.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 05:14:12.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14284, 14288) -->  Mary dropped the milk
2025-01-22 05:14:12.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the office.
2025-01-22 05:14:12.487 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19193, 19198) -->  John moved to the office
2025-01-22 05:14:12.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John got the football.
2025-01-22 05:14:12.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6518, 6522) -->  John got the football
2025-01-22 05:14:12.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:14:12.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13488, 13493) --> . Daniel travelled to the
2025-01-22 05:14:12.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:14:12.645 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10268, 10273) --> . Sandra went to the
2025-01-22 05:14:12.645 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:14:12.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (13825, 13829) -->  John dropped the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:14:15.358 | INFO     | test_jbb_embedding:begin_test:693 - the hallway<|eot_id|>
2025-01-22 05:14:15.358 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 05:14:23.725 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [88.25240384615384, 6.992225010347682, 51.342013888888886, 6.871514243655664, 11.475757264254385], 'topk_indices': array([13934, 11936, 11979,    14, 11937,    23, 11980, 11992, 11943,
          24, 24002, 11944, 11993, 24155, 11994,     0, 11995, 11952,
       11951, 24152]), 'topk_tokens': [',', 'BR', 'LE', '\n', 'ATION', '4', 'BR', '\n', 'AY', '\n\n', ' context', 'ING', 'AT', '<|end_header_id|>', 'L', '<|begin_of_text|>', 'ANT', 'IC', 'ANT', '<|eot_id|>'], 'evidence_proportions': [116.4, 154.01041666666666, 46.763020833333336, 87.59375, 31.509375]}, 'weight': {'score': [22.775240384615383, 23.436936051324505, 23.106770833333332, 23.43789587307182, 30.03782894736842], 'topk_indices': array([18762, 18806, 14662, 14626, 14707, 19407, 14671, 19465, 14525,
       14590, 20327, 20279, 18100, 18131, 23640, 23506, 21923, 21950,
       23588, 23722]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 23.817708333333332, 20.708333333333332, 26.685546875, 22.8953125]}, 'saliency': {'score': [0.5239246074969952, 0.03955606119522196, 0.3184611002604167, 0.03882567999987366, 0.08854072972347862], 'topk_indices': array([11915,    23, 24150, 11949,    24, 11987,  3036, 11944, 24046,
       11937, 11979, 11943, 11936, 11993, 24002, 11980, 11994, 11995,
       11952, 11951]), 'topk_tokens': ['LECTION', '4', 'Answer', 'AT', '\n\n', 'AY', ' bathroom', 'ING', ' location', 'ATION', 'LE', 'AY', 'BR', 'AT', ' context', 'BR', 'L', 'ANT', 'IC', 'ANT'], 'evidence_proportions': [0.6453369140625, 0.9566650390625, 0.23626200358072916, 0.58746337890625, 0.177587890625]}}, 'pred_res': 'the hallway<|eot_id|>', 'score': 0}
2025-01-22 05:14:23.734 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:14:23.734 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-35_pid-2_1-3-4-6-8.pkl | len: 3 |  size: 1.98 KB
Processing depth (1, 3, 4, 6, 8):   3%|▎         | 3/100 [01:24<45:49, 28.35s/it]is_0k: False
your chose emoji: ['☂', '😛', '🧙\u200d♀️', '🚴\u200d♀️', '🧏🏿\u200d♂️', '🚵🏾', '👨🏾\u200d⚕️', '🦹🏼\u200d♀️', '♈', '🛺']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.34s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.65s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]
Processing depth (1, 2, 4, 5, 6):   3%|▎         | 3/100 [01:43<45:49, 28.35s/it]2025-01-22 05:14:42.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bathroom.
2025-01-22 05:14:42.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary travelled to the
2025-01-22 05:14:42.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk there.
2025-01-22 05:14:42.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4993, 4999) --> . Mary picked up the milk
2025-01-22 05:14:42.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:14:42.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9793, 9799) --> . Mary went back to the
2025-01-22 05:14:42.312 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 05:14:42.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11873, 11877) -->  Mary dropped the milk
2025-01-22 05:14:42.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the office.
2025-01-22 05:14:42.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14368, 14373) --> . John moved to the
2025-01-22 05:14:42.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John got the football.
2025-01-22 05:14:42.487 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6539, 6543) -->  John got the football
2025-01-22 05:14:42.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:14:42.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13597, 13602) --> . Daniel travelled to the
2025-01-22 05:14:42.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:14:42.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10222, 10227) --> . Sandra went to the
2025-01-22 05:14:42.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:14:42.682 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (13918, 13922) -->  John dropped the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:14:45.330 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:14:45.331 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 05:14:53.688 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [123.65985576923077, 6.532829788002642, 46.16493055555556, 6.37737108631814, 9.455899325284092], 'topk_indices': array([24218,     3, 24192, 24108, 24191,  4999, 24213,     1, 24064,
        3036,    23, 24193,    24, 24211, 24098,    14, 24217,     0,
       24214, 24215]), 'topk_tokens': ['\n\n', '<|end_header_id|>', 'Question', ' location', '.\n\n', ' there', ':', '<|start_header_id|>', ' context', ' bathroom', '4', ':', '\n\n', '?\n', '.\n\n', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [163.4, 207.0, 53.114583333333336, 162.421875, 37.55625]}, 'weight': {'score': [22.00330528846154, 23.45326820658905, 23.106770833333332, 23.455085395297377, 29.960404829545453], 'topk_indices': array([18850, 18806, 14652, 14688, 19503, 19445, 14697, 14733, 14551,
       14616, 20333, 20381, 18175, 18144, 23556, 23690, 22022, 21995,
       23772, 23638]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 23.817708333333332, 20.708333333333332, 26.685546875, 18.88125]}, 'saliency': {'score': [0.7415689321664664, 0.035399750589920835, 0.2727322048611111, 0.03446367718029756, 0.07258280840787021], 'topk_indices': array([   19, 24198,  3033,  4994, 24204, 24216,    20, 24206, 24191,
        4998,    23,    24, 24212, 24211,  4999, 24098, 24108, 24192,
       24064,  3036]), 'topk_tokens': ['26', ' prior', ' travelled', ' Mary', ' milk', 'assistant', ' Jul', ' discarded', '.\n\n', ' milk', '4', '\n\n', 'Answer', '?\n', ' there', '.\n\n', ' location', 'Question', ' context', ' bathroom'], 'evidence_proportions': [0.91708984375, 1.2783610026041667, 0.283477783203125, 1.09906005859375, 0.185614013671875]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 05:14:53.694 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:14:53.694 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-35_pid-3_1-2-4-5-6.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 2, 4, 5, 6):   4%|▍         | 4/100 [01:54<46:22, 28.98s/it]is_0k: False
your chose emoji: ['👨🏽\u200d🍳', '🤽🏽\u200d♂️', '👩🏾\u200d❤\u200d👨🏾', '🙅🏽\u200d♂', '💆🏼\u200d♀️', '⛓️\u200d💥', '🤹🏾\u200d♀', '🈂', '⛹🏼\u200d♂', '💇🏽\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 1, 2, 8, 9):   4%|▍         | 4/100 [02:11<46:22, 28.98s/it]2025-01-22 05:15:10.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bathroom.
2025-01-22 05:15:10.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:15:10.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the milk there.
2025-01-22 05:15:10.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2961, 2967) --> . Mary picked up the milk
2025-01-22 05:15:10.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 05:15:10.830 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4864, 4870) --> . Mary went back to the
2025-01-22 05:15:10.830 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary dropped the milk.
2025-01-22 05:15:10.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19215, 19219) -->  dropped the milk.
2025-01-22 05:15:10.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John moved to the office.
2025-01-22 05:15:11.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21442, 21447) --> . John moved to the
2025-01-22 05:15:11.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John got the football.
2025-01-22 05:15:11.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6406, 6410) -->  John got the football
2025-01-22 05:15:11.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:15:11.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13598, 13603) --> . Daniel travelled to the
2025-01-22 05:15:11.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:15:11.188 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10138, 10143) --> . Sandra went to the
2025-01-22 05:15:11.188 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:15:11.254 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (13919, 13923) -->  John dropped the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:15:13.914 | INFO     | test_jbb_embedding:begin_test:693 - the bathroom<|eot_id|>
2025-01-22 05:15:13.914 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24177])
2025-01-22 05:15:22.228 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [131.06340144230768, 4.51987050248139, 26.55251736111111, 4.367122762678157, 4.368506588152985], 'topk_indices': array([   31, 24021, 24137, 24176,    33, 24056,     1, 24171,    35,
          32,    24,    14, 24022,    23,  4870,    34, 24175, 24173,
           0, 24172]), 'topk_tokens': [' travelled', ' you', ' return', '\n\n', ' the', '.\n\n', '<|start_header_id|>', ':', '.', ' to', '\n\n', '\n', ' context', '4', ' hallway', ' bathroom', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [346.75, 107.53125, 113.296875, 70.921875, 13.0484375]}, 'weight': {'score': [21.317908653846153, 23.439717741935485, 23.106770833333332, 23.44225171942327, 29.54804104477612], 'topk_indices': array([18827, 18783, 14689, 14653, 14734, 14698, 19427, 19485, 14552,
       14617, 20299, 20347, 18121, 18152, 23662, 23528, 21967, 21994,
       23744, 23610]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 23.817708333333332, 20.708333333333332, 20.90234375, 18.88125]}, 'saliency': {'score': [0.75543212890625, 0.02537328374592897, 0.15777757432725695, 0.02448809948985372, 0.034354594216417914], 'topk_indices': array([   22, 24150,  1214, 24056, 24021,    19,    30,    24, 24174,
          39,    38, 24020,    20, 24137,    23, 24170,    31, 24022,
        4870,    34]), 'topk_tokens': ['202', 'Question', 'nes', '.\n\n', ' you', '26', 'Mary', '\n\n', 'assistant', '\n\n\n', '***', ' provided', ' Jul', ' return', '4', 'Answer', ' travelled', ' context', ' hallway', ' bathroom'], 'evidence_proportions': [2.1158203125, 0.6593017578125, 0.5428466796875, 0.383270263671875, 0.063232421875]}}, 'pred_res': 'the bathroom<|eot_id|>', 'score': 100}
2025-01-22 05:15:22.236 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:15:22.236 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-35_pid-4_0-1-2-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 2, 8, 9):   5%|▌         | 5/100 [02:23<45:38, 28.82s/it]Processing depth (0, 1, 2, 8, 9):   5%|▌         | 5/100 [02:23<45:28, 28.72s/it]
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:82 - Selected idx: 36
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the garden? 
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:86 - Needle: [' Sandra went to the garden.', ' Mary moved to the bathroom.', ' John got the football.', ' Mary went to the garden.', ' Daniel travelled to the garden.', ' Mary discarded the football.', ' John dropped the football.']
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary went to the garden.', ' Mary discarded the football.', ' John dropped the football.']
2025-01-22 05:15:22.537 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏒', '🧇', '\U0001faf7🏻', '🤯', '🏄🏼\u200d♀️', '🧱', '👨\u200d👨\u200d👧', '👩🏻\u200d❤\u200d💋\u200d👨🏻', '🙅🏼\u200d♂', '♀️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.50s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.34s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  2.97s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
Processing depth (0, 1, 5, 8):   0%|          | 0/100 [00:15<?, ?it/s]2025-01-22 05:15:38.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:15:38.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:15:38.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the garden.
2025-01-22 05:15:38.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Mary went to the
2025-01-22 05:15:38.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 05:15:38.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12000, 12004) -->  discarded the football.
2025-01-22 05:15:38.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:15:38.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19255, 19259) -->  dropped the football.
2025-01-22 05:15:38.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:15:38.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3038, 3043) -->  Mary went to the garden
2025-01-22 05:15:38.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:15:38.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5496, 5500) -->  John got the football
2025-01-22 05:15:38.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:15:38.906 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10838, 10843) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:15:41.504 | INFO     | test_jbb_embedding:begin_test:693 - John<|eot_id|>
2025-01-22 05:15:41.504 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 05:15:49.843 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [499.46527777777777, 14.330088388017709, 284.8125, 13.81141634213034, 12.063209011130137], 'topk_indices': array([   66,     1, 24153, 19257,  5500,  5499,    14, 12002, 24162,
         185,    24,    34,    67, 24166, 24165, 24164,   105,    68,
       24161,     0]), 'topk_tokens': [' P', '<|start_header_id|>', ' football', ' football', '.', ' football', '\n', ' football', '<|start_header_id|>', 'ION', '\n\n', ' bathroom', 'ION', 'b', '\n\n', '<|end_header_id|>', 'ION', 'E', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [608.75, 285.025, 611.75, 518.625]}, 'weight': {'score': [21.04861111111111, 23.44574403161074, 23.378348214285715, 23.44757076790819, 29.92744006849315], 'topk_indices': array([18807, 18763, 14559, 14595, 14640, 19489, 19431, 14604, 14458,
       14523, 20303, 20351, 18101, 18132, 23522, 23656, 21953, 21980,
       23738, 23604]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 19.403125, 23.34765625, 21.23046875]}, 'saliency': {'score': [2.934054904513889, 0.08115430746126236, 1.8531930106026786, 0.07799895457602694, 0.09182195794092465], 'topk_indices': array([    0, 24163, 18906, 19255,    66, 24148,    24,    31, 12000,
       24165,   185, 24166, 24153,    68,    67, 19257,  5499, 12002,
         105,    34]), 'topk_tokens': ['<|begin_of_text|>', 'assistant', ' manner', ' dropped', ' P', 'Question', '\n\n', ' moved', ' discarded', '\n\n', 'ION', 'b', ' football', 'E', 'ION', ' football', ' football', ' football', 'ION', ' bathroom'], 'evidence_proportions': [3.6404296875, 1.41240234375, 3.80859375, 3.07861328125]}}, 'pred_res': 'John<|eot_id|>', 'score': 0}
2025-01-22 05:15:49.861 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:15:49.862 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-36_pid-0_0-1-5-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 1, 5, 8):   1%|          | 1/100 [00:27<44:51, 27.19s/it]is_0k: False
your chose emoji: ['🇹🇨', '🗞️', '🏃🏾\u200d♀️\u200d➡', '🏋🏻\u200d♀️', '👨🏼\u200d🦼', '🦄', '👰🏾\u200d♂️', '🏃🏻\u200d➡', '👯\u200d♂', '🅾️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.26s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.56s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.63s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (1, 4, 5, 7):   1%|          | 1/100 [00:44<44:51, 27.19s/it]2025-01-22 05:16:06.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:16:07.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary moved to
2025-01-22 05:16:07.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the garden.
2025-01-22 05:16:07.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9736, 9741) --> . Mary went to the
2025-01-22 05:16:07.062 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 05:16:07.125 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11903, 11907) -->  Mary discarded the football
2025-01-22 05:16:07.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:16:07.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16700, 16704) -->  John dropped the football
2025-01-22 05:16:07.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:16:07.273 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9737, 9742) -->  Mary went to the garden
2025-01-22 05:16:07.273 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:16:07.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5344, 5348) -->  John got the football
2025-01-22 05:16:07.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:16:07.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10616, 10621) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:16:09.986 | INFO     | test_jbb_embedding:begin_test:693 - John<|eot_id|>
2025-01-22 05:16:09.987 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24176])
2025-01-22 05:16:18.359 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [254.27430555555554, 12.776254704495637, 197.44196428571428, 12.489167909057025, 12.32958984375], 'topk_indices': array([ 7453,    24,  7484,     1,  7444,  7378,  2960,  7497, 24175,
        7483,  7422, 24176,  7489,  7423,  7445, 24171,  7454, 24174,
           0,  7498]), 'topk_tokens': [' or', '\n\n', ' five', '<|start_header_id|>', ' tele', 'nes', ' bathroom', ' two', '\n\n', ' or', ' Min', 'b', ' tele', 'nes', 'graph', '<|eot_id|>', ' three', '<|end_header_id|>', '<|begin_of_text|>', ' or'], 'evidence_proportions': [286.375, 130.8375, 256.0625, 366.65625]}, 'weight': {'score': [24.72439236111111, 23.443577071012037, 23.378348214285715, 23.44266012599909, 29.181690705128204], 'topk_indices': array([18800, 18756, 14595, 14631, 19395, 19453, 14676, 14640, 14494,
       14559, 20315, 20267, 18125, 18094, 23662, 23528, 21929, 21956,
       23610, 23744]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4546875, 19.403125, 29.130859375, 26.056640625]}, 'saliency': {'score': [1.6258138020833333, 0.07309270829025187, 1.2972935267857142, 0.07122548705584544, 0.0882428487141927], 'topk_indices': array([24175, 24163,  7452,  7443, 16703,  7484,  5347,  7483, 24176,
        7488,  7497,  7378,  7444,  7422,  7454,  7423,  2960,  7489,
        7445,  7498]), 'topk_tokens': ['\n\n', ' football', ' two', ' bogus', ' football', ' five', ' football', ' or', 'b', ' bogus', ' two', 'nes', ' tele', ' Min', ' three', 'nes', ' bathroom', ' tele', 'graph', ' or'], 'evidence_proportions': [1.614794921875, 0.641357421875, 1.9473876953125, 2.548583984375]}}, 'pred_res': 'John<|eot_id|>', 'score': 0}
2025-01-22 05:16:18.366 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:16:18.366 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-36_pid-1_1-4-5-7.pkl | len: 3 |  size: 1.99 KB
Processing depth (1, 4, 5, 7):   2%|▏         | 2/100 [00:55<45:40, 27.96s/it]is_0k: False
your chose emoji: ['👆🏽', '🦰', '⚕', '🧛', '👩🏿\u200d🎓', '\U0001faf1🏻\u200d\U0001faf2🏾', '👩\u200d👦\u200d👦', '👩🏾\u200d❤️\u200d💋\u200d👨🏾', '🖋️', '🛀🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.48s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.82s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (2, 4, 6, 7):   2%|▏         | 2/100 [01:12<45:40, 27.96s/it]2025-01-22 05:16:35.782 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:16:35.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4987, 4992) --> . Mary moved to the
2025-01-22 05:16:35.810 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the garden.
2025-01-22 05:16:35.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9678, 9683) --> . Mary went to the
2025-01-22 05:16:35.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 05:16:35.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14348, 14352) -->  discarded the football.
2025-01-22 05:16:35.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:16:36.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16690, 16694) -->  John dropped the football
2025-01-22 05:16:36.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:16:36.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9679, 9684) -->  Mary went to the garden
2025-01-22 05:16:36.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:16:36.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5452, 5456) -->  John got the football
2025-01-22 05:16:36.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:16:36.176 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10572, 10577) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:16:38.939 | INFO     | test_jbb_embedding:begin_test:693 - John got the football.<|eot_id|>
2025-01-22 05:16:38.939 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 05:16:47.328 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [224.36111111111111, 9.635036194415719, 236.65625, 9.343300853249389, 22.144968133223685], 'topk_indices': array([24026,  4988,     9,  5453,     4, 24060,  4989,  5455,  4999,
          14,    23,     1,    24,  4992,  5452, 24167, 24172, 24171,
       24170,     0]), 'topk_tokens': [' context', ' Mary', ':', ' got', '\n\n', '.\n\n', ' moved', ' football', '�', '\n', '4', '<|start_header_id|>', '\n\n', ' bathroom', ' John', '<|eot_id|>', 'b', '\n\n', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [369.475, 151.8875, 223.3125, 134.609375]}, 'weight': {'score': [21.82595486111111, 23.440245604963806, 23.378348214285715, 23.441485044837012, 28.803042763157894], 'topk_indices': array([18794, 18838, 14641, 14677, 19491, 14722, 14686, 19433, 14540,
       14605, 20353, 20305, 18120, 18151, 23516, 23650, 21955, 21982,
       23732, 23598]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 19.403125, 23.34765625, 26.056640625]}, 'saliency': {'score': [1.3082546657986112, 0.054456669709798344, 1.5526646205357142, 0.05265311276819368, 0.1604515878777755], 'topk_indices': array([14350,    22, 24169,    20,  9808,    23, 24070,  4999,  4997,
       24159, 24026,  5453,    24,  4988,  4989,  5455, 24171, 24172,
        5452,  4992]), 'topk_tokens': [' football', '202', 'assistant', ' Jul', ' barric', '4', ' location', '�', '�', ' football', ' context', ' got', '\n\n', ' Mary', ' moved', ' football', '\n\n', 'b', ' John', ' bathroom'], 'evidence_proportions': [2.02939453125, 0.860498046875, 1.3826904296875, 0.89208984375]}}, 'pred_res': 'John got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:16:47.357 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:16:47.358 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-36_pid-2_2-4-6-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (2, 4, 6, 7):   3%|▎         | 3/100 [01:24<45:57, 28.43s/it]is_0k: False
your chose emoji: ['🇸🇲', '🧖🏼', '🧗🏽\u200d♀', '☯️', '🕺🏽', '🧑🏽\u200d🤝\u200d🧑🏾', '🇰🇵', '🇸🇦', '🧎🏿\u200d♀', '👨🏻\u200d🎨']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.98s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.93s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (0, 3, 5, 8):   3%|▎         | 3/100 [01:42<45:57, 28.43s/it]2025-01-22 05:17:05.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:17:05.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:17:05.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the garden.
2025-01-22 05:17:05.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7618, 7623) --> . Mary went to the
2025-01-22 05:17:05.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 05:17:05.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11901, 11905) -->  Mary discarded the football
2025-01-22 05:17:05.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:17:05.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19235, 19239) -->  dropped the football.
2025-01-22 05:17:05.202 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:17:05.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7619, 7624) -->  Mary went to the garden
2025-01-22 05:17:05.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:17:05.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5446, 5450) -->  John got the football
2025-01-22 05:17:05.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:17:05.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10600, 10605) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:17:07.938 | INFO     | test_jbb_embedding:begin_test:693 - John<|eot_id|>
2025-01-22 05:17:07.939 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24142])
2025-01-22 05:17:16.345 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [474.9861111111111, 24.678354731828534, 284.5580357142857, 24.19132055322855, 23.85265112704918], 'topk_indices': array([17449, 24140, 17460, 17467, 24137, 17461, 17484, 17462, 17486,
       17481, 17478, 17468, 17488, 17469, 17487,     0, 17489, 17479,
       17480, 17490]), 'topk_tokens': ['.', '<|end_header_id|>', 'arp', '.L', '<|eot_id|>', 'ente', ' of', 'ur', '.L', 'ur', ' L', '.', ' L', ' The', '.', '<|begin_of_text|>', 'arp', 'arp', 'ente', 'ente'], 'evidence_proportions': [531.8, 318.95, 427.375, 646.625]}, 'weight': {'score': [22.33376736111111, 23.436063367156763, 23.378348214285715, 23.436919724318834, 29.286372950819672], 'topk_indices': array([18787, 18743, 14615, 14579, 19387, 14624, 14660, 19445, 14543,
       14478, 20283, 20331, 18081, 18112, 23494, 23628, 21933, 21960,
       23710, 23576]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 19.403125, 29.130859375, 21.23046875]}, 'saliency': {'score': [2.737155490451389, 0.1398085419794471, 1.8248814174107142, 0.13689130790120993, 0.1734979348104508], 'topk_indices': array([24142, 17484, 17458,    38, 17483, 17487, 17477, 17467, 17469,
       17462, 17481, 17478, 17486, 17460, 17461, 17488, 17489, 17479,
       17480, 17490]), 'topk_tokens': ['b', ' of', ' Rosa', '***', ' daughter', '.', ' Rosa', '.L', ' The', 'ur', 'ur', ' L', '.L', 'arp', 'ente', ' L', 'arp', 'arp', 'ente', 'ente'], 'evidence_proportions': [2.70439453125, 1.680712890625, 3.260498046875, 3.5753173828125]}}, 'pred_res': 'John<|eot_id|>', 'score': 0}
2025-01-22 05:17:16.353 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:17:16.353 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-36_pid-3_0-3-5-8.pkl | len: 3 |  size: 1.91 KB
Processing depth (0, 3, 5, 8):   4%|▍         | 4/100 [01:53<45:50, 28.65s/it]is_0k: False
your chose emoji: ['🚣\u200d♀', '💂\u200d♂️', '👩\u200d🚒', '🧵', '🧛🏾\u200d♂', '👩🏼', '👨🏾\u200d🎨', '🇪🇨', '↗️', '👇🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.53s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (0, 2, 4, 6):   4%|▍         | 4/100 [02:10<45:50, 28.65s/it]2025-01-22 05:17:33.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:17:33.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:17:33.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the garden.
2025-01-22 05:17:33.504 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4917, 4922) --> . Mary went to the
2025-01-22 05:17:33.504 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary discarded the football.
2025-01-22 05:17:33.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9791, 9795) -->  Mary discarded the football
2025-01-22 05:17:33.552 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the football.
2025-01-22 05:17:33.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14358, 14362) -->  John dropped the football
2025-01-22 05:17:33.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:17:33.645 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4918, 4923) -->  Mary went to the garden
2025-01-22 05:17:33.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John got the football.
2025-01-22 05:17:33.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5356, 5360) -->  John got the football
2025-01-22 05:17:33.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel travelled to the garden.
2025-01-22 05:17:33.731 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10587, 10592) --> . Daniel travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:17:36.489 | INFO     | test_jbb_embedding:begin_test:693 - John got the football.<|eot_id|>
2025-01-22 05:17:36.489 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24146])
2025-01-22 05:17:44.875 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [299.3298611111111, 12.032446995734814, 195.671875, 11.711415961769706, 13.399042038690476], 'topk_indices': array([  105,     4,   165,    31,    32, 11918,    23,   184,    14,
         181,     1, 24146, 24145, 22818,    24,    34, 24144,   185,
       24141,     0]), 'topk_tokens': ['ION', '\n\n', 'ION', ' moved', ' to', 'IVE', '4', ' P', '\n', 'G', '<|start_header_id|>', 'b', '\n\n', 'nes', '\n\n', ' bathroom', '<|end_header_id|>', 'ION', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [589.2, 202.525, 189.921875, 167.40625]}, 'weight': {'score': [23.40625, 23.438232431984762, 23.378348214285715, 23.438291065431024, 29.516369047619047], 'topk_indices': array([18776, 18820, 14600, 14636, 14681, 19473, 14645, 19415, 14499,
       14564, 20287, 20335, 18093, 18062, 23638, 23504, 21937, 21964,
       23720, 23586]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 19.403125, 29.130859375, 26.056640625]}, 'saliency': {'score': [1.8290337456597223, 0.06895924274005912, 1.2401035853794642, 0.06696573766689472, 0.09801035078745039], 'topk_indices': array([   23, 22817,     0,    20,   184,   105,  9799, 24133,   165,
         181, 24145, 11918, 24146,  5359,    24,    31, 18776, 22818,
         185,    34]), 'topk_tokens': ['4', ' Min', '<|begin_of_text|>', ' Jul', ' P', 'ION', ' barric', ' football', 'ION', 'G', '\n\n', 'IVE', 'b', ' football', '\n\n', ' moved', 'untlet', 'nes', 'ION', ' bathroom'], 'evidence_proportions': [3.49765625, 1.0107421875, 1.45318603515625, 1.1419677734375]}}, 'pred_res': 'John got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:17:44.888 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:17:44.888 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-36_pid-4_0-2-4-6.pkl | len: 3 |  size: 1.98 KB
Processing depth (0, 2, 4, 6):   5%|▌         | 5/100 [02:22<45:18, 28.61s/it]Processing depth (0, 2, 4, 6):   5%|▌         | 5/100 [02:22<45:08, 28.51s/it]
2025-01-22 05:17:45.201 | INFO     | __main__:<module>:82 - Selected idx: 37
2025-01-22 05:17:45.202 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 05:17:45.204 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:17:45.204 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:17:45.204 | INFO     | __main__:<module>:86 - Needle: [' Sandra went to the garden.', ' Mary moved to the bathroom.', ' Daniel travelled to the garden.', ' Mary picked up the apple.', ' John got the football.', ' Mary went to the garden.', ' Mary discarded the apple.', ' John dropped the football.']
2025-01-22 05:17:45.204 | INFO     | __main__:<module>:87 - Real Needle: [' Mary moved to the bathroom.', ' Mary picked up the apple.', ' Mary went to the garden.', ' Mary discarded the apple.', ' John dropped the football.']
2025-01-22 05:17:45.204 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🥅', '🧙🏿\u200d♀️', '🦵', '🧎🏿\u200d♀️\u200d➡️', '👨🏻\u200d🦲', '\U0001faab', '🕉', '🚴🏿\u200d♀️', '👩\u200d🚒', '📮']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.16s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.32s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.00s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.44s/it]
Processing depth (0, 2, 4, 6, 9):   0%|          | 0/100 [00:15<?, ?it/s]2025-01-22 05:18:01.198 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:18:01.199 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:18:01.199 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 05:18:01.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4911, 4916) --> . Mary picked up the
2025-01-22 05:18:01.225 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the garden.
2025-01-22 05:18:01.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9779, 9784) --> . Mary went to the
2025-01-22 05:18:01.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 05:18:01.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14331, 14335) -->  Mary discarded the apple
2025-01-22 05:18:01.372 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John dropped the football.
2025-01-22 05:18:01.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21526, 21530) -->  John dropped the football
2025-01-22 05:18:01.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:18:01.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9780, 9785) -->  Mary went to the garden
2025-01-22 05:18:01.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:18:01.666 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20513, 20518) --> . Daniel travelled to the
2025-01-22 05:18:01.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John got the football.
2025-01-22 05:18:01.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11156, 11160) -->  John got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:18:04.511 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the apple.<|eot_id|>
2025-01-22 05:18:04.512 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 05:18:12.897 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [445.76630434782606, 16.29153232473545, 123.63058035714286, 15.820380744152349, 15.091492491883116], 'topk_indices': array([   14, 22988, 24188, 14335,    31,    35, 24189,   105, 24187,
          24, 22989, 24176, 14332,    34,  4916,  4917,   185, 24185,
       24184,     0]), 'topk_tokens': ['\n', ' pot', '\n\n', '.', ' moved', '.', 'b', 'ION', '<|end_header_id|>', '\n\n', 'ation', ' discarded', ' discarded', ' bathroom', ' apple', '.', 'ION', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [675.85, 521.95, 245.325, 610.625, 148.625]}, 'weight': {'score': [23.077445652173914, 23.445867952215607, 23.378348214285715, 23.44625789174084, 29.674107142857142], 'topk_indices': array([18743, 18787, 14613, 14649, 19382, 14694, 19440, 14658, 14512,
       14577, 20254, 20302, 18112, 18081, 23524, 23658, 21947, 21974,
       23606, 23740]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 22.1, 19.403125, 28.873046875, 26.056640625]}, 'saliency': {'score': [2.7553498641304346, 0.09231125362335689, 0.7855115618024554, 0.089373779296875, 0.1133307915229302], 'topk_indices': array([  165,   181, 14331, 24162,    24,  4912, 24189, 24078,  4913,
         105, 24174, 22989, 14334,    31, 22988,   185,  4916, 24176,
       14332,    34]), 'topk_tokens': ['ION', 'G', ' Mary', 'Question', '\n\n', ' Mary', 'b', ' location', ' picked', 'ION', ' apple', 'ation', ' apple', ' moved', ' pot', 'ION', ' apple', ' discarded', ' discarded', ' bathroom'], 'evidence_proportions': [3.9609375, 2.8767578125, 1.21796875, 4.75, 1.023681640625]}}, 'pred_res': 'Mary discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:18:12.904 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:18:12.904 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-37_pid-0_0-2-4-6-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 2, 4, 6, 9):   1%|          | 1/100 [00:27<45:25, 27.53s/it]is_0k: False
your chose emoji: ['👆🏽', '👨🏽\u200d❤️\u200d💋\u200d👨🏾', '👛', '🈯', '🇧🇿', '💮', '👨🏿\u200d🔧', '🏊🏾\u200d♀️', '🧛🏾\u200d♀️', '🦹🏾\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.64s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.85s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (2, 5, 6, 7, 8):   1%|          | 1/100 [00:45<45:25, 27.53s/it]2025-01-22 05:18:30.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:18:30.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4951, 4956) --> . Mary moved to the
2025-01-22 05:18:30.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 05:18:30.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11896, 11901) --> . Mary picked up the
2025-01-22 05:18:30.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the garden.
2025-01-22 05:18:30.883 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14350, 14355) --> . Mary went to the
2025-01-22 05:18:30.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 05:18:30.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16699, 16703) -->  Mary discarded the apple
2025-01-22 05:18:30.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John dropped the football.
2025-01-22 05:18:31.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19247, 19251) -->  dropped the football.
2025-01-22 05:18:31.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:18:31.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14351, 14356) -->  Mary went to the garden
2025-01-22 05:18:31.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:18:31.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20496, 20501) --> . Daniel travelled to the
2025-01-22 05:18:31.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John got the football.
2025-01-22 05:18:31.307 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11154, 11158) -->  John got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:18:34.087 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the apple.<|eot_id|>
2025-01-22 05:18:34.088 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24185])
2025-01-22 05:18:42.490 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [255.58423913043478, 9.640539730444848, 90.05133928571429, 9.359704308310215, 10.173229166666667], 'topk_indices': array([   14, 24064, 24185, 16702, 24184,     1,    23,    24, 24170,
       16703, 24030, 11902, 24172, 11901, 24183, 16700,  4956,     0,
       24180, 24181]), 'topk_tokens': ['\n', '.\n\n', 'b', ' apple', '\n\n', '<|start_header_id|>', '4', '\n\n', ' apple', '.', ' context', '.', ' discarded', ' apple', '<|end_header_id|>', ' discarded', ' bathroom', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [337.275, 301.825, 125.1125, 490.0, 24.34375]}, 'weight': {'score': [22.007133152173914, 23.44303735323301, 23.378348214285715, 23.44444232381682, 29.39], 'topk_indices': array([18755, 18799, 14594, 14630, 19457, 19399, 14675, 14639, 14558,
       14493, 20319, 20271, 18124, 18093, 23534, 23668, 21939, 21966,
       23616, 23750]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 22.1, 19.403125, 28.873046875, 21.23046875]}, 'saliency': {'score': [1.5672819718070652, 0.05410275050031266, 0.5711713518415179, 0.05236194960143343, 0.07509918212890625], 'topk_indices': array([ 4952, 24029,    23,    24, 24158, 24185, 24028, 24168, 24074,
       11898, 16699, 24164,  4953, 16702, 24170, 24030, 11901, 24172,
       16700,  4956]), 'topk_tokens': [' Mary', ' you', '4', '\n\n', 'Question', 'b', ' provided', ' where', ' location', ' picked', ' Mary', ' prior', ' moved', ' apple', ' apple', ' context', ' apple', ' discarded', ' discarded', ' bathroom'], 'evidence_proportions': [1.70966796875, 1.66767578125, 0.64150390625, 3.8485107421875, 0.139801025390625]}}, 'pred_res': 'Mary discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:18:42.496 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:18:42.496 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-37_pid-1_2-5-6-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 5, 6, 7, 8):   2%|▏         | 2/100 [00:57<46:56, 28.74s/it]is_0k: False
your chose emoji: ['🧛🏿\u200d♂️', '🙅🏼', '🚶\u200d♀️\u200d➡', '🎢', '🛌🏿', '👨🏾\u200d🤝\u200d👨🏻', '🐹', '👨🏾\u200d✈', '👳🏻\u200d♂', '\U0001faf0']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.40s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.69s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.54s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Processing depth (0, 1, 2, 4, 6):   2%|▏         | 2/100 [01:13<46:56, 28.74s/it]2025-01-22 05:18:59.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:18:59.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:18:59.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 05:18:59.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Mary picked up the
2025-01-22 05:18:59.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the garden.
2025-01-22 05:18:59.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4907, 4912) --> . Mary went to the
2025-01-22 05:18:59.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 05:18:59.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9724, 9728) -->  Mary discarded the apple
2025-01-22 05:18:59.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John dropped the football.
2025-01-22 05:18:59.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14328, 14332) -->  John dropped the football
2025-01-22 05:18:59.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:18:59.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4908, 4913) -->  Mary went to the garden
2025-01-22 05:18:59.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:18:59.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20510, 20515) --> . Daniel travelled to the
2025-01-22 05:18:59.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John got the football.
2025-01-22 05:18:59.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11127, 11131) -->  John got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:19:02.549 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 05:19:02.549 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 05:19:10.910 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [627.0625, 20.744690345612582, 148.03125, 20.0927265990963, 21.39821337090164], 'topk_indices': array([  185,    33,    24,   116,    28,   117,   114, 20628, 20646,
          32,    31, 20627, 24152,    29,   118,    34, 20647,    35,
       24153,     0]), 'topk_tokens': ['ION', ' the', '\n\n', ' Collection', '<|end_header_id|>', ' of', '\n\n', 'ot', ' Min', ' to', ' moved', 'nes', '<|eot_id|>', '\n\n', ' Articles', ' bathroom', 'nes', '.', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1755.8, 367.85, 186.1125, 547.5, 170.90625]}, 'weight': {'score': [23.077445652173914, 23.437575020695363, 23.378348214285715, 23.437952757741574, 29.665727459016395], 'topk_indices': array([18784, 18740, 14610, 14646, 19379, 19437, 14655, 14691, 14509,
       14574, 20251, 20299, 18109, 18078, 23494, 23628, 21917, 21944,
       23710, 23576]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 22.1, 19.403125, 28.873046875, 26.056640625]}, 'saliency': {'score': [3.5285326086956523, 0.11744148304920322, 0.9318760463169643, 0.1137165243054476, 0.16411978299500513], 'topk_indices': array([   35,   114,    52,    30,    27,   185,    38, 20628,    39,
           0,  9725,    29,   116, 24144, 20646,    31, 20627,   118,
       20647,    34]), 'topk_tokens': ['.', '\n\n', ' Gutenberg', 'Mary', 'user', 'ION', '***', 'ot', '\n\n\n', '<|begin_of_text|>', ' discarded', '\n\n', ' Collection', ' discarded', ' Min', ' moved', 'nes', ' Articles', 'nes', ' bathroom'], 'evidence_proportions': [9.002734375, 2.02802734375, 0.9494140625, 4.260498046875, 1.0533447265625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 05:19:10.917 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:19:10.918 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-37_pid-2_0-1-2-4-6.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 2, 4, 6):   3%|▎         | 3/100 [01:25<46:13, 28.60s/it]is_0k: False
your chose emoji: ['🦻🏿', '😫', '😷', '🦹🏾\u200d♀', '🚴🏾\u200d♂', '🧎🏿\u200d➡', '🙏🏻', '🦶🏻', '🛋️', '🧛🏿\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.95s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (0, 1, 3, 5, 7):   3%|▎         | 3/100 [01:43<46:13, 28.60s/it]2025-01-22 05:19:28.768 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:19:28.768 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the bathroom.
2025-01-22 05:19:28.768 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 05:19:28.785 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2964, 2969) -->  tragedy. Mary picked up
2025-01-22 05:19:28.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the garden.
2025-01-22 05:19:28.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7588, 7593) -->  war. Mary went to
2025-01-22 05:19:28.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 05:19:28.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11902, 11906) -->  discarded the apple.
2025-01-22 05:19:28.890 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John dropped the football.
2025-01-22 05:19:28.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16760, 16764) -->  John dropped the football
2025-01-22 05:19:28.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:19:29.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7590, 7595) -->  Mary went to the garden
2025-01-22 05:19:29.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:19:29.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20494, 20499) --> . Daniel travelled to the
2025-01-22 05:19:29.125 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John got the football.
2025-01-22 05:19:29.182 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11164, 11168) -->  John got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:19:31.868 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:19:31.868 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24175])
2025-01-22 05:19:40.246 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [999.476902173913, 65.79235255190669, 479.74107142857144, 64.66273792510667, 111.75552455357143], 'topk_indices': array([18899,   125,  2991, 20556, 18900, 20577,    49, 20646, 24171,
         126,   124, 20664, 18901,   127, 20645, 18902,   128,  8213,
       20665,     0]), 'topk_tokens': ['\n', 'A', ' its', '.', 'the', 'Mr', '\n\n\n', 'ot', '<|start_header_id|>', ' Collection', '\n\n', ' Min', ' manner', ' of', 'nes', ' in', ' Articles', ' Minneapolis', 'nes', '<|begin_of_text|>'], 'evidence_proportions': [2256.4, 672.925, 781.25, 1003.25, 105.5234375]}, 'weight': {'score': [24.087296195652176, 23.438867462155677, 23.378348214285715, 23.438284777453294, 29.088169642857142], 'topk_indices': array([18758, 18802, 14633, 14597, 19455, 14678, 14642, 19397, 14496,
       14561, 20269, 20317, 18096, 18127, 23652, 23518, 21957, 21984,
       23734, 23600]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.709375, 27.9078125, 22.8671875, 23.08984375, 26.056640625]}, 'saliency': {'score': [5.862357761548913, 0.38079402310468197, 3.0035574776785716, 0.3740505305257394, 0.8388163975306919], 'topk_indices': array([   48,  2991,    67,    31, 20646, 18900,     0,   124, 18902,
          49,    62,    34, 20577,   126, 20664, 18901, 20645,   128,
        8213, 20665]), 'topk_tokens': ['***', ' its', ' Proof', ' moved', 'ot', 'the', '<|begin_of_text|>', '\n\n', ' in', '\n\n\n', ' Gutenberg', ' bathroom', 'Mr', ' Collection', ' Min', ' manner', 'nes', ' Articles', ' Minneapolis', 'nes'], 'evidence_proportions': [13.10546875, 4.311328125, 4.1146484375, 6.07763671875, 0.71661376953125]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:19:40.254 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:19:40.254 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-37_pid-3_0-1-3-5-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 1, 3, 5, 7):   4%|▍         | 4/100 [01:54<46:13, 28.89s/it]is_0k: False
your chose emoji: ['🕵\u200d♀', '👆🏼', '👨🏻\u200d🎓', '👮🏼\u200d♀️', '🥃', '👴🏻', '👰🏾\u200d♀️', '🤶🏼', '🧑🏼\u200d🦲', '\U0001fac4🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.26s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (2, 4, 6, 7, 8):   4%|▍         | 4/100 [02:12<46:13, 28.89s/it]2025-01-22 05:19:58.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bathroom.
2025-01-22 05:19:58.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4925, 4930) --> . Mary moved to the
2025-01-22 05:19:58.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary picked up the apple.
2025-01-22 05:19:58.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9686, 9691) -->  war. Mary picked up
2025-01-22 05:19:58.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the garden.
2025-01-22 05:19:58.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14334, 14339) --> . Mary went to the
2025-01-22 05:19:58.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the apple.
2025-01-22 05:19:58.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16715, 16719) -->  Mary discarded the apple
2025-01-22 05:19:58.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John dropped the football.
2025-01-22 05:19:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19217, 19221) -->  dropped the football.
2025-01-22 05:19:58.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 05:19:58.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14335, 14340) -->  Mary went to the garden
2025-01-22 05:19:58.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel travelled to the garden.
2025-01-22 05:19:58.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20478, 20483) --> . Daniel travelled to the
2025-01-22 05:19:58.802 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John got the football.
2025-01-22 05:19:58.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11138, 11142) -->  John got the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:20:01.650 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the apple.<|eot_id|>
2025-01-22 05:20:01.651 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24187])
2025-01-22 05:20:10.112 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [617.6711956521739, 19.27027955766846, 242.15178571428572, 18.571254088519023, 22.57259971217105], 'topk_indices': array([24185, 16714,  4927, 24173, 24167,  9687, 24183, 16715, 24170,
       16718,  9689, 16719,  4930, 24172, 24174, 24182, 16716,  9693,
           0,  9692]), 'topk_tokens': ['<|end_header_id|>', '.', ' moved', ' was', ' to', '.', '<|start_header_id|>', ' Mary', ' where', ' apple', ' picked', '.', ' bathroom', ' apple', ' discarded', '<|eot_id|>', ' discarded', '.', '<|begin_of_text|>', ' apple'], 'evidence_proportions': [591.25, 794.5, 441.25, 1188.875, 78.984375]}, 'weight': {'score': [22.760190217391305, 23.444016122364612, 23.378348214285715, 23.444705367863204, 29.46792763157895], 'topk_indices': array([18829, 18785, 14618, 14654, 19487, 14699, 19429, 14663, 14582,
       14517, 20349, 20301, 18154, 18123, 23670, 23536, 21978, 21951,
       23618, 23752]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.646875, 25.5640625, 19.403125, 28.873046875, 21.23046875]}, 'saliency': {'score': [3.9384420643682065, 0.10978408441310976, 1.5555419921875, 0.10530017996033748, 0.1646682337710732], 'topk_indices': array([24169, 24173, 24165, 24162,  9693, 24160, 24032,  9688, 14339,
        4927, 24166, 24170, 16715, 16718,  9689, 24172,  4930, 24174,
       16716,  9692]), 'topk_tokens': [' place', ' was', ' location', ' Where', '.', 'Question', ' context', ' Mary', ' garden', ' moved', ' prior', ' where', ' Mary', ' apple', ' picked', ' apple', ' bathroom', ' discarded', ' discarded', ' apple'], 'evidence_proportions': [3.068359375, 4.8873046875, 2.2732421875, 9.4462890625, 0.4136199951171875]}}, 'pred_res': 'Mary discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:20:10.119 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:20:10.119 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-37_pid-4_2-4-6-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 4, 6, 7, 8):   5%|▌         | 5/100 [02:24<46:17, 29.24s/it]Processing depth (2, 4, 6, 7, 8):   5%|▌         | 5/100 [02:25<45:56, 29.02s/it]
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:82 - Selected idx: 38
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the hallway? 
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:86 - Needle: [' Daniel travelled to the garden.', ' John moved to the office.', ' Mary got the football.', ' Sandra went to the garden.', ' Mary travelled to the office.', ' Sandra picked up the milk.', ' Daniel journeyed to the office.', ' Mary journeyed to the hallway.', ' John went to the kitchen.']
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:87 - Real Needle: [' Mary got the football.', ' Mary travelled to the office.', ' Mary journeyed to the hallway.', ' John went to the kitchen.']
2025-01-22 05:20:10.472 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🙆🏽\u200d♀️', '🥳', '👩\u200d🦯', '🤷🏼', '👩🏻\u200d💼', '🧎🏾\u200d♀️\u200d➡', '🙇🏽\u200d♀', '👩🏾\u200d🤝\u200d👨🏽', '😩', '🤤']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.73s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.69s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]
Processing depth (4, 7, 8, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:20:27.763 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:20:27.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9753, 9757) -->  Mary got the football
2025-01-22 05:20:27.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:20:27.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16811, 16816) --> . Mary travelled to the
2025-01-22 05:20:27.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:20:28.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19298, 19304) -->  Mary journeyed to the hallway
2025-01-22 05:20:28.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:20:28.160 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21504, 21509) --> . John went to the
2025-01-22 05:20:28.160 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:20:28.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22290, 22295) --> . Daniel travelled to the
2025-01-22 05:20:28.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:20:28.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14767, 14772) --> . John moved to the
2025-01-22 05:20:28.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:20:28.407 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7718, 7723) --> . Sandra went to the
2025-01-22 05:20:28.408 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:20:28.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2841, 2846) --> . Sandra picked up the
2025-01-22 05:20:28.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:20:28.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4344, 4350) -->  Daniel journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:20:31.263 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:20:31.263 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24220])
2025-01-22 05:20:39.673 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [567.596875, 22.30942944430683, 374.4014423076923, 21.47967509099934, 42.53483780570652], 'topk_indices': array([   24,  9766, 24108,    14,  9752, 24214, 24203, 24210,  9753,
        2847,  9754, 24208, 24207, 24218, 24215,  9755, 24074,  9757,
           0,  9756]), 'topk_tokens': ['\n\n', ' part', '.\n\n', '\n', '.', ':', ':', ' hallway', ' Mary', '.', ' got', ' before', ' football', '<|end_header_id|>', '<|eot_id|>', ' the', ' context', '.', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1844.5, 272.0, 395.8333333333333, 47.7875]}, 'weight': {'score': [22.71015625, 23.453043720584592, 21.64032451923077, 23.45560776958554, 29.68546195652174], 'topk_indices': array([18807, 18851, 14691, 14655, 14700, 19453, 19511, 14736, 14554,
       14619, 20385, 20337, 18160, 18129, 23688, 23554, 21987, 22014,
       23636, 23770]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.75625, 25.131510416666668, 18.6375]}, 'saliency': {'score': [3.849298095703125, 0.12691771644514802, 2.1897348257211537, 0.12161986517006794, 0.31965521107549255], 'topk_indices': array([24118, 24072, 24108,    51,    54, 24204, 24189, 24213, 14769,
        9766,  2842, 24202, 19303, 24208,  9754,  9753, 24207, 24210,
       24074,  9756]), 'topk_tokens': [' location', ' provided', '.\n\n', ' Proof', '\n\n\n\n', ' Where', ' return', 'Answer', ' moved', ' part', ' Sandra', 'Question', ' hallway', ' before', ' got', ' Mary', ' football', ' hallway', ' context', ' football'], 'evidence_proportions': [12.904296875, 1.559716796875, 2.7454427083333335, 0.2195068359375]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:20:39.680 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:20:39.680 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-38_pid-0_4-7-8-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (4, 7, 8, 9):   1%|          | 1/100 [00:29<47:59, 29.09s/it]is_0k: False
your chose emoji: ['🧔\u200d♀', '👨\u200d❤️\u200d👨', '👩🏾\u200d❤\u200d👩🏼', '🚣🏼\u200d♂️', '🧷', '🚶🏽\u200d♀', '👩🏽\u200d❤\u200d👨🏻', '🙍🏼\u200d♂️', '👴🏽', '🧑🏾\u200d🍳']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.38s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (1, 4, 6, 9):   1%|          | 1/100 [00:46<47:59, 29.09s/it]2025-01-22 05:20:56.944 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:20:56.961 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3038, 3042) -->  Mary got the football
2025-01-22 05:20:56.961 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:20:57.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9885, 9890) --> . Mary travelled to the
2025-01-22 05:20:57.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:20:57.109 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14449, 14455) --> . Mary journeyed to the
2025-01-22 05:20:57.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:20:57.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21460, 21465) --> . John went to the
2025-01-22 05:20:57.224 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:20:57.346 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22260, 22265) --> . Daniel travelled to the
2025-01-22 05:20:57.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:20:57.435 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15011, 15016) -->  bonds. John moved to
2025-01-22 05:20:57.435 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:20:57.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7857, 7862) --> . Sandra went to the
2025-01-22 05:20:57.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:20:57.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2891, 2896) --> . Sandra picked up the
2025-01-22 05:20:57.495 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:20:57.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4463, 4469) -->  Daniel journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:21:00.344 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:21:00.344 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24194])
2025-01-22 05:21:08.733 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [795.571875, 30.923380930732353, 553.1610576923077, 29.727888198757764, 35.866371637658226], 'topk_indices': array([14455,  3037, 24082, 24184, 24177, 24192,  3039,    14,  2892,
        2891, 24048,  3040, 24189,  2897, 24188, 24182, 24181,  3042,
        3041,     0]), 'topk_tokens': [' hallway', '.', '.\n\n', ' hallway', ':', '<|end_header_id|>', ' got', '\n', ' Sandra', '.', ' context', ' the', '<|eot_id|>', '.', ':', ' before', ' football', '.', ' football', '<|begin_of_text|>'], 'evidence_proportions': [2200.25, 516.25, 710.7916666666666, 52.8875]}, 'weight': {'score': [21.186328125, 23.449867746735, 22.48798076923077, 23.452777885610764, 30.22448575949367], 'topk_indices': array([18786, 18830, 14654, 14618, 19483, 14699, 14663, 19425, 14517,
       14582, 20335, 20383, 18155, 18124, 23686, 23552, 22012, 21985,
       23634, 23768]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.75625, 20.052083333333332, 18.6375]}, 'saliency': {'score': [4.9038543701171875, 0.17602722702745288, 3.2360088641826925, 0.16881745206149715, 0.2681328495846519], 'topk_indices': array([ 2899,  2864,  2890, 24046, 24092,     0, 24178,  3038, 24187,
        2896, 24176,  2944,  3039, 24048, 24182, 14455, 24184,  2892,
       24181,  3041]), 'topk_tokens': ['ible', ' During', ' Gov', ' provided', ' location', '<|begin_of_text|>', ' Where', ' Mary', 'Answer', ' milk', 'Question', 'lyn', ' got', ' context', ' before', ' hallway', ' hallway', ' Sandra', ' football', ' football'], 'evidence_proportions': [14.947265625, 2.976953125, 3.69091796875, 0.25155029296875]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:21:08.740 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:21:08.740 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-38_pid-1_1-4-6-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 4, 6, 9):   2%|▏         | 2/100 [00:58<47:29, 29.07s/it]is_0k: False
your chose emoji: ['💁🏾\u200d♂', '™', '🚶🏻\u200d♂\u200d➡️', '💆🏻\u200d♀', '👩🏻\u200d❤️\u200d💋\u200d👩🏽', '⛵', '🙎🏻\u200d♂️', '🤝🏻', '🚵🏽\u200d♀️', '🥬']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.64s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.93s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.38s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
Processing depth (5, 6, 7, 8):   2%|▏         | 2/100 [01:15<47:29, 29.07s/it]2025-01-22 05:21:26.685 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:21:26.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11982, 11986) -->  got the football.
2025-01-22 05:21:26.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:21:26.819 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14593, 14598) --> . Mary travelled to the
2025-01-22 05:21:26.820 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:21:26.906 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16806, 16812) -->  affair. Mary journeyed to
2025-01-22 05:21:26.906 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:21:27.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19436, 19441) --> . John went to the
2025-01-22 05:21:27.014 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:21:27.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22226, 22231) --> . Daniel travelled to the
2025-01-22 05:21:27.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:21:27.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15071, 15076) --> . John moved to the
2025-01-22 05:21:27.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:21:27.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7820, 7825) --> . Sandra went to the
2025-01-22 05:21:27.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:21:27.275 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2891, 2896) --> . Sandra picked up the
2025-01-22 05:21:27.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:21:27.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4458, 4464) -->  Daniel journeyed to the office
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:21:29.944 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 05:21:29.944 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24156])
2025-01-22 05:21:38.307 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [428.4578125, 15.74730549921351, 192.8221153846154, 15.21403761612475, 28.782291666666666], 'topk_indices': array([24008, 24129, 11962,    23, 24139, 11981, 24044, 24144, 11983,
       24009, 24150, 11982, 24143, 11985, 24154, 24152, 24010, 11984,
           0, 24151]), 'topk_tokens': [' provided', '.', ' way', '4', ':', ' Mary', '.\n\n', ' before', ' the', ' you', ':', ' got', ' football', '.', '<|end_header_id|>', '<|start_header_id|>', ' context', ' football', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [1228.125, 463.65, 172.49479166666666, 60.6875]}, 'weight': {'score': [21.275390625, 23.43465415183376, 21.64032451923077, 23.438380007879893, 29.307552083333334], 'topk_indices': array([18732, 18776, 14553, 14589, 14604, 19429, 19371, 14640, 14452,
       14517, 20249, 20297, 18101, 18070, 23508, 23642, 21905, 21932,
       23724, 23590]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.828125, 20.75625, 24.204427083333332, 18.6375]}, 'saliency': {'score': [2.478265380859375, 0.08981304798746792, 1.1756638746995192, 0.08666104200903596, 0.21018473307291666], 'topk_indices': array([11966, 11980, 24140, 24125, 24149, 11962, 16813, 24054, 24009,
       24138, 24008,  7821, 24144, 14595, 24146, 11981, 11982, 24143,
       24010, 11984]), 'topk_tokens': ['low', '!"', ' Where', ' return', 'Answer', ' way', ' hallway', ' location', ' you', 'Question', ' provided', ' Sandra', ' before', ' travelled', ' hallway', ' Mary', ' got', ' football', ' context', ' football'], 'evidence_proportions': [7.2529296875, 2.64384765625, 0.9771728515625, 0.2942626953125]}}, 'pred_res': 'the office<|eot_id|>', 'score': 100}
2025-01-22 05:21:38.321 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:21:38.322 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-38_pid-2_5-6-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (5, 6, 7, 8):   3%|▎         | 3/100 [01:27<47:22, 29.31s/it]is_0k: False
your chose emoji: ['🈵', '🧏🏿\u200d♂', '👷🏼\u200d♂', '\U0001fac3', '🧫', '🚶🏻', '🧚🏿\u200d♀️', '🧜🏾\u200d♀️', '🥳', '🙆🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.70s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.47s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.07s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (0, 1, 2, 5):   3%|▎         | 3/100 [01:44<47:22, 29.31s/it]2025-01-22 05:21:54.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:21:55.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:21:55.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2921, 2926) -->  Mary travelled to the office
2025-01-22 05:21:55.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:21:55.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4895, 4901) --> . Mary journeyed to the
2025-01-22 05:21:55.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:21:55.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11890, 11895) --> . John went to the
2025-01-22 05:21:55.183 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:21:55.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22288, 22293) --> . Daniel travelled to the
2025-01-22 05:21:55.301 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:21:55.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14669, 14674) -->  bonds. John moved to
2025-01-22 05:21:55.377 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:21:55.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7730, 7735) --> . Sandra went to the
2025-01-22 05:21:55.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:21:55.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2846, 2851) --> . Sandra picked up the
2025-01-22 05:21:55.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:21:55.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4348, 4354) --> . Daniel journeyed to the
2025-01-22 05:21:55.460 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:21:55.460 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:21:55.460 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:21:55.460 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🧑🏽\u200d🍼', '👩🏿\u200d❤️\u200d👨🏽', '👩🏽\u200d❤️\u200d👩🏼', '🏊\u200d♀️', '\U0001faf1🏽\u200d\U0001faf2🏿', '🌯', '🙆🏿\u200d♂', '🏃🏿\u200d♀\u200d➡', '🦹\u200d♂️', '🧘🏿\u200d♀']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.91s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (0, 1, 3, 5):   3%|▎         | 3/100 [02:01<47:22, 29.31s/it]2025-01-22 05:22:12.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:22:12.712 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:22:12.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2965, 2970) -->  tragedy. Mary travelled to
2025-01-22 05:22:12.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:22:12.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7604, 7610) -->  war. Mary journeyed to
2025-01-22 05:22:12.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:22:12.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11862, 11867) --> . John went to the
2025-01-22 05:22:12.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:22:12.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22235, 22240) -->  Daniel travelled to the garden
2025-01-22 05:22:12.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:22:13.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14744, 14749) --> . John moved to the
2025-01-22 05:22:13.025 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:22:13.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7712, 7717) --> . Sandra went to the
2025-01-22 05:22:13.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:22:13.084 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2846, 2851) --> . Sandra picked up the
2025-01-22 05:22:13.084 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:22:13.107 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4395, 4401) -->  execution. Daniel journeyed to
2025-01-22 05:22:13.107 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:22:13.107 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:22:13.107 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:22:13.107 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🧑🏻\u200d🦯\u200d➡', '🧎🏿\u200d➡️', '🛃', '🦙', '🤾🏿\u200d♂️', '👩🏼\u200d⚖', '✍️', '🦸🏾\u200d♀', '🏵', '👩🏾\u200d🦯\u200d➡️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.40s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.36s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it]
Processing depth (0, 2, 6, 7):   3%|▎         | 3/100 [02:18<47:22, 29.31s/it]2025-01-22 05:22:29.637 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:22:29.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:22:29.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4885, 4890) --> . Mary travelled to the
2025-01-22 05:22:29.787 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:22:29.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14259, 14265) --> . Mary journeyed to the
2025-01-22 05:22:29.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:22:29.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16706, 16711) --> . John went to the
2025-01-22 05:22:29.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:22:30.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22250, 22255) --> . Daniel travelled to the
2025-01-22 05:22:30.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:22:30.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14631, 14636) -->  bonds. John moved to
2025-01-22 05:22:30.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:22:30.178 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7677, 7682) --> . Sandra went to the
2025-01-22 05:22:30.178 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:22:30.192 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2804, 2809) --> . Sandra picked up the
2025-01-22 05:22:30.192 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:22:30.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4338, 4344) --> . Daniel journeyed to the
2025-01-22 05:22:30.214 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:22:30.214 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:22:30.214 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:22:30.214 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['👵🏼', '🪂', '🇨🇴', '🏃🏼\u200d♀️\u200d➡', '🕵🏽\u200d♀', '🤷🏿\u200d♂', '🏌🏻\u200d♀', '👨🏽\u200d🌾', '🏌🏻\u200d♂️', '♂️']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.28s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.64s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (0, 4, 5, 7):   3%|▎         | 3/100 [02:35<47:22, 29.31s/it]2025-01-22 05:22:46.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:22:46.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:22:46.857 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9871, 9876) --> . Mary travelled to the
2025-01-22 05:22:46.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:22:46.932 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12001, 12007) --> . Mary journeyed to the
2025-01-22 05:22:46.932 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:22:47.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16821, 16826) -->  affair. John went to
2025-01-22 05:22:47.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:22:47.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22256, 22261) --> . Daniel travelled to the
2025-01-22 05:22:47.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:22:47.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (15064, 15069) --> . John moved to the
2025-01-22 05:22:47.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:22:47.251 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7849, 7854) --> . Sandra went to the
2025-01-22 05:22:47.251 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:22:47.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2896, 2901) --> . Sandra picked up the
2025-01-22 05:22:47.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:22:47.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4463, 4469) -->  Daniel journeyed to the office
2025-01-22 05:22:47.298 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:22:47.298 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:22:47.299 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:22:47.299 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['✏', '🕤', '👩🏿\u200d❤️\u200d👨🏻', '🧑🏾\u200d❤\u200d💋\u200d🧑🏻', '🤶🏻', '🇧🇩', '🏄🏻', '🧎🏽\u200d♂\u200d➡', '💂🏾\u200d♀', '🧑🏽\u200d🦽\u200d➡']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]
Processing depth (0, 1, 4, 7):   3%|▎         | 3/100 [02:53<47:22, 29.31s/it]2025-01-22 05:23:04.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:23:04.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:23:04.404 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2979, 2984) -->  tragedy. Mary travelled to
2025-01-22 05:23:04.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:23:04.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9709, 9715) --> . Mary journeyed to the
2025-01-22 05:23:04.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:23:04.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16780, 16785) --> . John went to the
2025-01-22 05:23:04.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:23:04.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22246, 22251) --> . Daniel travelled to the
2025-01-22 05:23:04.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:23:04.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14737, 14742) -->  John moved to the office
2025-01-22 05:23:04.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:23:04.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7683, 7688) --> . Sandra went to the
2025-01-22 05:23:04.795 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:23:04.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2860, 2865) --> . Sandra picked up the
2025-01-22 05:23:04.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:23:04.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4397, 4403) -->  Daniel journeyed to the office
2025-01-22 05:23:04.842 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:23:04.842 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:23:04.842 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:23:04.842 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🙆🏾\u200d♂️', '👩🏽\u200d❤\u200d👩🏾', '👩🏽\u200d🔬', '🇨🇨', '🦸🏼\u200d♂️', '🗒', '🤷🏿\u200d♂', '🌫', '⚽', '🇬🇳']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.06s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.54s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]
Processing depth (2, 3, 4, 7):   3%|▎         | 3/100 [03:10<47:22, 29.31s/it]2025-01-22 05:23:21.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:23:21.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4853, 4857) -->  Mary got the football
2025-01-22 05:23:21.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:23:21.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7451, 7456) --> . Mary travelled to the
2025-01-22 05:23:21.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:23:21.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9702, 9708) -->  war. Mary journeyed to
2025-01-22 05:23:21.387 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:23:21.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16742, 16747) --> . John went to the
2025-01-22 05:23:21.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:23:21.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22258, 22263) --> . Daniel travelled to the
2025-01-22 05:23:21.620 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:23:21.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14742, 14747) --> . John moved to the
2025-01-22 05:23:21.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:23:21.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7669, 7674) --> . Sandra went to the
2025-01-22 05:23:21.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:23:21.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2785, 2790) --> . Sandra picked up the
2025-01-22 05:23:21.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:23:21.788 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4305, 4311) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:23:24.549 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:23:24.549 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24204])
2025-01-22 05:23:32.946 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [619.0453125, 30.028995910104932, 414.9206730769231, 29.127195002069538, 43.27392578125], 'topk_indices': array([ 4854, 24190, 10064,    24,  2791, 24198, 24058, 24187, 10100,
       24194, 24202,  4853,    14, 24192, 10065,  4857, 24191, 24199,
        4856,     0]), 'topk_tokens': [' got', ' the', 'po', '\n\n', '.', ':', ' context', ':', 'po', ' hallway', '<|end_header_id|>', ' Mary', '\n', ' before', 'or', '.', ' football', '<|eot_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [1995.5, 344.025, 396.6354166666667, 59.79375]}, 'weight': {'score': [22.05234375, 23.447156696686772, 20.868389423076923, 23.451086506622516, 29.430803571428573], 'topk_indices': array([18864, 18820, 14666, 14630, 19517, 19459, 14711, 14675, 14529,
       14594, 20331, 20379, 18189, 18158, 23684, 23550, 21983, 22010,
       23766, 23632]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.75625, 22.938802083333332, 18.6375]}, 'saliency': {'score': [4.04197998046875, 0.17119484354343964, 2.3550274188701925, 0.16564040026127896, 0.32206199282691594], 'topk_indices': array([ 2786, 24092, 24173,    51,    54, 24102, 24188, 24197, 24186,
        4854, 10064, 10065, 24058, 10100, 24192,  9709,  4853, 24194,
       24191,  4856]), 'topk_tokens': [' Sandra', '.\n\n', ' return', ' Proof', '\n\n\n\n', ' location', ' Where', 'Answer', 'Question', ' got', 'po', 'or', ' context', 'po', ' before', ' hallway', ' Mary', ' hallway', ' football', ' football'], 'evidence_proportions': [14.173828125, 1.922265625, 2.177978515625, 0.293017578125]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:23:32.952 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:23:32.953 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-38_pid-3_2-3-4-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (2, 3, 4, 7):   4%|▍         | 4/100 [03:22<1:40:47, 62.99s/it]is_0k: False
your chose emoji: ['👩🏿\u200d❤️\u200d👨🏼', '🐫', '⚰️', '🪛', '👨🏾\u200d🦲', '🏃🏿\u200d♂\u200d➡️', '🧑🏼\u200d🤝\u200d🧑🏽', '🏊🏻', '\U0001faab', '🤾🏾\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.55s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (0, 1, 5, 8):   4%|▍         | 4/100 [03:39<1:40:47, 62.99s/it]2025-01-22 05:23:50.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:23:50.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:23:50.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3042, 3047) --> . Mary travelled to the
2025-01-22 05:23:50.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:23:50.717 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11891, 11897) --> . Mary journeyed to the
2025-01-22 05:23:50.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:23:50.810 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19175, 19180) -->  John went to the kitchen
2025-01-22 05:23:50.810 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:23:50.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22218, 22223) --> . Daniel travelled to the
2025-01-22 05:23:50.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:23:50.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14650, 14655) --> . John moved to the
2025-01-22 05:23:50.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:23:51.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7831, 7836) --> . Sandra went to the
2025-01-22 05:23:51.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:23:51.046 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2896, 2901) --> . Sandra picked up the
2025-01-22 05:23:51.046 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:23:51.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4469, 4475) -->  Daniel journeyed to the office
2025-01-22 05:23:51.069 | INFO     | test_jbb_embedding:begin_test:685 - evidence_list:
2025-01-22 05:23:51.069 | INFO     | test_jbb_embedding:begin_test:686 - disturb_tok_needles:
2025-01-22 05:23:51.069 | INFO     | test_jbb_embedding:begin_test:687 - search_pos:
2025-01-22 05:23:51.069 | INFO     | test_jbb_embedding:begin_test:688 - attack_pos:
is_0k: False
your chose emoji: ['🗻', '😱', '🩲', '🗼', '🆎', '👷🏿\u200d♂', '👩\u200d❤️\u200d👨', '👩🏾\u200d❤️\u200d💋\u200d👨🏾', '🍒', '👩🏽\u200d🤝\u200d👨🏼']

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.04s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (2, 5, 6, 9):   4%|▍         | 4/100 [03:56<1:40:47, 62.99s/it]2025-01-22 05:24:07.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary got the football.
2025-01-22 05:24:07.737 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4845, 4849) -->  Mary got the football
2025-01-22 05:24:07.738 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the office.
2025-01-22 05:24:07.797 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11787, 11792) --> . Mary travelled to the
2025-01-22 05:24:07.797 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:24:07.874 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14293, 14299) --> . Mary journeyed to the
2025-01-22 05:24:07.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the kitchen.
2025-01-22 05:24:07.990 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21432, 21437) --> . John went to the
2025-01-22 05:24:07.990 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the garden.
2025-01-22 05:24:08.115 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22232, 22237) --> . Daniel travelled to the
2025-01-22 05:24:08.115 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John moved to the office.
2025-01-22 05:24:08.232 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14621, 14626) -->  John moved to the office
2025-01-22 05:24:08.232 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:24:08.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7638, 7643) -->  war. Sandra went to
2025-01-22 05:24:08.288 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the milk.
2025-01-22 05:24:08.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2713, 2718) --> . Sandra picked up the
2025-01-22 05:24:08.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel journeyed to the office.
2025-01-22 05:24:08.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (4297, 4303) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:24:11.138 | INFO     | test_jbb_embedding:begin_test:693 - Mary got the football.<|eot_id|>
2025-01-22 05:24:11.138 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 05:24:19.562 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [589.2046875, 32.55152732952665, 479.31850961538464, 31.60844612801592, 73.709375], 'topk_indices': array([24156, 24165,  4861, 24149, 24019,    24, 24154, 24054, 24153,
          14,  4864,  4849, 24162,  4862, 24164,  4848, 24020,  4863,
           0, 24161]), 'topk_tokens': [' hallway', '\n\n', ' one', ':', ' you', '\n\n', ' before', '.\n\n', ' football', '\n', ' a', '.', '<|start_header_id|>', ' supposed', '<|end_header_id|>', ' football', ' context', ' for', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [1738.75, 407.75, 432.0, 39.66875]}, 'weight': {'score': [21.186328125, 23.439126634392586, 22.306490384615383, 23.44221528842965, 29.686778846153846], 'topk_indices': array([18826, 18782, 14658, 14694, 14703, 19479, 14739, 19421, 14537,
       14602, 20341, 20293, 18151, 18120, 23524, 23658, 21984, 21957,
       23606, 23740]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.611328125, 20.75625, 20.052083333333332, 18.6375]}, 'saliency': {'score': [3.7042022705078126, 0.18374246889611262, 2.8428203876201925, 0.177957491617319, 0.5486046424278846], 'topk_indices': array([   24, 24135,  4864, 24163,  4846, 14299, 24054,  4861, 24018,
       24019, 24148,  4845, 24154,  2714,  4863, 24156, 24153,  4862,
       24020,  4848]), 'topk_tokens': ['\n\n', ' return', ' a', 'assistant', ' got', ' hallway', '.\n\n', ' one', ' provided', ' you', 'Question', ' Mary', ' before', ' Sandra', ' for', ' hallway', ' football', ' supposed', ' context', ' football'], 'evidence_proportions': [12.029296875, 2.357421875, 2.1946614583333335, 0.20235595703125]}}, 'pred_res': 'Mary got the football.<|eot_id|>', 'score': 0}
2025-01-22 05:24:19.569 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:24:19.569 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-38_pid-4_2-5-6-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 5, 6, 9):   5%|▌         | 5/100 [04:08<1:30:23, 57.09s/it]Processing depth (2, 5, 6, 9):   5%|▌         | 5/100 [04:09<1:18:57, 49.87s/it]
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:82 - Selected idx: 39
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:86 - Needle: [' Mary travelled to the office.', ' John moved to the office.', ' Daniel journeyed to the office.', ' Mary got the football.', ' Sandra went to the garden.', ' Daniel travelled to the garden.', ' Mary journeyed to the hallway.', ' Sandra picked up the milk.', ' Mary discarded the football.', ' John went to the kitchen.']
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the office.', ' Mary got the football.', ' Mary journeyed to the hallway.', ' Mary discarded the football.', ' John went to the kitchen.']
2025-01-22 05:24:19.939 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['⚠', '🧎🏿\u200d♀️\u200d➡', '💿', '📝', '🧝\u200d♂️', '🚶\u200d➡️', '🕰', '🕵🏿', '👨🏼\u200d❤️\u200d💋\u200d👨🏽', '🕊']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.40s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (2, 3, 6, 7, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:24:36.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the office.
2025-01-22 05:24:36.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4912, 4917) --> . Mary travelled to the
2025-01-22 05:24:36.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:24:36.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7391, 7395) -->  Mary got the football
2025-01-22 05:24:36.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:24:37.017 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14341, 14347) --> . Mary journeyed to the
2025-01-22 05:24:37.017 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the football.
2025-01-22 05:24:37.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16711, 16715) -->  Mary discarded the football
2025-01-22 05:24:37.102 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:24:37.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21425, 21430) --> . John went to the
2025-01-22 05:24:37.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the office.
2025-01-22 05:24:37.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9405, 9410) --> . John moved to the
2025-01-22 05:24:37.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:24:37.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3469, 3475) --> . Daniel journeyed to the
2025-01-22 05:24:37.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:24:37.381 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15812, 15817) --> . Sandra went to the
2025-01-22 05:24:37.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:24:37.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6848, 6853) --> . Daniel travelled to the
2025-01-22 05:24:37.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the milk.
2025-01-22 05:24:37.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22547, 22552) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:24:40.229 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:24:40.230 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 05:24:48.588 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [397.6647135416667, 16.9801972132572, 425.95552884615387, 16.160486332821957, 17.041217672413794], 'topk_indices': array([24153,  4916, 24140,  3471, 24011,    14,  3472,  4915,    24,
        4917,    23,  6850, 24164,  3476,  4914,  6851,  6852, 24161,
       24162,     0]), 'topk_tokens': [' discarded', ' the', ':', ' journey', ' context', '\n', 'ed', ' to', '\n\n', ' office', '4', ' travelled', '<|end_header_id|>', '.', ' travelled', ' to', ' the', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [916.0, 362.96875, 212.85416666666666, 519.1875, 31.640625]}, 'weight': {'score': [22.510416666666668, 23.430698651108905, 20.868389423076923, 23.43437668442657, 28.764547413793103], 'topk_indices': array([18833, 18789, 14626, 14662, 14671, 19486, 19428, 14707, 14525,
       14590, 20348, 20300, 18142, 18111, 23517, 23651, 21977, 21950,
       23733, 23599]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 26.611328125, 20.052083333333332, 29.130859375, 18.6375]}, 'saliency': {'score': [2.4698715209960938, 0.09526631248900923, 2.251666729266827, 0.09057865448086622, 0.12469442959489493], 'topk_indices': array([24055,  6852, 16714,    23,  3475,  6849,  4913,  9354,  6851,
        7394, 24151, 24139, 16712, 24011, 14347,  4917,  3471, 24153,
        6850,  4914]), 'topk_tokens': [' location', ' the', ' football', '4', ' office', ' Daniel', ' Mary', ' cause', ' to', ' football', ' football', 'Question', ' discarded', ' context', ' hallway', ' office', ' journey', ' discarded', ' travelled', ' travelled'], 'evidence_proportions': [5.06484375, 2.57958984375, 1.1188557942708333, 4.04345703125, 0.14947509765625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 05:24:48.592 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:24:48.592 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-39_pid-0_2-3-6-7-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 3, 6, 7, 9):   1%|          | 1/100 [00:28<47:05, 28.54s/it]is_0k: False
your chose emoji: ['⚜', '🤾🏾\u200d♀', '👮🏻\u200d♂️', '📟', '👨🏽\u200d🏫', '🧑🏾\u200d🚒', '🔩', '⚰', '🇿🇲', '😘']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.26s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.05s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.18s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.02s/it]
Processing depth (1, 4, 5, 6, 8):   1%|          | 1/100 [00:46<47:05, 28.54s/it]2025-01-22 05:25:06.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the office.
2025-01-22 05:25:06.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2947, 2952) --> . Mary travelled to the
2025-01-22 05:25:06.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:25:06.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9731, 9735) -->  Mary got the football
2025-01-22 05:25:06.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:25:07.027 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11867, 11873) --> . Mary journeyed to the
2025-01-22 05:25:07.028 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the football.
2025-01-22 05:25:07.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14389, 14393) -->  Mary discarded the football
2025-01-22 05:25:07.099 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:25:07.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19389, 19394) --> . John went to the
2025-01-22 05:25:07.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the office.
2025-01-22 05:25:07.239 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9436, 9441) --> . John moved to the
2025-01-22 05:25:07.239 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:25:07.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3505, 3511) --> . Daniel journeyed to the
2025-01-22 05:25:07.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:25:07.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15952, 15957) -->  Sandra went to the garden
2025-01-22 05:25:07.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:25:07.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6926, 6931) --> . Daniel travelled to the
2025-01-22 05:25:07.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the milk.
2025-01-22 05:25:07.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22579, 22584) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:25:10.143 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 05:25:10.143 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24214])
2025-01-22 05:25:18.551 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [551.1979166666666, 24.724928766105055, 220.02884615384616, 23.99194632955392, 42.77086509146341], 'topk_indices': array([    9, 24207, 24193, 14392, 24194,    24, 24199,  9734,    23,
       24078, 24187, 24208, 24186, 24093, 24188, 24206,    14,     0,
       24209, 24210]), 'topk_tokens': [':', 'Answer', ' prior', ' football', ' to', '\n\n', ' football', ' football', '4', '.', 'Question', ':', '.\n\n', '.\n\n', ':', '?\n', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [680.15, 958.25, 301.5208333333333, 982.5, 51.175]}, 'weight': {'score': [22.510416666666668, 23.44902647010241, 21.736478365384617, 23.451801148824796, 29.906631097560975], 'topk_indices': array([18845, 18801, 14707, 14671, 19446, 14752, 14716, 19504, 14570,
       14635, 20380, 20332, 18139, 18170, 23549, 23683, 22003, 21976,
       23765, 23631]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 26.611328125, 20.052083333333332, 29.130859375, 18.6375]}, 'saliency': {'score': [3.7438532511393228, 0.13252266870302692, 1.2416006234975963, 0.1277428970898874, 0.3286516143054497], 'topk_indices': array([24197, 14389,    14, 24188,    23, 24189,  2949,  2948, 24186,
        9731, 24207, 24193, 14392, 24093, 24206, 24201, 24199,  9734,
       14390, 24187]), 'topk_tokens': [' where', ' Mary', '\n', ':', '4', ' Where', ' travelled', ' Mary', '.\n\n', ' Mary', 'Answer', ' prior', ' football', '.\n\n', '?\n', ' discarded', ' football', ' football', ' discarded', 'Question'], 'evidence_proportions': [4.19921875, 6.797607421875, 1.6424560546875, 7.644775390625, 0.24642333984375]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 05:25:18.567 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:25:18.567 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-39_pid-1_1-4-5-6-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 4, 5, 6, 8):   2%|▏         | 2/100 [00:58<47:59, 29.38s/it]is_0k: False
your chose emoji: ['🚶🏾\u200d♂\u200d➡️', '👨🏽\u200d⚖️', '⏯️', '👨🏻\u200d❤️\u200d💋\u200d👨🏻', '\U0001fae3', '🙍🏽\u200d♀', '🧘🏼\u200d♀️', '💢', '📟', '🙆🏽\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Processing depth (1, 3, 4, 5, 6):   2%|▏         | 2/100 [01:15<47:59, 29.38s/it]2025-01-22 05:25:35.664 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the office.
2025-01-22 05:25:35.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary travelled to
2025-01-22 05:25:35.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:25:35.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7527, 7531) -->  Mary got the football
2025-01-22 05:25:35.724 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:25:35.775 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9735, 9741) --> . Mary journeyed to the
2025-01-22 05:25:35.775 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the football.
2025-01-22 05:25:35.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11897, 11901) -->  Mary discarded the football
2025-01-22 05:25:35.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:25:35.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14419, 14424) --> . John went to the
2025-01-22 05:25:35.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the office.
2025-01-22 05:25:35.954 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9493, 9498) --> . John moved to the
2025-01-22 05:25:35.954 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:25:35.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3461, 3467) --> . Daniel journeyed to the
2025-01-22 05:25:35.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:25:36.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15949, 15954) --> . Sandra went to the
2025-01-22 05:25:36.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:25:36.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6914, 6919) --> . Daniel travelled to the
2025-01-22 05:25:36.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the milk.
2025-01-22 05:25:36.234 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22529, 22534) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:25:39.040 | INFO     | test_jbb_embedding:begin_test:693 - Mary discarded the football.<|eot_id|>
2025-01-22 05:25:39.040 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24164])
2025-01-22 05:25:47.434 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [383.9967447916667, 20.430902404204254, 264.3942307692308, 19.8060630131448, 32.148094846491226], 'topk_indices': array([11989, 24163, 24009, 24137, 24151, 11900,    23,    24, 24138,
        7530,  7531, 11901, 24149, 11990, 24162, 11947, 11946, 24159,
           0, 24160]), 'topk_tokens': ['L', '\n\n', ' context', 'Question', ' discarded', ' football', '4', '\n\n', ':', ' football', '.', '.', ' football', 'ANT', '<|end_header_id|>', 'IC', 'ANT', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [369.6, 718.4375, 174.44791666666666, 804.5, 45.896875]}, 'weight': {'score': [23.720377604166668, 23.4322731316726, 20.868389423076923, 23.434750593485653, 29.19188596491228], 'topk_indices': array([18761, 18717, 14623, 14587, 14632, 14668, 19362, 19420, 14486,
       14551, 20300, 20252, 18055, 18086, 23633, 23499, 21916, 21943,
       23581, 23715]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.5640625, 26.611328125, 20.052083333333332, 29.130859375, 18.6375]}, 'saliency': {'score': [2.6250991821289062, 0.11585934509615782, 1.4171048677884615, 0.1119592729582085, 0.23462315609580592], 'topk_indices': array([24157,     0, 24146, 24147,    24, 11989,    23, 11975, 24143,
       24053, 24009, 11898, 11900, 24137, 11990, 24151,  7530, 11947,
       24149, 11946]), 'topk_tokens': ['Answer', '<|begin_of_text|>', ' place', ' where', '\n\n', 'L', '4', 'BR', ' prior', ' location', ' context', ' discarded', ' football', 'Question', 'ANT', ' discarded', ' football', 'IC', ' football', 'ANT'], 'evidence_proportions': [2.2359375, 4.98193359375, 0.9670817057291666, 6.26220703125, 0.20872802734375]}}, 'pred_res': 'Mary discarded the football.<|eot_id|>', 'score': 0}
2025-01-22 05:25:47.442 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:25:47.442 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-39_pid-2_1-3-4-5-6.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 3, 4, 5, 6):   3%|▎         | 3/100 [01:27<47:07, 29.15s/it]is_0k: False
your chose emoji: ['🧗🏽\u200d♀', '🏄🏽\u200d♂', '🔠', '㊙️', '‼️', '🤸🏽\u200d♀', '👳', '🪰', '🙍🏾', '🧑🏿\u200d🦰']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.51s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.56s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.47s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (1, 2, 3, 5, 6):   3%|▎         | 3/100 [01:44<47:07, 29.15s/it]2025-01-22 05:26:04.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the office.
2025-01-22 05:26:04.388 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary travelled to the
2025-01-22 05:26:04.388 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:26:04.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4969, 4973) -->  Mary got the football
2025-01-22 05:26:04.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:26:04.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7529, 7535) --> . Mary journeyed to the
2025-01-22 05:26:04.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the football.
2025-01-22 05:26:04.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11909, 11913) -->  Mary discarded the football
2025-01-22 05:26:04.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:26:04.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14429, 14434) --> . John went to the
2025-01-22 05:26:04.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the office.
2025-01-22 05:26:04.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9498, 9503) --> . John moved to the
2025-01-22 05:26:04.650 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:26:04.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3547, 3553) --> . Daniel journeyed to the
2025-01-22 05:26:04.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:26:04.755 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15959, 15964) --> . Sandra went to the
2025-01-22 05:26:04.755 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:26:04.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6929, 6934) --> . Daniel travelled to the
2025-01-22 05:26:04.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the milk.
2025-01-22 05:26:04.912 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22595, 22600) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:26:07.894 | INFO     | test_jbb_embedding:begin_test:693 - THE FIVE MILLION LOAN ELECTION.<|eot_id|>
2025-01-22 05:26:07.894 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24190])
2025-01-22 05:26:16.321 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [550.1888020833334, 29.946774760251323, 290.25721153846155, 29.14924845704581, 35.59810267857143], 'topk_indices': array([24069,    14,    24,  7244, 11898, 11891, 24163,  4973, 24175,
       24164, 11913,  4972, 11910, 24177, 24035, 11912, 11892,     0,
       24185, 24186]), 'topk_tokens': ['.\n\n', '\n', '\n\n', 'ot', '.', 'IVE', 'Question', '.', ' football', ':', '.', ' football', ' discarded', ' discarded', ' context', ' football', ' MILL', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [447.525, 921.25, 328.0208333333333, 1235.125, 74.65625]}, 'weight': {'score': [22.510416666666668, 23.440352182539684, 20.868389423076923, 23.44404655268826, 29.518526785714286], 'topk_indices': array([18727, 18771, 14597, 14633, 19424, 14678, 14642, 19366, 14561,
       14496, 20286, 20238, 18096, 18065, 23541, 23675, 21919, 21892,
       23623, 23757]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 26.611328125, 20.052083333333332, 29.130859375, 18.6375]}, 'saliency': {'score': [3.7194061279296875, 0.1705211457752046, 1.6572735126201923, 0.16539196007767842, 0.27197134835379466], 'topk_indices': array([ 3033, 24172, 24183, 11938, 24173,  7243, 11909,  7244, 11897,
       11902, 11891, 24169, 24175, 24163,  4972, 24035, 11912, 24177,
       11910, 11892]), 'topk_tokens': [' travelled', ' place', 'Answer', 'ANT', ' where', 'nes', ' Mary', 'ot', 'LECTION', ' MILL', 'IVE', ' prior', ' football', 'Question', ' football', ' context', ' football', ' discarded', ' discarded', ' MILL'], 'evidence_proportions': [2.70234375, 6.26953125, 1.6993815104166667, 9.6357421875, 0.3873291015625]}}, 'pred_res': 'THE FIVE MILLION LOAN ELECTION.<|eot_id|>', 'score': 0}
2025-01-22 05:26:16.329 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:26:16.329 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-39_pid-3_1-2-3-5-6.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 3, 5, 6):   4%|▍         | 4/100 [01:56<46:28, 29.05s/it]is_0k: False
your chose emoji: ['❔', '🧑\u200d🧑\u200d🧒', '\U0001fac4', '🧍🏻\u200d♀', '👮🏾\u200d♂', '👨🏻\u200d❤\u200d👨🏾', '😒', '👩🏼\u200d❤\u200d👨🏾', '🚮', '👜']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.22s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.90s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.53s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 3, 6, 7, 8):   4%|▍         | 4/100 [02:13<46:28, 29.05s/it]2025-01-22 05:26:33.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the office.
2025-01-22 05:26:33.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the office.
2025-01-22 05:26:33.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the football.
2025-01-22 05:26:33.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7531, 7535) -->  Mary got the football
2025-01-22 05:26:33.420 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary journeyed to the hallway.
2025-01-22 05:26:33.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14351, 14357) --> . Mary journeyed to the
2025-01-22 05:26:33.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary discarded the football.
2025-01-22 05:26:33.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16733, 16737) -->  Mary discarded the football
2025-01-22 05:26:33.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 05:26:33.749 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19280, 19285) -->  John went to the kitchen
2025-01-22 05:26:33.749 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John moved to the office.
2025-01-22 05:26:33.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9407, 9412) --> . John moved to the
2025-01-22 05:26:33.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel journeyed to the office.
2025-01-22 05:26:33.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3515, 3521) --> . Daniel journeyed to the
2025-01-22 05:26:33.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the garden.
2025-01-22 05:26:33.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15834, 15839) --> . Sandra went to the
2025-01-22 05:26:33.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel travelled to the garden.
2025-01-22 05:26:33.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6880, 6885) --> . Daniel travelled to the
2025-01-22 05:26:33.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the milk.
2025-01-22 05:26:34.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22569, 22574) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:26:36.845 | INFO     | test_jbb_embedding:begin_test:693 - Mary had the football.<|eot_id|>
2025-01-22 05:26:36.845 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24210])
2025-01-22 05:26:45.301 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [476.0983072916667, 22.95218796464563, 251.71875, 22.25591127079712, 29.690869140625], 'topk_indices': array([24190, 24193, 16737, 24183,    23, 16736,    24, 16750, 24182,
       24196, 24195, 16787, 16749, 24184, 16734, 24197, 16788, 24205,
       24206,     0]), 'topk_tokens': [' to', ' where', '.', 'Question', '4', ' football', '\n\n', 'ets', '.\n\n', ' was', ' football', ' bay', 'on', ':', ' discarded', ' discarded', 'on', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [497.4, 605.875, 337.4166666666667, 1055.75, 53.671875]}, 'weight': {'score': [23.380533854166668, 23.44454196266314, 20.868389423076923, 23.447377661720886, 29.3865234375], 'topk_indices': array([18833, 18789, 14672, 14636, 14681, 19434, 19492, 14717, 14535,
       14600, 20306, 20354, 18127, 18158, 23539, 23673, 21999, 21972,
       23755, 23621]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.14375, 26.611328125, 20.052083333333332, 29.130859375, 23.4265625]}, 'saliency': {'score': [3.1125590006510415, 0.1298465322270465, 1.3747464693509615, 0.12554421877586705, 0.2198474884033203], 'topk_indices': array([18932, 14357, 24192,     0, 24196, 24182, 24055, 24189, 24099,
       24193, 16749, 16750,  7534, 16736, 24183, 16788, 16787, 24195,
       16734, 24197]), 'topk_tokens': [' manner', ' hallway', ' place', '<|begin_of_text|>', ' was', '.\n\n', ' context', ' prior', ' location', ' where', 'on', 'ets', ' football', ' football', 'Question', 'on', ' bay', ' football', ' discarded', ' discarded'], 'evidence_proportions': [2.55283203125, 4.20751953125, 1.768798828125, 8.208984375, 0.331689453125]}}, 'pred_res': 'Mary had the football.<|eot_id|>', 'score': 0}
2025-01-22 05:26:45.305 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:26:45.306 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-39_pid-4_0-3-6-7-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (0, 3, 6, 7, 8):   5%|▌         | 5/100 [02:25<45:57, 29.02s/it]Processing depth (0, 3, 6, 7, 8):   5%|▌         | 5/100 [02:25<46:04, 29.11s/it]
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:82 - Selected idx: 40
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the office? 
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the bathroom.', ' John went back to the garden.', ' John travelled to the kitchen.', ' Daniel moved to the kitchen.', ' Sandra went to the bathroom.', ' Daniel moved to the office.', ' Mary travelled to the bedroom.', ' Daniel left the football.', ' Sandra journeyed to the bedroom.']
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' Daniel left the football.', ' Sandra journeyed to the bedroom.']
2025-01-22 05:26:45.584 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🇲🇷', '👨🏾\u200d❤\u200d💋\u200d👨🏿', '🙅🏽\u200d♀️', '🥢', '🎊', '🕍', '🇼🇫', '👩🏽\u200d⚕', '🧝🏾\u200d♂️', '👩🏼\u200d🦼\u200d➡']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.89s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]
Processing depth (0, 1, 4, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:27:03.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:27:03.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:27:03.276 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:27:03.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2964, 2969) -->  tragedy. Daniel moved to
2025-01-22 05:27:03.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:27:03.338 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9859, 9863) -->  Daniel left the football
2025-01-22 05:27:03.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 05:27:03.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19156, 19162) -->  Sandra journeyed to the bedroom
2025-01-22 05:27:03.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:27:03.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2377, 2382) --> . John travelled to the
2025-01-22 05:27:03.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:27:03.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4500, 4506) -->  John went back to the garden
2025-01-22 05:27:03.472 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:27:03.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2377, 2382) --> . John travelled to the
2025-01-22 05:27:03.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:27:03.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17763, 17768) --> . Sandra went to the
2025-01-22 05:27:03.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:27:03.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6130, 6135) --> . Mary travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:27:06.502 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the office.<|eot_id|>
2025-01-22 05:27:06.502 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24142])
2025-01-22 05:27:14.891 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [232.7625, 10.305835835576724, 132.73076923076923, 9.989134663264036, 17.173791273584907], 'topk_indices': array([    1, 24141,     4,    24, 24136, 24130,     9, 23996, 24040,
       24124, 24030, 24125,  9863, 24140,    14, 24129,  9862,     0,
       24137, 24138]), 'topk_tokens': ['<|start_header_id|>', '\n\n', '\n\n', '\n\n', ':', ' before', ':', ' context', ' location', 'Question', '.\n\n', ':', '.', '<|end_header_id|>', '\n', ' football', ' football', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [273.65, 176.91875, 508.125, 61.651041666666664]}, 'weight': {'score': [24.460546875, 23.429444501967282, 21.17938701923077, 23.43101632847836, 28.84375], 'topk_indices': array([18725, 18769, 14571, 14607, 19429, 19371, 14616, 14652, 14470,
       14535, 20297, 20249, 18094, 18063, 23606, 23472, 21920, 21893,
       23554, 23688]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 25.7171875, 26.791015625, 25.735677083333332]}, 'saliency': {'score': [1.4750091552734375, 0.05771674469998964, 0.7293935922475961, 0.0558158589268162, 0.12428312481574293], 'topk_indices': array([ 9859,     4, 24111,    34,    14, 24125, 24126,     0,  9860,
          31,     8, 24030,  2382, 24130, 24135, 24040, 23996, 24124,
       24129,  9862]), 'topk_tokens': [' Daniel', '\n\n', ' return', ' kitchen', '\n', ':', ' Where', '<|begin_of_text|>', ' left', ' moved', ' Date', '.\n\n', ' bathroom', ' before', 'Answer', ' location', ' context', 'Question', ' football', ' football'], 'evidence_proportions': [1.509423828125, 1.032958984375, 3.568115234375, 0.4193013509114583]}}, 'pred_res': 'The football was left in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:27:14.898 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:27:14.899 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-40_pid-0_0-1-4-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 4, 8):   1%|          | 1/100 [00:29<48:10, 29.20s/it]is_0k: False
your chose emoji: ['🩸', '💀', '🥩', '🕶️', '🦴', '🏋🏽\u200d♂', '🏴', '👮🏿', '\U0001fa77', '👩🏾\u200d🤝\u200d👩🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.44s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (2, 5, 6, 7):   1%|          | 1/100 [00:45<48:10, 29.20s/it]2025-01-22 05:27:31.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:27:31.629 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4863, 4868) -->  Daniel moved to the kitchen
2025-01-22 05:27:31.629 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:27:31.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4864, 4869) -->  moved to the kitchen.
2025-01-22 05:27:31.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:27:31.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14317, 14321) -->  Daniel left the football
2025-01-22 05:27:31.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 05:27:31.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16689, 16695) --> . Sandra journeyed to the
2025-01-22 05:27:31.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:27:31.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2453, 2458) --> . John travelled to the
2025-01-22 05:27:31.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:27:31.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4494, 4500) -->  John went back to the garden
2025-01-22 05:27:31.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:27:31.851 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2453, 2458) --> . John travelled to the
2025-01-22 05:27:31.851 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:27:31.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17798, 17803) --> . Sandra went to the
2025-01-22 05:27:31.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:27:31.966 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6025, 6030) -->  Mary travelled to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:27:34.885 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the field.<|eot_id|>
2025-01-22 05:27:34.885 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24156])
2025-01-22 05:27:43.285 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [276.334375, 7.526119924251832, 74.28125, 7.231183645751255, 16.458805338541666], 'topk_indices': array([24144, 24054, 24155, 14316, 14319,    14, 24150,  4863,    23,
       14318,    24,     3, 24143,  4864, 14321, 24152, 24154,     0,
       24151, 14320]), 'topk_tokens': [' before', ' location', '\n\n', '.', ' the', '\n', ':', ' Daniel', '4', ' left', '\n\n', '<|end_header_id|>', ' football', ' moved', '.', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', ' football'], 'evidence_proportions': [334.225, 295.75, 539.1875, 36.677083333333336]}, 'weight': {'score': [22.84921875, 23.437073140444554, 22.22295673076923, 23.438869852361798, 29.761979166666666], 'topk_indices': array([18774, 18818, 14599, 14635, 14680, 14644, 19485, 19427, 14498,
       14563, 20353, 20305, 18123, 18092, 23510, 23644, 21949, 21976,
       23726, 23592]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.6984375, 19.809375, 26.791015625, 21.213541666666668]}, 'saliency': {'score': [1.7828094482421875, 0.04247540585612505, 0.42954136775090146, 0.04061456747612316, 0.11885782877604166], 'topk_indices': array([24155,    20, 11845,  4867, 24146, 24138, 24153,    24,    23,
       24125, 24144, 24010, 14317, 24054, 24149, 14318,  4863,  4864,
       24143, 14320]), 'topk_tokens': ['\n\n', ' Jul', ' Daniel', ' kitchen', ' office', 'Question', 'assistant', '\n\n', '4', ' return', ' before', ' context', ' Daniel', ' location', 'Answer', ' left', ' Daniel', ' moved', ' football', ' football'], 'evidence_proportions': [2.210888671875, 1.634033203125, 3.776123046875, 0.22118123372395834]}}, 'pred_res': 'The football was left in the field.<|eot_id|>', 'score': 0}
2025-01-22 05:27:43.293 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:27:43.293 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-40_pid-1_2-5-6-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (2, 5, 6, 7):   2%|▏         | 2/100 [00:57<46:54, 28.72s/it]is_0k: False
your chose emoji: ['🔓', '🎙️', '🧑🏽', '🗞', '🤹🏼\u200d♀️', '🚵🏽\u200d♀', '👩🏿\u200d✈️', '👨\u200d✈️', '😮\u200d💨', '💇🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.23s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.28s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.00s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.44s/it]
Processing depth (0, 1, 4, 5):   2%|▏         | 2/100 [01:13<46:54, 28.72s/it]2025-01-22 05:27:59.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:27:59.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:27:59.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:27:59.301 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2968, 2973) -->  tragedy. Daniel moved to
2025-01-22 05:27:59.301 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:27:59.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9675, 9679) -->  Daniel left the football
2025-01-22 05:27:59.348 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 05:27:59.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11849, 11855) --> . Sandra journeyed to the
2025-01-22 05:27:59.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:27:59.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2381, 2386) --> . John travelled to the
2025-01-22 05:27:59.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:27:59.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4504, 4510) -->  John went back to the garden
2025-01-22 05:27:59.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:27:59.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2381, 2386) --> . John travelled to the
2025-01-22 05:27:59.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:27:59.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17736, 17741) --> . Sandra went to the
2025-01-22 05:27:59.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:27:59.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6134, 6139) --> . Mary travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:28:02.548 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the football field.<|eot_id|>
2025-01-22 05:28:02.548 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24148])
2025-01-22 05:28:10.925 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [200.02578125, 8.86390315100824, 101.48798076923077, 8.605389830947937, 7.941275460379464], 'topk_indices': array([    3,  2386, 24130,  9679, 24136, 24129, 24121, 24046, 24147,
       24036,    24, 24131, 24135, 24002,    14,  9678, 24146,     0,
       24143, 24144]), 'topk_tokens': ['<|end_header_id|>', ' bathroom', 'Question', '.', ' before', '.\n\n', '.', ' location', '\n\n', '.\n\n', '\n\n', ':', ' football', ' context', '\n', ' football', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [293.425, 140.571875, 385.125, 48.338541666666664]}, 'weight': {'score': [23.10390625, 23.432192248768168, 21.17938701923077, 23.4348945369218, 29.146484375], 'topk_indices': array([18828, 18784, 14660, 14624, 14735, 19481, 14699, 19423, 14588,
       14523, 20301, 20349, 18122, 18153, 23640, 23506, 21945, 21972,
       23588, 23722]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 25.7171875, 26.791015625, 21.213541666666668]}, 'saliency': {'score': [1.2288482666015625, 0.04966613458681835, 0.5466156005859375, 0.04815174633723229, 0.05844463620867048], 'topk_indices': array([24131, 24001,     8,  9675, 24147,  9676, 24129, 24141,    24,
       24036, 24117,    34, 24136,    31, 24046, 24130,  2386, 24002,
       24135,  9678]), 'topk_tokens': [':', ' you', ' Date', ' Daniel', '\n\n', ' left', '.\n\n', 'Answer', '\n\n', '.\n\n', ' return', ' kitchen', ' before', ' moved', ' location', 'Question', ' bathroom', ' context', ' football', ' football'], 'evidence_proportions': [1.5818359375, 0.822216796875, 2.700439453125, 0.2924906412760417]}}, 'pred_res': 'The football was left in the football field.<|eot_id|>', 'score': 0}
2025-01-22 05:28:10.930 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:28:10.930 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-40_pid-2_0-1-4-5.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 4, 5):   3%|▎         | 3/100 [01:25<45:38, 28.23s/it]is_0k: False
your chose emoji: ['🚣🏻\u200d♀', '🛷', '🇦🇪', '👩🏽\u200d❤\u200d👨🏿', '👷', '🛤️', '🗒️', '⤵️', '👉🏻', '🍘']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 3, 6, 9):   3%|▎         | 3/100 [01:41<45:38, 28.23s/it]2025-01-22 05:28:27.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:28:27.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:28:27.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:28:27.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7546, 7551) --> . Daniel moved to the
2025-01-22 05:28:27.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:28:28.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14233, 14237) -->  Daniel left the football
2025-01-22 05:28:28.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 05:28:28.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21425, 21431) --> . Sandra journeyed to the
2025-01-22 05:28:28.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:28:28.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2207, 2212) --> . John travelled to the
2025-01-22 05:28:28.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:28:28.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4453, 4459) --> . John went back to the
2025-01-22 05:28:28.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:28:28.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2207, 2212) --> . John travelled to the
2025-01-22 05:28:28.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:28:28.306 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17735, 17740) --> . Sandra went to the
2025-01-22 05:28:28.306 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:28:28.337 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6039, 6044) -->  Mary travelled to the bedroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:28:31.239 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the field.<|eot_id|>
2025-01-22 05:28:31.239 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24160])
2025-01-22 05:28:39.627 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [203.8390625, 10.129115279559658, 72.58082932692308, 9.901145651926027, 26.735131048387096], 'topk_indices': array([14235,  1502,  1441,    86,    89, 24156, 24014,    14, 24147,
       14237, 14312, 14341, 14359,   125,    87, 24158,    88, 24155,
       14236,     0]), 'topk_tokens': [' the', ' the', ' question', ' P', 'ER', '<|start_header_id|>', ' context', '\n', ' football', '.', 'ance', 'ord', 'ible', 'ION', 'ION', '<|end_header_id|>', 'E', '<|eot_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [200.875, 150.6875, 555.5, 16.161458333333332]}, 'weight': {'score': [21.651953125, 23.438014733269874, 21.35486778846154, 23.44174169154538, 29.72782258064516], 'topk_indices': array([18827, 18783, 14671, 14635, 14716, 19480, 19422, 14680, 14599,
       14534, 20348, 20300, 18152, 18121, 23646, 23512, 21978, 21951,
       23728, 23594]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 19.909375, 26.791015625, 21.213541666666668]}, 'saliency': {'score': [1.2765151977539062, 0.0573728043813113, 0.4166823167067308, 0.0559744175509463, 0.19918694034699472], 'topk_indices': array([ 4127,     0, 24129,  1503,    89,    86, 14234, 14340,  1501,
       14310,  1441, 24014, 14312, 14341, 14359,    88,   125, 24147,
          87, 14236]), 'topk_tokens': [' column', '<|begin_of_text|>', ' return', ' question', 'ER', ' P', ' left', 'acc', ' until', 'acc', ' question', ' context', 'ance', 'ord', 'ible', 'E', 'ION', ' football', 'ION', ' football'], 'evidence_proportions': [1.084521484375, 0.81748046875, 3.871337890625, 0.0891571044921875]}}, 'pred_res': 'The football was left in the field.<|eot_id|>', 'score': 0}
2025-01-22 05:28:39.634 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:28:39.635 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-40_pid-3_0-3-6-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 3, 6, 9):   4%|▍         | 4/100 [01:53<45:27, 28.42s/it]is_0k: False
your chose emoji: ['🖨️', '🇪🇹', '🇫🇮', '💂🏽\u200d♂️', '🏊🏼\u200d♀️', '🚶🏽\u200d➡️', '💤', '🇹🇴', '☝🏼', '🖕🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.77s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (3, 5, 6, 8):   4%|▍         | 4/100 [02:10<45:27, 28.42s/it]2025-01-22 05:28:56.913 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:28:56.954 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7626, 7631) --> . Daniel moved to the
2025-01-22 05:28:56.955 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:28:56.997 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7626, 7631) --> . Daniel moved to the
2025-01-22 05:28:56.997 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:28:57.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14527, 14531) -->  Daniel left the football
2025-01-22 05:28:57.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra journeyed to the bedroom.
2025-01-22 05:28:57.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19451, 19457) --> . Sandra journeyed to the
2025-01-22 05:28:57.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:28:57.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2453, 2458) --> . John travelled to the
2025-01-22 05:28:57.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:28:57.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4523, 4529) --> . John went back to the
2025-01-22 05:28:57.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:28:57.235 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2453, 2458) --> . John travelled to the
2025-01-22 05:28:57.235 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:28:57.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18021, 18026) --> . Sandra went to the
2025-01-22 05:28:57.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:28:57.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6120, 6125) --> . Mary travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:29:00.228 | INFO     | test_jbb_embedding:begin_test:693 - The football was in the office.<|eot_id|>
2025-01-22 05:29:00.228 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24142])
2025-01-22 05:29:08.610 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [715.3734375, 17.70262864982398, 238.6153846153846, 16.885285696501928, 41.52513266509434], 'topk_indices': array([ 2458,    14, 24130,  7631, 18843,  7626, 14529, 14526, 23996,
       14527, 14528, 24140,  7627, 24138, 14531, 24137, 24129,  7628,
           0, 14530]), 'topk_tokens': [' bathroom', '\n', ' before', ' kitchen', ' in', '.', ' the', '.', ' context', ' Daniel', ' left', '<|end_header_id|>', ' Daniel', '<|start_header_id|>', '.', '<|eot_id|>', ' football', ' moved', '<|begin_of_text|>', ' football'], 'evidence_proportions': [888.75, 888.75, 1291.25, 42.494791666666664]}, 'weight': {'score': [21.676953125, 23.43033754400497, 20.311298076923077, 23.43515777366281, 29.06308962264151], 'topk_indices': array([18699, 18743, 14599, 14563, 19338, 14644, 19396, 14608, 14457,
       14522, 20235, 20283, 18037, 18068, 23634, 23500, 21966, 21939,
       23582, 23716]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 26.791015625, 21.213541666666668]}, 'saliency': {'score': [4.445831298828125, 0.10117278502925037, 1.2724280724158654, 0.09630346232922009, 0.2944989474314564], 'topk_indices': array([    0, 11988,  2455, 24040, 23994, 24132, 24124, 24130, 24135,
       11987, 18842,  7631,  2458, 14528, 23996, 14527,  7627,  7628,
       24129, 14530]), 'topk_tokens': ['<|begin_of_text|>', ' moved', ' travelled', ' location', ' provided', ' office', 'Question', ' before', 'Answer', ' Daniel', ' manner', ' kitchen', ' bathroom', ' left', ' context', ' Daniel', ' Daniel', ' moved', ' football', ' football'], 'evidence_proportions': [5.07578125, 5.07578125, 9.16064453125, 0.2527058919270833]}}, 'pred_res': 'The football was in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:29:08.617 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:29:08.617 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-40_pid-4_3-5-6-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (3, 5, 6, 8):   5%|▌         | 5/100 [02:22<45:18, 28.62s/it]Processing depth (3, 5, 6, 8):   5%|▌         | 5/100 [02:23<45:21, 28.65s/it]
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:82 - Selected idx: 41
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:86 - Needle: [' John went back to the garden.', ' Sandra went to the bathroom.', ' John travelled to the kitchen.', ' Daniel moved to the kitchen.', ' Mary travelled to the bedroom.', ' Daniel got the milk.', ' Daniel moved to the office.', ' John travelled to the bathroom.', ' Daniel left the milk.', ' Sandra journeyed to the bedroom.']
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel got the milk.', ' Daniel moved to the office.', ' Daniel left the milk.', ' Sandra journeyed to the bedroom.']
2025-01-22 05:29:08.937 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🐔', '☃️', '🤚', '\U0001f7f0', '🧝🏼\u200d♀️', '🇸🇱', '👐🏾', '🧑🏼\u200d🦯\u200d➡', '2️⃣', '🧛']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (0, 1, 2, 8, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:29:26.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:29:26.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:29:26.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:29:26.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3050, 3054) -->  Daniel got the milk
2025-01-22 05:29:26.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:29:26.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4879, 4884) -->  Daniel moved to the office
2025-01-22 05:29:26.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:29:26.560 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19346, 19350) -->  Daniel left the milk
2025-01-22 05:29:26.560 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-22 05:29:26.666 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21440, 21446) --> . Sandra journeyed to the
2025-01-22 05:29:26.666 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the garden.
2025-01-22 05:29:26.726 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11848, 11854) --> . John went back to the
2025-01-22 05:29:26.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 05:29:26.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1513, 1518) --> . Sandra went to the
2025-01-22 05:29:26.734 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:29:26.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1101, 1106) --> . John travelled to the
2025-01-22 05:29:26.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 05:29:26.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10536, 10541) --> . Mary travelled to the
2025-01-22 05:29:26.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:29:26.796 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1101, 1106) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:29:29.454 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:29:29.454 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24184])
2025-01-22 05:29:37.815 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [40.850911458333336, 2.1327960267912514, 19.917818509615383, 2.0751399239238513, 2.982567630597015], 'topk_indices': array([24144,    34,     4, 24157, 24029, 24156,     1, 24178,    25,
       24176, 24183, 24158, 24063,     9,    24,    14, 24182,     0,
       24179, 24180]), 'topk_tokens': [' return', ' kitchen', '\n\n', 'Question', ' context', '.\n\n', '<|start_header_id|>', ':', '<|eot_id|>', '?\n', '\n\n', ':', '.\n\n', ':', '\n\n', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [85.00625, 44.8046875, 39.125, 37.84765625, 4.859375]}, 'weight': {'score': [23.260091145833332, 23.437507752098234, 20.311298076923077, 23.44105166601069, 29.252798507462686], 'topk_indices': array([18781, 18737, 14613, 14649, 19459, 14658, 14694, 19401, 14577,
       14512, 20273, 20321, 18075, 18106, 23661, 23527, 21993, 21966,
       23609, 23743]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 23.9234375, 26.462890625, 21.213541666666668]}, 'saliency': {'score': [0.24666706720987955, 0.011565230949137837, 0.10562544602614182, 0.011230143338322082, 0.021650399734724813], 'topk_indices': array([24147,     9,    12,    20, 24073, 24163, 24027, 24176,    14,
       24156, 24183,    24, 24144, 24181, 24063,     8, 24177,    34,
       24029, 24157]), 'topk_tokens': [' directly', ':', '202', ' Jul', ' location', ' prior', ' provided', '?\n', '\n', '.\n\n', '\n\n', '\n\n', ' return', 'assistant', '.\n\n', ' Date', 'Answer', ' kitchen', ' context', 'Question'], 'evidence_proportions': [0.4852294921875, 0.30291748046875, 0.225244140625, 0.2481231689453125, 0.027246475219726562]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 100}
2025-01-22 05:29:37.822 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:29:37.822 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-41_pid-0_0-1-2-8-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 1, 2, 8, 9):   1%|          | 1/100 [00:28<47:28, 28.77s/it]is_0k: False
your chose emoji: ['🍄', '👩🏽\u200d🎤', '👩🏻\u200d🏫', '🏃🏿\u200d♀\u200d➡', '🦒', '👨🏽\u200d🎤', '🏊🏽\u200d♀', '🥭', '🔌', '🇵🇸']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.52s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.72s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.98s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.39s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.11s/it]
Processing depth (0, 3, 4, 6, 8):   1%|          | 1/100 [00:47<47:28, 28.77s/it]2025-01-22 05:29:57.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:29:57.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:29:57.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:29:57.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7536, 7540) -->  Daniel got the milk
2025-01-22 05:29:57.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:29:57.204 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9738, 9743) --> . Daniel moved to the
2025-01-22 05:29:57.205 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:29:57.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14444, 14448) -->  Daniel left the milk
2025-01-22 05:29:57.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-22 05:29:57.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19271, 19277) -->  Sandra journeyed to the bedroom
2025-01-22 05:29:57.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the garden.
2025-01-22 05:29:57.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11898, 11904) --> . John went back to the
2025-01-22 05:29:57.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 05:29:57.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1395, 1400) --> . Sandra went to the
2025-01-22 05:29:57.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:29:57.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1051, 1056) --> . John travelled to the
2025-01-22 05:29:57.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 05:29:57.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10576, 10581) --> . Mary travelled to the
2025-01-22 05:29:57.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:29:57.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1051, 1056) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:30:00.171 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:30:00.171 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24222])
2025-01-22 05:30:09.058 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [124.224609375, 5.239580753353973, 56.30889423076923, 5.0665324457083765, 8.307072129360465], 'topk_indices': array([24200, 24062, 24111, 24205, 24207, 14447, 24209, 24101, 24216,
       24194, 24214, 24220, 24196,  7540,  7539, 24201, 24202,     0,
       24217, 24218]), 'topk_tokens': [' location', '\n', ' location', ' where', ' milk', ' milk', ' discarded', '.\n\n', ':', '.\n\n', '?\n', '<|end_header_id|>', ':', '.', ' milk', ' prior', ' to', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [144.8, 229.09375, 82.575, 196.578125, 23.638020833333332]}, 'weight': {'score': [23.554361979166668, 23.442574819401443, 20.311298076923077, 23.445831502068252, 28.68168604651163], 'topk_indices': array([18824, 18780, 14622, 14658, 14703, 19432, 19490, 14667, 14574,
       14509, 20352, 20304, 18118, 18149, 23677, 23543, 22009, 21982,
       23625, 23759]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 25.735677083333332]}, 'saliency': {'score': [0.7523701985677084, 0.02857882441627967, 0.3066523625300481, 0.027561206837351346, 0.05965747389682503], 'topk_indices': array([   31, 24182,  6418, 24101, 24197, 24214, 24067, 24194, 24202,
       24204, 24215, 24200, 24205, 24111, 24195, 24207, 14447, 24209,
        7539, 24201]), 'topk_tokens': [' moved', ' return', ' kitchen', '.\n\n', ' Where', '?\n', ' context', '.\n\n', ' to', ' place', 'Answer', ' location', ' where', ' location', 'Question', ' milk', ' milk', ' discarded', ' milk', ' prior'], 'evidence_proportions': [0.75712890625, 1.5560302734375, 0.396435546875, 1.2940673828125, 0.14811197916666666]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 100}
2025-01-22 05:30:09.094 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:30:09.106 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-41_pid-1_0-3-4-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 3, 4, 6, 8):   2%|▏         | 2/100 [01:00<49:24, 30.25s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d🍳', '👩🏼\u200d🦼', '🐦\u200d⬛', '🇹🇭', '👩\u200d🎓', '👩🏼\u200d🤝\u200d👨🏽', '🥂', '🤰🏾', '🪲', '👩🏿\u200d🤝\u200d👩🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.05s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.32s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.97s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]
Processing depth (0, 1, 2, 4, 5):   2%|▏         | 2/100 [01:15<49:24, 30.25s/it]2025-01-22 05:30:25.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:30:25.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:30:25.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:30:25.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2936, 2940) -->  Daniel got the milk
2025-01-22 05:30:25.063 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:30:25.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4928, 4933) --> . Daniel moved to the
2025-01-22 05:30:25.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:30:25.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9746, 9750) -->  Daniel left the milk
2025-01-22 05:30:25.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-22 05:30:25.207 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (11915, 11921) --> . Sandra journeyed to the
2025-01-22 05:30:25.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the garden.
2025-01-22 05:30:25.270 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11850, 11856) --> . John went back to the
2025-01-22 05:30:25.270 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 05:30:25.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1297, 1302) --> . Sandra went to the
2025-01-22 05:30:25.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:30:25.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (991, 996) --> . John travelled to the
2025-01-22 05:30:25.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 05:30:25.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10582, 10587) --> . Mary travelled to the
2025-01-22 05:30:25.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:30:25.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (991, 996) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:30:28.007 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:30:28.007 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 05:30:36.340 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [60.25732421875, 2.1782665898414866, 18.637770432692307, 2.1027090051115627, 5.840021306818182], 'topk_indices': array([   25,     4, 24132,    23, 24131, 24004,    34,     1, 24133,
       24151, 24153,     9, 24038,    24, 24158,    14, 24157,     0,
       24154, 24155]), 'topk_tokens': ['<|eot_id|>', '\n\n', 'Question', '4', '.\n\n', ' context', ' kitchen', '<|start_header_id|>', ':', '?\n', ':', ':', '.\n\n', '\n\n', '\n\n', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [105.3125, 76.296875, 73.325, 50.72265625, 7.485026041666667]}, 'weight': {'score': [22.423828125, 23.43442699279861, 20.311298076923077, 23.4388005712923, 28.321022727272727], 'topk_indices': array([18782, 18738, 14638, 14602, 14647, 14683, 19377, 19435, 14566,
       14501, 20263, 20311, 18107, 18076, 23496, 23630, 21934, 21907,
       23578, 23712]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 21.213541666666668]}, 'saliency': {'score': [0.35614267985026044, 0.011848781255092578, 0.09715593778170072, 0.011414099783792982, 0.03548743508078835], 'topk_indices': array([   20,    31,  2936,  6437, 24131,    14, 24151, 24002,    24,
       24119,     8, 24158, 24156, 24038, 24048, 24138, 24152, 24004,
       24132,    34]), 'topk_tokens': [' Jul', ' moved', ' Daniel', ' kitchen', '.\n\n', '\n', '?\n', ' provided', '\n\n', ' return', ' Date', '\n\n', 'assistant', '.\n\n', ' location', ' prior', 'Answer', ' context', 'Question', ' kitchen'], 'evidence_proportions': [0.6032958984375, 0.5469970703125, 0.3491455078125, 0.3408966064453125, 0.0389404296875]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 100}
2025-01-22 05:30:36.347 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:30:36.347 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-41_pid-2_0-1-2-4-5.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 2, 4, 5):   3%|▎         | 3/100 [01:27<46:40, 28.88s/it]is_0k: False
your chose emoji: ['🧎🏿\u200d♂️\u200d➡️', '🕉', '⛈', '🧑🏻\u200d⚕', '✌🏾', '#⃣', '🐣', '😳', '📢', '🏋🏾\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (3, 4, 5, 7, 8):   3%|▎         | 3/100 [01:43<46:40, 28.88s/it]2025-01-22 05:30:53.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:30:53.098 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7604, 7609) -->  war. Daniel moved to
2025-01-22 05:30:53.099 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:30:53.149 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9714, 9718) -->  Daniel got the milk
2025-01-22 05:30:53.149 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:30:53.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7604, 7609) -->  war. Daniel moved to
2025-01-22 05:30:53.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:30:53.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16790, 16794) -->  Daniel left the milk
2025-01-22 05:30:53.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-22 05:30:53.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19279, 19285) -->  Sandra journeyed to the bedroom
2025-01-22 05:30:53.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the garden.
2025-01-22 05:30:53.448 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11828, 11834) --> . John went back to the
2025-01-22 05:30:53.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 05:30:53.456 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1405, 1410) --> . Sandra went to the
2025-01-22 05:30:53.456 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:30:53.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1063, 1068) -->  travelled to the bathroom.
2025-01-22 05:30:53.464 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 05:30:53.523 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10510, 10515) --> . Mary travelled to the
2025-01-22 05:30:53.523 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:30:53.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1062, 1067) -->  John travelled to the bathroom
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:30:56.174 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 05:30:56.175 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24200])
2025-01-22 05:31:04.561 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [375.0104166666667, 10.632815405115068, 171.81009615384616, 10.09724335486275, 18.152096354166666], 'topk_indices': array([ 1066, 24199, 24192, 24194, 16793, 24196,  9718,    14,    23,
       24089, 24079,  7605,  9717,  7606,  9713, 24198,  7608,  7607,
       24195,     0]), 'topk_tokens': [' bathroom', '\n\n', '?\n', ':', ' milk', '<|start_header_id|>', '.', '\n', '4', ' location', '.\n\n', '.', ' milk', ' Daniel', '.', '<|end_header_id|>', ' to', ' moved', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [606.9, 379.9375, 606.9, 300.9375, 34.625]}, 'weight': {'score': [25.0185546875, 23.44452134032971, 21.756911057692307, 23.444773941125327, 29.775625], 'topk_indices': array([18832, 18788, 14633, 14669, 19434, 14678, 14714, 19492, 14532,
       14597, 20374, 20326, 18106, 18137, 23683, 23549, 21970, 21997,
       23765, 23631]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.3734375, 26.611328125, 23.3734375, 26.462890625, 25.735677083333332]}, 'saliency': {'score': [2.2673924763997397, 0.06031753230487956, 1.0632840670072115, 0.057044774114984424, 0.13673004150390625], 'topk_indices': array([24187,    23, 18931, 24045,  7608, 24079, 24193, 24179,  9712,
        1063, 11890,  6426,  9714, 24185, 16793, 24089,  1066,  9717,
        7606,  7607]), 'topk_tokens': [' discarded', '4', ' manner', ' context', ' to', '.\n\n', 'Answer', ' prior', ' Capt', ' travelled', ' Daniel', ' kitchen', ' Daniel', ' milk', ' milk', ' location', ' bathroom', ' milk', ' Daniel', ' moved'], 'evidence_proportions': [3.41591796875, 2.644775390625, 3.41591796875, 2.06640625, 0.23558553059895834]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 05:31:04.570 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:31:04.571 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-41_pid-3_3-4-5-7-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (3, 4, 5, 7, 8):   4%|▍         | 4/100 [01:55<45:47, 28.62s/it]is_0k: False
your chose emoji: ['🥡', '🙋🏼\u200d♂️', '🏄🏻\u200d♀️', '🍮', '🧑🏼\u200d❤\u200d🧑🏾', '🧕🏿', '🧎🏾\u200d♀\u200d➡', '🙆🏾\u200d♂️', '🙅\u200d♀', '🐐']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.53s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]
Processing depth (0, 1, 4, 5, 9):   4%|▍         | 4/100 [02:12<45:47, 28.62s/it]2025-01-22 05:31:21.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:31:21.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:31:21.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:31:21.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3050, 3054) -->  Daniel got the milk
2025-01-22 05:31:21.490 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:31:21.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9786, 9791) --> . Daniel moved to the
2025-01-22 05:31:21.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:31:21.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11998, 12002) -->  Daniel left the milk
2025-01-22 05:31:21.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra journeyed to the bedroom.
2025-01-22 05:31:21.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21480, 21486) --> . Sandra journeyed to the
2025-01-22 05:31:21.723 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the garden.
2025-01-22 05:31:21.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11902, 11908) --> . John went back to the
2025-01-22 05:31:21.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 05:31:21.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1513, 1518) --> . Sandra went to the
2025-01-22 05:31:21.817 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 05:31:21.823 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1101, 1106) --> . John travelled to the
2025-01-22 05:31:21.823 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary travelled to the bedroom.
2025-01-22 05:31:21.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10602, 10607) --> . Mary travelled to the
2025-01-22 05:31:21.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:31:21.896 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (1101, 1106) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:31:24.597 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:31:24.597 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24170])
2025-01-22 05:31:32.931 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [56.904947916666664, 3.033810643589956, 27.08533653846154, 2.9542912236247565, 2.812003580729167], 'topk_indices': array([   25, 24142,    23, 24164,     1, 11934, 24144, 24169,     9,
          24, 24049, 24015,    28,  7415,    14,  7416, 24168,     0,
       24165, 24166]), 'topk_tokens': ['<|eot_id|>', '.\n\n', '4', ':', '<|start_header_id|>', 'ANT', ':', '\n\n', ':', '\n\n', '.\n\n', ' context', '<|end_header_id|>', 'nes', '\n', 'ot', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [100.9875, 58.203125, 85.775, 40.80078125, 5.981770833333333]}, 'weight': {'score': [22.423828125, 23.433950068257975, 20.311298076923077, 23.438320663889236, 29.21484375], 'topk_indices': array([18756, 18712, 14624, 14588, 19351, 19409, 14633, 14669, 14552,
       14487, 20255, 20303, 18050, 18081, 23653, 23519, 21967, 21940,
       23735, 23601]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 21.213541666666668]}, 'saliency': {'score': [0.3190422058105469, 0.016757181570250124, 0.14288799579326922, 0.016320492860240357, 0.02084814707438151], 'topk_indices': array([    8,    38, 11935, 24013,    64, 24149,    34, 24167, 11978,
          63, 24049, 24163, 11963, 24130,  6535, 11934, 24143, 24015,
        7415,  7416]), 'topk_tokens': [' Date', '***', 'IC', ' provided', 'ENCES', ' prior', ' kitchen', 'assistant', 'ANT', 'ISC', '.\n\n', 'Answer', 'BR', ' return', ' kitchen', 'ANT', 'Question', ' context', 'nes', 'ot'], 'evidence_proportions': [0.5381103515625, 0.404083251953125, 0.4125244140625, 0.2682952880859375, 0.0357208251953125]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:31:32.951 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:31:32.952 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-41_pid-4_0-1-4-5-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 4, 5, 9):   5%|▌         | 5/100 [02:23<45:10, 28.53s/it]Processing depth (0, 1, 4, 5, 9):   5%|▌         | 5/100 [02:24<45:39, 28.84s/it]
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:82 - Selected idx: 42
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the office? 
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the bathroom.', ' John went back to the garden.', ' Sandra journeyed to the bedroom.', ' Sandra went to the kitchen.', ' Daniel moved to the kitchen.', ' Mary travelled to the bedroom.', ' Sandra went to the bathroom.', ' John travelled to the kitchen.', ' Daniel moved to the office.', ' Daniel left the football.', ' John went back to the office.']
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' Daniel left the football.', ' John went back to the office.']
2025-01-22 05:31:33.236 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🙇🏽\u200d♀️', '🤦', '🏃🏻\u200d♀', '🧎🏻', '🧙🏻', '✋🏽', '🧑\u200d🦳', '👨🏻\u200d🦽', '👦', '🌸']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.81s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (0, 5, 6, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:31:50.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:31:50.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:31:50.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:31:50.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (12008, 12013) -->  Daniel moved to the office
2025-01-22 05:31:50.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:31:50.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14550, 14554) -->  Daniel left the football
2025-01-22 05:31:50.556 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 05:31:50.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9135, 9141) --> . John went back to the
2025-01-22 05:31:50.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:31:50.649 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8303, 8308) --> . John travelled to the
2025-01-22 05:31:50.649 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:31:50.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9135, 9141) --> . John went back to the
2025-01-22 05:31:50.697 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 05:31:50.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9348, 9354) --> . Sandra journeyed to the
2025-01-22 05:31:50.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:31:50.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4993, 4998) --> . Sandra went to the
2025-01-22 05:31:50.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:31:50.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21279, 21284) --> . Mary travelled to the
2025-01-22 05:31:50.888 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went to the bathroom.
2025-01-22 05:31:50.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4993, 4998) --> . Sandra went to the
2025-01-22 05:31:50.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John travelled to the kitchen.
2025-01-22 05:31:50.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8303, 8308) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:31:53.877 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the office.<|eot_id|>
2025-01-22 05:31:53.877 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 05:32:02.279 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [285.753125, 11.50610874101314, 134.74493243243242, 11.090089821909299, 17.516614583333332], 'topk_indices': array([24198, 14551, 24172, 24087,  4994, 24195, 24187,     9,     3,
       18896,    35, 14554,    31,    24, 24186,    14, 24197, 14553,
       24194,     0]), 'topk_tokens': ['\n\n', ' left', '.', '.\n\n', ' Sandra', '<|start_header_id|>', ' before', ':', '<|end_header_id|>', ' in', '.', '.', ' moved', '\n\n', ' football', '\n', '<|end_header_id|>', ' football', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [435.8, 208.025, 533.1875, 60.53125]}, 'weight': {'score': [22.3125, 23.442003760019833, 20.52322635135135, 23.447412119486437, 29.394583333333333], 'topk_indices': array([18796, 18752, 14622, 14586, 14667, 14631, 19398, 19456, 14545,
       14480, 20324, 20276, 18090, 18121, 23551, 23685, 22017, 21990,
       23767, 23633]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 23.9234375, 26.791015625, 20.0703125]}, 'saliency': {'score': [1.740118408203125, 0.06522101999737884, 0.8237717087204391, 0.06267124475418695, 0.12630096435546875], 'topk_indices': array([17519,    30,     0, 17529,    24, 24168,    39, 24189, 24192,
       24097, 14551,  4998, 24187,    38,    34, 18895,    31,  4994,
       24186, 14553]), 'topk_tokens': ['ente', 'Daniel', '<|begin_of_text|>', 'ente', '\n\n', ' return', '\n\n\n', ' office', 'Answer', ' location', ' left', ' bathroom', ' before', '***', ' kitchen', ' manner', ' moved', ' Sandra', ' football', ' football'], 'evidence_proportions': [2.25537109375, 1.322900390625, 3.7685546875, 0.3061319986979167]}}, 'pred_res': 'The football was left in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:32:02.292 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:32:02.293 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-42_pid-0_0-5-6-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 5, 6, 8):   1%|          | 1/100 [00:28<47:42, 28.91s/it]is_0k: False
your chose emoji: ['🇰🇵', '👩\u200d🏫', '⛔', '🧑🏽\u200d❤️\u200d🧑🏼', '👩\u200d⚖', '💷', '😪', '👷🏾\u200d♀', '🧑🏾\u200d❤\u200d💋\u200d🧑🏽', '🤲🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.22s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (3, 6, 8, 9):   1%|          | 1/100 [00:46<47:42, 28.91s/it]2025-01-22 05:32:19.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:32:19.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7443, 7448) --> . Daniel moved to the
2025-01-22 05:32:19.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:32:19.977 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7443, 7448) --> . Daniel moved to the
2025-01-22 05:32:19.978 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:32:20.074 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19190, 19194) -->  Daniel left the football
2025-01-22 05:32:20.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 05:32:20.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9027, 9033) --> . John went back to the
2025-01-22 05:32:20.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:32:20.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8133, 8138) --> . John travelled to the
2025-01-22 05:32:20.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:32:20.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9027, 9033) --> . John went back to the
2025-01-22 05:32:20.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 05:32:20.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9131, 9137) -->  Press. Sandra journeyed to
2025-01-22 05:32:20.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:32:20.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4949, 4954) --> . Sandra went to the
2025-01-22 05:32:20.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:32:20.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21074, 21079) --> . Mary travelled to the
2025-01-22 05:32:20.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went to the bathroom.
2025-01-22 05:32:20.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4949, 4954) --> . Sandra went to the
2025-01-22 05:32:20.428 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John travelled to the kitchen.
2025-01-22 05:32:20.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8133, 8138) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:32:23.322 | INFO     | test_jbb_embedding:begin_test:693 - The football was in the office.<|eot_id|>
2025-01-22 05:32:23.322 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 05:32:31.752 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [310.484375, 10.411810544133433, 109.25675675675676, 10.012112042209807, 12.028894761029411], 'topk_indices': array([    1,  7445, 24202, 24218, 19191, 24207,     3, 19192,    24,
           9, 24117, 24107,    14, 24206, 19194, 24217, 24215, 19193,
       24214,     0]), 'topk_tokens': ['<|start_header_id|>', ' moved', ':', '\n\n', ' left', ' before', '<|end_header_id|>', ' the', '\n\n', ':', ' location', '.\n\n', '\n', ' football', '.', '<|end_header_id|>', '<|start_header_id|>', ' football', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [335.05, 335.05, 602.8125, 74.65625]}, 'weight': {'score': [21.333984375, 23.44985963173974, 21.01288006756757, 23.455342178770948, 29.823345588235295], 'topk_indices': array([18875, 18831, 14675, 14639, 14720, 14684, 19533, 19475, 14538,
       14603, 20401, 20353, 18200, 18169, 23705, 23571, 22037, 22010,
       23653, 23787]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 26.791015625, 20.0703125]}, 'saliency': {'score': [1.856732177734375, 0.05858064691963917, 0.6156170819256757, 0.056239518067517584, 0.0890878116383272], 'topk_indices': array([24209, 24203, 12835, 24201, 14357, 24188, 19767,     0, 24116,
       24212, 24207,  7445, 19191, 24107,  7448, 19190,  7444, 24117,
       24206, 19193]), 'topk_tokens': [' office', ' Where', 'present', 'Question', ' moved', ' return', ' kitchen', '<|begin_of_text|>', ' first', 'Answer', ' before', ' moved', ' left', '.\n\n', ' kitchen', ' Daniel', ' Daniel', ' location', ' football', ' football'], 'evidence_proportions': [1.8083984375, 1.8083984375, 4.190185546875, 0.38165283203125]}}, 'pred_res': 'The football was in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:32:31.760 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:32:31.760 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-42_pid-1_3-6-8-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (3, 6, 8, 9):   2%|▏         | 2/100 [00:58<47:45, 29.24s/it]is_0k: False
your chose emoji: ['👨🏽\u200d🔧', '💁\u200d♂️', '♎', '🌸', '🇺🇳', '👩🏻\u200d❤\u200d💋\u200d👨🏼', '👨🏻\u200d❤️\u200d💋\u200d👨🏻', '💑🏼', '🚶🏽\u200d♂️', '👫🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.62s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (1, 2, 3, 7):   2%|▏         | 2/100 [01:15<47:45, 29.24s/it]2025-01-22 05:32:48.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:32:48.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2966, 2971) -->  tragedy. Daniel moved to
2025-01-22 05:32:48.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:32:48.892 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2966, 2971) -->  tragedy. Daniel moved to
2025-01-22 05:32:48.892 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:32:48.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7610, 7614) -->  Daniel left the football
2025-01-22 05:32:48.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 05:32:48.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9054, 9060) --> . John went back to the
2025-01-22 05:32:48.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:32:49.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8160, 8165) --> . John travelled to the
2025-01-22 05:32:49.015 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:32:49.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9054, 9060) --> . John went back to the
2025-01-22 05:32:49.061 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 05:32:49.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9158, 9164) -->  Press. Sandra journeyed to
2025-01-22 05:32:49.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:32:49.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4925, 4930) -->  the office. Sandra went
2025-01-22 05:32:49.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:32:49.247 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21101, 21106) --> . Mary travelled to the
2025-01-22 05:32:49.247 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went to the bathroom.
2025-01-22 05:32:49.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4925, 4930) -->  the office. Sandra went
2025-01-22 05:32:49.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John travelled to the kitchen.
2025-01-22 05:32:49.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8160, 8165) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:32:52.171 | INFO     | test_jbb_embedding:begin_test:693 - The football was in the office.<|eot_id|>
2025-01-22 05:32:52.171 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 05:33:00.576 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [221.640625, 9.277011310065339, 101.49630489864865, 8.959523801813472, 17.783158052884616], 'topk_indices': array([24178, 24172, 24152, 24067, 24162,  7999,    24, 24167, 24173,
          14,  7614, 24077,     3, 24033, 24166, 24175,  7613, 24177,
           0, 24174]), 'topk_tokens': ['\n\n', 'Answer', '.', '.\n\n', ':', 'nes', '\n\n', ' before', ':', '\n', '.', ' location', '<|end_header_id|>', ' context', ' football', '<|start_header_id|>', ' football', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [240.1, 240.1, 446.875, 40.71875]}, 'weight': {'score': [24.237890625, 23.435845877098668, 21.87183277027027, 23.437579663212436, 29.177644230769232], 'topk_indices': array([18831, 18787, 14636, 14672, 19444, 19502, 14681, 14717, 14529,
       14594, 20370, 20322, 18156, 18125, 23537, 23671, 21972, 21999,
       23619, 23753]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.7171875, 25.7171875, 26.791015625, 20.0703125]}, 'saliency': {'score': [1.426715087890625, 0.05268428386978176, 0.5936019485061234, 0.05071559704400097, 0.1271990262545072], 'topk_indices': array([ 7642, 24031, 24148,  8079,  2968,  7611, 24161,  7948,  4923,
       24169,  2969,  4928,  7610, 24167, 24172,  7999, 24077, 24033,
       24166,  7613]), 'topk_tokens': ['inen', ' provided', ' return', 'nes', ' Daniel', ' left', 'Question', 'nes', ' moved', ' office', ' moved', ' Sandra', ' Daniel', ' before', 'Answer', 'nes', ' location', ' context', ' football', ' football'], 'evidence_proportions': [1.456640625, 1.456640625, 3.1785888671875, 0.20892333984375]}}, 'pred_res': 'The football was in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:33:00.582 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:33:00.582 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-42_pid-2_1-2-3-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 3, 7):   3%|▎         | 3/100 [01:27<46:57, 29.05s/it]is_0k: False
your chose emoji: ['🧑🏼\u200d💻', '\U0001faf7🏿', '👩🏽\u200d💻', '🚣🏻\u200d♀', '🦟', '♐', '👩🏿\u200d💻', '🤵🏾', '🤦🏽\u200d♂️', '🧐']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.25s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.03s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
Processing depth (3, 5, 6, 8):   3%|▎         | 3/100 [01:43<46:57, 29.05s/it]2025-01-22 05:33:16.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:33:17.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7592, 7597) -->  war. Daniel moved to
2025-01-22 05:33:17.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:33:17.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7592, 7597) -->  war. Daniel moved to
2025-01-22 05:33:17.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:33:17.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14568, 14572) -->  Daniel left the football
2025-01-22 05:33:17.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 05:33:17.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9065, 9071) --> . John went back to the
2025-01-22 05:33:17.196 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:33:17.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8233, 8238) --> . John travelled to the
2025-01-22 05:33:17.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:33:17.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9065, 9071) --> . John went back to the
2025-01-22 05:33:17.285 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 05:33:17.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9178, 9184) --> . Sandra journeyed to the
2025-01-22 05:33:17.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:33:17.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4911, 4916) --> . Sandra went to the
2025-01-22 05:33:17.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:33:17.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21203, 21208) --> . Mary travelled to the
2025-01-22 05:33:17.466 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went to the bathroom.
2025-01-22 05:33:17.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4911, 4916) --> . Sandra went to the
2025-01-22 05:33:17.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John travelled to the kitchen.
2025-01-22 05:33:17.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8233, 8238) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:33:20.428 | INFO     | test_jbb_embedding:begin_test:693 - The football was left in the office.<|eot_id|>
2025-01-22 05:33:20.428 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 05:33:28.812 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [354.184375, 9.940281968093899, 138.36782094594594, 9.458211426529683, 16.52060275607639], 'topk_indices': array([   23, 24187, 24081, 12008, 14567,     9, 24091,     3, 11965,
       11964, 14570, 14569,    14, 24189, 24180, 24191, 14572, 24188,
           0, 14571]), 'topk_tokens': ['4', ':', '.\n\n', 'ANT', '.', ':', ' location', '<|end_header_id|>', 'IC', 'ANT', ' the', ' left', '\n', '<|start_header_id|>', ' football', '<|end_header_id|>', '.', '<|eot_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [315.84375, 315.84375, 842.0625, 92.83333333333333]}, 'weight': {'score': [23.066015625, 23.44187055711688, 20.52322635135135, 23.446655632482706, 29.631944444444443], 'topk_indices': array([18756, 18800, 14640, 14604, 14649, 19395, 14685, 19453, 14563,
       14498, 20292, 20340, 18086, 18117, 23679, 23545, 21984, 22011,
       23761, 23627]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.3734375, 23.3734375, 26.791015625, 20.0703125]}, 'saliency': {'score': [2.26773681640625, 0.056092928058550484, 0.7796004011824325, 0.05315151978676882, 0.12175189124213324], 'topk_indices': array([    0, 24175, 14572, 24081, 24181,  8238,  8788, 11934, 11935,
        7595, 12008, 24186, 11965, 24091, 14568,  7594, 11964, 14569,
       24180, 14571]), 'topk_tokens': ['<|begin_of_text|>', 'Question', '.', '.\n\n', ' before', ' bathroom', '�', ' Daniel', ' moved', ' moved', 'ANT', 'Answer', 'IC', ' location', ' Daniel', ' Daniel', 'ANT', ' left', ' football', ' football'], 'evidence_proportions': [1.902490234375, 1.902490234375, 5.867919921875, 0.4763590494791667]}}, 'pred_res': 'The football was left in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:33:28.817 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:33:28.817 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-42_pid-3_3-5-6-8.pkl | len: 3 |  size: 2.05 KB
Processing depth (3, 5, 6, 8):   4%|▍         | 4/100 [01:55<45:57, 28.73s/it]is_0k: False
your chose emoji: ['🙇🏽\u200d♂', '🦹🏿', '🕷️', '🏌️\u200d♀️', '🏌🏼\u200d♀️', '🇨🇲', '🦸🏻\u200d♀️', '👨\u200d🦯\u200d➡', '🇨🇮', '🇪🇸']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.23s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.63s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (2, 4, 6, 7):   4%|▍         | 4/100 [02:12<45:57, 28.73s/it]2025-01-22 05:33:45.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:33:45.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4731, 4736) -->  the senate. Daniel moved
2025-01-22 05:33:45.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:33:46.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4731, 4736) -->  the senate. Daniel moved
2025-01-22 05:33:46.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the football.
2025-01-22 05:33:46.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14372, 14376) -->  Daniel left the football
2025-01-22 05:33:46.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 05:33:46.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9003, 9009) --> . John went back to the
2025-01-22 05:33:46.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:33:46.188 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8073, 8078) --> . John travelled to the
2025-01-22 05:33:46.188 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:33:46.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9003, 9009) --> . John went back to the
2025-01-22 05:33:46.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra journeyed to the bedroom.
2025-01-22 05:33:46.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9082, 9088) --> . Sandra journeyed to the
2025-01-22 05:33:46.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:33:46.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (4736, 4741) -->  to the kitchen. Sandra
2025-01-22 05:33:46.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary travelled to the bedroom.
2025-01-22 05:33:46.424 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21087, 21092) --> . Mary travelled to the
2025-01-22 05:33:46.425 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra went to the bathroom.
2025-01-22 05:33:46.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4737, 4742) -->  the kitchen. Sandra went
2025-01-22 05:33:46.453 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John travelled to the kitchen.
2025-01-22 05:33:46.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8073, 8078) --> . John travelled to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:33:49.363 | INFO     | test_jbb_embedding:begin_test:693 - The football was in the office.<|eot_id|>
2025-01-22 05:33:49.363 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 05:33:57.755 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [218.34296875, 7.6749314194494955, 121.86993243243244, 7.325348761340569, 7.368462456597222], 'topk_indices': array([24176, 24189,     1, 24187, 14373, 24192,  9693, 24091, 18935,
          23,    14,    24, 24047, 24180, 14376,     3, 24191, 14375,
           0, 24188]), 'topk_tokens': [':', '<|start_header_id|>', '<|start_header_id|>', ':', ' left', '\n\n', ' the', ' location', ' in', '4', '\n', '\n\n', ' context', ' football', '.', '<|end_header_id|>', '<|end_header_id|>', ' football', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [201.925, 201.925, 512.875, 49.3515625]}, 'weight': {'score': [23.698046875, 23.439271987105307, 21.169341216216218, 23.442536908633333, 29.19509548611111], 'topk_indices': array([18835, 18791, 14614, 14650, 14659, 19430, 19488, 14695, 14513,
       14578, 20308, 20356, 18160, 18129, 23551, 23685, 21985, 21958,
       23767, 23633]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.6375, 24.6375, 26.791015625, 20.0703125]}, 'saliency': {'score': [1.460687255859375, 0.042918228752298934, 0.6401796083192568, 0.040828082035942875, 0.054434617360432945], 'topk_indices': array([ 9724, 24162, 24190,    20, 24181, 24175,    24,    23, 14372,
        4735,  4734, 24186,  9691,  9690, 14373, 18934, 24091, 24047,
       24180, 14375]), 'topk_tokens': [' kitchen', ' return', 'assistant', ' Jul', ' before', 'Question', '\n\n', '4', ' Daniel', ' moved', ' Daniel', 'Answer', ' moved', ' Daniel', ' left', ' manner', ' location', ' context', ' football', ' football'], 'evidence_proportions': [1.328271484375, 1.328271484375, 3.596435546875, 0.2575480143229167]}}, 'pred_res': 'The football was in the office.<|eot_id|>', 'score': 0}
2025-01-22 05:33:57.760 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:33:57.760 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-42_pid-4_2-4-6-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 4, 6, 7):   5%|▌         | 5/100 [02:24<45:36, 28.81s/it]Processing depth (2, 4, 6, 7):   5%|▌         | 5/100 [02:24<45:47, 28.92s/it]
2025-01-22 05:33:57.995 | INFO     | __main__:<module>:82 - Selected idx: 43
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:86 - Needle: [' Daniel moved to the kitchen.', ' Mary travelled to the bedroom.', ' John went back to the garden.', ' Sandra went to the bathroom.', ' Daniel got the milk.', ' Sandra went to the kitchen.', ' Daniel moved to the office.', ' John travelled to the bathroom.', ' John travelled to the kitchen.', ' Sandra journeyed to the bedroom.', ' Daniel left the milk.', ' John went back to the office.']
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel got the milk.', ' Daniel moved to the office.', ' Daniel left the milk.', ' John went back to the office.']
2025-01-22 05:33:57.996 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏃🏽\u200d♀', '📡', '🆚', '👩🏾\u200d❤\u200d👩🏽', '👩🏼\u200d✈', '\U0001faf2🏼', '🛸', '👨🏿\u200d❤️\u200d👨🏼', '🇦🇷', '🙎']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.54s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.27s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.98s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.37s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
Processing depth (0, 3, 5, 7, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:34:16.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:34:16.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:34:16.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:34:16.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7478, 7482) -->  Daniel got the milk
2025-01-22 05:34:16.069 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:34:16.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11843, 11848) --> . Daniel moved to the
2025-01-22 05:34:16.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:34:16.207 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16744, 16748) -->  Daniel left the milk
2025-01-22 05:34:16.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 05:34:16.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19204, 19210) --> . John went back to the
2025-01-22 05:34:16.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:34:16.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17536, 17541) --> . Mary travelled to the
2025-01-22 05:34:16.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:34:16.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19204, 19210) --> . John went back to the
2025-01-22 05:34:16.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 05:34:16.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6865, 6870) --> . Sandra went to the
2025-01-22 05:34:16.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:34:16.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6865, 6870) --> . Sandra went to the
2025-01-22 05:34:16.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:34:16.559 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (901, 906) --> . John travelled to the
2025-01-22 05:34:16.559 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John travelled to the kitchen.
2025-01-22 05:34:16.564 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (901, 906) --> . John travelled to the
2025-01-22 05:34:16.564 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the bedroom.
2025-01-22 05:34:16.618 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10920, 10926) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:34:19.312 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:34:19.312 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24239])
2025-01-22 05:34:27.711 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [207.10416666666666, 9.737495874927811, 118.05130912162163, 9.37587265053141, 5.345492276278409], 'topk_indices': array([24213,    24,    23,    35,  6866,    29,    34,    14, 24231,
       24233, 24118,    31,  7482, 24218, 24219, 24237,  7481,     0,
       24234, 24235]), 'topk_tokens': [':', '\n\n', '4', '.', ' Sandra', '\n\n', ' kitchen', '\n', '?\n', ':', '.\n\n', ' moved', '.', ' prior', ' to', '<|end_header_id|>', ' milk', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [368.725, 408.5, 127.875, 194.0, 12.916666666666666]}, 'weight': {'score': [22.138020833333332, 23.45053780628661, 20.52322635135135, 23.456319656962076, 29.756214488636363], 'topk_indices': array([18846, 18890, 14707, 14671, 14716, 19550, 19492, 14752, 14635,
       14570, 20425, 20377, 18144, 18175, 23582, 23716, 22021, 22048,
       23798, 23664]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 20.0703125]}, 'saliency': {'score': [1.2526175181070964, 0.054137000253949755, 0.667978235193201, 0.052008235599004644, 0.03970081155950373], 'topk_indices': array([16781, 24128, 18989, 16747,    27, 24118, 24212,   906,  7479,
        7478, 24232, 24224,  6870,    30, 24226,    34,  6866,    31,
       24218,  7481]), 'topk_tokens': ['present', ' location', ' manner', ' milk', 'user', '.\n\n', 'Question', ' bathroom', ' got', ' Daniel', 'Answer', ' milk', ' kitchen', 'Daniel', ' discarded', ' kitchen', ' Sandra', ' moved', ' prior', ' milk'], 'evidence_proportions': [1.96728515625, 2.810791015625, 0.6693359375, 1.3076171875, 0.06768035888671875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:34:27.719 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:34:27.719 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-43_pid-0_0-3-5-7-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 3, 5, 7, 8):   1%|          | 1/100 [00:29<48:49, 29.59s/it]is_0k: False
your chose emoji: ['🏴\u200d☠️', '🤽🏾\u200d♀️', '⏬', '👷\u200d♀️', '🚣🏻\u200d♂', '👨🏻\u200d🦯\u200d➡', '👩\u200d❤\u200d💋\u200d👨', '👱🏿\u200d♀', '👨\u200d👨\u200d👧\u200d👦', '🕺🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.51s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.84s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.18s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.54s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.18s/it]
Processing depth (0, 3, 4, 7, 8):   1%|          | 1/100 [00:48<48:49, 29.59s/it]2025-01-22 05:34:46.504 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:34:46.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:34:46.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:34:46.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7478, 7482) -->  Daniel got the milk
2025-01-22 05:34:46.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:34:46.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9624, 9629) --> . Daniel moved to the
2025-01-22 05:34:46.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:34:46.676 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16744, 16748) -->  Daniel left the milk
2025-01-22 05:34:46.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 05:34:46.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19273, 19279) -->  John went back to the office
2025-01-22 05:34:46.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:34:46.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17524, 17529) --> . Mary travelled to the
2025-01-22 05:34:46.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:34:46.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19274, 19280) -->  went back to the office.
2025-01-22 05:34:46.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 05:34:47.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6911, 6916) --> . Sandra went to the
2025-01-22 05:34:47.009 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:34:47.048 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6911, 6916) --> . Sandra went to the
2025-01-22 05:34:47.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:34:47.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (821, 826) --> . John travelled to the
2025-01-22 05:34:47.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John travelled to the kitchen.
2025-01-22 05:34:47.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (821, 826) --> . John travelled to the
2025-01-22 05:34:47.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the bedroom.
2025-01-22 05:34:47.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10740, 10746) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:34:49.802 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:34:49.802 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24243])
2025-01-22 05:34:58.157 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [55.610188802083336, 3.3657968066897634, 36.8777977195946, 3.262682916192888, 2.8419325086805554], 'topk_indices': array([   23,  5903,  5896, 24242, 24215,    28, 24216,    14,  5898,
        5904,  6040, 24217,  6033,  6034,  6041,  5905, 24239, 24241,
           0, 24238]), 'topk_tokens': ['4', ' to', '\n', '\n\n', '.\n\n', '<|end_header_id|>', 'Question', '\n', ' way', ' the', ' to', ':', '\n', 'the', ' the', ' office', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [89.625, 90.296875, 39.2625, 76.65625, 3.732421875]}, 'weight': {'score': [22.974283854166668, 23.45119555803019, 20.54391891891892, 23.456116588277858, 29.7046875], 'topk_indices': array([18842, 18886, 14707, 14743, 19546, 14788, 19488, 14752, 14606,
       14671, 20373, 20421, 18211, 18180, 23720, 23586, 22052, 22025,
       23802, 23668]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 23.415364583333332]}, 'saliency': {'score': [0.33086903889973956, 0.01865363488798732, 0.20564558699324326, 0.018057733630010207, 0.0207974116007487], 'topk_indices': array([ 6039, 24230,  6912,    20,    23, 24242, 24228, 24240,  6041,
       24203, 24215, 24236, 24088,  7481,  6916,  5898, 24222,  6034,
       24216,  5905]), 'topk_tokens': [' landing', ' discarded', ' Sandra', ' Jul', '4', '\n\n', ' milk', 'assistant', ' the', ' return', '.\n\n', 'Answer', ' context', ' milk', ' kitchen', ' way', ' prior', 'the', 'Question', ' office'], 'evidence_proportions': [0.4736572265625, 0.618988037109375, 0.193359375, 0.499603271484375, 0.021901448567708332]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:34:58.164 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:34:58.164 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-43_pid-1_0-3-4-7-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 3, 4, 7, 8):   2%|▏         | 2/100 [01:00<49:09, 30.09s/it]is_0k: False
your chose emoji: ['🅿️', '🐦', '🔤', '🚧', '🧑🏽\u200d❤️\u200d🧑🏼', '🧎🏽\u200d♂️\u200d➡', '🚶🏻\u200d♀\u200d➡', '👩🏿\u200d❤️\u200d👨🏼', '👩🏽\u200d❤\u200d👩🏻', '🦻🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.53s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.59s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.54s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (0, 1, 6, 7, 9):   2%|▏         | 2/100 [01:16<49:09, 30.09s/it]2025-01-22 05:35:15.320 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:35:15.320 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:35:15.320 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:35:15.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2915, 2919) -->  got the milk.
2025-01-22 05:35:15.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:35:15.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14371, 14376) --> . Daniel moved to the
2025-01-22 05:35:15.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:35:15.521 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16806, 16810) -->  Daniel left the milk
2025-01-22 05:35:15.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 05:35:15.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19624, 19630) --> . John went back to the
2025-01-22 05:35:15.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:35:15.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17558, 17563) --> . Mary travelled to the
2025-01-22 05:35:15.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:35:15.830 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19624, 19630) --> . John went back to the
2025-01-22 05:35:15.830 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 05:35:15.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6850, 6855) --> . Sandra went to the
2025-01-22 05:35:15.869 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:35:15.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6850, 6855) --> . Sandra went to the
2025-01-22 05:35:15.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:35:15.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (901, 906) --> . John travelled to the
2025-01-22 05:35:15.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John travelled to the kitchen.
2025-01-22 05:35:15.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (901, 906) --> . John travelled to the
2025-01-22 05:35:15.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the bedroom.
2025-01-22 05:35:15.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10914, 10920) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:35:18.653 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:35:18.654 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24267])
2025-01-22 05:35:26.996 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [83.43440755208333, 4.77695972393902, 23.492081925675677, 4.670377945702012, 5.258140414368873], 'topk_indices': array([17372, 17399, 17377, 17371, 17378,    14, 24265, 17406, 17400,
       17407, 17409, 17379, 17408,     0, 24262, 17380, 17381, 24263,
       17410, 17382]), 'topk_tokens': [' Col', ' H', '\n', '.', 'still', '\n', '<|end_header_id|>', '\n', '.', 'still', ' the', ' in', ' in', '<|begin_of_text|>', '<|eot_id|>', ' the', ' service', '<|start_header_id|>', ' service', ' of'], 'evidence_proportions': [172.1875, 111.078125, 75.31875, 71.90625, 5.492838541666667]}, 'weight': {'score': [21.119466145833332, 23.4588123197363, 20.52322635135135, 23.46561809399397, 29.875], 'topk_indices': array([18846, 18890, 14655, 14691, 19543, 14736, 14700, 19485, 14554,
       14619, 20370, 20418, 18184, 18215, 23722, 23588, 22054, 22027,
       23804, 23670]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 20.5, 19.909375, 26.462890625, 20.0703125]}, 'saliency': {'score': [0.4533831278483073, 0.02597778974170787, 0.12471255740603886, 0.025403172263987243, 0.039255347906374465], 'topk_indices': array([ 2681,  1323, 24156, 17406,    31, 24260, 24240, 17399,     0,
          34, 17409, 17372, 17408, 17379, 17378, 17407, 17380, 17382,
       17381, 17410]), 'topk_tokens': [' Hill', ' use', ' location', '\n', ' moved', 'Answer', 'Question', ' H', '<|begin_of_text|>', ' kitchen', ' the', ' Col', ' in', ' in', 'still', 'still', ' the', ' of', ' service', ' service'], 'evidence_proportions': [0.964501953125, 0.55377197265625, 0.3637451171875, 0.46771240234375, 0.025670369466145832]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:35:27.003 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:35:27.003 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-43_pid-2_0-1-6-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 6, 7, 9):   3%|▎         | 3/100 [01:28<47:43, 29.52s/it]is_0k: False
your chose emoji: ['🏃🏾\u200d♀️', '👩🏻\u200d❤️\u200d💋\u200d👨🏼', '👩🏽\u200d❤️\u200d💋\u200d👨🏼', '👩\u200d🦽\u200d➡️', '🏅', '🥣', '😴', '🚶🏾\u200d➡', '👩🏿\u200d🏫', '👩🏾\u200d🤝\u200d👩🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.27s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
Processing depth (0, 1, 5, 6, 7):   3%|▎         | 3/100 [01:45<47:43, 29.52s/it]2025-01-22 05:35:43.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:35:43.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:35:43.782 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:35:43.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3044, 3048) -->  Daniel got the milk
2025-01-22 05:35:43.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:35:43.866 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11833, 11838) --> . Daniel moved to the
2025-01-22 05:35:43.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:35:43.943 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14354, 14358) -->  Daniel left the milk
2025-01-22 05:35:43.943 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 05:35:44.036 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16714, 16720) --> . John went back to the
2025-01-22 05:35:44.037 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:35:44.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17509, 17514) --> . Mary travelled to the
2025-01-22 05:35:44.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:35:44.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16714, 16720) --> . John went back to the
2025-01-22 05:35:44.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 05:35:44.252 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6830, 6835) -->  went to the kitchen.
2025-01-22 05:35:44.253 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:35:44.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6829, 6834) -->  Sandra went to the kitchen
2025-01-22 05:35:44.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:35:44.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (901, 906) --> . John travelled to the
2025-01-22 05:35:44.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John travelled to the kitchen.
2025-01-22 05:35:44.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (901, 906) --> . John travelled to the
2025-01-22 05:35:44.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the bedroom.
2025-01-22 05:35:44.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10908, 10914) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:35:47.036 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:35:47.037 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24229])
2025-01-22 05:35:55.371 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [55.818196614583336, 3.747528771356058, 35.937288851351354, 3.6465516851288733, 4.643107586596385], 'topk_indices': array([24203,    34, 17333,  5984,     9,  5855, 24223,  5991,  5985,
       24108,     1,    24, 24228,  5992,  5856,    14,     0, 24227,
       24224, 24225]), 'topk_tokens': [':', ' kitchen', ' of', '\n', ':', ' the', ':', ' to', 'the', '.\n\n', '<|start_header_id|>', '\n\n', '\n\n', ' the', ' office', '\n', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [123.0, 70.390625, 18.371875, 77.4453125, 6.905598958333333]}, 'weight': {'score': [22.138020833333332, 23.443205265764277, 21.004011824324323, 23.448235036303835, 29.065323795180724], 'topk_indices': array([18823, 18779, 14636, 14672, 19418, 14723, 19476, 14687, 14535,
       14600, 20351, 20303, 18148, 18117, 23682, 23548, 21994, 21967,
       23630, 23764]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 26.611328125, 19.909375, 26.462890625, 20.0703125]}, 'saliency': {'score': [0.3428030014038086, 0.020562414683773008, 0.20186656230204814, 0.01996492068091185, 0.03397914013230657], 'topk_indices': array([24072, 24118,     8, 24208,    24,    20,  5849, 17332, 24228,
       14357, 24074, 17361, 24108, 24189, 24226, 24222,  5985, 24202,
          34,  5856]), 'topk_tokens': [' provided', ' location', ' Date', ' prior', '\n\n', ' Jul', ' way', ' service', '\n\n', ' milk', ' context', ' service', '.\n\n', ' return', 'assistant', 'Answer', 'the', 'Question', ' kitchen', ' office'], 'evidence_proportions': [0.7072509765625, 0.478118896484375, 0.097802734375, 0.520477294921875, 0.034603118896484375]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:35:55.375 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:35:55.376 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-43_pid-3_0-1-5-6-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 5, 6, 7):   4%|▍         | 4/100 [01:57<46:30, 29.07s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d🚀', '👉', '🥨', '💟', '🪥', '👩🏽\u200d🚀', '👩🏼\u200d❤\u200d💋\u200d👨🏻', '👩🏿\u200d🦰', '🇲🇩', '🧑🏽\u200d❤\u200d💋\u200d🧑🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.01s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.76s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.62s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (2, 3, 5, 6, 9):   4%|▍         | 4/100 [02:14<46:30, 29.07s/it]2025-01-22 05:36:12.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:36:12.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4857, 4862) --> . Daniel moved to the
2025-01-22 05:36:12.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel got the milk.
2025-01-22 05:36:12.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7528, 7532) -->  Daniel got the milk
2025-01-22 05:36:12.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:36:12.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4857, 4862) --> . Daniel moved to the
2025-01-22 05:36:12.578 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 05:36:12.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14380, 14384) -->  Daniel left the milk
2025-01-22 05:36:12.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 05:36:12.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19572, 19578) --> . John went back to the
2025-01-22 05:36:12.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:36:12.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17538, 17543) --> . Mary travelled to the
2025-01-22 05:36:12.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:36:12.971 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (19572, 19578) --> . John went back to the
2025-01-22 05:36:12.971 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 05:36:13.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6877, 6882) --> . Sandra went to the
2025-01-22 05:36:13.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the kitchen.
2025-01-22 05:36:13.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6877, 6882) --> . Sandra went to the
2025-01-22 05:36:13.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bathroom.
2025-01-22 05:36:13.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (895, 900) --> . John travelled to the
2025-01-22 05:36:13.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  John travelled to the kitchen.
2025-01-22 05:36:13.053 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (895, 900) --> . John travelled to the
2025-01-22 05:36:13.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra journeyed to the bedroom.
2025-01-22 05:36:13.107 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (10914, 10920) --> . Sandra journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:36:15.740 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:36:15.740 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24223])
2025-01-22 05:36:24.128 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [167.37923177083334, 5.370496509432015, 82.68623310810811, 5.091212756052141, 8.047003173828125], 'topk_indices': array([24197,     1, 24183, 24216, 14383, 24215,   900, 24195,    23,
        4858, 24217,    14,  7531,  7532, 24102, 24222, 24221,     0,
       24218, 24219]), 'topk_tokens': [':', '<|start_header_id|>', ' return', 'Answer', ' milk', '?\n', ' bathroom', '.\n\n', '4', ' Daniel', ':', '\n', ' milk', '.', '.\n\n', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [229.175, 233.78125, 229.175, 183.3125, 9.49609375]}, 'weight': {'score': [22.158854166666668, 23.447213221332454, 20.52322635135135, 23.452969816883922, 29.894140625], 'topk_indices': array([18838, 18794, 14658, 14622, 19433, 19491, 14667, 14703, 14586,
       14521, 20366, 20318, 18139, 18108, 23542, 23676, 21996, 21969,
       23758, 23624]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 26.611328125, 19.909375, 26.462890625, 20.0703125]}, 'saliency': {'score': [1.0139414469401042, 0.02992929152982591, 0.4592371760187922, 0.02829466775759589, 0.06037039756774902], 'topk_indices': array([   20,    23, 24195,  6878, 24202, 24112, 24208, 24196,  4859,
         897, 24222, 24183, 24102, 24220,  7528, 14383, 24216,  4858,
        7531,   900]), 'topk_tokens': [' Jul', '4', '.\n\n', ' Sandra', ' prior', ' location', ' milk', 'Question', ' moved', ' travelled', '\n\n', ' return', '.\n\n', 'assistant', ' Daniel', ' milk', 'Answer', ' Daniel', ' milk', ' bathroom'], 'evidence_proportions': [1.24052734375, 1.651123046875, 1.24052734375, 1.2576904296875, 0.04901123046875]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 100}
2025-01-22 05:36:24.137 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:36:24.137 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-43_pid-4_2-3-5-6-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (2, 3, 5, 6, 9):   5%|▌         | 5/100 [02:26<45:50, 28.96s/it]Processing depth (2, 3, 5, 6, 9):   5%|▌         | 5/100 [02:26<46:21, 29.28s/it]
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:82 - Selected idx: 44
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the garden? 
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:86 - Needle: [' Mary travelled to the bedroom.', ' John went back to the garden.', ' Daniel travelled to the bathroom.', ' Daniel went to the garden.', ' John travelled to the bathroom.', ' Sandra went to the bathroom.', ' Daniel dropped the apple.', ' John travelled to the kitchen.']
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel travelled to the bathroom.', ' Daniel went to the garden.', ' Daniel dropped the apple.', ' John travelled to the kitchen.']
2025-01-22 05:36:24.506 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🚴🏽\u200d♂', '🏄🏼\u200d♂️', '🏊🏽', '😮', '👩🏻\u200d🦲', '✒', '🧖🏼\u200d♂', '🏙️', '🏄🏻\u200d♂️', '👩🏻\u200d❤\u200d💋\u200d👩🏼']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.07s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.75s/it]
Processing depth (3, 5, 7, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:36:41.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:36:41.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7511, 7516) --> . Daniel travelled to the
2025-01-22 05:36:41.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 05:36:41.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11881, 11886) --> . Daniel went to the
2025-01-22 05:36:41.833 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:36:41.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16634, 16638) -->  Daniel dropped the apple
2025-01-22 05:36:41.918 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 05:36:41.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11116, 11121) -->  paper. John travelled to
2025-01-22 05:36:41.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:36:42.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9102, 9107) -->  Press. Mary travelled to
2025-01-22 05:36:42.024 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:36:42.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22180, 22186) --> . John went back to the
2025-01-22 05:36:42.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the bathroom.
2025-01-22 05:36:42.179 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7512, 7517) -->  Daniel travelled to the bathroom
2025-01-22 05:36:42.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:36:42.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19722, 19727) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:36:45.041 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the apple.<|eot_id|>
2025-01-22 05:36:45.041 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24115])
2025-01-22 05:36:53.409 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [318.31661184210526, 9.85941387138237, 202.60119047619048, 9.447906932677133, 18.42641715116279], 'topk_indices': array([24098,     9,    23, 16635, 16638, 24003, 16636,    14, 24013,
        7516, 24103, 23969, 24114, 24105, 24102, 16637,     0, 24113,
       24110, 24111]), 'topk_tokens': [':', ':', '4', ' dropped', '.', '.\n\n', ' the', '\n', ' location', ' bathroom', ' before', ' context', '\n\n', ' garden', ' apple', ' apple', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [419.7, 194.425, 634.3125, 88.028125]}, 'weight': {'score': [22.61924342105263, 23.42563386267518, 22.84970238095238, 23.42677249460088, 28.876453488372093], 'topk_indices': array([18738, 18782, 14609, 14573, 19377, 14618, 14654, 19435, 14537,
       14472, 20255, 20303, 18076, 18107, 23473, 23607, 21932, 21905,
       23689, 23555]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 19.665625, 27.083984375, 23.6015625]}, 'saliency': {'score': [1.9373329564144737, 0.055376282674636165, 1.324311755952381, 0.052784505046100175, 0.13266133153161339], 'topk_indices': array([24099,    23, 24009, 23967, 24015, 24003, 24097, 16634, 24114,
       24112,  7512,  7513, 16635, 24013, 24103, 23969, 24105,  7516,
       24102, 16637]), 'topk_tokens': [' Where', '4', ' item', ' provided', ' traveled', '.\n\n', 'Question', ' Daniel', '\n\n', 'assistant', ' Daniel', ' travelled', ' dropped', ' location', ' before', ' context', ' garden', ' bathroom', ' apple', ' apple'], 'evidence_proportions': [2.366015625, 1.041796875, 4.304443359375, 0.510498046875]}}, 'pred_res': 'Daniel dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:36:53.416 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:36:53.416 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-44_pid-0_3-5-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (3, 5, 7, 9):   1%|          | 1/100 [00:28<47:25, 28.75s/it]is_0k: False
your chose emoji: ['👯', '\U0001fa76', '🧗\u200d♂️', '⛹\u200d♀️', '🇰🇿', '⛪', '🚸', '🥚', '👴🏾', '🍊']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.32s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.55s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.36s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.55s/it]
Processing depth (0, 1, 4, 7):   1%|          | 1/100 [00:45<47:25, 28.75s/it]2025-01-22 05:37:09.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:37:09.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:37:09.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 05:37:09.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2962, 2967) -->  Daniel went to the garden
2025-01-22 05:37:09.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:37:09.932 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9716, 9720) -->  Daniel dropped the apple
2025-01-22 05:37:09.933 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 05:37:09.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11165, 11170) -->  paper. John travelled to
2025-01-22 05:37:09.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:37:10.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9146, 9151) -->  Press. Mary travelled to
2025-01-22 05:37:10.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:37:10.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22276, 22282) --> . John went back to the
2025-01-22 05:37:10.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the bathroom.
2025-01-22 05:37:10.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:37:10.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:37:10.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19852, 19857) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:37:12.978 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's house.<|eot_id|>
2025-01-22 05:37:12.978 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 05:37:21.357 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [346.7730263157895, 13.61137803945083, 219.75892857142858, 13.169858286388866, 16.9290625], 'topk_indices': array([24161,    14, 24033,  9719, 24163,  9720, 24162, 24164,   548,
         424,   541,   425, 24170, 24166, 24169, 24177, 24167,     0,
       24174, 24175]), 'topk_tokens': ['Question', '\n', ' context', ' apple', ' Where', '.', ':', ' was', ' per', ' to', ' to', ' ', '?', ' apple', ' garden', '<|end_header_id|>', ' before', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [523.55, 244.95, 486.125, 160.3375]}, 'weight': {'score': [24.01768092105263, 23.444163013811927, 21.68563988095238, 23.44524130664402, 29.526875], 'topk_indices': array([18816, 18772, 14666, 14630, 14675, 19487, 14711, 19429, 14529,
       14594, 20355, 20307, 18110, 18141, 23665, 23531, 21978, 21951,
       23613, 23747]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 24.1796875, 27.083984375, 23.6015625]}, 'saliency': {'score': [2.2442626953125, 0.07450081611742464, 1.3089454287574405, 0.07171940560586193, 0.1225897216796875], 'topk_indices': array([  424, 24160,   541,  1627, 24172,    31,  2956, 24171,   546,
       24164, 24033, 24163, 24170, 24161,  9719,    34,   548, 24166,
       24169, 24167]), 'topk_tokens': [' to', '.\n\n', ' to', ' work', 'Answer', ' travelled', '�', ' \n', ' em', ' was', ' context', ' Where', '?', 'Question', ' apple', ' bathroom', ' per', ' apple', ' garden', ' before'], 'evidence_proportions': [3.224609375, 1.59775390625, 3.40478515625, 0.9820068359375]}}, 'pred_res': "Daniel's house.<|eot_id|>", 'score': 0}
2025-01-22 05:37:21.363 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:37:21.364 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-44_pid-1_0-1-4-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 1, 4, 7):   2%|▏         | 2/100 [00:56<46:11, 28.28s/it]is_0k: False
your chose emoji: ['👩🏾\u200d💼', '🧚\u200d♀', '🚣🏼\u200d♂️', '👩🏻\u200d❤️\u200d💋\u200d👨🏽', '🇧🇦', '🌧', '🚾', '🥍', '👨\u200d🦽\u200d➡️', '👼🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.45s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.29s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.02s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.51s/it]
Processing depth (2, 3, 8, 9):   2%|▏         | 2/100 [01:12<46:11, 28.28s/it]2025-01-22 05:37:37.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:37:37.767 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4907, 4912) --> . Daniel travelled to the
2025-01-22 05:37:37.768 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 05:37:37.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7588, 7593) -->  war. Daniel went to
2025-01-22 05:37:37.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:37:37.913 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19237, 19241) -->  dropped the apple.
2025-01-22 05:37:37.914 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 05:37:37.974 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11137, 11142) --> . John travelled to the
2025-01-22 05:37:37.975 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:37:38.022 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9130, 9135) -->  Press. Mary travelled to
2025-01-22 05:37:38.022 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:37:38.144 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22210, 22216) --> . John went back to the
2025-01-22 05:37:38.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the bathroom.
2025-01-22 05:37:38.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4908, 4913) -->  Daniel travelled to the bathroom
2025-01-22 05:37:38.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:37:38.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19752, 19757) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:37:41.031 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's hand.<|eot_id|>
2025-01-22 05:37:41.032 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24153])
2025-01-22 05:37:49.440 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [392.49013157894734, 13.583553257989733, 198.60565476190476, 13.123912806850225, 22.84173387096774], 'topk_indices': array([24041, 24007, 24152, 24137, 24051, 24139, 24138, 24144, 24143,
       24153,    14, 19239, 24136, 19240, 24141, 24151, 24140,     0,
       24149, 24148]), 'topk_tokens': ['.\n\n', ' context', '\n\n', ' Where', ' location', ' the', ' was', '?', ' garden', 'b', '\n', ' apple', ':', '.', ' before', '<|end_header_id|>', ' apple', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [324.575, 227.725, 984.625, 151.4625]}, 'weight': {'score': [21.293996710526315, 23.438506478721642, 22.84970238095238, 23.44070877477608, 29.713709677419356], 'topk_indices': array([18745, 18789, 14615, 14651, 19389, 14696, 19447, 14660, 14579,
       14514, 20285, 20333, 18077, 18108, 23637, 23503, 21935, 21962,
       23585, 23719]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 23.1296875, 20.97265625, 19.990625]}, 'saliency': {'score': [2.1558902138157894, 0.07488543704203925, 1.266508556547619, 0.07220824446166446, 0.1662932365171371], 'topk_indices': array([22216, 24041, 24146, 24152, 19236, 24053, 24136, 19237, 24144,
       24145, 24138, 24153, 24007, 24135, 24051, 24137, 24143, 19239,
       24141, 24140]), 'topk_tokens': [' garden', '.\n\n', 'Answer', '\n\n', ' Daniel', ' traveled', ':', ' dropped', '?', ' \n', ' was', 'b', ' context', 'Question', ' location', ' Where', ' garden', ' apple', ' before', ' apple'], 'evidence_proportions': [2.041015625, 1.33984375, 5.037109375, 0.7818359375]}}, 'pred_res': "Daniel's hand.<|eot_id|>", 'score': 0}
2025-01-22 05:37:49.444 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:37:49.445 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-44_pid-2_2-3-8-9.pkl | len: 3 |  size: 2.01 KB
Processing depth (2, 3, 8, 9):   3%|▎         | 3/100 [01:24<45:34, 28.19s/it]is_0k: False
your chose emoji: ['👮🏻\u200d♀️', '🙆\u200d♂', '⛓\u200d💥', '🧑🏿\u200d✈️', '🌽', '👇', '🧚🏾\u200d♂️', '🖐️', '🗜️', '👩\u200d⚕']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.51s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (1, 6, 7, 8):   3%|▎         | 3/100 [01:42<45:34, 28.19s/it]2025-01-22 05:38:07.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:38:07.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2950, 2955) -->  tragedy. Daniel travelled to
2025-01-22 05:38:07.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 05:38:07.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14351, 14356) --> . Daniel went to the
2025-01-22 05:38:07.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:38:07.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16762, 16766) -->  Daniel dropped the apple
2025-01-22 05:38:07.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 05:38:07.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11119, 11124) --> . John travelled to the
2025-01-22 05:38:07.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:38:07.382 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9069, 9074) --> . Mary travelled to the
2025-01-22 05:38:07.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:38:07.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22218, 22224) --> . John went back to the
2025-01-22 05:38:07.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the bathroom.
2025-01-22 05:38:07.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2952, 2957) -->  Daniel travelled to the bathroom
2025-01-22 05:38:07.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:38:07.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19805, 19810) -->  Rev. Sandra went to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:38:10.271 | INFO     | test_jbb_embedding:begin_test:693 - Anthony street<|eot_id|>
2025-01-22 05:38:10.272 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24155])
2025-01-22 05:38:18.629 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [276.9358552631579, 10.829470310042222, 167.3608630952381, 10.483538617422672, 10.974221850198413], 'topk_indices': array([16763, 24144,    24, 10058, 16766, 24138,    14, 24154,    23,
       24043, 24053, 24146, 24142, 24143, 24145, 16765, 24150, 24151,
           0, 24153]), 'topk_tokens': [' dropped', ' the', '\n\n', ' as', '.', ':', '\n', '\n\n', '4', '.\n\n', ' location', '?', ' apple', ' before', ' garden', ' apple', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>'], 'evidence_proportions': [261.00625, 246.5, 546.6875, 107.5]}, 'weight': {'score': [23.19736842105263, 23.43889187846676, 23.031994047619047, 23.4394364426984, 29.688244047619047], 'topk_indices': array([18758, 18802, 14631, 14595, 14640, 19409, 14676, 19467, 14559,
       14494, 20299, 20347, 18109, 18078, 23645, 23511, 21970, 21943,
       23593, 23727]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.8265625, 19.665625, 27.083984375, 19.990625]}, 'saliency': {'score': [1.7646484375, 0.061094074200316664, 1.0771077473958333, 0.058867363028586014, 0.08344341459728423], 'topk_indices': array([10023, 24152, 24009, 10065, 14356, 24049, 22224,    23, 24043,
       24139, 24146,  2953, 16763, 24137,  2956, 24053, 24142, 24143,
       24145, 16765]), 'topk_tokens': [' if', 'assistant', ' context', 'po', ' garden', ' item', ' garden', '4', '.\n\n', ' Where', '?', ' travelled', ' dropped', 'Question', ' bathroom', ' location', ' apple', ' before', ' garden', ' apple'], 'evidence_proportions': [1.699072265625, 1.30205078125, 3.865966796875, 0.611767578125]}}, 'pred_res': 'Anthony street<|eot_id|>', 'score': 0}
2025-01-22 05:38:18.633 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:38:18.634 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-44_pid-3_1-6-7-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 6, 7, 8):   4%|▍         | 4/100 [01:53<45:43, 28.58s/it]is_0k: False
your chose emoji: ['🦸\u200d♀️', '🌊', '🧛🏽\u200d♂', '👩🏿\u200d❤\u200d👩🏾', '🤲🏻', '🤼\u200d♀', '🇬🇳', '™', '🙍🏼\u200d♀', '🛠️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.88s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]
Processing depth (4, 5, 6, 9):   4%|▍         | 4/100 [02:11<45:43, 28.58s/it]2025-01-22 05:38:36.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:38:36.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9769, 9774) --> . Daniel travelled to the
2025-01-22 05:38:36.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the garden.
2025-01-22 05:38:36.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11913, 11918) --> . Daniel went to the
2025-01-22 05:38:36.256 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:38:36.327 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14374, 14378) -->  Daniel dropped the apple
2025-01-22 05:38:36.327 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 05:38:36.384 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11167, 11172) --> . John travelled to the
2025-01-22 05:38:36.384 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the bedroom.
2025-01-22 05:38:36.429 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9243, 9248) --> . Mary travelled to the
2025-01-22 05:38:36.429 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:38:36.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22248, 22254) --> . John went back to the
2025-01-22 05:38:36.539 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the bathroom.
2025-01-22 05:38:36.586 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9770, 9775) -->  Daniel travelled to the bathroom
2025-01-22 05:38:36.586 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:38:36.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19968, 19973) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:38:39.410 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's pocket.<|eot_id|>
2025-01-22 05:38:39.410 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 05:38:47.769 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [336.1644736842105, 11.536844470617407, 196.20684523809524, 11.120998996191737, 16.137001129518072], 'topk_indices': array([24189, 24083, 24184, 24049, 24176, 24179, 14375, 24181, 24178,
       24180, 24186, 14378, 24185, 24182, 14377, 24183, 24193,     0,
       24191, 24190]), 'topk_tokens': [':', '.\n\n', ' the', ' context', '.\n\n', ' Where', ' dropped', ' the', ':', ' was', '?', '.', ' garden', ' apple', ' apple', ' before', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [302.35, 270.45, 782.75, 78.425]}, 'weight': {'score': [21.668996710526315, 23.446400528969335, 21.986979166666668, 23.449067079228413, 29.276731927710845], 'topk_indices': array([18784, 18740, 14616, 14652, 14661, 19379, 14697, 19437, 14515,
       14580, 20257, 20305, 18109, 18078, 23541, 23675, 21988, 21961,
       23757, 23623]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 19.665625, 27.083984375, 19.990625]}, 'saliency': {'score': [2.1377852590460527, 0.06409132308196959, 1.2169131324404763, 0.06145826393883496, 0.11989106327654367], 'topk_indices': array([24093,  9770,  9286, 24188,     0, 24176, 24187,  9771, 14374,
        9774, 24180, 24049, 24177, 24186, 24179, 14375, 24183, 24185,
       24182, 14377]), 'topk_tokens': [' location', ' Daniel', ' cause', 'Answer', '<|begin_of_text|>', '.\n\n', ' \n', ' travelled', ' Daniel', ' bathroom', ' was', ' context', 'Question', '?', ' Where', ' dropped', ' before', ' garden', ' apple', ' apple'], 'evidence_proportions': [1.8107421875, 1.47470703125, 5.49658203125, 0.440869140625]}}, 'pred_res': "Daniel's pocket.<|eot_id|>", 'score': 0}
2025-01-22 05:38:47.776 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:38:47.776 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-44_pid-4_4-5-6-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (4, 5, 6, 9):   5%|▌         | 5/100 [02:23<45:34, 28.78s/it]Processing depth (4, 5, 6, 9):   5%|▌         | 5/100 [02:23<45:24, 28.68s/it]
2025-01-22 05:38:48.074 | INFO     | __main__:<module>:82 - Selected idx: 45
2025-01-22 05:38:48.074 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 05:38:48.074 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 05:38:48.074 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:38:48.074 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the bathroom.', ' Daniel travelled to the bathroom.', ' John went back to the garden.', ' Daniel picked up the football.', ' Mary travelled to the bedroom.', ' Daniel went to the garden.', ' Sandra went to the bathroom.', ' Daniel dropped the football.', ' John travelled to the kitchen.']
2025-01-22 05:38:48.075 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel travelled to the bathroom.', ' Daniel picked up the football.', ' Daniel went to the garden.', ' Daniel dropped the football.', ' John travelled to the kitchen.']
2025-01-22 05:38:48.075 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👨🏼', '👏🏿', '🦸🏻\u200d♀', '👩🏽\u200d🦯\u200d➡️', '☄', '\U0001faf1🏾\u200d\U0001faf2🏿', '🤵🏿\u200d♂️', '🇸🇧', '🚴🏻\u200d♂', '🚴🏿\u200d♂']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.86s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (0, 1, 5, 7, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:39:05.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:39:05.685 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:39:05.685 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:39:05.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3043, 3048) --> . Daniel picked up the
2025-01-22 05:39:05.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 05:39:05.759 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11885, 11890) --> . Daniel went to the
2025-01-22 05:39:05.759 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:39:05.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16815, 16819) -->  Daniel dropped the football
2025-01-22 05:39:05.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the kitchen.
2025-01-22 05:39:05.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (9157, 9162) --> . John travelled to the
2025-01-22 05:39:05.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:39:05.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:39:05.884 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:39:05.950 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13195, 13201) --> . John went back to the
2025-01-22 05:39:05.950 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the bedroom.
2025-01-22 05:39:05.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6971, 6976) --> . Mary travelled to the
2025-01-22 05:39:05.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:39:05.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (189, 194) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:39:08.728 | INFO     | test_jbb_embedding:begin_test:693 - The line of march.<|eot_id|>
2025-01-22 05:39:08.729 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 05:39:17.143 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [506.3463541666667, 22.273271249121937, 349.3720238095238, 21.507961272561683, 15.079520089285714], 'topk_indices': array([20711,    31, 24185, 20692, 18828,    24, 24183, 24196,  3048,
       16819,  9318, 24087,    34,  3049, 18785, 16818, 20712, 24193,
       24194,     0]), 'topk_tokens': [' Min', ' travelled', ' discarded', 'nes', ' ga', '\n\n', ' football', '<|end_header_id|>', ' football', '.', ' cause', ' location', ' bathroom', '.', 'untlet', ' football', 'nes', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [791.55, 509.65, 182.8875, 881.4375, 241.225]}, 'weight': {'score': [22.023111979166668, 23.44563757695963, 20.822916666666668, 23.449330976258487, 29.62236201298701], 'topk_indices': array([18785, 18829, 14636, 14672, 19430, 19488, 14681, 14717, 14535,
       14600, 20370, 20322, 18123, 18154, 23547, 23681, 21997, 21970,
       23763, 23629]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 22.3625, 19.665625, 27.341796875, 19.990625]}, 'saliency': {'score': [3.1106465657552085, 0.12818997907498658, 2.143400646391369, 0.12347486141916734, 0.10949875472427963], 'topk_indices': array([   24,    30, 18784, 24177, 16816, 24171, 20711, 20692,  9805,
       18828,    31,  9318, 24087, 24183, 24185,  3048,    34, 20712,
       16818, 18785]), 'topk_tokens': ['\n\n', 'Daniel', ' ga', ' prior', ' dropped', 'Question', ' Min', 'nes', ' barric', ' ga', ' travelled', ' cause', ' location', ' football', ' discarded', ' football', ' bathroom', 'nes', ' football', 'untlet'], 'evidence_proportions': [5.141015625, 2.654296875, 1.00859375, 6.15283203125, 1.204931640625]}}, 'pred_res': 'The line of march.<|eot_id|>', 'score': 0}
2025-01-22 05:39:17.177 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:39:17.178 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-45_pid-0_0-1-5-7-8.pkl | len: 3 |  size: 2.12 KB
Processing depth (0, 1, 5, 7, 8):   1%|          | 1/100 [00:28<47:49, 28.99s/it]is_0k: False
your chose emoji: ['🧑🏿\u200d🎤', '👩🏼\u200d⚕', '👶🏼', '🧜🏼\u200d♀️', '🚴🏿\u200d♂️', '⛹🏽\u200d♀️', '🤦🏽\u200d♀', '🙎🏻\u200d♂️', '🌓', '🕤']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.76s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.56s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.45s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (1, 4, 5, 7, 8):   1%|          | 1/100 [00:45<47:49, 28.99s/it]2025-01-22 05:39:34.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:39:34.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3042) --> . Daniel travelled to the
2025-01-22 05:39:34.290 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:39:34.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9787, 9792) --> . Daniel picked up the
2025-01-22 05:39:34.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 05:39:34.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11985, 11990) --> . Daniel went to the
2025-01-22 05:39:34.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:39:34.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16889, 16893) -->  Daniel dropped the football
2025-01-22 05:39:34.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the kitchen.
2025-01-22 05:39:34.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (9143, 9148) --> . John travelled to the
2025-01-22 05:39:34.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:39:34.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3038, 3043) -->  Daniel travelled to the bathroom
2025-01-22 05:39:34.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:39:34.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13251, 13257) --> . John went back to the
2025-01-22 05:39:34.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the bedroom.
2025-01-22 05:39:34.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6957, 6962) --> . Mary travelled to the
2025-01-22 05:39:34.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:39:34.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (183, 188) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:39:37.458 | INFO     | test_jbb_embedding:begin_test:693 - The line of march.<|eot_id|>
2025-01-22 05:39:37.458 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24144])
2025-01-22 05:39:47.736 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [447.1380208333333, 18.30878965295896, 350.3199404761905, 17.592494917434237, 16.4544921875], 'topk_indices': array([16890, 24118, 24127, 24143,   189, 24033, 24124,  3042,  3038,
         188, 24142,  9792,  9793, 16893, 24131, 24129, 16892,     0,
       24139, 24140]), 'topk_tokens': [' dropped', ':', ' where', '\n\n', '.', ' location', ' to', ' bathroom', ' Daniel', ' bathroom', '<|end_header_id|>', ' football', '.', '.', ' discarded', ' football', ' football', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [594.2, 458.85, 237.3625, 770.5, 239.45]}, 'weight': {'score': [21.8564453125, 23.431575350975276, 21.986979166666668, 23.434402487345448, 29.579375], 'topk_indices': array([18795, 18751, 14610, 14574, 19454, 14619, 19396, 14655, 14538,
       14473, 20268, 20316, 18079, 18110, 23627, 23493, 21939, 21912,
       23709, 23575]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.01875, 22.3625, 19.665625, 27.341796875, 19.990625]}, 'saliency': {'score': [2.796905517578125, 0.10248300934122251, 2.267246791294643, 0.09791383750408421, 0.12104461669921875], 'topk_indices': array([24137, 24119,   184, 24117,  9148, 16889,   548, 24127, 24123,
       16890, 24033,  3039,  3038,  9797,  9792,  3042,   188, 24129,
       16892, 24131]), 'topk_tokens': ['Answer', ' Where', ' Sandra', 'Question', ' bathroom', ' Daniel', ' per', ' where', ' prior', ' dropped', ' location', ' travelled', ' Daniel', ' barric', ' football', ' bathroom', ' bathroom', ' football', ' football', ' discarded'], 'evidence_proportions': [3.74296875, 2.605078125, 1.351806640625, 5.4892578125, 1.33388671875]}}, 'pred_res': 'The line of march.<|eot_id|>', 'score': 0}
2025-01-22 05:39:47.761 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:39:47.766 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-45_pid-1_1-4-5-7-8.pkl | len: 3 |  size: 2.14 KB
Processing depth (1, 4, 5, 7, 8):   2%|▏         | 2/100 [00:59<48:53, 29.93s/it]is_0k: False
your chose emoji: ['🤝🏿', '🌇', '〽', '🏋🏿\u200d♀️', '☑', '☠', '👨🏽\u200d🦱', '🧙🏼\u200d♀️', '⚱️', '🔖']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.87s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (0, 1, 5, 6, 7):   2%|▏         | 2/100 [01:16<48:53, 29.93s/it]2025-01-22 05:40:04.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:40:04.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:40:04.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:40:04.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3043, 3048) --> . Daniel picked up the
2025-01-22 05:40:04.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 05:40:04.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11932, 11937) --> . Daniel went to the
2025-01-22 05:40:04.488 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:40:04.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14400, 14404) -->  Daniel dropped the football
2025-01-22 05:40:04.561 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the kitchen.
2025-01-22 05:40:04.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (9145, 9150) -->  Press. John travelled to
2025-01-22 05:40:04.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:40:04.608 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:40:04.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:40:04.680 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13196, 13202) --> . John went back to the
2025-01-22 05:40:04.680 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the bedroom.
2025-01-22 05:40:04.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6984, 6989) --> . Mary travelled to the
2025-01-22 05:40:04.714 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:40:04.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (189, 194) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:40:07.376 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 05:40:07.376 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 05:40:15.769 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [319.1484375, 12.404413436105141, 195.1235119047619, 11.94070768291168, 19.691966145833334], 'topk_indices': array([14404,  7948, 24180,    14, 24191,    23, 24082,  7999, 10258,
       10297,    24, 24178,  7949,  3048,  8000, 14403,  3049, 24188,
           0, 24189]), 'topk_tokens': ['.', 'nes', ' discarded', '\n', '<|end_header_id|>', '4', ' location', 'nes', ' of', ' corner', '\n\n', ' football', 'ot', ' football', 'ot', ' football', '.', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [371.15, 437.8, 135.6375, 563.0, 136.925]}, 'weight': {'score': [22.777994791666668, 23.443120763762604, 20.822916666666668, 23.446060075152168, 28.112083333333334], 'topk_indices': array([18788, 18832, 14678, 14642, 14687, 19485, 19427, 14723, 14541,
       14606, 20299, 20347, 18126, 18157, 23676, 23542, 21996, 21969,
       23758, 23624]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 22.3625, 19.665625, 27.341796875, 23.6140625]}, 'saliency': {'score': [1.9493916829427083, 0.06986033634659396, 1.1683465866815477, 0.06703739056483168, 0.11652374267578125], 'topk_indices': array([   23,  3049, 24166,  6989,  9151, 14401,    24,    34,  7948,
       10257,    31, 24082,  7949,  7999, 24180,  8000, 10297, 24178,
        3048, 14403]), 'topk_tokens': ['4', '.', 'Question', ' bedroom', ' bathroom', ' dropped', '\n\n', ' bathroom', 'nes', ' corner', ' travelled', ' location', 'ot', 'nes', ' discarded', 'ot', ' corner', ' football', ' football', ' football'], 'evidence_proportions': [2.3626953125, 2.3015625, 0.70830078125, 3.99853515625, 0.785693359375]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 05:40:15.775 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:40:15.776 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-45_pid-2_0-1-5-6-7.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 5, 6, 7):   3%|▎         | 3/100 [01:27<46:58, 29.05s/it]is_0k: False
your chose emoji: ['*⃣', '👨🏾\u200d❤\u200d💋\u200d👨🏾', '🧑🏼\u200d🎤', '🇸🇳', '🅰️', '🧑\u200d🔬', '\U0001faf5🏻', '🙇🏼\u200d♀️', '🤽🏿\u200d♀', '🧜']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.30s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.02s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.14s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]
Processing depth (0, 2, 6, 8, 9):   3%|▎         | 3/100 [01:44<46:58, 29.05s/it]2025-01-22 05:40:32.835 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:40:32.835 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:40:32.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:40:32.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4999, 5004) --> . Daniel picked up the
2025-01-22 05:40:32.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 05:40:32.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13198, 13203) -->  back to the garden.
2025-01-22 05:40:32.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:40:33.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19256, 19260) -->  dropped the football.
2025-01-22 05:40:33.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the kitchen.
2025-01-22 05:40:33.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (9171, 9176) --> . John travelled to the
2025-01-22 05:40:33.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:40:33.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:40:33.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:40:33.153 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13195, 13201) --> . John went back to the
2025-01-22 05:40:33.153 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the bedroom.
2025-01-22 05:40:33.192 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7006, 7011) -->  Mary travelled to the bedroom
2025-01-22 05:40:33.192 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:40:33.193 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (189, 194) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:40:35.893 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 05:40:35.893 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 05:40:44.276 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [267.0572916666667, 10.122081903147098, 217.95833333333334, 9.685762450281738, 19.335799032182837], 'topk_indices': array([ 5004, 13196,  5000, 24151,    29,    23, 24023,    14,    34,
       24152,    38, 19259, 24067, 24163, 19258,    24, 24176, 24174,
       24173,     0]), 'topk_tokens': [' football', ' John', ' Daniel', 'Question', '\n\n', '4', ' context', '\n', ' bathroom', ':', '***', '.', ' location', ' football', ' football', '\n\n', '<|end_header_id|>', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [376.4, 264.9, 84.275, 512.625, 146.2]}, 'weight': {'score': [20.833984375, 23.440418096852902, 22.114955357142858, 23.44416308885068, 29.60261194029851], 'topk_indices': array([18808, 18764, 14654, 14618, 14663, 14699, 19466, 19408, 14517,
       14582, 20328, 20280, 18098, 18129, 23527, 23661, 21993, 21966,
       23609, 23743]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 22.3625, 18.846875, 21.23046875, 19.990625]}, 'saliency': {'score': [1.5239206949869792, 0.057602880926011124, 1.2995837983630953, 0.055064215579489664, 0.14037755709975513], 'topk_indices': array([   37, 17537,  9176, 24157, 18907,    39, 19256, 13196,    31,
       24165,    24,  5004, 24023,  5000, 24151, 24067,    38,    34,
       24163, 19258]), 'topk_tokens': ['UL', 'ente', ' bathroom', ' prior', ' manner', '\n\n\n', ' dropped', ' John', ' travelled', ' discarded', '\n\n', ' football', ' context', ' Daniel', 'Question', ' location', '***', ' bathroom', ' football', ' football'], 'evidence_proportions': [2.228125, 1.62099609375, 0.4261474609375, 2.821044921875, 0.78271484375]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 05:40:44.282 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:40:44.283 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-45_pid-3_0-2-6-8-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (0, 2, 6, 8, 9):   4%|▍         | 4/100 [01:56<46:08, 28.84s/it]is_0k: False
your chose emoji: ['🛠', '👨\u200d🍼', '🆒', '⚙', '🧑🏽\u200d⚕', '👨🏽\u200d❤️\u200d👨🏻', '🙇🏻\u200d♂', '🏃🏽\u200d♀️', '\U0001faf5🏾', '🌙']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.45s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.92s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]
Processing depth (0, 3, 6, 7, 9):   4%|▍         | 4/100 [02:13<46:08, 28.84s/it]2025-01-22 05:41:01.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel travelled to the bathroom.
2025-01-22 05:41:01.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:41:01.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:41:01.914 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7528, 7533) -->  Daniel picked up the football
2025-01-22 05:41:01.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went to the garden.
2025-01-22 05:41:01.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13094, 13099) -->  back to the garden.
2025-01-22 05:41:01.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:41:02.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16777, 16781) -->  Daniel dropped the football
2025-01-22 05:41:02.073 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the kitchen.
2025-01-22 05:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (9063, 9068) --> . John travelled to the
2025-01-22 05:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the bathroom.
2025-01-22 05:41:02.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the garden.
2025-01-22 05:41:02.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13090, 13096) -->  winter. John went back to
2025-01-22 05:41:02.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary travelled to the bedroom.
2025-01-22 05:41:02.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6959, 6964) --> . Mary travelled to the
2025-01-22 05:41:02.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:41:02.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (189, 194) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:41:05.351 | INFO     | test_jbb_embedding:begin_test:693 - The Pioneer Guards were followed by the City Guards, under Capt.<|eot_id|>
2025-01-22 05:41:05.351 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 05:41:13.752 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [362.7395833333333, 17.879245326909558, 298.1927083333333, 17.29243596184123, 26.628935401119403], 'topk_indices': array([24170, 24176, 24080,  7533, 24066, 16781, 24163,    24, 24152,
          34, 24158, 16780,    14, 24178, 24057, 24177, 24067,     0,
       24173, 24174]), 'topk_tokens': ['?\n', '<|end_header_id|>', ' the', '.', ' first', '.', ' football', '\n\n', ':', ' bathroom', ' to', ' football', '\n', 'b', '.\n\n', '\n\n', ' location', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [580.2, 368.3, 175.9, 508.0, 210.35]}, 'weight': {'score': [22.85546875, 23.4384434059799, 21.856026785714285, 23.44039990833195, 29.246501865671643], 'topk_indices': array([18799, 18755, 14626, 14662, 14671, 19458, 14707, 19400, 14590,
       14525, 20334, 20286, 18124, 18093, 23527, 23661, 21993, 21966,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.81875, 27.1765625, 18.846875, 27.341796875, 19.990625]}, 'saliency': {'score': [2.3061421712239585, 0.10053022566141806, 1.7562372116815477, 0.09689646143371157, 0.193206103880014], 'topk_indices': array([   38, 24160, 24161,  7427, 24177, 24151, 24066,  9068,  7428,
       24057,  9330,  7532, 24178, 24157,    31, 24165, 24163, 16780,
          34, 24067]), 'topk_tokens': ['***', ' place', ' where', ' Min', '\n\n', 'Question', ' first', ' bathroom', 'nes', '.\n\n', ' cause', ' football', 'b', ' prior', ' travelled', ' discarded', ' football', ' football', ' bathroom', ' location'], 'evidence_proportions': [3.5744140625, 2.62626953125, 0.89326171875, 3.59423828125, 1.100146484375]}}, 'pred_res': 'The Pioneer Guards were followed by the City Guards, under Capt.<|eot_id|>', 'score': 0}
2025-01-22 05:41:13.758 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:41:13.758 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-45_pid-4_0-3-6-7-9.pkl | len: 3 |  size: 2.13 KB
Processing depth (0, 3, 6, 7, 9):   5%|▌         | 5/100 [02:25<46:01, 29.07s/it]Processing depth (0, 3, 6, 7, 9):   5%|▌         | 5/100 [02:25<46:11, 29.17s/it]
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:82 - Selected idx: 46
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the office? 
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the bathroom.', ' Mary travelled to the bedroom.', ' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' John went back to the garden.', ' Daniel dropped the apple.', ' Sandra went to the bathroom.']
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' Daniel dropped the apple.', ' Sandra went to the bathroom.']
2025-01-22 05:41:14.053 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['💂🏿\u200d♂️', '🇲🇲', '🧘🏿\u200d♂', '🈷', '🦹', '🧑\u200d🍳', '👩🏻\u200d❤\u200d👨🏻', '🐚', '🤦🏿\u200d♂', '🍱']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.58s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.72s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]
Processing depth (0, 1, 3, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:41:30.843 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:41:30.843 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:41:30.843 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:41:30.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Daniel moved to the
2025-01-22 05:41:30.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:41:30.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7637, 7641) -->  Daniel dropped the apple
2025-01-22 05:41:30.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:41:30.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21467, 21472) --> . Sandra went to the
2025-01-22 05:41:30.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:41:31.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17513, 17518) --> . John travelled to the
2025-01-22 05:41:31.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:41:31.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16793, 16798) --> . Mary travelled to the
2025-01-22 05:41:31.174 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:41:31.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7251, 7257) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:41:33.885 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:41:33.885 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24171])
2025-01-22 05:41:42.235 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [331.5061677631579, 14.747101741540497, 82.927734375, 14.452586128775012, 10.570404877533784], 'topk_indices': array([   29,  2091,    32, 24169,    68, 24167,  2031, 17696,    14,
          34,    31,    35,  1625, 17697,  1704, 24166,  1626,  1705,
           0,  1627]), 'topk_tokens': ['\n\n', ' the', ' to', '<|end_header_id|>', 'E', '<|start_header_id|>', 'ra', 'ar', '\n', ' kitchen', ' moved', '.', ' summer', 'ison', ' summer', '<|eot_id|>', "'s", "'s", '<|begin_of_text|>', ' work'], 'evidence_proportions': [801.8, 218.375, 277.71875, 17.3734375]}, 'weight': {'score': [21.62705592105263, 23.44515802101431, 20.259765625, 23.44870043032023, 29.825802364864863], 'topk_indices': array([18783, 18827, 14599, 14635, 19422, 19480, 14680, 14644, 14563,
       14498, 20342, 20294, 18152, 18121, 23523, 23657, 21977, 21950,
       23739, 23605]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.8021256296258223, 0.08349477579055752, 0.4529457092285156, 0.08189714535193632, 0.07884401888460726], 'topk_indices': array([24158, 17550,    68, 17560,  1634,    30, 17696,    67,  1641,
        2031, 17653,  2032,    31,    34,  1625, 17697,  1626,  1704,
        1705,  1627]), 'topk_tokens': [' apple', 'ente', 'E', 'ente', ' part', 'Daniel', 'ar', 'ION', 'ern', 'ra', 'ison', 'irie', ' moved', ' kitchen', ' summer', 'ison', "'s", ' summer', "'s", ' work'], 'evidence_proportions': [4.1470703125, 1.067578125, 1.9139404296875, 0.102276611328125]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:41:42.242 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:41:42.242 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-46_pid-0_0-1-3-9.pkl | len: 3 |  size: 1.98 KB
Processing depth (0, 1, 3, 9):   1%|          | 1/100 [00:28<46:15, 28.04s/it]is_0k: False
your chose emoji: ['🈚', '🕵🏻', '🤷🏽\u200d♂️', '🧑🏿\u200d❤️\u200d🧑🏾', '🚴🏼\u200d♀', '✌🏽', '🐈', '💂🏾\u200d♀️', '🏃🏽\u200d♀️', '🇳🇫']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.56s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.40s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.11s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]
Processing depth (0, 2, 4, 8):   1%|          | 1/100 [00:44<46:15, 28.04s/it]2025-01-22 05:41:58.939 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:41:58.939 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:41:58.939 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:41:58.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4915, 4920) --> . Daniel moved to the
2025-01-22 05:41:58.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:41:59.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9695, 9699) -->  Daniel dropped the apple
2025-01-22 05:41:59.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:41:59.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19345, 19350) --> . Sandra went to the
2025-01-22 05:41:59.125 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:41:59.223 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17525, 17530) --> . John travelled to the
2025-01-22 05:41:59.223 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:41:59.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16799, 16804) --> . Mary travelled to the
2025-01-22 05:41:59.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:41:59.356 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7166, 7172) -->  cold. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:42:02.128 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the apple.<|eot_id|>
2025-01-22 05:42:02.128 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 05:42:10.518 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [228.0234375, 9.991005375175895, 89.259765625, 9.766737278256725, 13.026539522058824], 'topk_indices': array([  547, 24142,    28, 24147,   546, 24047,  9698,   185, 24013,
          24,   539,   541,   548,    14,   424,   425, 24157,     0,
       24155, 24154]), 'topk_tokens': ['s', ':', '<|end_header_id|>', ' before', ' em', '.\n\n', ' apple', 'ION', ' context', '\n\n', ',', ' to', ' per', '\n', ' to', ' ', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [312.9, 264.05, 341.40625, 16.4140625]}, 'weight': {'score': [21.62705592105263, 23.441074828242694, 21.51611328125, 23.443779919799393, 29.66360294117647], 'topk_indices': array([18801, 18757, 14645, 14609, 14654, 19474, 14690, 19416, 14508,
       14573, 20336, 20288, 18095, 18126, 23633, 23499, 21938, 21965,
       23715, 23581]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.317543431332237, 0.05549489089110742, 0.4986686706542969, 0.054207134321927974, 0.09664569181554458], 'topk_indices': array([ 9696,   541,  7173, 24047,    34,    31,   424,    24, 24057,
       24152,  4920, 24141, 24147,  9325,   185,   546, 24146, 24013,
        9698,   548]), 'topk_tokens': [' dropped', ' to', ' garden', '.\n\n', ' kitchen', ' moved', ' to', '\n\n', ' location', 'Answer', ' office', 'Question', ' before', ' cause', 'ION', ' em', ' apple', ' context', ' apple', ' per'], 'evidence_proportions': [1.62890625, 1.3603515625, 2.39306640625, 0.1029541015625]}}, 'pred_res': 'Daniel dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:42:10.525 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:42:10.525 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-46_pid-1_0-2-4-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 2, 4, 8):   2%|▏         | 2/100 [00:56<46:01, 28.18s/it]is_0k: False
your chose emoji: ['🏊🏾\u200d♀️', '👊', '👱🏽\u200d♂', '🦹🏼\u200d♀️', '📽', '🚶🏼\u200d♂️\u200d➡', '🤸\u200d♂️', '🤸🏻', '\U0001fac4🏼', '🪶']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.20s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (0, 2, 5, 7):   2%|▏         | 2/100 [01:12<46:01, 28.18s/it]2025-01-22 05:42:27.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:42:27.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:42:27.339 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:42:27.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4951, 4956) --> . Daniel moved to the
2025-01-22 05:42:27.364 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:42:27.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11857, 11861) -->  Daniel dropped the apple
2025-01-22 05:42:27.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:42:27.507 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16691, 16696) --> . Sandra went to the
2025-01-22 05:42:27.507 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:42:27.594 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17463, 17468) --> . John travelled to the
2025-01-22 05:42:27.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:42:27.679 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16775, 16780) --> . Mary travelled to the
2025-01-22 05:42:27.679 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:42:27.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7131, 7137) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:42:30.412 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:42:30.413 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 05:42:38.797 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [358.79605263157896, 11.424917756351899, 83.4931640625, 11.103623839666819, 11.654129464285715], 'topk_indices': array([24154,    34,     4,    35,     9,  1626,    32, 24051, 24146,
       24061, 24153,    31, 24161, 24151,  1705,    14,  1627,     0,
       24158, 24159]), 'topk_tokens': ['?', ' kitchen', '\n\n', '.', ':', "'s", ' to', '.\n\n', ':', ' location', ' office', ' moved', '<|end_header_id|>', ' before', "'s", '\n', ' work', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [621.6, 440.15, 293.34375, 67.0]}, 'weight': {'score': [21.62705592105263, 23.44114147976496, 20.259765625, 23.44467923780614, 29.497544642857143], 'topk_indices': array([18781, 18825, 14647, 14611, 19478, 14656, 19420, 14692, 14510,
       14575, 20292, 20340, 18119, 18150, 23643, 23509, 21948, 21975,
       23591, 23725]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.9454281455592106, 0.06342879717371513, 0.425567626953125, 0.0617068540757998, 0.08825247628348214], 'topk_indices': array([    0,  1625, 24051, 24147,  1626, 24017, 24156,  4953,    30,
       11860,  1704,  1705, 24150, 24061, 24145,    34, 24153,    31,
       24151,  1627]), 'topk_tokens': ['<|begin_of_text|>', ' summer', '.\n\n', ' Where', "'s", ' context', 'Answer', ' moved', 'Daniel', ' apple', ' summer', "'s", ' apple', ' location', 'Question', ' kitchen', ' office', ' moved', ' before', ' work'], 'evidence_proportions': [3.265234375, 2.12021484375, 2.0472412109375, 0.369384765625]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:42:38.804 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:42:38.804 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-46_pid-2_0-2-5-7.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 2, 5, 7):   3%|▎         | 3/100 [01:24<45:37, 28.23s/it]is_0k: False
your chose emoji: ['🤱🏻', '👨🏿\u200d🦯\u200d➡', '👱🏽\u200d♂️', '🇰🇾', '🏵️', '🧎\u200d♂', '⚫', '🎨', '🕵🏾\u200d♂️', '🙅🏾\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.22s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.47s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.08s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]
Processing depth (1, 2, 3, 4):   3%|▎         | 3/100 [01:41<45:37, 28.23s/it]2025-01-22 05:42:55.433 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:42:55.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2917, 2922) --> . Daniel moved to the
2025-01-22 05:42:55.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:42:55.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2917, 2922) --> . Daniel moved to the
2025-01-22 05:42:55.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:42:55.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7601, 7605) -->  Daniel dropped the apple
2025-01-22 05:42:55.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:42:55.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9791, 9796) --> . Sandra went to the
2025-01-22 05:42:55.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:42:55.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17517, 17522) --> . John travelled to the
2025-01-22 05:42:55.656 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:42:55.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16801, 16806) --> . Mary travelled to the
2025-01-22 05:42:55.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:42:55.776 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7162, 7168) -->  cold. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:42:58.566 | INFO     | test_jbb_embedding:begin_test:693 - The doctor's office.<|eot_id|>
2025-01-22 05:42:58.566 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24175])
2025-01-22 05:43:06.955 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [196.46463815789474, 7.058985699809744, 77.361328125, 6.863337069751067, 6.303312602796052], 'topk_indices': array([24166,  7604, 24063,    23, 24165,  7616,     1,  4913,  7621,
       24163, 24158,    14,    24, 24168,  7169, 24169, 24171,     0,
       24173, 24170]), 'topk_tokens': ['?', ' apple', '.\n\n', '4', ' office', ' im', '<|start_header_id|>', ' moved', 'inen', ' before', ':', '\n', '\n\n', 'Answer', ' garden', ':', '<|start_header_id|>', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [221.55, 221.55, 322.9375, 45.115625]}, 'weight': {'score': [21.653371710526315, 23.441491231698237, 21.51611328125, 23.44417442426376, 29.085731907894736], 'topk_indices': array([18787, 18831, 14651, 14687, 19496, 19438, 14696, 14732, 14615,
       14550, 20310, 20358, 18156, 18125, 23661, 23527, 21993, 21966,
       23743, 23609]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.1579878957648027, 0.03972552335235674, 0.4300670623779297, 0.03858678957029794, 0.0464249661094264], 'topk_indices': array([24166, 24063,  7602,  2918, 24073,    24, 24172,  4912, 24162,
        7601,  7620, 24165, 24163,  7604,  7616,  4913,  7621, 24157,
        7169, 24168]), 'topk_tokens': ['?', '.\n\n', ' dropped', ' Daniel', ' location', '\n\n', 'assistant', ' Daniel', ' apple', ' Daniel', ' genu', ' office', ' before', ' apple', ' im', ' moved', 'inen', 'Question', ' garden', 'Answer'], 'evidence_proportions': [1.17578125, 1.17578125, 2.21435546875, 0.27730712890625]}}, 'pred_res': "The doctor's office.<|eot_id|>", 'score': 0}
2025-01-22 05:43:06.960 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:43:06.961 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-46_pid-3_1-2-3-4.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 3, 4):   4%|▍         | 4/100 [01:52<45:07, 28.20s/it]is_0k: False
your chose emoji: ['🤦🏻\u200d♂', '👩🏾\u200d🍳', '🇲🇽', '🕰️', '🇧🇷', '👨🏾', '🏌️\u200d♀', '👩🏼\u200d🦱', '👩🏼\u200d🦯\u200d➡', '🤙🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (4, 6, 7, 8):   4%|▍         | 4/100 [02:10<45:07, 28.20s/it]2025-01-22 05:43:24.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:43:24.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9693, 9698) -->  war. Daniel moved to
2025-01-22 05:43:24.674 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:43:24.721 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9693, 9698) -->  war. Daniel moved to
2025-01-22 05:43:24.722 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:43:24.804 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16727, 16731) -->  Daniel dropped the apple
2025-01-22 05:43:24.804 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:43:24.899 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19193, 19198) --> . Sandra went to the
2025-01-22 05:43:24.899 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:43:24.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (17491, 17496) --> . John travelled to the
2025-01-22 05:43:24.987 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:43:25.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (16809, 16814) --> . Mary travelled to the
2025-01-22 05:43:25.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:43:25.111 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7034, 7040) -->  John went back to the garden
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:43:27.878 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the apple.<|eot_id|>
2025-01-22 05:43:27.878 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 05:43:36.274 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [189.05016447368422, 6.328474919428146, 53.0966796875, 6.153856498531055, 6.0304343483664775], 'topk_indices': array([24198, 24186,     1, 24087, 24190, 18978, 16728,  9696,     9,
           3, 18979, 24187, 24189,    24,    14,    23, 16730, 24194,
       24197,     0]), 'topk_tokens': ['\n\n', ' apple', '<|start_header_id|>', '.\n\n', '?', ' manner', ' dropped', ' moved', ':', '<|end_header_id|>', ' in', ' before', ' office', '\n\n', '\n', '4', ' apple', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [209.3875, 209.3875, 354.1875, 16.265625]}, 'weight': {'score': [23.4765625, 23.45237480373523, 21.67041015625, 23.453535554682006, 29.812855113636363], 'topk_indices': array([18879, 18835, 14690, 14654, 19538, 14699, 19480, 14735, 14553,
       14618, 20400, 20352, 18178, 18147, 23557, 23691, 21996, 22023,
       23773, 23639]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.3734375, 23.3734375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.2227437872635691, 0.03600560758795761, 0.3157386779785156, 0.03488739868573203, 0.04430085962468928], 'topk_indices': array([24183, 16727, 24190,    19,     8, 24181,    22,    20, 24053,
          24, 14311,  9695,    23, 24186, 16728,  9696, 24187, 18978,
       24189, 16730]), 'topk_tokens': [' Where', ' Daniel', '?', '26', ' Date', 'Question', '202', ' Jul', ' context', '\n\n', ' Daniel', ' Daniel', '4', ' apple', ' dropped', ' moved', ' before', ' manner', ' office', ' apple'], 'evidence_proportions': [1.297314453125, 1.297314453125, 2.4564208984375, 0.0866607666015625]}}, 'pred_res': 'Daniel dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:43:36.282 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:43:36.282 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-46_pid-4_4-6-7-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (4, 6, 7, 8):   5%|▌         | 5/100 [02:22<45:17, 28.60s/it]Processing depth (4, 6, 7, 8):   5%|▌         | 5/100 [02:22<45:05, 28.48s/it]
2025-01-22 05:43:36.598 | INFO     | __main__:<module>:82 - Selected idx: 47
2025-01-22 05:43:36.598 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:43:36.607 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:43:36.608 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:43:36.608 | INFO     | __main__:<module>:86 - Needle: [' Daniel moved to the kitchen.', ' John travelled to the bathroom.', ' Daniel grabbed the milk.', ' Mary travelled to the bedroom.', ' John went back to the garden.', ' Daniel moved to the office.', ' Daniel dropped the milk.', ' Sandra went to the bathroom.']
2025-01-22 05:43:36.608 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel grabbed the milk.', ' Daniel moved to the office.', ' Daniel dropped the milk.', ' Sandra went to the bathroom.']
2025-01-22 05:43:36.608 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['💇🏽\u200d♂', '\U0001fac4🏾', '🏃🏾\u200d♂️\u200d➡', '🇬🇦', '🤽🏽\u200d♀️', '🚶🏽\u200d♂️\u200d➡️', '🇨🇿', '🧑\u200d🎄', '👩🏼\u200d❤️\u200d👩🏻', '🤚']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.87s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.07s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.61s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.15s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (0, 3, 5, 7, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:43:53.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:43:53.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:43:53.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 05:43:53.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7587, 7591) -->  Daniel grabbed the milk
2025-01-22 05:43:53.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:43:53.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11917, 11922) --> . Daniel moved to the
2025-01-22 05:43:53.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 05:43:53.887 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16802, 16806) -->  Daniel dropped the milk
2025-01-22 05:43:53.887 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:43:54.000 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21452, 21457) --> . Sandra went to the
2025-01-22 05:43:54.001 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:43:54.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8109, 8114) --> . John travelled to the
2025-01-22 05:43:54.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:43:54.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22374, 22379) --> . Mary travelled to the
2025-01-22 05:43:54.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:43:54.206 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6193, 6199) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:43:56.913 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 05:43:56.913 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 05:44:05.291 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [449.3016304347826, 18.91476414411059, 161.3359375, 18.40994978858351, 16.822873975409838], 'topk_indices': array([ 1705,    67,  7590,    32,    29,    68,    24, 24133,    14,
          35,    34,  1627, 18787,    31,  7591,   185, 24154, 24155,
       18744,     0]), 'topk_tokens': ["'s", 'ION', ' milk', ' to', '\n\n', 'E', '\n\n', ':', '\n', '.', ' kitchen', ' work', ' ga', ' moved', '.', 'ION', '<|eot_id|>', '<|start_header_id|>', 'untlet', '<|begin_of_text|>'], 'evidence_proportions': [861.1, 633.8125, 309.425, 416.28125, 56.1875]}, 'weight': {'score': [22.769021739130434, 23.433350922936842, 20.259765625, 23.43608926128591, 28.871157786885245], 'topk_indices': array([18744, 18788, 14589, 14625, 14634, 19441, 19383, 14670, 14553,
       14488, 20315, 20267, 18113, 18082, 23642, 23508, 21941, 21968,
       23590, 23724]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 28.263671875, 19.909375, 27.013671875, 20.796875]}, 'saliency': {'score': [2.6903341542119565, 0.10829140241844115, 0.8330039978027344, 0.10534888346070788, 0.12229244044569672], 'topk_indices': array([   68, 24146, 16805, 24144,    63,    38, 18743,    67, 24138,
        7588,  1627,  8114,    30,  7590, 24132,    34,   185,    31,
       18787, 18744]), 'topk_tokens': ['E', ' discarded', ' milk', ' milk', 'ISC', '***', ' ga', 'ION', ' prior', ' grabbed', ' work', ' bathroom', 'Daniel', ' milk', 'Question', ' kitchen', 'ION', ' moved', ' ga', 'untlet'], 'evidence_proportions': [4.598046875, 4.568359375, 1.54306640625, 2.814697265625, 0.327978515625]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 05:44:05.319 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:44:05.320 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-47_pid-0_0-3-5-7-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 3, 5, 7, 9):   1%|          | 1/100 [00:28<47:07, 28.57s/it]is_0k: False
your chose emoji: ['🇱🇧', '🧑🏻\u200d❤\u200d🧑🏽', '👵', '🙍🏼', '👨🏾\u200d🦽\u200d➡', '☝️', '🇹🇰', '🏸', '📟', '👻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.67s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.32s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.43s/it]
Processing depth (0, 1, 6, 7, 9):   1%|          | 1/100 [00:44<47:07, 28.57s/it]2025-01-22 05:44:21.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:44:21.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:44:21.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 05:44:21.415 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2960, 2964) -->  Daniel grabbed the milk
2025-01-22 05:44:21.415 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:44:21.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14341, 14346) --> . Daniel moved to the
2025-01-22 05:44:21.506 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 05:44:21.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16764, 16768) -->  Daniel dropped the milk
2025-01-22 05:44:21.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:44:21.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21444, 21449) --> . Sandra went to the
2025-01-22 05:44:21.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:44:21.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7984, 7989) -->  sell. John travelled to
2025-01-22 05:44:21.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:44:21.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22426, 22431) --> . Mary travelled to the
2025-01-22 05:44:21.875 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:44:21.912 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6134, 6140) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:44:24.601 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:44:24.601 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24147])
2025-01-22 05:44:32.958 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [527.3641304347826, 17.569642857142856, 132.478515625, 17.00708571813695, 13.972212357954545], 'topk_indices': array([24127,   541,   424, 24119,  2961,  1626, 24132, 16767,   425,
       16768, 24120,  2963,   185, 24121,  2964,  1705, 24142,  1627,
           0, 24143]), 'topk_tokens': [' to', ' to', ' to', '.\n\n', ' grabbed', "'s", ' milk', ' milk', ' ', '.', 'Question', ' milk', 'ION', ':', '.', "'s", '<|eot_id|>', ' work', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [762.9, 888.875, 297.0, 736.1875, 65.925]}, 'weight': {'score': [22.769021739130434, 23.429919772256728, 21.50048828125, 23.431830583032642, 28.711363636363636], 'topk_indices': array([18778, 18734, 14621, 14585, 14630, 19431, 14666, 19373, 14549,
       14484, 20293, 20245, 18052, 18083, 23616, 23482, 21930, 21903,
       23698, 23564]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 28.263671875, 19.909375, 27.013671875, 20.796875]}, 'saliency': {'score': [3.252016813858696, 0.09841637147386129, 0.7200698852539062, 0.09499555664265014, 0.10077375932173295], 'topk_indices': array([24134,  1625,  1626, 24129,    67, 24125, 16765,   548,    31,
        1704,  1705, 24126,    34,   185,  2961, 24132, 16767,  2963,
        1627, 24120]), 'topk_tokens': [' discarded', ' summer', "'s", ' place', 'ION', ' location', ' dropped', ' per', ' moved', ' summer', "'s", ' prior', ' kitchen', 'ION', ' grabbed', ' milk', ' milk', ' milk', ' work', 'Question'], 'evidence_proportions': [4.0796875, 6.4326171875, 1.49296875, 4.833740234375, 0.37353515625]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:44:32.966 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:44:32.966 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-47_pid-1_0-1-6-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 6, 7, 9):   2%|▏         | 2/100 [00:56<45:46, 28.03s/it]is_0k: False
your chose emoji: ['⬆️', '📇', '♍', '🇨🇫', '🛐', '🚶🏾\u200d♀', '🦸🏿', '👩🏼\u200d🦳', '👤', '👨🏾\u200d🔧']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.27s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.38s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.02s/it]
Processing depth (3, 5, 6, 8, 9):   2%|▏         | 2/100 [01:14<45:46, 28.03s/it]2025-01-22 05:44:51.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:44:51.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7593, 7598) -->  Daniel moved to the kitchen
2025-01-22 05:44:51.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 05:44:51.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11899, 11903) -->  Daniel grabbed the milk
2025-01-22 05:44:51.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:44:51.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7594, 7599) -->  moved to the kitchen.
2025-01-22 05:44:51.543 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 05:44:51.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19191, 19195) -->  dropped the milk.
2025-01-22 05:44:51.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:44:51.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21412, 21417) --> . Sandra went to the
2025-01-22 05:44:51.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:44:51.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8116, 8121) --> . John travelled to the
2025-01-22 05:44:51.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:44:51.927 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22370, 22375) --> . Mary travelled to the
2025-01-22 05:44:51.927 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:44:51.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6273, 6279) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:44:54.665 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 05:44:54.665 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24167])
2025-01-22 05:45:03.019 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [325.33627717391306, 11.300478382292098, 157.880859375, 10.90397142161535, 27.85447716346154], 'topk_indices': array([ 7597,    24, 24152, 22985, 14409, 14410, 14411, 11903,  8121,
        7596,    14, 24046,  7594,    23, 24056, 11882, 24165,     0,
       24162, 24163]), 'topk_tokens': [' kitchen', '\n\n', ' milk', 'ation', ' moved', ' to', ' the', '.', ' bathroom', ' the', '\n', '.\n\n', ' moved', '4', ' location', ' MILL', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [478.35, 363.9375, 465.9, 262.78125, 50.921875]}, 'weight': {'score': [22.747282608695652, 23.438870500620602, 20.259765625, 23.44163757200282, 29.575961538461538], 'topk_indices': array([18759, 18803, 14611, 14575, 14620, 14656, 19403, 19461, 14539,
       14474, 20275, 20323, 18128, 18097, 23504, 23638, 21937, 21964,
       23586, 23720]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.6984375, 28.263671875, 19.809375, 20.90234375, 20.796875]}, 'saliency': {'score': [1.9290134595788044, 0.06382090482131775, 0.8321990966796875, 0.061533661034108095, 0.20841451791616586], 'topk_indices': array([24150, 24046, 22984, 11881, 24154,  6279,  7569,    23, 24140,
       11900,  7593, 24146, 14409, 11902,  7597, 24152,  7594,  8121,
       24056, 11882]), 'topk_tokens': [' where', '.\n\n', ' pot', 'IVE', ' discarded', ' garden', '�', '4', 'Question', ' grabbed', ' Daniel', ' prior', ' moved', ' milk', ' kitchen', ' milk', ' moved', ' bathroom', ' location', ' MILL'], 'evidence_proportions': [2.9052734375, 2.6820068359375, 2.39951171875, 1.4200439453125, 0.2870361328125]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 05:45:03.024 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:45:03.024 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-47_pid-2_3-5-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (3, 5, 6, 8, 9):   3%|▎         | 3/100 [01:26<46:48, 28.95s/it]is_0k: False
your chose emoji: ['👽', '🏗', '👩🏻\u200d❤\u200d💋\u200d👨🏼', '👮🏾', '😵', '🐇', '🇧🇫', '🏃🏼\u200d♂\u200d➡', '⏹', '💆🏿\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (0, 2, 3, 5, 7):   3%|▎         | 3/100 [01:43<46:48, 28.95s/it]2025-01-22 05:45:20.652 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:45:20.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:45:20.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 05:45:20.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4952, 4956) -->  Daniel grabbed the milk
2025-01-22 05:45:20.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:45:20.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7451, 7456) --> . Daniel moved to the
2025-01-22 05:45:20.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 05:45:20.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11848, 11852) -->  Daniel dropped the milk
2025-01-22 05:45:20.781 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:45:20.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16776, 16781) --> . Sandra went to the
2025-01-22 05:45:20.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:45:20.917 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8000, 8005) -->  sell. John travelled to
2025-01-22 05:45:20.918 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:45:21.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22368, 22373) --> . Mary travelled to the
2025-01-22 05:45:21.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:45:21.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6192, 6198) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:45:23.765 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:45:23.765 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 05:45:32.150 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [458.3342391304348, 17.60910992841774, 125.39453125, 17.117533621368477, 14.37353515625], 'topk_indices': array([ 1705,    34,  4956,  5938,    32,     4,    24,  1627,    30,
        9332, 17539,    29,    35,  9379,    14, 17529,    31, 24160,
           0, 24161]), 'topk_tokens': ["'s", ' kitchen', '.', ' in', ' to', '\n\n', '\n\n', ' work', 'Daniel', ' cause', 'ente', '\n\n', '.', 'did', '\n', 'ente', ' moved', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [876.9, 471.4375, 451.6, 451.625, 41.3875]}, 'weight': {'score': [22.769021739130434, 23.44025674445548, 21.50048828125, 23.442182837560612, 29.920654296875], 'topk_indices': array([18762, 18806, 14590, 14626, 19401, 19459, 14671, 14635, 14489,
       14554, 20327, 20279, 18131, 18100, 23502, 23636, 21935, 21962,
       23718, 23584]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 28.263671875, 19.909375, 27.013671875, 20.796875]}, 'saliency': {'score': [2.6942483653192935, 0.09911046664235766, 0.6873016357421875, 0.09624672465656602, 0.11143064498901367], 'topk_indices': array([   27, 24144, 17538,  7456, 11851, 24152, 24010,  4955,    39,
       24138, 17528,  1627,    38,    34, 17539,  9332, 17529,    30,
        9379,    31]), 'topk_tokens': ['user', ' prior', 'arp', ' office', ' milk', ' discarded', ' context', ' milk', '\n\n\n', 'Question', 'arp', ' work', '***', ' kitchen', 'ente', ' cause', 'ente', 'Daniel', 'did', ' moved'], 'evidence_proportions': [4.72578125, 3.48046875, 2.195703125, 3.04443359375, 0.25213623046875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:45:32.158 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:45:32.158 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-47_pid-3_0-2-3-5-7.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 2, 3, 5, 7):   4%|▍         | 4/100 [01:55<46:26, 29.02s/it]is_0k: False
your chose emoji: ['♻️', '🕳', '👩🏼\u200d❤\u200d💋\u200d👨🏽', '👎🏿', '🛎', '🙅🏾', '☯️', '🤸🏽\u200d♂️', '✳️', '🧖🏼\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.01s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.80s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (1, 2, 3, 7, 9):   4%|▍         | 4/100 [02:12<46:26, 29.02s/it]2025-01-22 05:45:49.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:45:49.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2890, 2895) -->  Daniel moved to the kitchen
2025-01-22 05:45:49.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel grabbed the milk.
2025-01-22 05:45:49.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4882, 4886) -->  Daniel grabbed the milk
2025-01-22 05:45:49.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:45:49.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2891, 2896) -->  moved to the kitchen.
2025-01-22 05:45:49.906 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 05:45:49.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16708, 16712) -->  Daniel dropped the milk
2025-01-22 05:45:49.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:45:50.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21440, 21445) --> . Sandra went to the
2025-01-22 05:45:50.091 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:45:50.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (8016, 8021) -->  sell. John travelled to
2025-01-22 05:45:50.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:45:50.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22398, 22403) --> . Mary travelled to the
2025-01-22 05:45:50.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:45:50.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6166, 6172) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:45:52.941 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:45:52.942 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 05:46:01.348 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [832.1875, 24.58608724388632, 125.072265625, 23.751025332036907, 70.49895368303571], 'topk_indices': array([ 4886,  2886,  2894, 24190, 24184, 24185,  2891,  2889, 24178,
        4882,  2887, 24177, 24199, 24179, 24084, 24197,    14,     0,
       24200, 24201]), 'topk_tokens': ['.', '�', ' kitchen', ' milk', ' prior', ' to', ' moved', '�', 'Question', ' Daniel', '�', '.\n\n', ':', ':', '.\n\n', '?\n', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [1204.5, 1053.0, 1085.2, 753.375, 93.2625]}, 'weight': {'score': [23.810122282608695, 23.45289780237938, 21.50048828125, 23.453850360999628, 30.20889136904762], 'topk_indices': array([18800, 18844, 14671, 14707, 19497, 14752, 14716, 19439, 14635,
       14570, 20311, 20359, 18138, 18169, 23666, 23532, 21992, 21965,
       23614, 23748]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.6984375, 28.263671875, 19.809375, 27.013671875, 20.796875]}, 'saliency': {'score': [5.418929390285326, 0.13207390603054775, 0.6823272705078125, 0.12667849331303788, 0.5272794451032367], 'topk_indices': array([24050,  4883, 24177,  4885,  2889, 16711,  2887, 24084, 24192,
       24198,  2888, 24197,  2891,  2890, 24190,  2894, 24184,  2886,
       24178,  4882]), 'topk_tokens': [' context', ' grabbed', '.\n\n', ' milk', '�', ' milk', '�', '.\n\n', ' discarded', 'Answer', '�', '?\n', ' moved', ' Daniel', ' milk', ' kitchen', ' prior', '�', 'Question', ' Daniel'], 'evidence_proportions': [7.841796875, 7.96533203125, 5.9708984375, 5.250732421875, 0.5415283203125]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 100}
2025-01-22 05:46:01.384 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:46:01.384 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-47_pid-4_1-2-3-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (1, 2, 3, 7, 9):   5%|▌         | 5/100 [02:24<46:04, 29.10s/it]Processing depth (1, 2, 3, 7, 9):   5%|▌         | 5/100 [02:24<45:53, 28.99s/it]
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:82 - Selected idx: 48
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the office? 
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:86 - Needle: [' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' John travelled to the bathroom.', ' Mary travelled to the bedroom.', ' John went back to the garden.', ' Daniel dropped the apple.', ' Sandra went to the bathroom.']
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel moved to the office.', ' Daniel dropped the apple.', ' Sandra went to the bathroom.']
2025-01-22 05:46:01.686 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👩🏾\u200d❤️\u200d💋\u200d👨🏼', '☯', '🦏', '👩🏿\u200d❤️\u200d💋\u200d👨🏿', '💂🏿', '\U0001faf7🏾', '🚶🏿\u200d♀️\u200d➡', '❤️', '➖', '🧑🏾\u200d🦽\u200d➡️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.55s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.73s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.55s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.13s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Processing depth (1, 2, 5, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:46:18.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:46:18.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:46:18.573 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:46:18.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:46:18.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:46:18.647 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11985, 11989) -->  Daniel dropped the apple
2025-01-22 05:46:18.647 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:46:18.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21461, 21466) --> . Sandra went to the
2025-01-22 05:46:18.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:46:18.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20879, 20884) --> . John travelled to the
2025-01-22 05:46:18.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:46:18.923 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13132, 13137) -->  Mary travelled to the bedroom
2025-01-22 05:46:18.923 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:46:18.943 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3986, 3992) -->  John went back to the garden
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:46:21.657 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's hand<|eot_id|>
2025-01-22 05:46:21.657 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24151])
2025-01-22 05:46:30.030 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [289.9736842105263, 10.017841247826446, 136.24609375, 9.713566068244953, 15.646957397460938], 'topk_indices': array([24145, 12050, 24065, 24005, 11989, 24134, 24139,     9,  3033,
       11988, 24149, 24078, 24039, 12149,    14, 24079, 24049,     0,
       24146, 24147]), 'topk_tokens': [':', 'ight', '.\n', ' context', '.', ':', ' before', ':', ' moved', ' apple', '<|end_header_id|>', ' to', '.\n\n', ' the', '\n', ' the', ' location', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [351.65, 351.65, 464.875, 26.7]}, 'weight': {'score': [21.653371710526315, 23.438576426264802, 23.3662109375, 23.44003074598864, 29.593994140625], 'topk_indices': array([18801, 18757, 14613, 14577, 19416, 19474, 14658, 14622, 14476,
       14541, 20336, 20288, 18095, 18126, 23511, 23645, 21944, 21971,
       23593, 23727]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.8076541298314144, 0.056535161586848966, 0.8673553466796875, 0.0546178191034072, 0.11796331405639648], 'topk_indices': array([24141, 24043, 12036, 12101, 12037, 24048, 24077, 24039, 24133,
       24144,  3032, 24139, 12050, 11986, 24005, 11985, 24138,  3033,
       11988, 24049]), 'topk_tokens': [' office', ' obtained', 'ann', 'ann', 'ouncements', ' first', ' moved', '.\n\n', 'Question', 'Answer', ' Daniel', ' before', 'ight', ' dropped', ' context', ' Daniel', ' apple', ' moved', ' apple', ' location'], 'evidence_proportions': [2.03916015625, 2.03916015625, 3.2918701171875, 0.157269287109375]}}, 'pred_res': "Daniel's hand<|eot_id|>", 'score': 0}
2025-01-22 05:46:30.037 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:46:30.037 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-48_pid-0_1-2-5-9.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 2, 5, 9):   1%|          | 1/100 [00:28<46:33, 28.22s/it]is_0k: False
your chose emoji: ['🧙🏾\u200d♂️', '👊🏿', '👨🏼\u200d❤️\u200d💋\u200d👨🏻', '🥂', '▫', '🤾🏿\u200d♂️', '⛩', '🇸🇸', '🏡', '😁']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.29s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.47s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.56s/it]
Processing depth (0, 1, 2, 6):   1%|          | 1/100 [00:44<46:33, 28.22s/it]2025-01-22 05:46:46.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:46:46.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:46:46.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:46:46.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2966, 2971) -->  tragedy. Daniel moved to
2025-01-22 05:46:46.572 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:46:46.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4971, 4975) -->  Daniel dropped the apple
2025-01-22 05:46:46.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:46:46.676 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14363, 14368) --> . Sandra went to the
2025-01-22 05:46:46.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:46:46.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20895, 20900) --> . John travelled to the
2025-01-22 05:46:46.786 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:46:46.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13066, 13071) -->  winter. Mary travelled to
2025-01-22 05:46:46.860 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:46:46.880 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3983, 3989) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:46:49.571 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:46:49.571 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24181])
2025-01-22 05:46:57.969 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [364.7672697368421, 20.405739590224943, 141.9599609375, 20.05426596339393, 14.792727699762658], 'topk_indices': array([  434,  5630,   537, 24179,   438,    14,    34,   549, 24176,
         435,   437,   550,   613,  7987,   553,   556,   554,   520,
         551,     0]), 'topk_tokens': [',', 'ot', ' an', '<|end_header_id|>', '10', '\n', ' kitchen', ' ', '<|eot_id|>', '000', ' ', '6', ' Sunday', 'ot', ' to', ',', ' ', ',', ',', '<|begin_of_text|>'], 'evidence_proportions': [897.1, 239.46875, 268.21875, 34.971875]}, 'weight': {'score': [23.15542763157895, 23.445183282335428, 21.61572265625, 23.4466233720858, 29.42583069620253], 'topk_indices': array([18787, 18831, 14607, 14643, 14688, 19484, 19426, 14652, 14571,
       14506, 20352, 20304, 18156, 18125, 23533, 23667, 21972, 21999,
       23615, 23749]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 25.7171875, 27.083984375, 20.796875]}, 'saliency': {'score': [2.1483218544407894, 0.11404865150060474, 0.7824211120605469, 0.11200528944731795, 0.10730492314205894], 'topk_indices': array([  62, 5630,   52,  554,  551,   57,  552, 7986,  438,  404,  548,
        550,   31,   63,   64,  553, 7987,  435,   34,  613]), 'topk_tokens': ['MIN', 'ot', ' Gutenberg', ' ', ',', ' Proof', '000', 'nes', '10', '000', ' setting', '6', ' moved', 'ISC', 'ENCES', ' to', 'ot', '000', ' kitchen', ' Sunday'], 'evidence_proportions': [5.021484375, 1.436669921875, 1.885498046875, 0.1970703125]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:46:57.986 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:46:57.986 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-48_pid-1_0-1-2-6.pkl | len: 3 |  size: 1.92 KB
Processing depth (0, 1, 2, 6):   2%|▏         | 2/100 [00:56<45:49, 28.06s/it]is_0k: False
your chose emoji: ['\U0001fada', '🌰', '🤷🏽\u200d♂', '👨🏻\u200d🔬', '👨🏿\u200d❤\u200d💋\u200d👨🏿', '👩🏽\u200d❤️\u200d💋\u200d👨🏾', '👠', '🇲🇱', '😍', '👳🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.41s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]
Processing depth (1, 4, 5, 6):   2%|▏         | 2/100 [01:13<45:49, 28.06s/it]2025-01-22 05:47:15.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:47:15.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:47:15.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:47:15.496 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:47:15.496 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:47:15.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11861, 11865) -->  Daniel dropped the apple
2025-01-22 05:47:15.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:47:15.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14337, 14342) --> . Sandra went to the
2025-01-22 05:47:15.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:47:15.742 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20859, 20864) --> . John travelled to the
2025-01-22 05:47:15.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:47:15.828 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13015, 13020) --> . Mary travelled to the
2025-01-22 05:47:15.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:47:15.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3911, 3917) -->  the ground. John went back
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:47:18.627 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the apple.<|eot_id|>
2025-01-22 05:47:18.627 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24145])
2025-01-22 05:47:27.029 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [202.94736842105263, 7.263984724407819, 61.072265625, 7.074090609837017, 13.961417776639344], 'topk_indices': array([24130,  3033,     3, 24128, 11864, 24139, 24132,    23, 24033,
          24, 11865, 24043, 24136,    14, 24133, 24135, 24143, 24141,
           0, 24140]), 'topk_tokens': [' was', ' moved', '<|end_header_id|>', ':', ' apple', ':', ' apple', '4', '.\n\n', '\n\n', '.', ' location', '?', '\n', ' before', ' office', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [254.825, 254.825, 296.71875, 24.175]}, 'weight': {'score': [21.653371710526315, 23.436283543150573, 21.2724609375, 23.43912418871563, 29.442879098360656], 'topk_indices': array([18751, 18795, 14621, 14657, 19390, 14702, 14666, 19448, 14585,
       14520, 20316, 20268, 18120, 18089, 23491, 23625, 21945, 21918,
       23573, 23707]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.2267696982935856, 0.040753430121186277, 0.353973388671875, 0.03961106590138424, 0.10231068095222848], 'topk_indices': array([    0, 24130,  9686,    23,    24, 11861, 24033, 11862, 24127,
       24129, 23999, 24136,  3032,  3033, 24138, 11864, 24132, 24043,
       24133, 24135]), 'topk_tokens': ['<|begin_of_text|>', ' was', ' moved', '4', '\n\n', ' Daniel', '.\n\n', ' dropped', 'Question', ' Where', ' context', '?', ' Daniel', ' moved', 'Answer', ' apple', ' apple', ' location', ' before', ' office'], 'evidence_proportions': [1.43271484375, 1.43271484375, 2.0723876953125, 0.138385009765625]}}, 'pred_res': 'Daniel dropped the apple.<|eot_id|>', 'score': 0}
2025-01-22 05:47:27.035 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:47:27.036 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-48_pid-2_1-4-5-6.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 4, 5, 6):   3%|▎         | 3/100 [01:25<46:05, 28.51s/it]is_0k: False
your chose emoji: ['👩🏿\u200d💼', '🚴', '🙆🏾\u200d♂', '🌯', '🧗🏾', '✌️', '🛠', '👩🏽\u200d🦼\u200d➡', '🥫', '🧏🏽\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.32s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]
Processing depth (0, 1, 7, 9):   3%|▎         | 3/100 [01:43<46:05, 28.51s/it]2025-01-22 05:47:45.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:47:45.415 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:47:45.416 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:47:45.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Daniel moved to the
2025-01-22 05:47:45.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:47:45.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16705, 16709) -->  Daniel dropped the apple
2025-01-22 05:47:45.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:47:45.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21431, 21436) --> . Sandra went to the
2025-01-22 05:47:45.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:47:45.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20831, 20836) --> . John travelled to the
2025-01-22 05:47:45.745 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:47:45.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13118, 13123) --> . Mary travelled to the
2025-01-22 05:47:45.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:47:45.848 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4005, 4011) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:47:48.570 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's hand<|eot_id|>
2025-01-22 05:47:48.570 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 05:47:56.939 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [233.33963815789474, 9.426724516923205, 97.345703125, 9.192147422616335, 7.586198833626761], 'topk_indices': array([ 1625, 24153, 16729, 24148,    35,  1704,    14, 16730, 16722,
       24163, 16708,  1626, 16759, 16721,  1705,  1627, 16760, 24161,
           0, 24160]), 'topk_tokens': [' summer', ' before', 'dr', ':', '.', ' summer', '\n', 'um', 'ets', '<|end_header_id|>', ' apple', "'s", ' bay', 'on', "'s", ' work', 'on', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [399.75, 117.4625, 412.46875, 39.503125]}, 'weight': {'score': [21.62705592105263, 23.442198878682554, 20.259765625, 23.445737877077033, 29.591769366197184], 'topk_indices': array([18767, 18811, 14632, 14596, 19428, 14677, 19486, 14641, 14495,
       14560, 20348, 20300, 18136, 18105, 23651, 23517, 21956, 21983,
       23599, 23733]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 19.909375, 27.083984375, 20.796875]}, 'saliency': {'score': [1.367454127261513, 0.05293623953613766, 0.5283584594726562, 0.05158611334437674, 0.057079422641807875], 'topk_indices': array([16720, 24147, 24153,    34, 16706, 16768,    31, 16730,  1625,
       24152, 16729, 16721,  1626, 16722,  1704,  1705, 16759, 16760,
       16708,  1627]), 'topk_tokens': [' bay', 'Question', ' before', ' kitchen', ' dropped', 'dr', ' moved', 'um', ' summer', ' apple', 'dr', 'on', "'s", 'ets', ' summer', "'s", ' bay', 'on', ' apple', ' work'], 'evidence_proportions': [2.0529296875, 0.616455078125, 2.877685546875, 0.22479248046875]}}, 'pred_res': "Daniel's hand<|eot_id|>", 'score': 0}
2025-01-22 05:47:56.946 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:47:56.946 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-48_pid-3_0-1-7-9.pkl | len: 3 |  size: 1.99 KB
Processing depth (0, 1, 7, 9):   4%|▍         | 4/100 [01:55<46:30, 29.06s/it]is_0k: False
your chose emoji: ['🔋', '♈', '✖', '🙇🏻\u200d♂️', '🍧', '👨🏻\u200d⚕️', '🙋🏾\u200d♂', '\U0001faf2🏻', '👨🏻\u200d❤️\u200d💋\u200d👨🏽', '🧛🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.81s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (1, 2, 6, 8):   4%|▍         | 4/100 [02:12<46:30, 29.06s/it]2025-01-22 05:48:14.757 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:48:14.774 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:48:14.774 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 05:48:14.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:48:14.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the apple.
2025-01-22 05:48:14.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14291, 14295) -->  Daniel dropped the apple
2025-01-22 05:48:14.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went to the bathroom.
2025-01-22 05:48:14.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19208, 19213) -->  Sandra went to the bathroom
2025-01-22 05:48:14.959 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:48:15.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20837, 20842) --> . John travelled to the
2025-01-22 05:48:15.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:48:15.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (12996, 13001) --> . Mary travelled to the
2025-01-22 05:48:15.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:48:15.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3999, 4005) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:48:17.920 | INFO     | test_jbb_embedding:begin_test:693 - Daniel's pocket.<|eot_id|>
2025-01-22 05:48:17.920 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24185])
2025-01-22 05:48:26.324 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [172.44120065789474, 6.789052164296345, 62.9599609375, 6.621531551463586, 8.716411072530864], 'topk_indices': array([    1,     4, 24179, 24170, 24175, 24176, 24166, 24168,    24,
       24172,     9, 14295, 14294,    23,    14, 24173, 24183,     0,
       24181, 24180]), 'topk_tokens': ['<|start_header_id|>', '\n\n', ':', ' was', ' office', '?', '.\n\n', ':', '\n\n', ' apple', ':', '.', ' apple', '4', '\n', ' before', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [171.2625, 171.2625, 363.375, 22.0515625]}, 'weight': {'score': [23.15049342105263, 23.447078613362, 20.259765625, 23.44942333768062, 29.57175925925926], 'topk_indices': array([18777, 18821, 14669, 14633, 19422, 14678, 14714, 19480, 14532,
       14597, 20306, 20354, 18115, 18146, 23517, 23651, 21983, 21956,
       23599, 23733]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 19.909375, 27.083984375, 26.4859375]}, 'saliency': {'score': [1.1017552425986843, 0.03784230407139517, 0.34056663513183594, 0.03680483729174106, 0.0656993536301601], 'topk_indices': array([24182,    19,     8,  3033,    20, 24170,    24, 24169, 24166,
       24176,     0, 14292,    23, 24178, 24167, 14291, 24175, 24172,
       24173, 14294]), 'topk_tokens': ['assistant', '26', ' Date', ' moved', ' Jul', ' was', '\n\n', ' Where', '.\n\n', '?', '<|begin_of_text|>', ' dropped', '4', 'Answer', 'Question', ' Daniel', ' office', ' apple', ' before', ' apple'], 'evidence_proportions': [0.965478515625, 0.965478515625, 2.61083984375, 0.167041015625]}}, 'pred_res': "Daniel's pocket.<|eot_id|>", 'score': 0}
2025-01-22 05:48:26.332 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:48:26.332 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-48_pid-4_1-2-6-8.pkl | len: 3 |  size: 2.01 KB
Processing depth (1, 2, 6, 8):   5%|▌         | 5/100 [02:24<46:12, 29.18s/it]Processing depth (1, 2, 6, 8):   5%|▌         | 5/100 [02:24<45:53, 28.98s/it]
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:82 - Selected idx: 49
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:84 - Answer: kitchen
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the bathroom.', ' Mary travelled to the bedroom.', ' Daniel moved to the kitchen.', ' Daniel picked up the football.', ' Daniel moved to the office.', ' John went back to the garden.', ' Daniel dropped the football.', ' Sandra went to the bathroom.']
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the kitchen.', ' Daniel picked up the football.', ' Daniel moved to the office.', ' Daniel dropped the football.', ' Sandra went to the bathroom.']
2025-01-22 05:48:26.733 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🐡', '\U0001faf6🏽', '🚴🏽\u200d♀️', '👩🏿\u200d⚖', '🧏🏾\u200d♀️', '👶🏻', '🎬', '🚶🏽\u200d➡️', '🧑🏻\u200d🍼', '👩🏿\u200d❤️\u200d👨🏻']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.45s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (1, 4, 5, 6, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:48:44.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:48:44.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:48:44.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:48:44.074 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9757, 9762) --> . Daniel picked up the
2025-01-22 05:48:44.074 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:48:44.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:48:44.092 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:48:44.168 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14461, 14465) -->  Daniel dropped the football
2025-01-22 05:48:44.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:48:44.326 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21477, 21482) --> . Sandra went to the
2025-01-22 05:48:44.326 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:48:44.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4673, 4678) --> . John travelled to the
2025-01-22 05:48:44.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:48:44.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7628, 7633) -->  war. Mary travelled to
2025-01-22 05:48:44.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:48:44.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14099, 14105) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:48:47.257 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the football.<|eot_id|>
2025-01-22 05:48:47.257 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24214])
2025-01-22 05:48:55.689 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [241.14583333333334, 9.40810561795433, 89.212890625, 9.12525075484965, 7.464008678089488], 'topk_indices': array([24133, 24093, 24206,     3,    14, 14463,    23, 24201,    24,
       24103, 24212, 14462, 24199, 14465, 14464,  9762,  9763,     0,
       24209, 24210]), 'topk_tokens': [' the', '.\n\n', '?\n', '<|end_header_id|>', '\n', ' the', '4', ' discarded', '\n\n', ' location', '<|end_header_id|>', ' dropped', ' football', '.', ' football', ' football', '.', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [162.7, 354.65, 162.7, 569.0625, 22.2]}, 'weight': {'score': [21.844075520833332, 23.450195110872528, 21.34228515625, 23.453184457335485, 29.51864346590909], 'topk_indices': array([18849, 18805, 14627, 14663, 14708, 14672, 19444, 19502, 14591,
       14526, 20340, 20388, 18174, 18143, 23563, 23697, 22029, 22002,
       23779, 23645]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 19.909375, 27.341796875, 20.796875]}, 'saliency': {'score': [1.4709790547688801, 0.05215262933515196, 0.4905433654785156, 0.05045406929922098, 0.05427490581165661], 'topk_indices': array([24131, 24206,  9760, 24197, 24093, 24207,    23, 24187, 24203,
       14461,  9763,    24,  9759, 24193, 24103, 14462, 24201, 24199,
       14464,  9762]), 'topk_tokens': [' moved', '?\n', ' up', ' where', '.\n\n', 'Answer', '4', 'Question', ' left', ' Daniel', '.', '\n\n', ' picked', ' prior', ' location', ' dropped', ' discarded', ' football', ' football', ' football'], 'evidence_proportions': [0.9280517578125, 1.94140625, 0.9280517578125, 3.92724609375, 0.121392822265625]}}, 'pred_res': 'Daniel dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 05:48:55.702 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:48:55.703 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-49_pid-0_1-4-5-6-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (1, 4, 5, 6, 9):   1%|          | 1/100 [00:28<47:27, 28.76s/it]is_0k: False
your chose emoji: ['🤸🏾\u200d♀️', '🇮🇸', '🧑🏼\u200d🦼\u200d➡', '👦🏽', '🚶🏽\u200d♂\u200d➡', '👨🏾\u200d❤️\u200d💋\u200d👨🏿', '🇦🇲', '\U0001faf0🏼', '💇🏻\u200d♀️', '🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.47s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.90s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.97s/it]
Processing depth (0, 3, 5, 6, 9):   1%|          | 1/100 [00:46<47:27, 28.76s/it]2025-01-22 05:49:14.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:49:14.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:49:14.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:49:14.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7596, 7601) -->  war. Daniel picked up
2025-01-22 05:49:14.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:49:14.158 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11865, 11870) --> . Daniel moved to the
2025-01-22 05:49:14.158 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:49:14.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14369, 14373) -->  Daniel dropped the football
2025-01-22 05:49:14.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:49:14.346 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21551, 21556) --> . Sandra went to the
2025-01-22 05:49:14.346 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:49:14.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4689, 4694) -->  the senate. John travelled
2025-01-22 05:49:14.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:49:14.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7649, 7654) --> . Mary travelled to the
2025-01-22 05:49:14.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:49:14.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14071, 14077) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:49:17.846 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Daniel, and prior to that, Daniel picked up the football.<|eot_id|>
2025-01-22 05:49:17.847 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 05:49:26.269 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [386.9075520833333, 15.88875077527393, 142.298828125, 15.436192793539036, 24.25783962673611], 'topk_indices': array([ 1484, 14370, 24169, 14371,  1485,  1625,    28,    31, 24167,
          35, 14372,  1704, 24177,  7603,  1626,  7602,  1705,  1627,
       24178,     0]), 'topk_tokens': [' of', ' dropped', ' discarded', ' the', ' the', ' summer', '<|end_header_id|>', ' moved', ' football', '.', ' football', ' summer', '<|eot_id|>', '.', "'s", ' football', "'s", ' work', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [704.35, 391.625, 187.25, 674.9375, 33.98125]}, 'weight': {'score': [22.544921875, 23.4411153607608, 21.7373046875, 23.443135224684198, 29.33029513888889], 'topk_indices': array([18819, 18775, 14651, 14687, 14732, 19414, 19472, 14696, 14550,
       14615, 20286, 20334, 18144, 18113, 23663, 23529, 21963, 21936,
       23745, 23611]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 25.8265625, 19.909375, 27.341796875, 20.796875]}, 'saliency': {'score': [2.2054964701334634, 0.08951743841417717, 0.7867279052734375, 0.0869521510159162, 0.17938232421875], 'topk_indices': array([   39,   551,  1421, 24071,   185,  1483,    38,    34,    30,
       14370,  1625,  1626,    31, 24169, 24167, 14372,  1704,  1705,
        7602,  1627]), 'topk_tokens': ['\n\n\n', ' instead', ' question', ' location', 'ION', ' question', '***', ' kitchen', 'Daniel', ' dropped', ' summer', "'s", ' moved', ' discarded', ' football', ' football', ' summer', "'s", ' football', ' work'], 'evidence_proportions': [3.5779296875, 2.386328125, 0.9240234375, 4.35107421875, 0.217242431640625]}}, 'pred_res': 'The football was dropped by Daniel, and prior to that, Daniel picked up the football.<|eot_id|>', 'score': 0}
2025-01-22 05:49:26.276 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:49:26.277 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-49_pid-1_0-3-5-6-9.pkl | len: 3 |  size: 2.18 KB
Processing depth (0, 3, 5, 6, 9):   2%|▏         | 2/100 [00:59<48:43, 29.83s/it]is_0k: False
your chose emoji: ['🧔🏼', '🧎🏿\u200d➡', '🤦\u200d♀', '🧑🏾\u200d💼', '🇨🇿', '🏘️', '👩🏼\u200d❤\u200d👩🏼', '👨\u200d🦼\u200d➡️', '🥀', '🗣️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:13,  6.58s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.19s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.46s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.05s/it]
Processing depth (0, 2, 3, 6, 9):   2%|▏         | 2/100 [01:21<48:43, 29.83s/it]2025-01-22 05:49:48.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:49:48.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the kitchen.
2025-01-22 05:49:48.864 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:49:48.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4969, 4974) --> . Daniel picked up the
2025-01-22 05:49:48.889 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:49:48.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7637, 7642) --> . Daniel moved to the
2025-01-22 05:49:48.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:49:48.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14369, 14373) -->  Daniel dropped the football
2025-01-22 05:49:48.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:49:49.118 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21419, 21424) --> . Sandra went to the
2025-01-22 05:49:49.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:49:49.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4695, 4700) -->  the senate. John travelled
2025-01-22 05:49:49.142 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:49:49.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7691, 7696) --> . Mary travelled to the
2025-01-22 05:49:49.184 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:49:49.268 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14053, 14059) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:49:52.154 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Daniel.<|eot_id|>
2025-01-22 05:49:52.154 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24174])
2025-01-22 05:50:00.545 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [620.04296875, 22.16946891673905, 207.18359375, 21.45234624228363, 16.051140280330884], 'topk_indices': array([24169,    29, 14370,  2048,  2099,  2100,    30,  2109,  2053,
        2097, 24161,  4974,  2050,    31, 14373, 24159,  4975,  2049,
       14372,     0]), 'topk_tokens': ['<|eot_id|>', '\n\n', ' dropped', 'P', ' du', ' Ch', 'Daniel', ' the', 'ien', 'ra', ' discarded', ' football', 'irie', ' moved', '.', ' football', '.', 'ra', ' football', '<|begin_of_text|>'], 'evidence_proportions': [967.5, 590.7, 367.25, 1214.75, 78.95625]}, 'weight': {'score': [21.8232421875, 23.441493981883607, 21.7373046875, 23.444232726208725, 29.744255514705884], 'topk_indices': array([18827, 18783, 14647, 14611, 14656, 19480, 14692, 19422, 14510,
       14575, 20294, 20342, 18152, 18121, 23505, 23639, 21944, 21971,
       23587, 23721]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.809375, 22.3625, 19.909375, 27.341796875, 20.796875]}, 'saliency': {'score': [3.6717071533203125, 0.12646400561160917, 1.1684951782226562, 0.12224814057839106, 0.12385805915383731], 'topk_indices': array([24063,  2048,  2051,  2098,  2052,    34,  2097, 14370,  2099,
        2053,  2100,  4702,    30,    31,  4974,  2050,  2049, 24159,
       24161, 14372]), 'topk_tokens': [' location', 'P', ' du', 'irie', ' Ch', ' kitchen', 'ra', ' dropped', ' du', 'ien', ' Ch', ' bathroom', 'Daniel', ' moved', ' football', 'irie', 'ra', ' football', ' discarded', ' football'], 'evidence_proportions': [5.25546875, 3.242578125, 1.90302734375, 8.5234375, 0.4043701171875]}}, 'pred_res': 'The football was dropped by Daniel.<|eot_id|>', 'score': 0}
2025-01-22 05:50:00.553 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:50:00.554 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-49_pid-2_0-2-3-6-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 2, 3, 6, 9):   3%|▎         | 3/100 [01:33<51:30, 31.86s/it]is_0k: False
your chose emoji: ['🍫', '🙂\u200d↔️', '🥨', '👩🏾\u200d💻', '🌂', '👮🏿\u200d♀', '🙅🏿', '👩🏽\u200d❤\u200d👨🏼', '🐬', '🚶🏼\u200d♀\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (1, 2, 3, 5, 7):   3%|▎         | 3/100 [01:51<51:30, 31.86s/it]2025-01-22 05:50:18.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:50:18.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:50:18.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:50:18.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4999, 5004) --> . Daniel picked up the
2025-01-22 05:50:18.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:50:18.352 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 05:50:18.353 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:50:18.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12007, 12011) -->  dropped the football.
2025-01-22 05:50:18.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:50:18.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16731, 16736) --> . Sandra went to the
2025-01-22 05:50:18.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:50:18.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4777, 4782) --> . John travelled to the
2025-01-22 05:50:18.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:50:18.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7693, 7698) --> . Mary travelled to the
2025-01-22 05:50:18.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:50:18.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14174, 14180) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:50:21.300 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 05:50:21.300 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24200])
2025-01-22 05:50:29.689 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [170.84261067708334, 7.644447485848862, 55.6962890625, 7.4505317724930675, 7.076521508487654], 'topk_indices': array([    1,  5005,    20, 24185,    22, 24079,  5000,  4964,  5004,
          19, 12009,     3,    24, 24198,    14, 24045, 24195,    23,
           0, 24196]), 'topk_tokens': ['<|start_header_id|>', '.', ' Jul', ' football', '202', '.\n\n', ' Daniel', '4', ' football', '26', ' football', '<|end_header_id|>', '\n\n', '<|end_header_id|>', '\n', ' context', '<|eot_id|>', '4', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [179.7625, 229.75, 179.7625, 278.09375, 8.29453125]}, 'weight': {'score': [20.825520833333332, 23.446504565549724, 20.259765625, 23.45121802963208, 29.49189814814815], 'topk_indices': array([18789, 18833, 14621, 14657, 14702, 14666, 19486, 19428, 14490,
       14555, 20320, 20368, 18127, 18158, 23537, 23671, 22003, 21976,
       23619, 23753]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 19.909375, 21.23046875, 20.796875]}, 'saliency': {'score': [1.0109999974568684, 0.04318655797609026, 0.29740333557128906, 0.0420569386825811, 0.05228035538284867], 'topk_indices': array([24089,  5001, 24193, 24044, 24079,  3033,  3032,    16,    24,
       24043,    22, 24187,    19,    20, 24185,  5000,  5004,    23,
       12009, 24045]), 'topk_tokens': [' location', ' picked', 'Answer', ' you', '.\n\n', ' moved', ' Daniel', ' Date', '\n\n', ' provided', '202', ' discarded', '26', ' Jul', ' football', ' Daniel', ' football', '4', ' football', ' context'], 'evidence_proportions': [1.0342529296875, 1.4111328125, 1.0342529296875, 1.65576171875, 0.04855194091796875]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 05:50:29.699 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:50:29.700 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-49_pid-3_1-2-3-5-7.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 3, 5, 7):   4%|▍         | 4/100 [02:02<49:15, 30.79s/it]is_0k: False
your chose emoji: ['🚴\u200d♀️', '🤷🏻\u200d♀️', '👩🏽\u200d❤\u200d👨🏾', '👈🏻', '🅰️', '👩🏽\u200d🦰', '👨🏻\u200d🍳', '🤵\u200d♂', '🤟🏼', '🧎🏽\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.66s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (2, 4, 5, 8, 9):   4%|▍         | 4/100 [02:19<49:15, 30.79s/it]2025-01-22 05:50:47.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the kitchen.
2025-01-22 05:50:47.045 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4913, 4918) --> . Daniel moved to the
2025-01-22 05:50:47.045 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the football.
2025-01-22 05:50:47.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9674, 9679) -->  war. Daniel picked up
2025-01-22 05:50:47.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 05:50:47.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4913, 4918) --> . Daniel moved to the
2025-01-22 05:50:47.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the football.
2025-01-22 05:50:47.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19119, 19123) -->  Daniel dropped the football
2025-01-22 05:50:47.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went to the bathroom.
2025-01-22 05:50:47.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21401, 21406) --> . Sandra went to the
2025-01-22 05:50:47.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the bathroom.
2025-01-22 05:50:47.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4645, 4650) --> . John travelled to the
2025-01-22 05:50:47.357 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary travelled to the bedroom.
2025-01-22 05:50:47.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7606, 7611) -->  war. Mary travelled to
2025-01-22 05:50:47.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the garden.
2025-01-22 05:50:47.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13952, 13958) -->  law. John went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:50:50.350 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Daniel.<|eot_id|>
2025-01-22 05:50:50.351 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24138])
2025-01-22 05:50:58.726 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [190.0546875, 8.513598494262872, 61.440185546875, 8.29768166388324, 21.766015625], 'topk_indices': array([24118,    19, 24132,     1,  9680,    24, 24111, 24110,     3,
       19122, 24123, 24112, 24130, 24017,    14,    23, 24136,     0,
       24133, 24134]), 'topk_tokens': [' to', '26', ':', '<|start_header_id|>', ' football', '\n\n', 'Question', '.\n\n', '<|end_header_id|>', ' football', ' football', ':', '?\n', '.\n\n', '\n', '4', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [210.675, 168.96875, 210.675, 364.75, 30.14375]}, 'weight': {'score': [22.565755208333332, 23.429168737831905, 22.46875, 23.430666128998798, 29.054375], 'topk_indices': array([18760, 18804, 14630, 14666, 14711, 19404, 14675, 19462, 14529,
       14594, 20276, 20324, 18123, 18092, 23621, 23487, 21926, 21953,
       23703, 23569]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 25.8265625, 19.909375, 27.341796875, 20.796875]}, 'saliency': {'score': [1.2079200744628906, 0.046242632193739384, 0.3401756286621094, 0.04489068884873488, 0.16029266357421876], 'topk_indices': array([   14, 24116,  9676, 24121,    24,    22,  4914,    19, 24131,
          20, 24110, 24130, 24117, 24017, 24125,    23,  9680, 24111,
       24123, 19122]), 'topk_tokens': ['\n', ' location', ' Daniel', ' where', '\n\n', '202', ' Daniel', '26', 'Answer', ' Jul', '.\n\n', '?\n', ' prior', '.\n\n', ' discarded', '4', ' football', 'Question', ' football', ' football'], 'evidence_proportions': [1.239990234375, 1.104296875, 1.239990234375, 2.5699462890625, 0.157781982421875]}}, 'pred_res': 'The football was dropped by Daniel.<|eot_id|>', 'score': 0}
2025-01-22 05:50:58.734 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:50:58.734 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-49_pid-4_2-4-5-8-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 4, 5, 8, 9):   5%|▌         | 5/100 [02:31<47:44, 30.16s/it]Processing depth (2, 4, 5, 8, 9):   5%|▌         | 5/100 [02:32<48:11, 30.43s/it]
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:82 - Selected idx: 50
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the kitchen? 
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:86 - Needle: [' Sandra went back to the garden.', ' Daniel put down the football there.', ' Mary travelled to the hallway.', ' Daniel journeyed to the kitchen.', ' Daniel took the football.', ' Sandra picked up the apple.', ' Mary went to the kitchen.', ' Sandra moved to the hallway.', ' Sandra left the apple.', ' Mary put down the apple.', ' Daniel went to the garden.']
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the hallway.', ' Mary went to the kitchen.', ' Mary put down the apple.', ' Daniel went to the garden.']
2025-01-22 05:50:59.106 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🤲🏽', '🇬🇹', '⚽', '🤹🏾\u200d♀', '💁🏽\u200d♂', '💇🏼\u200d♂', '😘', '🌶', '🦆', '🥓']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.33s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.65s/it]
Processing depth (1, 2, 7, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:51:15.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:51:16.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2921, 2926) --> . Mary travelled to the
2025-01-22 05:51:16.007 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:51:16.036 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4882, 4887) --> . Mary went to the
2025-01-22 05:51:16.036 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:51:16.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16756, 16761) --> . Mary put down the
2025-01-22 05:51:16.122 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:51:16.155 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6447, 6452) -->  back to the garden.
2025-01-22 05:51:16.156 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 05:51:16.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6444, 6450) --> . Sandra went back to the
2025-01-22 05:51:16.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel put down the football there.
2025-01-22 05:51:16.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3783, 3789) --> . Daniel put down the football
2025-01-22 05:51:16.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel journeyed to the kitchen.
2025-01-22 05:51:16.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15388, 15394) -->  Daniel journeyed to the kitchen
2025-01-22 05:51:16.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:51:16.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7031, 7035) -->  took the football.
2025-01-22 05:51:16.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 05:51:16.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22236, 22241) --> . Sandra picked up the
2025-01-22 05:51:16.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra moved to the hallway.
2025-01-22 05:51:16.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4826, 4831) --> . Sandra moved to the
2025-01-22 05:51:16.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra left the apple.
2025-01-22 05:51:16.526 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9043, 9047) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:51:19.207 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:51:19.207 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24248])
2025-01-22 05:51:27.586 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [353.9125, 19.547719681662613, 527.9409722222222, 18.51488427361025, 23.05748046875], 'topk_indices': array([ 6453,  3788, 15393,  4830, 24235, 24246,  4829, 24238, 24236,
       24247,  2927,  4828,  3784,  4831,  2926,  4827, 24248, 24244,
       24243,     0]), 'topk_tokens': ['      ', ' football', ' kitchen', ' the', ' apple', '<|end_header_id|>', ' to', ' kitchen', ' before', '\n\n', '.', ' moved', ' Daniel', ' hallway', ' hallway', ' Sandra', 'hall', '<|start_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [641.35, 242.325, 236.075, 295.9]}, 'weight': {'score': [20.19453125, 23.4604268896128, 23.27777777777778, 23.46339830026865, 29.844375], 'topk_indices': array([18868, 18912, 14713, 14749, 19565, 14794, 14758, 19507, 14677,
       14612, 20427, 20379, 18206, 18237, 23596, 23730, 22029, 22056,
       23678, 23812]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [1.874951171875, 0.11075797695842439, 3.2928059895833335, 0.10456506138277537, 0.17355804443359374], 'topk_indices': array([ 3789, 24102,  9046, 24247,  3785, 24232,  6453,  2922, 15393,
        3788, 24235,  2923, 24236, 24238,  4828,  3784, 24248,  4831,
        2926,  4827]), 'topk_tokens': [' there', ' context', ' apple', '\n\n', ' put', ' Where', '      ', ' Mary', ' kitchen', ' football', ' apple', ' travelled', ' before', ' kitchen', ' moved', ' Daniel', 'hall', ' hallway', ' hallway', ' Sandra'], 'evidence_proportions': [3.6408203125, 1.170703125, 1.32841796875, 1.35986328125]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:51:27.593 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:51:27.594 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-50_pid-0_1-2-7-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 7, 9):   1%|          | 1/100 [00:28<46:46, 28.35s/it]is_0k: False
your chose emoji: ['👩🏿\u200d❤\u200d💋\u200d👩🏽', '👩🏽\u200d❤️\u200d💋\u200d👩🏿', '🧑\u200d🎨', '👩🏾\u200d⚖️', '👰🏻', '🧍🏼\u200d♀', '🧎🏽', '🤸🏿\u200d♀️', '👳🏽\u200d♀', '🏌🏽\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.99s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.87s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.92s/it]
Processing depth (0, 1, 3, 4):   1%|          | 1/100 [00:46<46:46, 28.35s/it]2025-01-22 05:51:45.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:51:45.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:51:45.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:51:45.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Mary went to the
2025-01-22 05:51:45.482 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:51:45.523 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7644, 7649) --> . Mary put down the
2025-01-22 05:51:45.523 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:51:45.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6541, 6546) -->  back to the garden.
2025-01-22 05:51:45.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 05:51:45.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6538, 6544) --> . Sandra went back to the
2025-01-22 05:51:45.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel put down the football there.
2025-01-22 05:51:45.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3813, 3819) --> . Daniel put down the football
2025-01-22 05:51:45.607 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel journeyed to the kitchen.
2025-01-22 05:51:45.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15449, 15455) --> . Daniel journeyed to the
2025-01-22 05:51:45.684 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:51:45.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7164, 7168) -->  Daniel took the football
2025-01-22 05:51:45.718 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 05:51:45.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22216, 22221) --> . Sandra picked up the
2025-01-22 05:51:45.825 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra moved to the hallway.
2025-01-22 05:51:45.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4904, 4909) --> . Sandra moved to the
2025-01-22 05:51:45.849 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra left the apple.
2025-01-22 05:51:45.892 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9109, 9113) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:51:48.589 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:51:48.589 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24168])
2025-01-22 05:51:56.922 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [377.35625, 16.213378118406354, 280.015625, 15.520044578063446, 26.0505859375], 'topk_indices': array([24150,    24, 24158,    34,  5851, 24151,    14,  4905,  4909,
        5853,  5988, 24155,    35, 24156, 24166,  4906,  5989, 24164,
           0, 24163]), 'topk_tokens': ['Question', '\n\n', ' kitchen', ' hallway', '\n', ':', '\n', ' Sandra', ' hallway', ' way', '\n', ' apple', '.', ' before', '<|end_header_id|>', ' moved', 'the', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [701.25, 319.675, 294.225, 194.275]}, 'weight': {'score': [20.56171875, 23.438671341690455, 23.291666666666668, 23.441276824590503, 29.710416666666667], 'topk_indices': array([18780, 18824, 14631, 14667, 14676, 19477, 14712, 19419, 14595,
       14530, 20345, 20297, 18100, 18131, 23512, 23646, 21941, 21968,
       23594, 23728]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [2.06246337890625, 0.09147422272558024, 1.7015041775173612, 0.08743603647237197, 0.1917065938313802], 'topk_indices': array([15455, 24153, 24161,  7649,  7787,    38,    39,  3042, 24152,
          31,  5853, 24150, 24158, 24156,  5989, 24155,    34,  4905,
        4906,  4909]), 'topk_tokens': [' kitchen', ' was', 'Answer', ' apple', ' Square', '***', '\n\n\n', ' kitchen', ' Where', ' travelled', ' way', 'Question', ' kitchen', ' before', 'the', ' apple', ' hallway', ' Sandra', ' moved', ' hallway'], 'evidence_proportions': [4.07333984375, 1.53291015625, 1.6529296875, 0.990673828125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:51:56.937 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:51:56.945 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-50_pid-1_0-1-3-4.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 1, 3, 4):   2%|▏         | 2/100 [00:57<47:15, 28.94s/it]is_0k: False
your chose emoji: ['🦼', '✂️', '👱🏾\u200d♂', '📁', '🧘🏻\u200d♂️', '🏃\u200d♀\u200d➡', '\U0001fad8', '👨🏽\u200d✈', '🩲', '🧚🏽\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.07s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
Processing depth (2, 5, 6, 9):   2%|▏         | 2/100 [01:15<47:15, 28.94s/it]2025-01-22 05:52:14.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:52:14.941 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4958, 4963) --> . Mary travelled to the
2025-01-22 05:52:14.941 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:52:15.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11889, 11894) --> . Mary went to the
2025-01-22 05:52:15.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:52:15.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14395, 14400) --> . Mary put down the
2025-01-22 05:52:15.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:52:15.115 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6509, 6514) -->  back to the garden.
2025-01-22 05:52:15.115 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 05:52:15.147 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6506, 6512) --> . Sandra went back to the
2025-01-22 05:52:15.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel put down the football there.
2025-01-22 05:52:15.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3801, 3807) --> . Daniel put down the football
2025-01-22 05:52:15.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel journeyed to the kitchen.
2025-01-22 05:52:15.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15505, 15511) --> . Daniel journeyed to the
2025-01-22 05:52:15.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:52:15.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7029, 7033) -->  took the football.
2025-01-22 05:52:15.284 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 05:52:15.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22244, 22249) --> . Sandra picked up the
2025-01-22 05:52:15.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra moved to the hallway.
2025-01-22 05:52:15.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4892, 4897) --> . Sandra moved to the
2025-01-22 05:52:15.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra left the apple.
2025-01-22 05:52:15.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9025, 9029) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:52:18.165 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:52:18.165 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24188])
2025-01-22 05:52:26.543 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [255.59375, 13.322786625191187, 315.09809027777777, 12.67189247980112, 12.945577566964285], 'topk_indices': array([24187, 24175, 24076, 24182,  8752, 22249,  3802,  4896,  4962,
          14,    24,  4895,  4894,    23, 24042,  4963,  4893, 24186,
           0, 24183]), 'topk_tokens': ['\n\n', ' apple', '.\n\n', ':', '2', ' apple', ' Daniel', ' the', ' the', '\n', '\n\n', ' to', ' moved', '4', ' context', ' hallway', ' Sandra', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [417.65, 150.225, 187.65, 266.85]}, 'weight': {'score': [20.19453125, 23.441000785416065, 22.612630208333332, 23.444926649575304, 29.2234375], 'topk_indices': array([18852, 18808, 14639, 14675, 14720, 19447, 19505, 14684, 14603,
       14538, 20319, 20367, 18146, 18177, 23548, 23682, 21969, 21996,
       23764, 23630]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [1.33162841796875, 0.07622912305486802, 1.9547220865885417, 0.07238683042650715, 0.09461310250418527], 'topk_indices': array([    0,  6512,    24, 24170, 24176, 24181,    23,  6507,  9028,
        4960, 24175, 24178,  9025, 22249,  3802,  4897, 24042,  4894,
        4963,  4893]), 'topk_tokens': ['<|begin_of_text|>', ' garden', '\n\n', 'Question', ' before', 'Answer', '4', ' Sandra', ' apple', ' travelled', ' apple', ' kitchen', ' Sandra', ' apple', ' Daniel', ' hallway', ' context', ' moved', ' hallway', ' Sandra'], 'evidence_proportions': [2.15048828125, 0.784228515625, 1.0302734375, 1.3615234375]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:52:26.550 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:52:26.551 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-50_pid-2_2-5-6-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (2, 5, 6, 9):   3%|▎         | 3/100 [01:27<47:16, 29.24s/it]is_0k: False
your chose emoji: ['💁\u200d♀', '🚷', '👨🏾\u200d⚕️', '🉑', '🤾🏼\u200d♀️', '🤷🏼\u200d♂', '🇧🇷', '👦🏻', '👩🏿\u200d🤝\u200d👩🏾', '✍']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.48s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.19s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
Processing depth (0, 3, 6, 9):   3%|▎         | 3/100 [01:44<47:16, 29.24s/it]2025-01-22 05:52:43.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:52:43.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:52:43.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:52:43.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7680, 7685) --> . Mary went to the
2025-01-22 05:52:43.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:52:43.589 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14605, 14610) --> . Mary put down the
2025-01-22 05:52:43.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:52:43.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6555, 6560) -->  back to the garden.
2025-01-22 05:52:43.622 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 05:52:43.654 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6552, 6558) --> . Sandra went back to the
2025-01-22 05:52:43.654 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel put down the football there.
2025-01-22 05:52:43.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3827, 3833) --> . Daniel put down the football
2025-01-22 05:52:43.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel journeyed to the kitchen.
2025-01-22 05:52:43.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15646, 15652) -->  engagement. Daniel journeyed to
2025-01-22 05:52:43.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:52:43.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7250, 7254) -->  Daniel took the football
2025-01-22 05:52:43.807 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 05:52:43.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22148, 22153) --> . Sandra picked up the
2025-01-22 05:52:43.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra moved to the hallway.
2025-01-22 05:52:43.966 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4952, 4957) --> . Sandra moved to the
2025-01-22 05:52:43.966 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra left the apple.
2025-01-22 05:52:44.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9103, 9107) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:52:46.723 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:52:46.723 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 05:52:55.188 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [389.79375, 18.41493952863345, 325.7170138888889, 17.64862302623399, 25.575064132462686], 'topk_indices': array([   39,    24, 24163,  4954,    14,    35,  4957, 24166, 24173,
       24168, 24180, 24172, 24167, 24164, 24165, 24169, 24170,     0,
       24178, 24177]), 'topk_tokens': ['\n\n\n', '\n\n', '.\n\n', ' moved', '\n', '.', ' hallway', ' Where', '?', ' the', '<|end_header_id|>', ' kitchen', ' was', 'Question', ':', ' apple', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [635.15, 281.35, 269.9, 372.775]}, 'weight': {'score': [20.56171875, 23.440091999173042, 23.922960069444443, 23.44175738996643, 29.311567164179106], 'topk_indices': array([18800, 18756, 14565, 14601, 19395, 14616, 14652, 19453, 14529,
       14464, 20321, 20273, 18125, 18094, 23508, 23642, 21968, 21941,
       23590, 23724]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [2.1150390625, 0.10346963348924954, 1.9634128146701388, 0.09902728846447739, 0.19147115678929572], 'topk_indices': array([24163,  7685,   185, 24175, 24173,    31,  9106, 24165,    38,
          39, 24167,  4953,  4954,    34, 24166, 24172,  4957, 24170,
       24164, 24169]), 'topk_tokens': ['.\n\n', ' kitchen', 'ION', 'Answer', '?', ' travelled', ' apple', ':', '***', '\n\n\n', ' was', ' Sandra', ' moved', ' hallway', ' Where', ' kitchen', ' hallway', ' before', 'Question', ' apple'], 'evidence_proportions': [3.6951171875, 1.3884765625, 1.51708984375, 1.85947265625]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:52:55.196 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:52:55.196 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-50_pid-3_0-3-6-9.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 3, 6, 9):   4%|▍         | 4/100 [01:55<46:24, 29.01s/it]is_0k: False
your chose emoji: ['🤦\u200d♂', '💂', '🍥', '🧘🏻\u200d♂', '🇿🇦', '👨🏾\u200d⚕', '🥋', '🍺', '🏇🏿', '👩🏻\u200d❤️\u200d💋\u200d👨🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.82s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (0, 3, 4, 5):   4%|▍         | 4/100 [02:13<46:24, 29.01s/it]2025-01-22 05:53:12.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:53:12.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:53:12.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:53:13.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7474, 7479) --> . Mary went to the
2025-01-22 05:53:13.040 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:53:13.093 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9698, 9703) -->  war. Mary put down
2025-01-22 05:53:13.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:53:13.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6521, 6526) -->  back to the garden.
2025-01-22 05:53:13.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 05:53:13.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (6518, 6524) --> . Sandra went back to the
2025-01-22 05:53:13.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel put down the football there.
2025-01-22 05:53:13.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3789, 3795) --> . Daniel put down the football
2025-01-22 05:53:13.187 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel journeyed to the kitchen.
2025-01-22 05:53:13.266 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (15391, 15397) --> . Daniel journeyed to the
2025-01-22 05:53:13.266 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:53:13.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (7025, 7029) -->  took the football.
2025-01-22 05:53:13.304 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 05:53:13.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (22162, 22167) --> . Sandra picked up the
2025-01-22 05:53:13.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra moved to the hallway.
2025-01-22 05:53:13.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (4858, 4863) --> . Sandra moved to the
2025-01-22 05:53:13.451 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra left the apple.
2025-01-22 05:53:13.499 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9017, 9021) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:53:16.208 | INFO     | test_jbb_embedding:begin_test:693 - The garden.<|eot_id|>
2025-01-22 05:53:16.208 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24170])
2025-01-22 05:53:24.594 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [213.35703125, 12.569497735076325, 284.30555555555554, 11.997359875813741, 14.998543160860656], 'topk_indices': array([    1, 24160,  4863,  4858,  1647, 24152, 24157, 24024, 24153,
          24,    14,  4861,  4859, 24168, 24158,  1640,  4860,     0,
       24165, 24166]), 'topk_tokens': ['<|start_header_id|>', ' kitchen', ' hallway', '.', 'ern', 'Question', ' apple', ' context', ':', '\n\n', '\n', ' to', ' Sandra', '<|end_header_id|>', ' before', ' part', ' moved', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [387.35, 206.425, 99.578125, 160.075]}, 'weight': {'score': [21.427734375, 23.438247217970464, 22.612630208333332, 23.441146934734835, 29.52356557377049], 'topk_indices': array([18782, 18826, 14631, 14667, 14712, 14676, 19421, 19479, 14530,
       14595, 20359, 20311, 18131, 18100, 23522, 23656, 21982, 21955,
       23738, 23604]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 19.403125, 25.2359375, 18.846875]}, 'saliency': {'score': [1.1787841796875, 0.07074955545055434, 1.6881442599826388, 0.06741635058063296, 0.11446593237704918], 'topk_indices': array([24139, 24155,    31, 24163,  1561,  1568,    24, 24154,    34,
       24068,  1647, 24160, 24024, 24157,  1640, 24158, 24152,  4863,
        4859,  4860]), 'topk_tokens': [' return', ' was', ' travelled', 'Answer', ' part', 'ern', '\n\n', ' Where', ' hallway', ' location', 'ern', ' kitchen', ' context', ' apple', ' part', ' before', 'Question', ' hallway', ' Sandra', ' moved'], 'evidence_proportions': [2.2619140625, 1.0294921875, 0.61455078125, 0.8091796875]}}, 'pred_res': 'The garden.<|eot_id|>', 'score': 0}
2025-01-22 05:53:24.599 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:53:24.599 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-50_pid-4_0-3-4-5.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 3, 4, 5):   5%|▌         | 5/100 [02:25<46:09, 29.15s/it]Processing depth (0, 3, 4, 5):   5%|▌         | 5/100 [02:25<46:06, 29.13s/it]
2025-01-22 05:53:24.873 | INFO     | __main__:<module>:82 - Selected idx: 51
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:86 - Needle: [' Daniel put down the football there.', ' Sandra picked up the apple.', ' Mary travelled to the hallway.', ' Sandra left the apple.', ' Mary got the milk.', ' Mary went to the kitchen.', ' Sandra moved to the hallway.', ' Sandra went back to the garden.', ' Daniel journeyed to the kitchen.', ' Daniel took the football.', ' Mary put down the milk.', ' Daniel went to the garden.']
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the hallway.', ' Mary got the milk.', ' Mary went to the kitchen.', ' Mary put down the milk.', ' Daniel went to the garden.']
2025-01-22 05:53:24.874 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🍦', '🌖', '👨🏿\u200d🔬', '🧙🏾\u200d♀️', '🎑', '👂🏻', '🦸🏽\u200d♀️', '🦜', '🧏🏾\u200d♂', '🏃\u200d♂']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.36s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Processing depth (1, 2, 5, 8, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:53:42.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:53:42.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2966, 2971) -->  tragedy. Mary travelled to
2025-01-22 05:53:42.522 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 05:53:42.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4916, 4920) -->  Mary got the milk
2025-01-22 05:53:42.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:53:42.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11916, 11921) --> . Mary went to the
2025-01-22 05:53:42.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:53:42.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19212, 19217) -->  Mary put down the milk
2025-01-22 05:53:42.744 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:53:42.778 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5753, 5758) -->  back to the garden.
2025-01-22 05:53:42.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:53:42.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16800, 16806) --> . Daniel put down the football
2025-01-22 05:53:42.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 05:53:42.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5150, 5155) --> . Sandra picked up the
2025-01-22 05:53:42.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:53:42.951 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11350, 11354) -->  Sandra left the apple
2025-01-22 05:53:42.951 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the hallway.
2025-01-22 05:53:43.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10289, 10294) --> . Sandra moved to the
2025-01-22 05:53:43.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 05:53:43.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5750, 5756) --> . Sandra went back to the
2025-01-22 05:53:43.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the kitchen.
2025-01-22 05:53:43.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6321, 6327) --> . Daniel journeyed to the
2025-01-22 05:53:43.070 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel took the football.
2025-01-22 05:53:43.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8689, 8693) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:53:45.792 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:53:45.792 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24194])
2025-01-22 05:53:54.210 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1149.3020833333333, 35.279696346654546, 363.4826388888889, 33.68248694949662, 50.70016571969697], 'topk_indices': array([24167, 24168, 24189, 24178, 24180, 24177, 19214, 19215,  4919,
       10294, 24190,  4920, 19216, 24192, 24179,     0, 19217,  2972,
       24193, 24194]), 'topk_tokens': ['Question', ':', '<|eot_id|>', ' the', ' was', ' where', ' down', ' the', ' milk', ' hallway', '<|start_header_id|>', '.', ' milk', '<|end_header_id|>', ' milk', '<|begin_of_text|>', '.', ' hallway', '\n\n', 'hall'], 'evidence_proportions': [966.45, 1860.375, 351.975, 2379.0, 330.925]}, 'weight': {'score': [23.367513020833332, 23.43986082985494, 23.291666666666668, 23.44015379655715, 29.476089015151516], 'topk_indices': array([18825, 18781, 14644, 14680, 19484, 19426, 14689, 14725, 14608,
       14543, 20298, 20346, 18119, 18150, 23653, 23519, 21985, 21958,
       23601, 23735]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.5640625, 26.283203125, 19.403125, 26.3234375, 18.846875]}, 'saliency': {'score': [7.226806640625, 0.2014802971442741, 2.1825358072916665, 0.19154012932686332, 0.37657396721117425], 'topk_indices': array([19212, 19211, 24180, 24176,  4917, 19583,  2969, 19213, 24173,
       24177, 19217, 19214, 24167,  4919, 24193, 19216, 24179, 10294,
        2972, 24194]), 'topk_tokens': [' Mary', '?"', ' was', ' place', ' got', ' hall', ' travelled', ' put', ' prior', ' where', '.', ' down', 'Question', ' milk', '\n\n', ' milk', ' milk', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [5.67734375, 12.572265625, 1.83681640625, 15.434375, 1.68232421875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 05:53:54.228 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:53:54.229 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-51_pid-0_1-2-5-8-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 5, 8, 9):   1%|          | 1/100 [00:29<48:11, 29.20s/it]is_0k: False
your chose emoji: ['🖱️', '👨🏻\u200d🦯\u200d➡️', '🔊', '🔻', '🕶️', '🧑🏽\u200d🍼', '\U0001faf7🏿', '💆🏽', '🥈', '👩🏻\u200d❤\u200d👩🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.26s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.88s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
Processing depth (3, 4, 6, 8, 9):   1%|          | 1/100 [00:47<48:11, 29.20s/it]2025-01-22 05:54:12.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:54:12.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7485, 7490) --> . Mary travelled to the
2025-01-22 05:54:12.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 05:54:12.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9675, 9679) -->  Mary got the milk
2025-01-22 05:54:12.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:54:12.479 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14286, 14291) --> . Mary went to the
2025-01-22 05:54:12.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:54:12.576 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19252, 19257) -->  Mary put down the milk
2025-01-22 05:54:12.576 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:54:12.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5686, 5691) -->  back to the garden.
2025-01-22 05:54:12.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:54:12.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16840, 16846) --> . Daniel put down the football
2025-01-22 05:54:12.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 05:54:12.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5043, 5048) --> . Sandra picked up the
2025-01-22 05:54:12.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:54:12.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11274, 11278) -->  Sandra left the apple
2025-01-22 05:54:12.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the hallway.
2025-01-22 05:54:12.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10159, 10164) --> . Sandra moved to the
2025-01-22 05:54:12.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 05:54:12.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5683, 5689) --> . Sandra went back to the
2025-01-22 05:54:12.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the kitchen.
2025-01-22 05:54:12.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6238, 6244) --> . Daniel journeyed to the
2025-01-22 05:54:12.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel took the football.
2025-01-22 05:54:12.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8574, 8578) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:54:15.595 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:54:15.595 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24212])
2025-01-22 05:54:23.995 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [345.1015625, 15.739637105100144, 213.30381944444446, 15.117944007451873, 14.160677083333333], 'topk_indices': array([ 9679, 24191, 10113,    24, 10149, 14291, 19253, 24192, 24197,
       19256, 24210, 10114, 19257, 10164, 24207, 24211,  7490,     0,
       24208, 24212]), 'topk_tokens': ['.', ' prior', 'po', '\n\n', 'po', ' kitchen', ' put', ' to', ' milk', ' milk', '<|end_header_id|>', 'or', '.', ' hallway', '<|eot_id|>', '\n\n', ' hallway', '<|begin_of_text|>', '<|start_header_id|>', 'hall'], 'evidence_proportions': [392.4, 351.5, 254.975, 501.6, 226.3125]}, 'weight': {'score': [22.157552083333332, 23.446644641750982, 23.291666666666668, 23.448156437590562, 29.846458333333334], 'topk_indices': array([18821, 18865, 14726, 14690, 14771, 19466, 14735, 19524, 14654,
       14589, 20386, 20338, 18190, 18159, 23555, 23689, 21988, 22015,
       23637, 23771]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 26.283203125, 19.403125, 26.3234375, 18.846875]}, 'saliency': {'score': [2.0788981119791665, 0.08987919903727029, 1.3361375596788194, 0.08604555155667823, 0.10586649576822917], 'topk_indices': array([ 5684, 24195, 24199, 24185,  7487,  9678, 10149, 10113, 19253,
       10114, 10160, 24191, 19256, 14291, 24197,  5044, 24211, 10164,
        7490, 24212]), 'topk_tokens': [' Sandra', ' where', ' discarded', 'Question', ' travelled', ' milk', 'po', 'po', ' put', 'or', ' Sandra', ' prior', ' milk', ' kitchen', ' milk', ' Sandra', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [2.171484375, 2.37353515625, 1.3734375, 3.3181640625, 1.216796875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 05:54:24.024 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:54:24.025 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-51_pid-1_3-4-6-8-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (3, 4, 6, 8, 9):   2%|▏         | 2/100 [00:58<48:16, 29.55s/it]is_0k: False
your chose emoji: ['🤹\u200d♀️', '🧙\u200d♀️', '☺️', '🏃🏾\u200d➡️', '🧑🏻\u200d🚒', '🟦', '👨🏿\u200d💻', '🐴', '👨🏻\u200d❤️\u200d💋\u200d👨🏼', '👩🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.28s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.91s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
Processing depth (1, 2, 3, 4, 9):   2%|▏         | 2/100 [01:18<48:16, 29.55s/it]2025-01-22 05:54:44.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:54:44.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary travelled to
2025-01-22 05:54:44.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 05:54:44.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4952, 4956) -->  Mary got the milk
2025-01-22 05:54:44.169 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:54:44.229 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7595, 7600) -->  war. Mary went to
2025-01-22 05:54:44.229 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:54:44.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9793, 9798) --> . Mary put down the
2025-01-22 05:54:44.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:54:44.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6003, 6008) -->  back to the garden.
2025-01-22 05:54:44.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:54:44.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16902, 16908) --> . Daniel put down the football
2025-01-22 05:54:44.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 05:54:44.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5238, 5243) --> . Sandra picked up the
2025-01-22 05:54:44.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:54:44.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11376, 11380) -->  Sandra left the apple
2025-01-22 05:54:44.505 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the hallway.
2025-01-22 05:54:44.564 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10351, 10356) --> . Sandra moved to the
2025-01-22 05:54:44.564 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 05:54:44.613 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6001, 6007) -->  Sandra went back to the garden
2025-01-22 05:54:44.613 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the kitchen.
2025-01-22 05:54:44.646 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6325, 6331) --> . Daniel journeyed to the
2025-01-22 05:54:44.647 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel took the football.
2025-01-22 05:54:44.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8783, 8787) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:54:47.350 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:54:47.350 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24250])
2025-01-22 05:54:55.751 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [166.95247395833334, 7.698327268791489, 112.57204861111111, 7.3842879396933, 6.891362616356383], 'topk_indices': array([24249,    23,  9798,     1,  6331, 24230, 24224, 24139,    14,
        4955,  2960, 24235,  4956,  7601,    24, 24248, 10356,     0,
       24245, 24246]), 'topk_tokens': ['\n\n', '4', ' milk', '<|start_header_id|>', ' kitchen', ' to', ':', ' location', '\n', ' milk', ' hallway', ' milk', '.', ' kitchen', '\n\n', '<|end_header_id|>', ' hallway', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [150.56875, 260.5625, 134.278125, 164.1875, 143.8875]}, 'weight': {'score': [23.140950520833332, 23.45554673648621, 23.918619791666668, 23.455169754681105, 29.7016289893617], 'topk_indices': array([18841, 18885, 14696, 14660, 14741, 19480, 14705, 19538, 14559,
       14624, 20352, 20400, 18190, 18159, 23589, 23723, 22037, 22010,
       23671, 23805]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.5640625, 26.283203125, 22.8671875, 21.771875, 18.846875]}, 'saliency': {'score': [0.9959208170572916, 0.04359390421739063, 0.6973537868923612, 0.0416763577500465, 0.05143145297436004], 'topk_indices': array([ 6001, 18984,  2944, 24247,  2957, 24243,  5239, 10352, 24237,
          24,  9798, 24229, 24139, 24223,  6331,  4955, 24235,  7601,
        2960, 10356]), 'topk_tokens': [' Sandra', ' manner', 'lyn', 'assistant', ' travelled', 'Answer', ' Sandra', ' Sandra', ' discarded', '\n\n', ' milk', ' prior', ' location', 'Question', ' kitchen', ' milk', ' milk', ' kitchen', ' hallway', ' hallway'], 'evidence_proportions': [0.935009765625, 1.771484375, 0.759033203125, 0.918798828125, 0.750390625]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:54:55.756 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:54:55.757 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-51_pid-2_1-2-3-4-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (1, 2, 3, 4, 9):   3%|▎         | 3/100 [01:30<49:23, 30.55s/it]is_0k: False
your chose emoji: ['🤵🏿\u200d♂', '🏋🏼\u200d♂️', '💚', '👨🏼\u200d❤️\u200d💋\u200d👨🏼', '👨🏾\u200d❤\u200d💋\u200d👨🏿', '🦵🏽', '🗳️', '👨\u200d👩\u200d👧\u200d👦', '🥨', '🚴🏽\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.16s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.51s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.99s/it]
Processing depth (0, 2, 4, 6, 8):   3%|▎         | 3/100 [01:48<49:23, 30.55s/it]2025-01-22 05:55:14.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:55:14.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:55:14.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 05:55:14.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4952, 4956) -->  Mary got the milk
2025-01-22 05:55:14.114 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:55:14.164 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9680, 9685) -->  war. Mary went to
2025-01-22 05:55:14.165 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:55:14.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14334, 14339) --> . Mary put down the
2025-01-22 05:55:14.241 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:55:14.274 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6003, 6008) -->  back to the garden.
2025-01-22 05:55:14.275 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:55:14.365 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16808, 16814) --> . Daniel put down the football
2025-01-22 05:55:14.366 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 05:55:14.391 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5238, 5243) --> . Sandra picked up the
2025-01-22 05:55:14.391 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:55:14.447 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11324, 11328) -->  Sandra left the apple
2025-01-22 05:55:14.447 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the hallway.
2025-01-22 05:55:14.496 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10219, 10224) --> . Sandra moved to the
2025-01-22 05:55:14.496 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 05:55:14.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (6001, 6007) -->  Sandra went back to the garden
2025-01-22 05:55:14.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the kitchen.
2025-01-22 05:55:14.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6325, 6331) --> . Daniel journeyed to the
2025-01-22 05:55:14.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel took the football.
2025-01-22 05:55:14.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8641, 8645) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:55:17.259 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:55:17.259 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24154])
2025-01-22 05:55:25.612 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [267.2630208333333, 12.049136896137766, 157.33506944444446, 11.577898701083122, 9.396505604619565], 'topk_indices': array([10909, 10843,   185, 24133, 24128, 10888,  9686, 10912,    35,
        4956, 24134,    24,    34, 10911, 10889, 24152, 24149, 10224,
           0, 24150]), 'topk_tokens': [' as', 'ed', 'ION', ' prior', ':', '\n', ' kitchen', ' I', '.', '.', ' to', '\n\n', ' hallway', ' as', 'walk', '<|end_header_id|>', '<|eot_id|>', ' hallway', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [438.85, 346.4375, 180.0, 232.2, 154.6625]}, 'weight': {'score': [22.236979166666668, 23.43159591422776, 23.918619791666668, 23.43205812704901, 29.920516304347824], 'topk_indices': array([18805, 18761, 14654, 14618, 14663, 19472, 14699, 19414, 14517,
       14582, 20286, 20334, 18130, 18099, 23505, 23639, 21957, 21930,
       23721, 23587]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 26.283203125, 22.8671875, 21.771875, 18.846875]}, 'saliency': {'score': [1.591827392578125, 0.0682468855672657, 0.9644622802734375, 0.06539052641975013, 0.0707166920537534], 'topk_indices': array([10912,    38, 24043,    39, 10911, 10910, 24147,   185, 10220,
       10842, 24141,    31,  4955, 24139, 24127, 24133,  9686, 10889,
          34, 10224]), 'topk_tokens': [' I', '***', ' location', '\n\n\n', ' as', ' rapidly', 'Answer', 'ION', ' Sandra', 'walk', ' discarded', ' travelled', ' milk', ' milk', 'Question', ' prior', ' kitchen', 'walk', ' hallway', ' hallway'], 'evidence_proportions': [2.609765625, 2.3524169921875, 1.009521484375, 1.3357421875, 0.80380859375]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:55:25.619 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:55:25.619 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-51_pid-3_0-2-4-6-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 2, 4, 6, 8):   4%|▍         | 4/100 [02:00<48:26, 30.28s/it]is_0k: False
your chose emoji: ['📽️', '⏳', '👍', '🧍🏽\u200d♀', '🏃\u200d➡️', '😳', '🚵\u200d♂', '🙅🏾\u200d♀', '🐵', '💃']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.78s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.25s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.97s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.42s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]
Processing depth (0, 1, 6, 7, 9):   4%|▍         | 4/100 [02:18<48:26, 30.28s/it]2025-01-22 05:55:43.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:55:43.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:55:43.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary got the milk.
2025-01-22 05:55:43.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2962, 2966) -->  Mary got the milk
2025-01-22 05:55:43.901 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:55:43.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14551, 14556) -->  bonds. Mary went to
2025-01-22 05:55:43.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:55:44.056 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16904, 16909) --> . Mary put down the
2025-01-22 05:55:44.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:55:44.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5749, 5754) -->  back to the garden.
2025-01-22 05:55:44.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:55:44.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16953, 16959) -->  Daniel put down the football there
2025-01-22 05:55:44.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 05:55:44.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5238, 5243) --> . Sandra picked up the
2025-01-22 05:55:44.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:55:44.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11414, 11418) -->  Sandra left the apple
2025-01-22 05:55:44.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra moved to the hallway.
2025-01-22 05:55:44.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10349, 10354) --> . Sandra moved to the
2025-01-22 05:55:44.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 05:55:44.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (5746, 5752) --> . Sandra went back to the
2025-01-22 05:55:44.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel journeyed to the kitchen.
2025-01-22 05:55:44.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (6317, 6323) --> . Daniel journeyed to the
2025-01-22 05:55:44.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel took the football.
2025-01-22 05:55:44.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8683, 8687) -->  Daniel took the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:55:47.092 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 05:55:47.092 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24162])
2025-01-22 05:55:55.525 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [337.5130208333333, 13.867548624043037, 212.20486111111111, 13.24910288321925, 22.120625], 'topk_indices': array([24149,    31,    24, 24134,  2965, 10350,    35, 24135,    34,
       24147, 16909, 24136, 24141,  2966, 24142, 24160, 10354, 24157,
           0, 24158]), 'topk_tokens': [' discarded', ' travelled', '\n\n', '.\n\n', ' milk', ' Sandra', '.', 'Question', ' hallway', ' milk', ' milk', ':', ' prior', '.', ' to', '<|end_header_id|>', ' hallway', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [524.15, 407.6875, 205.95, 395.45, 168.3625]}, 'weight': {'score': [22.43359375, 23.431605627974342, 23.71462673611111, 23.432176610143124, 29.4034375], 'topk_indices': array([18781, 18737, 14590, 14626, 19454, 14671, 14635, 19396, 14483,
       14548, 20268, 20316, 18075, 18106, 23503, 23637, 21924, 21951,
       23585, 23719]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 26.283203125, 23.8109375, 21.771875, 18.846875]}, 'saliency': {'score': [2.0015665690104165, 0.07851055326013863, 1.2888387044270833, 0.07478828991975213, 0.1631602478027344], 'topk_indices': array([16906,    30, 24007,  6319,  5239, 24144,     0, 24145, 16907,
       14557,  2965,    31, 24149, 24147, 16909, 10350, 24135, 24141,
          34, 10354]), 'topk_tokens': [' put', 'Mary', ' context', ' journey', ' Sandra', ' place', '<|begin_of_text|>', ' where', ' down', ' kitchen', ' milk', ' travelled', ' discarded', ' milk', ' milk', ' Sandra', 'Question', ' prior', ' hallway', ' hallway'], 'evidence_proportions': [3.098828125, 2.7421875, 1.2216796875, 2.2392578125, 0.85400390625]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:55:55.533 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:55:55.533 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-51_pid-4_0-1-6-7-9.pkl | len: 3 |  size: 2.12 KB
Processing depth (0, 1, 6, 7, 9):   5%|▌         | 5/100 [02:30<47:43, 30.15s/it]Processing depth (0, 1, 6, 7, 9):   5%|▌         | 5/100 [02:30<47:45, 30.17s/it]
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:82 - Selected idx: 52
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the kitchen? 
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:86 - Needle: [' Sandra moved to the hallway.', ' Sandra went back to the garden.', ' Sandra left the apple.', ' Mary travelled to the hallway.', ' Daniel took the football.', ' Daniel put down the football there.', ' Sandra picked up the apple.', ' Mary went to the kitchen.', ' Daniel journeyed to the kitchen.', ' Mary put down the apple.', ' Daniel went to the garden.']
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the hallway.', ' Mary went to the kitchen.', ' Mary put down the apple.', ' Daniel went to the garden.']
2025-01-22 05:55:55.862 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🎋', '⚧', '♨️', '⏰', '🙍🏾\u200d♂️', '🚝', '🤰🏾', '💻', '👨🏼\u200d🦱', '💇🏽\u200d♀']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
Processing depth (0, 1, 5, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 05:56:13.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:56:13.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:56:13.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:56:13.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2966, 2971) -->  tragedy. Mary went to
2025-01-22 05:56:13.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:56:13.695 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11979, 11984) --> . Mary put down the
2025-01-22 05:56:13.695 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:56:13.797 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19333, 19338) --> . Daniel went to the
2025-01-22 05:56:13.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 05:56:13.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (21989, 21994) -->  state. Sandra moved to
2025-01-22 05:56:13.907 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 05:56:14.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20093, 20099) --> . Sandra went back to the
2025-01-22 05:56:14.019 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:56:14.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9145, 9149) -->  Sandra left the apple
2025-01-22 05:56:14.071 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:56:14.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3908, 3912) -->  Daniel took the football
2025-01-22 05:56:14.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel put down the football there.
2025-01-22 05:56:14.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17662, 17668) --> . Daniel put down the football
2025-01-22 05:56:14.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:56:14.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9144, 9149) --> . Sandra left the apple
2025-01-22 05:56:14.240 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel journeyed to the kitchen.
2025-01-22 05:56:14.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (17531, 17537) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:56:17.038 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:56:17.038 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24162])
2025-01-22 05:56:25.398 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [169.90859375, 9.226202669149597, 141.6796875, 8.895124097847276, 9.883326480263158], 'topk_indices': array([24149, 17491, 17501,    25, 24050,     9, 24152, 24153,    23,
           1, 24150, 17492,    24,    14, 17502, 21995, 24160, 24157,
       24158,     0]), 'topk_tokens': [' apple', 'arp', 'arp', '<|eot_id|>', '.\n\n', ':', ' kitchen', '?', '4', '<|start_header_id|>', ' before', 'ente', '\n\n', '\n', 'ente', ' hallway', '<|end_header_id|>', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [287.1, 113.909375, 153.1375, 125.4875]}, 'weight': {'score': [22.218359375, 23.43462135319677, 23.825303819444443, 23.435046948235097, 29.18174342105263], 'topk_indices': array([18745, 18789, 14575, 14611, 19390, 19448, 14656, 14620, 14521,
       14456, 20287, 20335, 18077, 18108, 23516, 23650, 21949, 21976,
       23732, 23598]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 25.2109375, 21.771875, 19.665625]}, 'saliency': {'score': [0.960888671875, 0.05207730685359766, 0.9147169325086806, 0.05003527882157597, 0.07458723637095668], 'topk_indices': array([24144,    31, 24131,    20,    38,     8, 24153,  9145,    23,
       24146,    24, 24149, 17501, 17491,    34, 24150, 24152, 17492,
       17502, 21995]), 'topk_tokens': ['Question', ' travelled', ' return', ' Jul', '***', ' Date', '?', ' Sandra', '4', ' Where', '\n\n', ' apple', 'arp', 'arp', ' hallway', ' before', ' kitchen', 'ente', 'ente', ' hallway'], 'evidence_proportions': [1.70908203125, 0.617333984375, 0.851171875, 0.665966796875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:56:25.413 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:56:25.414 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-52_pid-0_0-1-5-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 1, 5, 8):   1%|          | 1/100 [00:29<48:31, 29.41s/it]is_0k: False
your chose emoji: ['🧎🏼\u200d♂️', '🪧', '🦓', '🏖', '🧏🏻\u200d♂', '🦻🏿', '🙇🏽\u200d♂', '🏯', '👨🏾', '💆🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.90s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (3, 5, 7, 8):   1%|          | 1/100 [00:46<48:31, 29.41s/it]2025-01-22 05:56:43.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:56:43.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7583, 7588) -->  war. Mary travelled to
2025-01-22 05:56:43.096 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:56:43.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11849, 11854) --> . Mary went to the
2025-01-22 05:56:43.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:56:43.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16771, 16776) --> . Mary put down the
2025-01-22 05:56:43.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:56:43.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19224, 19229) -->  Daniel went to the garden
2025-01-22 05:56:43.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 05:56:43.436 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22043, 22048) -->  state. Sandra moved to
2025-01-22 05:56:43.436 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 05:56:43.534 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20027, 20033) --> . Sandra went back to the
2025-01-22 05:56:43.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:56:43.578 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9139, 9143) -->  Sandra left the apple
2025-01-22 05:56:43.578 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:56:43.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3890, 3894) -->  Daniel took the football
2025-01-22 05:56:43.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel put down the football there.
2025-01-22 05:56:43.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17560, 17566) --> . Daniel put down the football
2025-01-22 05:56:43.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:56:43.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9138, 9143) --> . Sandra left the apple
2025-01-22 05:56:43.751 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel journeyed to the kitchen.
2025-01-22 05:56:43.837 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (17467, 17473) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:56:46.474 | INFO     | test_jbb_embedding:begin_test:693 - The garden<|eot_id|>
2025-01-22 05:56:46.474 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24212])
2025-01-22 05:56:54.818 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [131.0796875, 7.873646887259963, 160.34071180555554, 7.5444554824289085, 11.168981040396341], 'topk_indices': array([    9, 24211, 24194, 24208, 24181, 24100, 24193, 24195, 24203,
       24202, 24200,     1,    14, 22049,    23,    24,  9139, 24210,
           0, 24207]), 'topk_tokens': [':', '\n\n', 'Question', '<|start_header_id|>', ' return', '.\n\n', '.\n\n', ':', '?', ' kitchen', ' before', '<|start_header_id|>', '\n', ' hallway', '4', '\n\n', ' Sandra', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [156.53125, 126.725, 115.9125, 125.15]}, 'weight': {'score': [22.39375, 23.449912244476565, 23.825303819444443, 23.45022720569974, 29.693025914634145], 'topk_indices': array([18793, 18837, 14593, 14629, 14674, 14638, 19496, 19438, 14557,
       14492, 20365, 20317, 18131, 18162, 23570, 23704, 22003, 22030,
       23786, 23652]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [24.2203125, 19.403125, 21.771875, 24.1796875]}, 'saliency': {'score': [0.776263427734375, 0.04476500595253458, 1.0829603407118056, 0.042612391999681795, 0.08323781083269817], 'topk_indices': array([    8,    20,  9142, 24100, 24193, 24110, 24205, 24209, 24199,
       24203,    24,    23, 24066, 24181, 24200, 24194,  7589, 24202,
       22049,  9139]), 'topk_tokens': [' Date', ' Jul', ' apple', '.\n\n', '.\n\n', ' location', 'Answer', 'assistant', ' apple', '?', '\n\n', '4', ' context', ' return', ' before', 'Question', ' hallway', ' kitchen', ' hallway', ' Sandra'], 'evidence_proportions': [0.9820556640625, 0.6638671875, 0.64833984375, 0.810791015625]}}, 'pred_res': 'The garden<|eot_id|>', 'score': 0}
2025-01-22 05:56:54.825 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:56:54.826 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-52_pid-1_3-5-7-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (3, 5, 7, 8):   2%|▏         | 2/100 [00:58<48:02, 29.41s/it]is_0k: False
your chose emoji: ['👩🏾\u200d❤\u200d💋\u200d👨🏼', '👩\u200d❤\u200d👨', '🧑🏼\u200d⚖', '🌈', '🕉', '🙅🏼\u200d♀', '👨🏿\u200d❤️\u200d👨🏼', '🧑🏽\u200d🎨', '🙂', '☘️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.73s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.64s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]
Processing depth (1, 3, 7, 8):   2%|▏         | 2/100 [01:15<48:02, 29.41s/it]2025-01-22 05:57:12.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:57:12.171 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Mary travelled to the
2025-01-22 05:57:12.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:57:12.212 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7575, 7580) -->  war. Mary went to
2025-01-22 05:57:12.212 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:57:12.301 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16777, 16782) --> . Mary put down the
2025-01-22 05:57:12.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:57:12.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19258, 19263) -->  Daniel went to the garden
2025-01-22 05:57:12.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 05:57:12.527 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22048, 22053) --> . Sandra moved to the
2025-01-22 05:57:12.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 05:57:12.639 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20097, 20103) --> . Sandra went back to the
2025-01-22 05:57:12.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:57:12.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9103, 9107) -->  Sandra left the apple
2025-01-22 05:57:12.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:57:12.707 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3896, 3900) -->  Daniel took the football
2025-01-22 05:57:12.707 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel put down the football there.
2025-01-22 05:57:12.804 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17566, 17572) --> . Daniel put down the football
2025-01-22 05:57:12.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:57:12.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20594, 20599) --> . Sandra picked up the
2025-01-22 05:57:12.920 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel journeyed to the kitchen.
2025-01-22 05:57:13.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (17489, 17495) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:57:15.704 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:57:15.705 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24178])
2025-01-22 05:57:24.112 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [98.65078125, 7.827056237335098, 120.79600694444444, 7.583186528497409, 14.97578125], 'topk_indices': array([24168, 24148,  9103,     4, 24169, 24161, 24177, 24172, 24147,
       24159,    24,     9,     1, 24066,    23, 22053,    14,     0,
       24176, 24173]), 'topk_tokens': [' kitchen', ' the', ' Sandra', '\n\n', '?', ':', '\n\n', ':', ' return', '.\n\n', '\n\n', ':', '<|start_header_id|>', '.\n\n', '4', ' hallway', '\n', '<|begin_of_text|>', '<|end_header_id|>', '<|eot_id|>'], 'evidence_proportions': [98.4, 80.790625, 123.15, 92.2625]}, 'weight': {'score': [22.39375, 23.439482444894754, 23.291666666666668, 23.44056994818653, 29.38605769230769], 'topk_indices': array([18767, 18811, 14597, 14561, 14642, 19412, 14606, 19470, 14460,
       14525, 20291, 20339, 18136, 18105, 23536, 23670, 21980, 21953,
       23752, 23618]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.75625, 22.8671875, 21.771875, 24.1796875]}, 'saliency': {'score': [0.566131591796875, 0.044419120585700037, 0.774017333984375, 0.04289788596988342, 0.10784020057091347], 'topk_indices': array([   20,    14, 24169, 24166, 24159,    24, 24175, 24032, 24076,
       24165,     8,    23, 24171, 24066, 24160,  9106, 24168, 24147,
        9103, 22053]), 'topk_tokens': [' Jul', '\n', '?', ' before', '.\n\n', '\n\n', 'assistant', ' context', ' location', ' apple', ' Date', '4', 'Answer', '.\n\n', 'Question', ' apple', ' kitchen', ' return', ' Sandra', ' hallway'], 'evidence_proportions': [0.558251953125, 0.4533935546875, 0.660595703125, 0.59228515625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:57:24.118 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:57:24.118 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-52_pid-2_1-3-7-8.pkl | len: 3 |  size: 1.99 KB
Processing depth (1, 3, 7, 8):   3%|▎         | 3/100 [01:28<47:27, 29.36s/it]is_0k: False
your chose emoji: ['🙇🏼\u200d♀', '🇧🇿', '🗄️', '🇻🇬', '👷🏻\u200d♀', '☣', '🧑🏿', '👒', '👩🏽\u200d❤️\u200d💋\u200d👩🏾', '🔩']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
Processing depth (0, 5, 7, 9):   3%|▎         | 3/100 [01:45<47:27, 29.36s/it]2025-01-22 05:57:41.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:57:41.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:57:41.894 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:57:41.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11913, 11918) --> . Mary went to the
2025-01-22 05:57:41.952 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:57:42.042 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16783, 16788) --> . Mary put down the
2025-01-22 05:57:42.042 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:57:42.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20094, 20099) -->  back to the garden.
2025-01-22 05:57:42.151 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 05:57:42.274 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22005, 22010) -->  state. Sandra moved to
2025-01-22 05:57:42.274 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 05:57:42.391 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20091, 20097) --> . Sandra went back to the
2025-01-22 05:57:42.392 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:57:42.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9151, 9155) -->  Sandra left the apple
2025-01-22 05:57:42.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:57:42.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3906, 3910) -->  Daniel took the football
2025-01-22 05:57:42.456 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel put down the football there.
2025-01-22 05:57:42.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17590, 17596) --> . Daniel put down the football
2025-01-22 05:57:42.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:57:42.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9150, 9155) --> . Sandra left the apple
2025-01-22 05:57:42.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel journeyed to the kitchen.
2025-01-22 05:57:42.690 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (17541, 17547) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:57:45.401 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:57:45.401 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 05:57:53.764 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [150.496875, 8.09699069286453, 112.21440972222223, 7.823504032090883, 11.38689201108871], 'topk_indices': array([24171, 24060, 18910,     9, 24154, 24168, 24155, 24153, 18911,
       24162, 24163, 24160,     1,    23,    24, 22011,    14, 24170,
           0, 24167]), 'topk_tokens': ['\n\n', '.\n\n', ' manner', ':', 'Question', '<|start_header_id|>', ':', '.\n\n', ' in', ' kitchen', '?', ' before', '<|start_header_id|>', '4', '\n\n', ' hallway', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [206.85, 171.825, 121.4875, 101.825]}, 'weight': {'score': [20.56171875, 23.438304033092038, 23.825303819444443, 23.44011172467764, 29.43623991935484], 'topk_indices': array([18767, 18811, 14635, 14599, 19406, 14680, 14644, 19464, 14498,
       14563, 20285, 20333, 18124, 18093, 23666, 23532, 21992, 21965,
       23748, 23614]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [0.819647216796875, 0.04585548692799896, 0.7277747260199653, 0.044196009868225, 0.08482533116494456], 'topk_indices': array([ 9151, 11918,    20, 24141, 24026, 24159,     0, 24169, 24153,
           8,    23, 24163,    24,    34, 20681, 24160, 18910, 24154,
       24162, 22011]), 'topk_tokens': [' Sandra', ' kitchen', ' Jul', ' return', ' context', ' apple', '<|begin_of_text|>', 'assistant', '.\n\n', ' Date', '4', '?', '\n\n', ' hallway', 'nes', ' before', ' manner', 'Question', ' kitchen', ' hallway'], 'evidence_proportions': [1.24013671875, 0.833642578125, 0.66845703125, 0.5363525390625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:57:53.773 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:57:53.773 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-52_pid-3_0-5-7-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 5, 7, 9):   4%|▍         | 4/100 [01:57<47:09, 29.47s/it]is_0k: False
your chose emoji: ['🧘🏿\u200d♂', '🤾🏼\u200d♀', '🤷🏽\u200d♂️', '🌗', '🦭', '🧛🏾\u200d♀', '🧗🏽\u200d♂️', '🗓', '👬🏽', '♒']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.87s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.35s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.97s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.40s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]
Processing depth (0, 3, 5, 9):   4%|▍         | 4/100 [02:15<47:09, 29.47s/it]2025-01-22 05:58:12.074 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:58:12.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:58:12.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went to the kitchen.
2025-01-22 05:58:12.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7448, 7453) --> . Mary went to the
2025-01-22 05:58:12.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary put down the apple.
2025-01-22 05:58:12.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11827, 11832) --> . Mary put down the
2025-01-22 05:58:12.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the garden.
2025-01-22 05:58:12.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20026, 20031) -->  back to the garden.
2025-01-22 05:58:12.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 05:58:12.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22029, 22034) -->  state. Sandra moved to
2025-01-22 05:58:12.396 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 05:58:12.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20023, 20029) --> . Sandra went back to the
2025-01-22 05:58:12.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 05:58:12.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9037, 9041) -->  Sandra left the apple
2025-01-22 05:58:12.557 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel took the football.
2025-01-22 05:58:12.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3850, 3854) -->  Daniel took the football
2025-01-22 05:58:12.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel put down the football there.
2025-01-22 05:58:12.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17566, 17572) --> . Daniel put down the football
2025-01-22 05:58:12.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:58:12.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (20572, 20577) --> . Sandra picked up the
2025-01-22 05:58:12.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Daniel journeyed to the kitchen.
2025-01-22 05:58:12.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (17489, 17495) --> . Daniel journeyed to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:58:15.550 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 05:58:15.550 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 05:58:23.934 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [310.284375, 12.191872236684434, 157.99565972222223, 11.727561348105198, 14.735455729166667], 'topk_indices': array([    1,  9037, 24086, 24188,    23,    32,    31,  7936, 24186,
          24,    14, 22035, 24194,  7937,  7987, 24196,    34, 24193,
        7988,     0]), 'topk_tokens': ['<|start_header_id|>', ' Sandra', '.\n\n', ' kitchen', '4', ' to', ' travelled', 'nes', ' before', '\n\n', '\n', ' hallway', '<|start_header_id|>', 'ot', 'nes', '<|end_header_id|>', ' hallway', '<|eot_id|>', 'ot', '<|begin_of_text|>'], 'evidence_proportions': [721.3, 231.55, 145.325, 142.9625]}, 'weight': {'score': [20.56171875, 23.44242489979753, 23.71506076388889, 23.44440457392835, 29.061458333333334], 'topk_indices': array([18811, 18767, 14665, 14629, 14710, 14674, 19464, 19406, 14593,
       14528, 20313, 20361, 18136, 18105, 23690, 23556, 22016, 21989,
       23638, 23772]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [1.9149658203125, 0.0694252368187265, 1.0243089463975694, 0.06647279841679178, 0.10765238444010417], 'topk_indices': array([   20,    23, 24182,    30,  9040, 24167,    24, 24052, 24185,
         263, 24186, 24188,  7936,  9037,    31,  7937, 22035,  7987,
        7988,    34]), 'topk_tokens': [' Jul', '4', ' Where', 'Mary', ' apple', ' return', '\n\n', ' context', ' apple', 'hue', ' before', ' kitchen', 'nes', ' Sandra', ' travelled', 'ot', ' hallway', 'nes', 'ot', ' hallway'], 'evidence_proportions': [4.8978515625, 1.17861328125, 0.827734375, 0.7556640625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 05:58:23.943 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:58:23.943 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-52_pid-4_0-3-5-9.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 3, 5, 9):   5%|▌         | 5/100 [02:27<47:03, 29.73s/it]Processing depth (0, 3, 5, 9):   5%|▌         | 5/100 [02:28<46:56, 29.65s/it]
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:82 - Selected idx: 53
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:86 - Needle: [' Daniel put down the football there.', ' Mary travelled to the hallway.', ' Sandra left the apple.', ' Mary grabbed the milk.', ' Sandra went back to the garden.', ' Daniel journeyed to the kitchen.', ' Daniel took the football.', ' Sandra picked up the apple.', ' Mary went to the kitchen.', ' Sandra moved to the hallway.', ' Mary put down the milk.', ' Daniel went to the garden.']
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:87 - Real Needle: [' Mary travelled to the hallway.', ' Mary grabbed the milk.', ' Mary went to the kitchen.', ' Mary put down the milk.', ' Daniel went to the garden.']
2025-01-22 05:58:24.266 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👩🏻\u200d🤝\u200d👨🏾', '◾', '✊🏻', '👩🏽\u200d🦯\u200d➡️', '⛓', '👨🏼\u200d🦱', '🏄🏼\u200d♀', '👨🏽\u200d🎨', '🖕', '🪆']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.42s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.79s/it]
Processing depth (0, 1, 4, 6, 8):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 05:58:41.485 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:58:41.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:58:41.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the milk.
2025-01-22 05:58:41.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2976, 2980) -->  Mary grabbed the milk
2025-01-22 05:58:41.500 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:58:41.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9692, 9697) --> . Mary went to the
2025-01-22 05:58:41.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:58:41.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14330, 14335) --> . Mary put down the
2025-01-22 05:58:41.625 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:58:41.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7800, 7805) -->  back to the garden.
2025-01-22 05:58:41.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:58:41.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12990, 12996) --> . Daniel put down the football
2025-01-22 05:58:41.735 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra left the apple.
2025-01-22 05:58:41.771 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6917, 6921) -->  Sandra left the apple
2025-01-22 05:58:41.771 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 05:58:41.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7796, 7802) -->  city. Sandra went back to
2025-01-22 05:58:41.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 05:58:41.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15749, 15755) -->  engagement. Daniel journeyed to
2025-01-22 05:58:41.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel took the football.
2025-01-22 05:58:41.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (13614, 13618) -->  Daniel took the football
2025-01-22 05:58:41.964 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:58:42.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9348, 9353) --> . Sandra picked up the
2025-01-22 05:58:42.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the hallway.
2025-01-22 05:58:42.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (7924, 7929) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:58:44.726 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:58:44.726 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24186])
2025-01-22 05:58:53.123 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [389.0026041666667, 19.679809262474677, 153.87131076388889, 19.112249847175597, 10.695816532258064], 'topk_indices': array([   14,  1646, 17567, 17558, 17579,    38, 17585, 17576, 17586,
       17559,  1725,  1647, 17560, 17587,     0, 17577, 17588, 24181,
       17578, 24182]), 'topk_tokens': ['\n', "'s", ' The', 'arp', 'ur', '***', '.', ' L', ' L', 'ente', "'s", ' work', 'ur', 'arp', '<|begin_of_text|>', 'arp', 'ente', '<|eot_id|>', 'ente', '<|start_header_id|>'], 'evidence_proportions': [603.1, 417.8125, 325.6, 396.05, 208.2125]}, 'weight': {'score': [21.790690104166668, 23.43833715738559, 24.39409722222222, 23.438550020203905, 29.5078125], 'topk_indices': array([18859, 18815, 14674, 14710, 19518, 14755, 19460, 14719, 14573,
       14638, 20380, 20332, 18149, 18180, 23537, 23671, 22003, 21976,
       23753, 23619]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 27.935546875, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [2.2729695638020835, 0.11107999762288644, 0.9910829332139757, 0.10761671877711662, 0.08007123393397178], 'topk_indices': array([ 1645,  1646,    39, 17576,    34, 17586,  7929, 17579, 17575,
        1724,  1725, 17558,    38,  1647, 17559, 17560, 17587, 17577,
       17588, 17578]), 'topk_tokens': [' summer', "'s", '\n\n\n', ' L', ' hallway', ' L', ' hallway', 'ur', ' Rosa', ' summer', "'s", 'arp', '***', ' work', 'ente', 'ur', 'arp', 'arp', 'ente', 'ente'], 'evidence_proportions': [3.4365234375, 3.0546875, 1.89228515625, 2.1251953125, 1.0125]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:58:53.170 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:58:53.171 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-53_pid-0_0-1-4-6-8.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 1, 4, 6, 8):   1%|          | 1/100 [00:28<47:26, 28.76s/it]is_0k: False
your chose emoji: ['🗳️', '🧗🏻\u200d♂️', '👨🏻\u200d🦽', '🤲🏼', '☃', '🤷🏻\u200d♂️', '🧖🏽\u200d♀️', '⛹🏼', '🪳', '💤']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.50s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.92s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.95s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.45s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
Processing depth (1, 4, 6, 8, 9):   1%|          | 1/100 [00:46<47:26, 28.76s/it]2025-01-22 05:59:11.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:59:11.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary travelled to
2025-01-22 05:59:11.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the milk.
2025-01-22 05:59:11.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9690, 9694) -->  Mary grabbed the milk
2025-01-22 05:59:11.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:59:11.705 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14370, 14375) --> . Mary went to the
2025-01-22 05:59:11.705 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:59:11.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19262, 19267) -->  Mary put down the milk
2025-01-22 05:59:11.800 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:59:11.838 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7763, 7768) -->  back to the garden.
2025-01-22 05:59:11.838 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:59:11.904 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13008, 13014) --> . Daniel put down the football
2025-01-22 05:59:11.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra left the apple.
2025-01-22 05:59:11.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6880, 6884) -->  Sandra left the apple
2025-01-22 05:59:11.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 05:59:11.976 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7759, 7765) -->  city. Sandra went back to
2025-01-22 05:59:11.976 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 05:59:12.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15794, 15800) --> . Daniel journeyed to the
2025-01-22 05:59:12.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel took the football.
2025-01-22 05:59:12.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (13782, 13786) -->  Daniel took the football
2025-01-22 05:59:12.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:59:12.170 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9421, 9426) --> . Sandra picked up the
2025-01-22 05:59:12.170 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the hallway.
2025-01-22 05:59:12.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (7887, 7892) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:59:14.899 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 05:59:14.899 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24196])
2025-01-22 05:59:23.330 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [483.4088541666667, 21.625834228687136, 283.03819444444446, 20.77684970379883, 21.367595615671643], 'topk_indices': array([24178, 24169, 24075,  9694, 19299,  2961, 24179, 24182, 19266,
       17685, 24181, 19267, 24194, 24191,     0, 24192,  7892,  2960,
       24195, 24196]), 'topk_tokens': [' place', 'Question', '.\n\n', '.', ' hall', '.', ' where', ' was', ' milk', 'ison', ' milk', '.', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', ' hallway', ' hallway', '\n\n', 'hall'], 'evidence_proportions': [490.2375, 500.0, 297.15, 781.7, 351.275]}, 'weight': {'score': [23.642903645833332, 23.438039795032854, 23.762803819444443, 23.43735176995733, 29.00722947761194], 'topk_indices': array([18815, 18771, 14614, 14650, 19474, 19416, 14695, 14659, 14513,
       14578, 20336, 20288, 18140, 18109, 23653, 23519, 21950, 21977,
       23735, 23601]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.5640625, 27.935546875, 19.403125, 26.3234375, 18.846875]}, 'saliency': {'score': [2.9748128255208335, 0.12396220813295487, 1.7649078369140625, 0.11868052879854049, 0.1575704546117071], 'topk_indices': array([22697, 19264, 24175, 24178,  9693, 19573,  2957, 24193, 14375,
       24179, 19299,  6880, 24169, 17685, 19266, 24181, 24195,  7892,
        2960, 24196]), 'topk_tokens': [' hall', ' down', ' prior', ' place', ' milk', ' hall', ' travelled', 'assistant', ' kitchen', ' where', ' hall', ' Sandra', 'Question', 'ison', ' milk', ' milk', '\n\n', ' hallway', ' hallway', 'hall'], 'evidence_proportions': [2.920703125, 3.594970703125, 1.53701171875, 5.117578125, 1.82783203125]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 05:59:23.347 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:59:23.348 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-53_pid-1_1-4-6-8-9.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 4, 6, 8, 9):   2%|▏         | 2/100 [00:58<48:20, 29.59s/it]is_0k: False
your chose emoji: ['👼', '🛢️', '🧔', '🧑🏽\u200d🎓', '🙉', '👨🏾\u200d🦰', '🇲🇾', '🤽🏽\u200d♂', '🌠', '👨🏼\u200d❤️\u200d👨🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.77s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.87s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.04s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
Processing depth (0, 1, 3, 8, 9):   2%|▏         | 2/100 [01:19<48:20, 29.59s/it]2025-01-22 05:59:44.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 05:59:44.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 05:59:44.055 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the milk.
2025-01-22 05:59:44.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3038, 3042) -->  Mary grabbed the milk
2025-01-22 05:59:44.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 05:59:44.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7625, 7630) --> . Mary went to the
2025-01-22 05:59:44.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 05:59:44.230 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19351, 19356) --> . Mary put down the
2025-01-22 05:59:44.231 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 05:59:44.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7882, 7887) -->  back to the garden.
2025-01-22 05:59:44.272 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 05:59:44.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13012, 13018) --> . Daniel put down the football
2025-01-22 05:59:44.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra left the apple.
2025-01-22 05:59:44.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6976, 6980) -->  left the apple.
2025-01-22 05:59:44.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 05:59:44.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7879, 7885) --> . Sandra went back to the
2025-01-22 05:59:44.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 05:59:44.526 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15792, 15798) --> . Daniel journeyed to the
2025-01-22 05:59:44.526 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel took the football.
2025-01-22 05:59:44.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (13786, 13790) -->  Daniel took the football
2025-01-22 05:59:44.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 05:59:44.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9510, 9515) --> . Sandra picked up the
2025-01-22 05:59:44.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the hallway.
2025-01-22 05:59:44.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8056, 8061) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 05:59:47.951 | INFO     | test_jbb_embedding:begin_test:693 - kitchen<|eot_id|>
2025-01-22 05:59:47.951 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24214])
2025-01-22 05:59:56.687 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [320.875, 12.602877111120288, 139.44965277777777, 12.107574926522332, 8.499338250411185], 'topk_indices': array([   29,  7410, 24212,  7432,  7476,    30, 24193,   185,  7630,
       24188,    34, 24194,    31,    35,  3042,  7485, 24210,  8061,
       24209,     0]), 'topk_tokens': ['\n\n', 'nes', '<|end_header_id|>', 'graph', ' tele', 'Mary', ' prior', 'ION', ' kitchen', ':', ' hallway', ' to', ' travelled', '.', '.', ' or', '<|start_header_id|>', ' hallway', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [572.4, 455.5625, 255.425, 244.9, 103.025]}, 'weight': {'score': [21.790690104166668, 23.445064417557912, 22.45551215277778, 23.448182721364407, 29.461143092105264], 'topk_indices': array([18807, 18763, 14612, 14648, 19466, 14693, 14657, 19408, 14576,
       14511, 20296, 20344, 18081, 18112, 23697, 23563, 22029, 22002,
       23645, 23779]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 27.935546875, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [1.9349924723307292, 0.0721200045987194, 0.7902069091796875, 0.06919910929754444, 0.0629627830103824], 'topk_indices': array([ 7475,  7365,  3038, 24201,   185,  7409,  7431,  3041,  7410,
        7485, 24187,  3039,  7432, 24193,    30,  7630,  7476,    31,
          34,  8061]), 'topk_tokens': [' bogus', 'nes', ' Mary', ' discarded', 'ION', ' Min', ' tele', ' milk', 'nes', ' or', 'Question', ' grabbed', 'graph', ' prior', 'Mary', ' kitchen', ' tele', ' travelled', ' hallway', ' hallway'], 'evidence_proportions': [3.40087890625, 3.3897705078125, 1.30107421875, 1.35908203125, 0.5151123046875]}}, 'pred_res': 'kitchen<|eot_id|>', 'score': 0}
2025-01-22 05:59:56.720 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 05:59:56.721 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-53_pid-2_0-1-3-8-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 3, 8, 9):   3%|▎         | 3/100 [01:32<50:37, 31.32s/it]is_0k: False
your chose emoji: ['👨🏻\u200d❤\u200d💋\u200d👨🏽', '📽️', '🧑🏽\u200d🦱', '🧘🏻\u200d♀', '↖', '🧍🏼\u200d♀', '🧑🏻\u200d🦲', '⛹️', '🙍🏼\u200d♂', '🧛']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.43s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.99s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.39s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]
Processing depth (0, 4, 5, 6, 9):   3%|▎         | 3/100 [01:50<50:37, 31.32s/it]2025-01-22 06:00:15.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 06:00:15.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  travelled to the hallway.
2025-01-22 06:00:15.121 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the milk.
2025-01-22 06:00:15.173 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9860, 9864) -->  Mary grabbed the milk
2025-01-22 06:00:15.174 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 06:00:15.232 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11900, 11905) --> . Mary went to the
2025-01-22 06:00:15.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 06:00:15.309 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14422, 14427) --> . Mary put down the
2025-01-22 06:00:15.310 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 06:00:15.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7953, 7958) -->  back to the garden.
2025-01-22 06:00:15.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 06:00:15.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13050, 13056) --> . Daniel put down the football
2025-01-22 06:00:15.432 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra left the apple.
2025-01-22 06:00:15.467 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6969, 6973) -->  left the apple.
2025-01-22 06:00:15.467 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 06:00:15.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7949, 7955) -->  sell. Sandra went back to
2025-01-22 06:00:15.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 06:00:15.588 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15776, 15782) --> . Daniel journeyed to the
2025-01-22 06:00:15.589 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel took the football.
2025-01-22 06:00:15.654 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (13788, 13792) -->  Daniel took the football
2025-01-22 06:00:15.655 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:00:15.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9589, 9594) --> . Sandra picked up the
2025-01-22 06:00:15.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the hallway.
2025-01-22 06:00:15.741 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8097, 8102) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:00:18.443 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 06:00:18.443 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24188])
2025-01-22 06:00:26.806 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [293.2395833333333, 14.144751663841925, 183.57552083333334, 13.614405484646305, 12.212053571428571], 'topk_indices': array([11905,    29, 24167, 11932,    34,    35, 11931,    14, 24168,
        7402, 24187, 24067,  7403,    24, 24186,  7358,  8102,     0,
       24183, 24184]), 'topk_tokens': [' kitchen', '\n\n', ' prior', 'IC', ' hallway', '.', 'ANT', '\n', ' to', ' Min', '\n\n', '.\n\n', 'nes', '\n\n', '<|end_header_id|>', 'nes', ' hallway', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [504.7, 344.71875, 205.15, 228.5, 193.425]}, 'weight': {'score': [21.790690104166668, 23.43877888470919, 23.006944444444443, 23.441062261717292, 29.496279761904763], 'topk_indices': array([18797, 18841, 14590, 14626, 14635, 19494, 19436, 14671, 14554,
       14489, 20320, 20368, 18109, 18140, 23531, 23665, 21970, 21997,
       23613, 23747]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.225, 27.935546875, 19.403125, 21.771875, 18.846875]}, 'saliency': {'score': [1.7130330403645833, 0.07956381089719576, 1.0608876546223958, 0.07647521449085849, 0.08831048390221974], 'topk_indices': array([ 6971,  9863, 11975, 14427, 24173, 24067,    38, 24161, 11932,
          31, 11931, 11905, 24175, 24167,  7403,  7402,  7358,  6968,
          34,  8102]), 'topk_tokens': [' apple', ' milk', 'ANT', ' milk', ' milk', '.\n\n', '***', 'Question', 'IC', ' travelled', 'ANT', ' kitchen', ' discarded', ' prior', 'nes', ' Min', 'nes', ' Sandra', ' hallway', ' hallway'], 'evidence_proportions': [2.908203125, 2.50244140625, 1.0341796875, 1.302294921875, 0.975927734375]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 06:00:26.815 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:00:26.815 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-53_pid-3_0-4-5-6-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 4, 5, 6, 9):   4%|▍         | 4/100 [02:02<49:20, 30.84s/it]is_0k: False
your chose emoji: ['🚶🏾\u200d♀️\u200d➡️', '👭🏾', '⏱️', '🤰🏻', '👩🏻\u200d❤️\u200d👩🏿', '🏙', '🩰', '🏇', '🛶', '🇸🇭']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.96s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.87s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (1, 2, 4, 7, 9):   4%|▍         | 4/100 [02:20<49:20, 30.84s/it]2025-01-22 06:00:44.748 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary travelled to the hallway.
2025-01-22 06:00:44.762 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Mary travelled to
2025-01-22 06:00:44.763 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the milk.
2025-01-22 06:00:44.788 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4952, 4956) -->  Mary grabbed the milk
2025-01-22 06:00:44.788 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went to the kitchen.
2025-01-22 06:00:44.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9864, 9869) --> . Mary went to the
2025-01-22 06:00:44.842 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary put down the milk.
2025-01-22 06:00:44.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16820, 16825) -->  affair. Mary put down
2025-01-22 06:00:44.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the garden.
2025-01-22 06:00:44.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (7958, 7963) -->  back to the garden.
2025-01-22 06:00:44.968 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel put down the football there.
2025-01-22 06:00:45.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (13148, 13154) --> . Daniel put down the football
2025-01-22 06:00:45.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra left the apple.
2025-01-22 06:00:45.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (6974, 6978) -->  left the apple.
2025-01-22 06:00:45.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went back to the garden.
2025-01-22 06:00:45.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7954, 7960) -->  sell. Sandra went back to
2025-01-22 06:00:45.116 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel journeyed to the kitchen.
2025-01-22 06:00:45.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15932, 15938) --> . Daniel journeyed to the
2025-01-22 06:00:45.195 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel took the football.
2025-01-22 06:00:45.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (13896, 13900) -->  Daniel took the football
2025-01-22 06:00:45.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:00:45.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (9594, 9599) --> . Sandra picked up the
2025-01-22 06:00:45.311 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra moved to the hallway.
2025-01-22 06:00:45.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8102, 8107) --> . Sandra moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:00:48.038 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 06:00:48.038 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24212])
2025-01-22 06:00:56.416 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [194.73307291666666, 10.637709709890563, 135.35329861111111, 10.268922712688884, 12.587005208333334], 'topk_indices': array([24186, 24191, 24204,  3040,  2961, 24206, 24192, 24131,    14,
       24211, 24101,  9869,    24, 24091,  2960,  8107, 24210,     0,
       24207, 24208]), 'topk_tokens': [':', ' prior', '?\n', ' of', '.', ':', ' to', ' the', '\n', '\n\n', ' location', ' kitchen', '\n\n', '.\n\n', ' hallway', ' hallway', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [252.9875, 217.0, 155.175, 167.79375, 185.1625]}, 'weight': {'score': [23.732747395833332, 23.446654965930207, 23.006944444444443, 23.447026042744774, 29.804375], 'topk_indices': array([18783, 18739, 14570, 14606, 19436, 14615, 14651, 19378, 14469,
       14534, 20266, 20314, 18077, 18108, 23669, 23535, 21991, 21964,
       23751, 23617]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.5640625, 27.935546875, 19.403125, 26.7546875, 18.846875]}, 'saliency': {'score': [1.1964619954427083, 0.059812623204560704, 0.8074391682942709, 0.0575690239266909, 0.094957275390625], 'topk_indices': array([ 8104, 24172,    24,  4955, 24205,  8103,  7961, 24209,  3039,
       24185, 24197, 24091, 24199,  6973,  2957, 24191, 24101,  9869,
        2960,  8107]), 'topk_tokens': [' moved', ' return', '\n\n', ' milk', 'Answer', ' Sandra', ' garden', 'assistant', ' Fourth', 'Question', ' milk', '.\n\n', ' discarded', ' Sandra', ' travelled', ' prior', ' location', ' kitchen', ' hallway', ' hallway'], 'evidence_proportions': [1.55712890625, 1.596923828125, 0.800537109375, 1.09189453125, 1.01591796875]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 06:00:56.435 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:00:56.436 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-53_pid-4_1-2-4-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 2, 4, 7, 9):   5%|▌         | 5/100 [02:32<48:07, 30.40s/it]Processing depth (1, 2, 4, 7, 9):   5%|▌         | 5/100 [02:32<48:14, 30.47s/it]
2025-01-22 06:00:56.755 | INFO     | __main__:<module>:82 - Selected idx: 54
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:86 - Needle: [' Sandra went back to the garden.', ' Daniel picked up the apple.', ' Sandra went back to the bathroom.', ' Daniel moved to the office.', ' Sandra moved to the hallway.', ' Mary went back to the hallway.', ' John went back to the hallway.', ' Daniel went back to the bedroom.', ' John went to the garden.']
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel picked up the apple.', ' Daniel moved to the office.', ' Daniel went back to the bedroom.', ' John went to the garden.']
2025-01-22 06:00:56.756 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['😩', '🪀', '👩🏽\u200d❤\u200d💋\u200d👩🏼', '🦹🏿', '👨\u200d🦽\u200d➡', '☺️', '🏃🏾\u200d♀', '⁉️', '\U0001faf2🏻', '👩🏾\u200d❤️\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.99s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.87s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
Processing depth (0, 4, 6, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:01:14.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 06:01:14.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-22 06:01:14.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:01:14.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9796, 9801) --> . Daniel moved to the
2025-01-22 06:01:14.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:01:14.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14550, 14556) -->  bonds. Daniel went back to
2025-01-22 06:01:14.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the garden.
2025-01-22 06:01:14.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3456, 3461) -->  John went back to the
2025-01-22 06:01:14.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 06:01:14.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11241, 11247) -->  state. Sandra went back to
2025-01-22 06:01:14.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the bathroom.
2025-01-22 06:01:14.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11241, 11247) -->  state. Sandra went back to
2025-01-22 06:01:14.696 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the hallway.
2025-01-22 06:01:14.710 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2459, 2464) --> . Sandra moved to the
2025-01-22 06:01:14.711 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary went back to the hallway.
2025-01-22 06:01:14.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3456, 3462) -->  John went back to the hallway
2025-01-22 06:01:14.736 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the hallway.
2025-01-22 06:01:14.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3455, 3461) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:01:17.457 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:01:17.457 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24138])
2025-01-22 06:01:25.865 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [333.3965773809524, 21.361547224523612, 213.99030172413794, 20.85764645599834, 21.490672831632654], 'topk_indices': array([  185,    35, 24120,    26, 24125, 24121,     9,    67, 24136,
          23, 24126,    68,    14,   105,    24,     1, 22804,     0,
       24133, 24134]), 'topk_tokens': ['ION', '.', 'Question', '<|start_header_id|>', ' apple', ':', ':', 'ION', '<|end_header_id|>', '4', ' before', 'E', '\n', 'ION', '\n\n', '<|start_header_id|>', 'nes', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [690.3, 407.3, 117.32552083333333, 161.875]}, 'weight': {'score': [22.301339285714285, 23.43252381938691, 23.084321120689655, 23.433929081050227, 29.770089285714285], 'topk_indices': array([18739, 18783, 14626, 14590, 14683, 14647, 19436, 19378, 14482,
       14547, 20316, 20268, 18108, 18077, 23490, 23624, 21929, 21956,
       23706, 23572]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.08125, 19.909375, 24.600260416666668, 22.1546875]}, 'saliency': {'score': [1.7340494791666667, 0.12054278508828707, 1.2544871363146552, 0.11777117750168638, 0.1649443957270408], 'topk_indices': array([   20,    41, 24131,    52,   185, 22755,    38, 22803,    23,
       22783,  2464,    68,    24, 24128,    67, 24126, 24125, 24120,
         105, 22804]), 'topk_tokens': [' Jul', '-text', 'Answer', ' Gutenberg', 'ION', 'nes', '***', ' Min', '4', ' sort', ' hallway', 'E', '\n\n', ' bedroom', 'ION', ' before', ' apple', 'Question', 'ION', 'nes'], 'evidence_proportions': [3.5138671875, 2.09765625, 0.6399739583333334, 0.903515625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:01:25.872 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:01:25.873 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-54_pid-0_0-4-6-8.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 4, 6, 8):   1%|          | 1/100 [00:28<47:48, 28.97s/it]is_0k: False
your chose emoji: ['🐺', '✍️', '\U0001fabf', '☦️', '👩🏼\u200d❤\u200d👩🏽', '💆🏿', '💻', '\U0001faf7🏼', '🏀', '❤️\u200d🩹']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.60s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Processing depth (3, 4, 5, 9):   1%|          | 1/100 [00:46<47:48, 28.97s/it]2025-01-22 06:01:43.145 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 06:01:43.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7370, 7375) --> . Daniel picked up the
2025-01-22 06:01:43.189 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:01:43.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9673, 9678) -->  war. Daniel moved to
2025-01-22 06:01:43.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:01:43.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11839, 11845) --> . Daniel went back to the
2025-01-22 06:01:43.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the garden.
2025-01-22 06:01:43.342 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3256, 3261) -->  John went back to the
2025-01-22 06:01:43.343 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 06:01:43.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11092, 11098) --> . Sandra went back to the
2025-01-22 06:01:43.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the bathroom.
2025-01-22 06:01:43.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11092, 11098) --> . Sandra went back to the
2025-01-22 06:01:43.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the hallway.
2025-01-22 06:01:43.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2191, 2196) --> . Sandra moved to the
2025-01-22 06:01:43.481 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary went back to the hallway.
2025-01-22 06:01:43.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3256, 3262) -->  John went back to the hallway
2025-01-22 06:01:43.501 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the hallway.
2025-01-22 06:01:43.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3255, 3261) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:01:46.228 | INFO     | test_jbb_embedding:begin_test:693 - the press room<|eot_id|>
2025-01-22 06:01:46.228 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 06:01:54.983 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [424.4375, 16.628805460221635, 341.38793103448273, 15.883715660478993, 20.257069762323944], 'topk_indices': array([ 9679,  2193, 24164, 24172, 24035,  2194,  9677, 24181, 24165,
       11845, 24170,    14,  7376,  9676, 24176,  7375, 24036, 24180,
           0, 24177]), 'topk_tokens': [' office', ' moved', 'Question', ' bedroom', ' you', ' to', ' to', '\n\n', ':', ' bedroom', ' before', '\n', '.', ' moved', ':', ' apple', ' context', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [504.75, 601.3625, 363.4166666666667, 240.425]}, 'weight': {'score': [22.14360119047619, 23.440053341051936, 22.033135775862068, 23.44287202442612, 29.100572183098592], 'topk_indices': array([18813, 18769, 14638, 14674, 19484, 14683, 19426, 14719, 14537,
       14602, 20346, 20298, 18107, 18138, 23534, 23668, 22000, 21973,
       23616, 23750]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.3625, 23.3734375, 20.927083333333332, 22.1546875]}, 'saliency': {'score': [2.370791480654762, 0.09381179229100335, 1.9809738685344827, 0.08956283755218261, 0.14687981404049297], 'topk_indices': array([24151,  7372,  9675, 24034, 24035,  7371, 24080,  2196,  9679,
        2193, 24175, 24170, 24169,  2192, 24164, 24172,  9676, 11845,
        7375, 24036]), 'topk_tokens': [' return', ' picked', ' Daniel', ' provided', ' you', ' Daniel', ' location', ' hallway', ' office', ' moved', 'Answer', ' before', ' apple', ' Sandra', 'Question', ' bedroom', ' moved', ' bedroom', ' apple', ' context'], 'evidence_proportions': [2.9208984375, 3.38037109375, 1.8934733072916667, 1.38388671875]}}, 'pred_res': 'the press room<|eot_id|>', 'score': 0}
2025-01-22 06:01:54.987 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:01:54.988 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-54_pid-1_3-4-5-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (3, 4, 5, 9):   2%|▏         | 2/100 [00:58<47:27, 29.06s/it]is_0k: False
your chose emoji: ['🤸🏿\u200d♀', '🧑🏻\u200d🎤', '🇾🇪', '\U0001faf3🏻', '👩🏽\u200d❤\u200d💋\u200d👨🏼', '🖤', '👰🏼\u200d♂', '📝', '💶', '👲🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.02s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.03s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.53s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]
Processing depth (1, 3, 7, 8):   2%|▏         | 2/100 [01:16<47:27, 29.06s/it]2025-01-22 06:02:13.283 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 06:02:13.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3042) --> . Daniel picked up the
2025-01-22 06:02:13.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:02:13.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7646, 7651) --> . Daniel moved to the
2025-01-22 06:02:13.375 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:02:13.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16753, 16759) --> . Daniel went back to the
2025-01-22 06:02:13.463 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the garden.
2025-01-22 06:02:13.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3456, 3461) -->  John went back to the
2025-01-22 06:02:13.484 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 06:02:13.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11147, 11153) -->  paper. Sandra went back to
2025-01-22 06:02:13.546 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the bathroom.
2025-01-22 06:02:13.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11147, 11153) -->  paper. Sandra went back to
2025-01-22 06:02:13.604 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the hallway.
2025-01-22 06:02:13.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2453, 2458) --> . Sandra moved to the
2025-01-22 06:02:13.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary went back to the hallway.
2025-01-22 06:02:13.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3456, 3462) -->  John went back to the hallway
2025-01-22 06:02:13.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the hallway.
2025-01-22 06:02:13.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3455, 3461) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:02:16.284 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 06:02:16.285 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24222])
2025-01-22 06:02:24.634 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [413.01190476190476, 19.674793077113605, 294.4897629310345, 19.003422452841896, 31.336838942307693], 'topk_indices': array([ 3038,  3042,  2459,    14,  9226,  9222,  9284, 24220,  9281,
        9223,  9227,  9285,  9224,  9228, 24217,     0,  9282,  9225,
        9283,  9286]), 'topk_tokens': [' Daniel', ' apple', '.', '\n', ' in', ' resort', ' draft', '<|end_header_id|>', ' resort', ' to', ' a', ' in', ' a', ' few', '<|eot_id|>', '<|begin_of_text|>', ' to', ' draft', ' a', ' a'], 'evidence_proportions': [708.6, 478.05, 171.75, 341.9]}, 'weight': {'score': [21.318824404761905, 23.454915579590487, 23.278286637931036, 23.456983096508644, 29.834649725274726], 'topk_indices': array([18839, 18883, 14717, 14681, 14726, 14762, 19546, 19488, 14560,
       14625, 20408, 20360, 18208, 18177, 23706, 23572, 22038, 22011,
       23654, 23788]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.3625, 19.909375, 20.927083333333332, 22.1546875]}, 'saliency': {'score': [2.323300316220238, 0.11138594512737884, 1.7964751144935345, 0.10744295730142664, 0.23149410708919987], 'topk_indices': array([9279, 3039, 7616, 9223, 9285, 3042, 9227, 2458, 9224, 3038, 3461,
       2454, 9282, 9222, 9284, 9283, 9286, 9281, 9228, 9225]), 'topk_tokens': ['necessary', ' picked', 'inen', ' to', ' in', ' apple', ' a', ' hallway', ' a', ' Daniel', ' hallway', ' Sandra', ' to', ' resort', ' draft', ' a', ' a', ' resort', ' few', ' draft'], 'evidence_proportions': [4.094140625, 2.626953125, 0.8755289713541666, 1.9861328125]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 06:02:24.650 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:02:24.650 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-54_pid-2_1-3-7-8.pkl | len: 3 |  size: 1.99 KB
Processing depth (1, 3, 7, 8):   3%|▎         | 3/100 [01:27<47:25, 29.33s/it]is_0k: False
your chose emoji: ['👨🏿\u200d🏫', '🤸\u200d♀️', '🚶🏾\u200d♂\u200d➡️', '👱🏾\u200d♂️', '😚', '👩🏿\u200d❤\u200d👩🏾', '👩🏽\u200d❤️\u200d💋\u200d👨🏾', '👩🏽', '📎', '👩\u200d🚀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.61s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.03s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.81s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (0, 3, 5, 7):   3%|▎         | 3/100 [01:45<47:25, 29.33s/it]2025-01-22 06:02:42.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 06:02:42.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-22 06:02:42.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:02:42.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7526, 7531) --> . Daniel moved to the
2025-01-22 06:02:42.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:02:42.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11891, 11897) --> . Daniel went back to the
2025-01-22 06:02:42.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the garden.
2025-01-22 06:02:42.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3298, 3303) -->  John went back to the
2025-01-22 06:02:42.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 06:02:42.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11125, 11131) -->  paper. Sandra went back to
2025-01-22 06:02:42.389 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the bathroom.
2025-01-22 06:02:42.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11125, 11131) -->  paper. Sandra went back to
2025-01-22 06:02:42.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the hallway.
2025-01-22 06:02:42.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2191, 2196) --> . Sandra moved to the
2025-01-22 06:02:42.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary went back to the hallway.
2025-01-22 06:02:42.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3298, 3304) -->  John went back to the hallway
2025-01-22 06:02:42.470 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the hallway.
2025-01-22 06:02:42.486 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3297, 3303) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:02:45.176 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:02:45.176 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 06:02:53.554 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [451.7261904761905, 17.761554643565994, 229.58620689655172, 17.129410271815697, 17.282529159330984], 'topk_indices': array([20669,  7527,    26,    23,    35, 24169,    67, 24172,    14,
          24,   185, 24170,    68, 24180, 20689,     1,   105, 24177,
           0, 24178]), 'topk_tokens': ['nes', ' Daniel', '<|start_header_id|>', '4', '.', ' apple', 'ION', ' bedroom', '\n', '\n\n', 'ION', ' before', 'E', '<|end_header_id|>', 'nes', '<|start_header_id|>', 'ION', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [724.6, 694.4, 230.8125, 201.275]}, 'weight': {'score': [21.25186011904762, 23.43883869500496, 23.278286637931036, 23.440934599527637, 28.881822183098592], 'topk_indices': array([18791, 18747, 14640, 14604, 19472, 19414, 14649, 14685, 14568,
       14503, 20286, 20334, 18116, 18085, 23534, 23668, 21943, 21970,
       23616, 23750]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.08125, 19.909375, 20.927083333333332, 22.1546875]}, 'saliency': {'score': [2.4335239955357144, 0.10069248150249131, 1.3381095096982758, 0.09717567717613118, 0.12643583055952906], 'topk_indices': array([ 7528,    24, 24166,    38,    34,    39,  2196,  7240, 20669,
       20688,    31,    68,    67,   185,  7527, 24169, 24170, 20689,
       24172,   105]), 'topk_tokens': [' moved', '\n\n', ' Where', '***', ' apple', '\n\n\n', ' hallway', 'nes', 'nes', ' Min', ' picked', 'E', 'ION', 'ION', ' Daniel', ' apple', ' before', 'nes', ' bedroom', 'ION'], 'evidence_proportions': [3.994921875, 3.719140625, 1.16015625, 1.11455078125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:02:53.559 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:02:53.560 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-54_pid-3_0-3-5-7.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 3, 5, 7):   4%|▍         | 4/100 [01:56<46:39, 29.17s/it]is_0k: False
your chose emoji: ['🚕', '🫀', '\U0001faf1🏿\u200d\U0001faf2🏼', '💪🏾', '🔀', '🧑🏿\u200d❤\u200d🧑🏾', '🧑🏿\u200d🦼\u200d➡️', '🇫🇲', '🥶', '🏋🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.91s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.42s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.97s/it]
Processing depth (0, 2, 3, 8):   4%|▍         | 4/100 [02:14<46:39, 29.17s/it]2025-01-22 06:03:11.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel picked up the apple.
2025-01-22 06:03:11.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  picked up the apple.
2025-01-22 06:03:11.847 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:03:11.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4924, 4929) --> . Daniel moved to the
2025-01-22 06:03:11.873 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:03:11.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7605, 7611) -->  war. Daniel went back to
2025-01-22 06:03:11.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went to the garden.
2025-01-22 06:03:11.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3424, 3429) -->  John went back to the
2025-01-22 06:03:11.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went back to the garden.
2025-01-22 06:03:11.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (11147, 11153) --> . Sandra went back to the
2025-01-22 06:03:11.994 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the bathroom.
2025-01-22 06:03:12.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11147, 11153) --> . Sandra went back to the
2025-01-22 06:03:12.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the hallway.
2025-01-22 06:03:12.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (2393, 2398) --> . Sandra moved to the
2025-01-22 06:03:12.068 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary went back to the hallway.
2025-01-22 06:03:12.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3424, 3430) -->  John went back to the hallway
2025-01-22 06:03:12.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the hallway.
2025-01-22 06:03:12.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3423, 3429) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:03:14.826 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:03:14.827 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24232])
2025-01-22 06:03:23.224 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [603.9464285714286, 22.255292151522653, 222.50646551724137, 21.510055718656965, 15.986043294270834], 'topk_indices': array([  185,  4927,     1,  1479,    14,  7195,    35,  4926,    85,
          26, 24230, 24215,  7243, 24219, 24220,  7166,  7244,     0,
       24227, 24228]), 'topk_tokens': ['ION', ' to', '<|start_header_id|>', 'nes', '\n', 'nes', '.', ' moved', 'ol', '<|start_header_id|>', '<|end_header_id|>', ':', ' Min', ' apple', ' before', 'nes', 'nes', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [943.8, 1010.3, 274.1666666666667, 253.475]}, 'weight': {'score': [22.076636904761905, 23.45441064207312, 22.033135775862068, 23.457311329287958, 29.429850260416668], 'topk_indices': array([18795, 18839, 14652, 14688, 19498, 14697, 19440, 14733, 14616,
       14551, 20398, 20350, 18164, 18133, 23574, 23708, 22013, 22040,
       23790, 23656]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.08125, 19.909375, 23.813802083333332, 22.1546875]}, 'saliency': {'score': [3.2556036086309526, 0.12499342148105451, 1.2818308863146552, 0.1208877689665688, 0.11975161234537761], 'topk_indices': array([ 4929,  2398, 24225,    48, 24216,    34,  4925,   185,    31,
          85,  1479, 24222,  7195, 24214,  4926, 24220,  7166,  7243,
       24219,  7244]), 'topk_tokens': [' office', ' hallway', 'Answer', 'ucci', ' Where', ' apple', ' Daniel', 'ION', ' picked', 'ol', 'nes', ' bedroom', 'nes', 'Question', ' moved', ' before', 'nes', ' Min', ' apple', 'nes'], 'evidence_proportions': [4.9974609375, 5.4017578125, 1.5439453125, 1.42158203125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:03:23.240 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:03:23.240 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-54_pid-4_0-2-3-8.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 2, 3, 8):   5%|▌         | 5/100 [02:26<46:28, 29.35s/it]Processing depth (0, 2, 3, 8):   5%|▌         | 5/100 [02:26<46:25, 29.32s/it]
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:82 - Selected idx: 55
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:86 - Needle: [' Daniel moved to the office.', ' Sandra moved to the hallway.', ' Daniel picked up the apple.', ' Mary went back to the hallway.', ' John went back to the hallway.', ' Sandra went back to the bathroom.', ' Daniel went back to the bedroom.', ' Sandra went back to the garden.', ' Daniel discarded the apple.', ' John went to the garden.']
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the office.', ' Daniel picked up the apple.', ' Daniel went back to the bedroom.', ' Daniel discarded the apple.', ' John went to the garden.']
2025-01-22 06:03:23.515 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👲', '👩🏿\u200d🚀', '⏯️', '🧑🏿\u200d🎓', '👱🏿\u200d♂️', '👩🏿\u200d❤️\u200d💋\u200d👨🏽', '🦸\u200d♀️', '🏃🏼', '🧑🏼\u200d🤝\u200d🧑🏿', '👨🏽\u200d⚕️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]
Processing depth (0, 1, 3, 5, 7):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:03:41.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:03:41.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:03:41.011 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the apple.
2025-01-22 06:03:41.026 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3037, 3042) --> . Daniel picked up the
2025-01-22 06:03:41.026 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:03:41.067 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7477, 7483) --> . Daniel went back to the
2025-01-22 06:03:41.067 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel discarded the apple.
2025-01-22 06:03:41.126 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11793, 11797) -->  Daniel discarded the apple
2025-01-22 06:03:41.127 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the garden.
2025-01-22 06:03:41.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14755, 14760) -->  John went back to the
2025-01-22 06:03:41.203 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:03:41.226 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4164, 4169) -->  Sandra moved to the hallway
2025-01-22 06:03:41.227 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:03:41.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13047, 13053) --> . Mary went back to the
2025-01-22 06:03:41.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:03:41.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13048, 13054) -->  Mary went back to the hallway
2025-01-22 06:03:41.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the bathroom.
2025-01-22 06:03:41.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16833, 16839) --> . Sandra went back to the
2025-01-22 06:03:41.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 06:03:41.527 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16833, 16839) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:03:44.305 | INFO     | test_jbb_embedding:begin_test:693 - Daniel discarded the apple.<|eot_id|>
2025-01-22 06:03:44.305 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24190])
2025-01-22 06:03:52.710 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [286.27875, 20.45692920180225, 92.81142241379311, 20.09468602411136, 23.368451286764707], 'topk_indices': array([24170, 24183, 24054,     9, 24178, 24169,    24, 24177, 24163,
       24079, 24184, 24162, 24069,    14, 24164, 24188, 24182,     0,
       24185, 24186]), 'topk_tokens': [' to', 'Answer', '.', ':', ',', ' prior', '\n\n', ' discarded', 'Question', ' location', ':', '.\n\n', '.\n\n', '\n', ':', '<|end_header_id|>', '?\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [344.825, 313.3, 214.27083333333334, 606.125, 31.24375]}, 'weight': {'score': [22.405, 23.43785652281746, 23.348060344827587, 23.439034147402435, 28.988511029411764], 'topk_indices': array([18830, 18786, 14678, 14642, 14723, 19425, 19483, 14687, 14606,
       14541, 20297, 20345, 18124, 18155, 23535, 23669, 21975, 21948,
       23617, 23751]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 22.3625, 20.927083333333332, 29.201171875, 22.1546875]}, 'saliency': {'score': [1.7355712890625, 0.10986536015909185, 0.606085415544181, 0.10758542686599579, 0.17630094640395222], 'topk_indices': array([ 7429, 24165,  7473,    24, 24164, 18883, 24172,  7474, 24173,
       24168, 24175, 11794, 24069, 24162, 24183, 24182, 24169, 24079,
       24163, 24177]), 'topk_tokens': ['nes', ' Where', ' Min', '\n\n', ':', 'deal', ' place', 'nes', ' where', ' location', ' apple', ' discarded', '.\n\n', '.\n\n', 'Answer', '?\n', ' prior', ' location', 'Question', ' discarded'], 'evidence_proportions': [1.72646484375, 1.71279296875, 1.1479085286458333, 4.60986328125, 0.1732177734375]}}, 'pred_res': 'Daniel discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 06:03:52.733 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:03:52.733 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-55_pid-0_0-1-3-5-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 1, 3, 5, 7):   1%|          | 1/100 [00:29<47:55, 29.05s/it]is_0k: False
your chose emoji: ['🐈\u200d⬛', '🪆', '👨🏻\u200d🔬', '👩🏼\u200d🍼', '👨🏽\u200d🔬', '🐁', '🚝', '🤽🏽\u200d♂', '🙎🏼\u200d♂', '✍🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.81s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (0, 1, 3, 4, 8):   1%|          | 1/100 [00:46<47:55, 29.05s/it]2025-01-22 06:04:10.459 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:04:10.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:04:10.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the apple.
2025-01-22 06:04:10.474 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2921, 2926) --> . Daniel picked up the
2025-01-22 06:04:10.475 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:04:10.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7376, 7382) --> . Daniel went back to the
2025-01-22 06:04:10.512 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel discarded the apple.
2025-01-22 06:04:10.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9657, 9661) -->  discarded the apple.
2025-01-22 06:04:10.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the garden.
2025-01-22 06:04:10.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14730, 14735) -->  John went back to the
2025-01-22 06:04:10.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:04:10.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4076, 4081) --> . Sandra moved to the
2025-01-22 06:04:10.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:04:10.719 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13083, 13089) -->  winter. Mary went back to
2025-01-22 06:04:10.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:04:10.785 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13085, 13091) -->  Mary went back to the hallway
2025-01-22 06:04:10.785 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the bathroom.
2025-01-22 06:04:10.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16780, 16786) --> . Sandra went back to the
2025-01-22 06:04:10.886 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 06:04:10.985 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16780, 16786) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:04:13.703 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:04:13.703 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 06:04:22.098 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [430.43375, 20.004330684217063, 107.94059806034483, 19.472868861491392, 107.25709885817308], 'topk_indices': array([ 9657,     1,  4082,  9644,  2927,  9295,    34, 24151,    31,
       24149, 24036, 24131,  9660, 24155,    23,    24,    14,     0,
       24152, 24153]), 'topk_tokens': [' discarded', '<|start_header_id|>', '.', '�', '.', ' a', ' office', ':', ' moved', '?\n', '.\n\n', ':', '.', '<|end_header_id|>', '4', '\n\n', '\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [768.9, 448.3, 242.47916666666666, 752.6875, 41.84375]}, 'weight': {'score': [21.4271875, 23.433016681154022, 23.045258620689655, 23.435563485272766, 28.13671875], 'topk_indices': array([18761, 18805, 14617, 14653, 14698, 19406, 19464, 14662, 14581,
       14516, 20326, 20278, 18099, 18130, 23628, 23494, 21960, 21933,
       23576, 23710]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 22.3625, 20.927083333333332, 23.08984375, 22.1546875]}, 'saliency': {'score': [2.3631201171875, 0.11090110375532927, 0.6165624158135776, 0.1079569156870657, 0.7291051424466647], 'topk_indices': array([   38,    39,  9554,    22,    19,  2926,  9580, 24150,  9656,
          20, 24144,  7382,    23, 24130,    24,    34,    31,  4081,
        9644,  9657]), 'topk_tokens': ['***', '\n\n\n', ' place', '202', '26', ' apple', 'character', 'Answer', ' Daniel', ' Jul', ' discarded', ' bedroom', '4', 'Question', '\n\n', ' office', ' moved', ' hallway', '�', ' discarded'], 'evidence_proportions': [3.9556640625, 2.5869140625, 1.330078125, 4.319580078125, 0.2212646484375]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:04:22.106 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:04:22.106 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-55_pid-1_0-1-3-4-8.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 1, 3, 4, 8):   2%|▏         | 2/100 [00:58<47:45, 29.24s/it]is_0k: False
your chose emoji: ['🌓', '🇲🇲', '⛹🏻\u200d♀️', '🐭', '©', '🧑\u200d🦽\u200d➡️', '😙', '🎺', '🕺🏿', '🤹🏾\u200d♀️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.35s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.89s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.36s/it]
Processing depth (1, 3, 5, 7, 9):   2%|▏         | 2/100 [01:18<47:45, 29.24s/it]2025-01-22 06:04:42.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:04:42.095 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 06:04:42.095 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the apple.
2025-01-22 06:04:42.137 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7661, 7666) --> . Daniel picked up the
2025-01-22 06:04:42.137 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:04:42.199 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11903, 11909) --> . Daniel went back to the
2025-01-22 06:04:42.199 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel discarded the apple.
2025-01-22 06:04:42.289 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16803, 16807) -->  Daniel discarded the apple
2025-01-22 06:04:42.289 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the garden.
2025-01-22 06:04:42.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (15070, 15075) -->  John went back to the
2025-01-22 06:04:42.371 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:04:42.393 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4164, 4169) -->  Sandra moved to the hallway
2025-01-22 06:04:42.393 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:04:42.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13180, 13186) --> . Mary went back to the
2025-01-22 06:04:42.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:04:42.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13181, 13187) -->  Mary went back to the hallway
2025-01-22 06:04:42.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the bathroom.
2025-01-22 06:04:42.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16943, 16949) --> . Sandra went back to the
2025-01-22 06:04:42.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 06:04:42.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16943, 16949) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:04:45.489 | INFO     | test_jbb_embedding:begin_test:693 - Daniel discarded the apple.<|eot_id|>
2025-01-22 06:04:45.489 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 06:04:53.900 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [555.88, 23.183148243801654, 154.76508620689654, 22.473577404124907, 26.75493706597222], 'topk_indices': array([24171,  7665,  7662, 24196,    24, 11999, 24172, 24077, 16806,
       11956, 11955, 16807,  7666,    14,  7667, 24185, 16804,     0,
       24193, 24194]), 'topk_tokens': ['Question', ' the', ' Daniel', '<|end_header_id|>', '\n\n', 'ANT', ':', '.\n\n', ' apple', 'IC', 'ANT', '.', ' apple', '\n', '.', ' discarded', ' discarded', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [555.8, 857.1, 232.6875, 1077.875, 224.975]}, 'weight': {'score': [22.58, 23.44032541322314, 23.348060344827587, 23.441326979623955, 29.106336805555557], 'topk_indices': array([18804, 18760, 14633, 14597, 14678, 19475, 14642, 19417, 14496,
       14561, 20289, 20337, 18098, 18129, 23673, 23539, 21958, 21985,
       23621, 23755]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 20.927083333333332, 29.201171875, 22.1546875]}, 'saliency': {'score': [3.497939453125, 0.13000480210485538, 1.0523639547413792, 0.12540997142060278, 0.1979045867919922], 'topk_indices': array([   24,  4168, 24181,  3033, 24191,     0, 16803, 24077,  3032,
       24183,  7663, 24171, 11999, 11956,  7662, 11955, 16806,  7666,
       24185, 16804]), 'topk_tokens': ['\n\n', ' hallway', ' where', ' moved', 'Answer', '<|begin_of_text|>', ' Daniel', '.\n\n', ' Daniel', ' apple', ' picked', 'Question', 'ANT', 'IC', ' Daniel', 'ANT', ' apple', ' apple', ' discarded', ' discarded'], 'evidence_proportions': [3.16650390625, 4.9078125, 1.2921142578125, 8.38671875, 1.15546875]}}, 'pred_res': 'Daniel discarded the apple.<|eot_id|>', 'score': 0}
2025-01-22 06:04:53.907 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:04:53.908 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-55_pid-2_1-3-5-7-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 3, 5, 7, 9):   3%|▎         | 3/100 [01:30<49:09, 30.41s/it]is_0k: False
your chose emoji: ['👨🏾\u200d🦼', '👨🏻\u200d🏭', '🧍🏽\u200d♂️', '🌡️', '👩\u200d🔬', '🤾🏿\u200d♀️', '🐸', '👆🏼', '👨🏾', '🧑\u200d🔧']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.87s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.40s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]
Processing depth (0, 5, 7, 8, 9):   3%|▎         | 3/100 [01:48<49:09, 30.41s/it]2025-01-22 06:05:12.020 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:05:12.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:05:12.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the apple.
2025-01-22 06:05:12.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11741, 11746) --> . Daniel picked up the
2025-01-22 06:05:12.079 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:05:12.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16689, 16695) --> . Daniel went back to the
2025-01-22 06:05:12.166 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel discarded the apple.
2025-01-22 06:05:12.257 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19156, 19160) -->  Daniel discarded the apple
2025-01-22 06:05:12.258 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the garden.
2025-01-22 06:05:12.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14687, 14692) -->  John went back to the
2025-01-22 06:05:12.330 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:05:12.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4051, 4056) --> . Sandra moved to the
2025-01-22 06:05:12.350 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:05:12.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (12985, 12991) --> . Mary went back to the
2025-01-22 06:05:12.421 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:05:12.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (12986, 12992) -->  Mary went back to the hallway
2025-01-22 06:05:12.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the bathroom.
2025-01-22 06:05:12.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16810, 16816) --> . Sandra went back to the
2025-01-22 06:05:12.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 06:05:12.671 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16810, 16816) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:05:15.329 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 06:05:15.330 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 06:05:23.716 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [239.8275, 12.625736540688058, 111.39224137931035, 12.27164318276005, 18.02008056640625], 'topk_indices': array([24162, 24161, 24165,    23, 24167, 24061, 19157,     1, 24154,
       24071, 24174, 16695, 24155,    14, 24169, 24156,    24,     0,
       24177, 24178]), 'topk_tokens': [' to', ' prior', ' where', '4', ' apple', '.\n\n', ' discarded', '<|start_header_id|>', '.\n\n', ' location', '?\n', ' bedroom', 'Question', '\n', ' discarded', ':', '\n\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [239.25, 247.625, 220.70833333333334, 417.25, 113.6125]}, 'weight': {'score': [22.405, 23.438810267118757, 22.297144396551722, 23.441253431931205, 29.51513671875], 'topk_indices': array([18797, 18841, 14646, 14682, 14734, 19499, 19441, 14698, 14545,
       14610, 20313, 20361, 18135, 18166, 23531, 23665, 21997, 21970,
       23747, 23613]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 22.3625, 20.927083333333332, 29.201171875, 22.1546875]}, 'saliency': {'score': [1.45974609375, 0.06974971164270541, 0.6746868265086207, 0.06758257173869406, 0.13371777534484863], 'topk_indices': array([24061,  4056,    20, 24160,     0, 24164, 19159, 24174,    16,
       24154, 24165, 24175, 24161, 24167, 24071,    24, 19157, 24155,
       16695, 24169]), 'topk_tokens': ['.\n\n', ' hallway', ' Jul', ' location', '<|begin_of_text|>', ' place', ' apple', '?\n', ' Date', '.\n\n', ' where', 'Answer', ' prior', ' apple', ' location', '\n\n', ' discarded', 'Question', ' bedroom', ' discarded'], 'evidence_proportions': [1.2306640625, 1.43212890625, 1.1688639322916667, 3.2769775390625, 0.61171875]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:05:23.722 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:05:23.722 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-55_pid-3_0-5-7-8-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (0, 5, 7, 8, 9):   4%|▍         | 4/100 [02:00<48:16, 30.17s/it]is_0k: False
your chose emoji: ['🤸🏼\u200d♀', '🤹🏿\u200d♂️', '🏋🏿\u200d♀️', '🙅🏻\u200d♂️', '📲', '⬜', '✌🏻', '👩🏽\u200d🦽', '🗜', '🥝']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.11s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.51s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]
Processing depth (1, 3, 5, 6, 8):   4%|▍         | 4/100 [02:18<48:16, 30.17s/it]2025-01-22 06:05:42.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:05:42.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3036) --> . Daniel moved to the
2025-01-22 06:05:42.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the apple.
2025-01-22 06:05:42.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7625, 7630) --> . Daniel picked up the
2025-01-22 06:05:42.335 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:05:42.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11893, 11899) --> . Daniel went back to the
2025-01-22 06:05:42.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel discarded the apple.
2025-01-22 06:05:42.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14352, 14356) -->  Daniel discarded the apple
2025-01-22 06:05:42.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the garden.
2025-01-22 06:05:42.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14989, 14994) -->  John went back to the
2025-01-22 06:05:42.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:05:42.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4164, 4169) -->  Sandra moved to the hallway
2025-01-22 06:05:42.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:05:42.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (13148, 13154) --> . Mary went back to the
2025-01-22 06:05:42.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:05:42.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (13149, 13155) -->  Mary went back to the hallway
2025-01-22 06:05:42.730 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the bathroom.
2025-01-22 06:05:42.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16832, 16838) -->  affair. Sandra went back to
2025-01-22 06:05:42.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra went back to the garden.
2025-01-22 06:05:42.938 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16832, 16838) -->  affair. Sandra went back to
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:05:45.625 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:05:45.625 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24172])
2025-01-22 06:05:53.989 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [379.045, 23.062913667576737, 99.96955818965517, 22.60147731395108, 34.47911149364407], 'topk_indices': array([24160, 24150, 24155,  7630, 24159, 24036,  7631, 24152, 24151,
       24061, 24166,    14, 24051, 24144, 24145, 24164, 24146,     0,
       24167, 24168]), 'topk_tokens': [',', ' location', ' where', ' apple', ' discarded', '.', '.', ' to', ' prior', ' location', ':', '\n', '.\n\n', '.\n\n', 'Question', '?\n', ':', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [431.15, 713.8, 148.91145833333334, 646.3125, 54.53125]}, 'weight': {'score': [22.58, 23.435837573425996, 25.06627155172414, 23.434764329394692, 29.435381355932204], 'topk_indices': array([18808, 18764, 14594, 14630, 19409, 14675, 19467, 14639, 14493,
       14558, 20329, 20281, 18096, 18127, 23515, 23649, 21981, 21954,
       23731, 23597]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 20.927083333333332, 29.201171875, 22.1546875]}, 'saliency': {'score': [2.38105712890625, 0.12322920787183544, 0.6943001582704741, 0.12020239379275498, 0.2566665390790519], 'topk_indices': array([    0, 24148,  7626, 24168, 24154, 24147, 24146, 24155, 24150,
       24165, 24157, 24051, 24144,  7630, 14353, 24061, 24164, 24151,
       24159, 24145]), 'topk_tokens': ['<|begin_of_text|>', ' was', ' Daniel', '<|start_header_id|>', ' place', ' Where', ':', ' where', ' location', 'Answer', ' apple', '.\n\n', '.\n\n', ' apple', ' discarded', ' location', '?\n', ' prior', ' discarded', 'Question'], 'evidence_proportions': [2.50634765625, 3.996484375, 0.8836873372395834, 5.018310546875, 0.32738037109375]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:05:53.997 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:05:53.998 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-55_pid-4_1-3-5-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 3, 5, 6, 8):   5%|▌         | 5/100 [02:30<47:50, 30.21s/it]Processing depth (1, 3, 5, 6, 8):   5%|▌         | 5/100 [02:30<47:41, 30.12s/it]
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:82 - Selected idx: 56
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the office? 
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:86 - Needle: [' Sandra moved to the hallway.', ' Sandra went back to the garden.', ' Mary went back to the hallway.', ' Sandra picked up the apple.', ' Daniel went to the hallway.', ' Daniel moved to the office.', ' Sandra left the apple.', ' Daniel dropped the football.', ' Sandra picked up the apple.']
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel went to the hallway.', ' Daniel moved to the office.', ' Daniel dropped the football.', ' Sandra picked up the apple.']
2025-01-22 06:05:54.295 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧑\u200d🧒\u200d🧒', '⛰️', '🇮🇲', '👩🏿\u200d🍳', '🏂', '💆🏾\u200d♀️', '🔇', '\U0001fab9', '🏌\u200d♂', '👨🏼']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.25s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.57s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
Processing depth (1, 3, 4, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 06:06:11.266 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:06:11.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2959) -->  tragedy. Daniel went to
2025-01-22 06:06:11.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:06:11.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7379, 7384) --> . Daniel moved to the
2025-01-22 06:06:11.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the football.
2025-01-22 06:06:11.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9689, 9693) -->  Daniel dropped the football
2025-01-22 06:06:11.363 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the apple.
2025-01-22 06:06:11.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16711, 16716) --> . Sandra picked up the
2025-01-22 06:06:11.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:06:11.527 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14745, 14750) --> . Sandra moved to the
2025-01-22 06:06:11.527 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 06:06:11.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5133, 5139) --> . Sandra went back to the
2025-01-22 06:06:11.558 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 06:06:11.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7029, 7035) -->  Mary went back to the hallway
2025-01-22 06:06:11.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the apple.
2025-01-22 06:06:11.639 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9012, 9016) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:06:14.356 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:06:14.356 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 06:06:22.820 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [395.68092105263156, 13.984143489777336, 252.38095238095238, 13.475952968244755, 18.67002704326923], 'topk_indices': array([24153,  2956,  9688, 24150,    24, 24149, 24047,    23, 24147,
       24142,  9693,  9690, 24157,  9691,    14, 24146, 24154, 24155,
        9692,     0]), 'topk_tokens': [':', ' Daniel', '.', '?', '\n\n', ' office', '.\n\n', '4', ' before', ':', '.', ' dropped', '<|end_header_id|>', ' the', '\n', ' football', '<|eot_id|>', '<|start_header_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [346.2125, 332.0, 886.5, 116.175]}, 'weight': {'score': [23.88157894736842, 23.43705767320586, 23.949404761904763, 23.436261504021225, 28.888461538461538], 'topk_indices': array([18769, 18813, 14633, 14669, 14714, 19466, 19408, 14678, 14532,
       14597, 20328, 20280, 18107, 18138, 23491, 23625, 21951, 21924,
       23707, 23573]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4734375, 19.909375, 27.341796875, 23.49375]}, 'saliency': {'score': [2.4580913342927633, 0.079577112304445, 1.6669573102678572, 0.07632171248788709, 0.1358892587515024], 'topk_indices': array([ 7380, 24128,  5139,  9012,  9652,  7381, 24057, 24152,  2960,
       24149, 24141, 24147, 14750,  9689,  2956,  5134,  7034,  9690,
       24146,  9692]), 'topk_tokens': [' Daniel', ' return', ' garden', ' Sandra', ' Press', ' moved', ' location', 'Answer', ' hallway', ' office', 'Question', ' before', ' hallway', ' Daniel', ' Daniel', ' Sandra', ' hallway', ' dropped', ' football', ' football'], 'evidence_proportions': [2.098828125, 1.83330078125, 5.994140625, 0.6133056640625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:06:22.829 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:06:22.829 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-56_pid-0_1-3-4-7.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 3, 4, 7):   1%|          | 1/100 [00:28<46:49, 28.38s/it]is_0k: False
your chose emoji: ['🤳🏾', '👩🏽\u200d🦱', '🌬', '🤵🏼\u200d♀️', '⬜', '🏧', '👩🏼\u200d🎓', '🧜🏼', '👩🏽\u200d💻', '🤏🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.04s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.94s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.95s/it]
Processing depth (1, 2, 6, 7):   1%|          | 1/100 [00:46<46:49, 28.38s/it]2025-01-22 06:06:40.813 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:06:40.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2968, 2973) -->  tragedy. Daniel went to
2025-01-22 06:06:40.832 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:06:40.856 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4911, 4916) --> . Daniel moved to the
2025-01-22 06:06:40.856 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the football.
2025-01-22 06:06:40.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14359, 14363) -->  Daniel dropped the football
2025-01-22 06:06:40.931 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the apple.
2025-01-22 06:06:41.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16785, 16790) --> . Sandra picked up the
2025-01-22 06:06:41.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:06:41.095 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14994, 14999) -->  bonds. Sandra moved to
2025-01-22 06:06:41.095 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 06:06:41.123 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5147, 5153) --> . Sandra went back to the
2025-01-22 06:06:41.124 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 06:06:41.162 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7268, 7274) --> . Mary went back to the
2025-01-22 06:06:41.162 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the apple.
2025-01-22 06:06:41.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9060, 9064) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:06:43.999 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the football.<|eot_id|>
2025-01-22 06:06:43.999 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24156])
2025-01-22 06:06:52.432 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [418.68092105263156, 16.09007900782317, 178.75595238095238, 15.631303381151788, 22.6973876953125], 'topk_indices': array([14361,  4912, 14359, 24142,    14, 24154, 24146,  7274, 14360,
        2970, 24144, 15000, 14363, 24152, 24155, 24143, 24151, 14362,
       24156,     0]), 'topk_tokens': [' the', ' Daniel', ' Daniel', ' the', '\n', '<|end_header_id|>', ' office', ' hallway', ' dropped', ' Daniel', ' before', ' hallway', '.', '<|start_header_id|>', '\n\n', ' football', '<|eot_id|>', ' football', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [439.5875, 349.775, 925.375, 61.325]}, 'weight': {'score': [23.88157894736842, 23.4348120783145, 23.547619047619047, 23.43436191384386, 28.087158203125], 'topk_indices': array([18801, 18757, 14637, 14601, 19416, 14682, 19474, 14646, 14500,
       14565, 20288, 20336, 18126, 18095, 23508, 23642, 21974, 21947,
       23724, 23590]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.4734375, 19.909375, 27.341796875, 23.49375]}, 'saliency': {'score': [2.791773745888158, 0.09202269421463016, 1.0779041108630953, 0.08903754641698454, 0.13943719863891602], 'topk_indices': array([24140, 17627, 14253, 24010,  2971, 17671, 24138, 24155,  2974,
       14359,  4912, 24146, 24144, 14360,  2970,  7274, 15000, 24143,
       24156, 14362]), 'topk_tokens': [' Where', 'ison', 'rail', ' context', ' went', 'ison', 'Question', '\n\n', ' hallway', ' Daniel', ' Daniel', ' office', ' before', ' dropped', ' Daniel', ' hallway', ' hallway', ' football', 'hall', ' football'], 'evidence_proportions': [2.83251953125, 2.08623046875, 6.638427734375, 0.379248046875]}}, 'pred_res': 'Daniel dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 06:06:52.439 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:06:52.439 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-56_pid-1_1-2-6-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 2, 6, 7):   2%|▏         | 2/100 [00:57<47:32, 29.10s/it]is_0k: False
your chose emoji: ['🇲🇼', '©', '🤸🏼\u200d♂', '🧑🏻\u200d🏫', '🧑🏻\u200d🦳', '👨🏼\u200d🚒', '👔', '👨\u200d👧', '⚔', '🧗🏿']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.90s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.02s/it]
Processing depth (2, 6, 8, 9):   2%|▏         | 2/100 [01:16<47:32, 29.10s/it]2025-01-22 06:07:10.924 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:07:10.949 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4945, 4950) --> . Daniel went to the
2025-01-22 06:07:10.950 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:07:11.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14356, 14361) --> . Daniel moved to the
2025-01-22 06:07:11.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the football.
2025-01-22 06:07:11.135 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19234, 19238) -->  dropped the football.
2025-01-22 06:07:11.136 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the apple.
2025-01-22 06:07:11.247 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21461, 21466) --> . Sandra picked up the
2025-01-22 06:07:11.247 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:07:11.323 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14765, 14770) -->  Sandra moved to the hallway
2025-01-22 06:07:11.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 06:07:11.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5134, 5140) -->  Sandra went back to the garden
2025-01-22 06:07:11.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 06:07:11.390 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7254, 7260) --> . Mary went back to the
2025-01-22 06:07:11.391 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the apple.
2025-01-22 06:07:11.438 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9048, 9052) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:07:14.202 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the football.<|eot_id|>
2025-01-22 06:07:14.202 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 06:07:22.656 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [573.7952302631579, 21.114834259565324, 521.875, 20.245001836561542, 38.98483455882353], 'topk_indices': array([ 4951, 14768,    23, 19234, 19235, 24189, 24195,    24,  4950,
           1, 24187, 19237, 24186, 24194, 19236, 24197,     0, 14769,
       24198, 24199]), 'topk_tokens': ['.', ' the', '4', ' dropped', ' the', ' office', '<|start_header_id|>', '\n\n', ' hallway', '<|start_header_id|>', ' before', '.', ' football', '<|eot_id|>', ' football', '<|end_header_id|>', '<|begin_of_text|>', ' hallway', '\n\n', 'hall'], 'evidence_proportions': [551.8, 477.45, 1381.0, 46.371875]}, 'weight': {'score': [21.066611842105264, 23.450778861251138, 25.024181547619047, 23.45128617508898, 29.559007352941176], 'topk_indices': array([18802, 18846, 14676, 14640, 19504, 19446, 14721, 14685, 14604,
       14539, 20318, 20366, 18140, 18171, 23547, 23681, 21986, 22013,
       23763, 23629]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.665625, 19.909375, 21.23046875, 23.49375]}, 'saliency': {'score': [3.2863351922286186, 0.12116135724191182, 3.773158482142857, 0.1154983226219438, 0.2938616584329044], 'topk_indices': array([24184,    24, 14358, 19329, 19603, 19233, 24183, 24181, 24196,
        4946, 19234, 24189, 24187,  7260,  4950, 24186, 19236, 24198,
       14769, 24199]), 'topk_tokens': [' was', '\n\n', ' moved', ' hall', ' hall', ' Daniel', ' Where', 'Question', 'assistant', ' Daniel', ' dropped', ' office', ' before', ' hallway', ' hallway', ' football', ' football', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [3.1775390625, 2.7263671875, 7.85986328125, 0.29627685546875]}}, 'pred_res': 'Daniel dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 06:07:22.663 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:07:22.664 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-56_pid-2_2-6-8-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (2, 6, 8, 9):   3%|▎         | 3/100 [01:28<47:52, 29.62s/it]is_0k: False
your chose emoji: ['👩🏼\u200d🤝\u200d👨🏻', '👩🏿\u200d⚕️', '\U0001fa89', '🏋🏽\u200d♀', '🧑🏾\u200d🎤', '🏃\u200d➡️', '🛌🏻', '🚵\u200d♀️', '🤾🏿\u200d♂', '🤹🏾\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.79s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.25s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (0, 2, 7, 8):   3%|▎         | 3/100 [01:45<47:52, 29.62s/it]2025-01-22 06:07:40.128 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:07:40.128 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 06:07:40.129 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:07:40.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4903, 4908) --> . Daniel moved to the
2025-01-22 06:07:40.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the football.
2025-01-22 06:07:40.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16695, 16699) -->  Daniel dropped the football
2025-01-22 06:07:40.237 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the apple.
2025-01-22 06:07:40.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19135, 19140) --> . Sandra picked up the
2025-01-22 06:07:40.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:07:40.399 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14798, 14803) --> . Sandra moved to the
2025-01-22 06:07:40.399 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 06:07:40.425 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5139, 5145) --> . Sandra went back to the
2025-01-22 06:07:40.425 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 06:07:40.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7124, 7130) --> . Mary went back to the
2025-01-22 06:07:40.460 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the apple.
2025-01-22 06:07:40.502 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9026, 9030) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:07:43.255 | INFO     | test_jbb_embedding:begin_test:693 - Daniel dropped the football.<|eot_id|>
2025-01-22 06:07:43.256 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24141])
2025-01-22 06:07:51.646 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [333.7763157894737, 16.00473073020212, 170.4404761904762, 15.619698753318952, 16.000069754464285], 'topk_indices': array([24029, 24123,    23, 16750, 24124,    24,   546,    14, 24129,
         547,   451,   452, 16698, 24128, 24137,   429,   568, 24136,
         430,     0]), 'topk_tokens': ['.\n\n', 'Question', '4', 'on', ':', '\n\n', ' em', '\n', ' before', 's', ' em', 's', ' football', ' football', '<|start_header_id|>', ' em', ' em', '<|eot_id|>', 's', '<|begin_of_text|>'], 'evidence_proportions': [435.2, 298.65, 617.25, 40.7]}, 'weight': {'score': [22.67064144736842, 23.435291894466534, 22.49813988095238, 23.43671110085463, 29.369140625], 'topk_indices': array([18821, 18777, 14590, 14626, 19422, 14671, 19480, 14635, 14554,
       14489, 20294, 20342, 18140, 18109, 23499, 23633, 21965, 21938,
       23715, 23581]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 19.909375, 27.341796875, 23.49375]}, 'saliency': {'score': [2.0425896895559212, 0.09114561087252526, 0.9797479538690477, 0.08883321099292389, 0.11872972760881696], 'topk_indices': array([   24,   441, 24125,    39, 24131,     0,   547,    34,  7130,
         452,   122, 24123, 24129,   546,   451,   430, 16698, 24128,
         568,   429]), 'topk_tokens': ['\n\n', 'positor', ' Where', '\n\n\n', ' office', '<|begin_of_text|>', 's', ' hallway', ' hallway', 's', ' Published', 'Question', ' before', ' em', ' em', 's', ' football', ' football', ' em', ' em'], 'evidence_proportions': [2.369921875, 1.631884765625, 4.412841796875, 0.2297607421875]}}, 'pred_res': 'Daniel dropped the football.<|eot_id|>', 'score': 0}
2025-01-22 06:07:51.653 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:07:51.653 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-56_pid-3_0-2-7-8.pkl | len: 3 |  size: 2.0 KB
Processing depth (0, 2, 7, 8):   4%|▍         | 4/100 [01:57<46:59, 29.37s/it]is_0k: False
your chose emoji: ['🙆🏼', '🚣🏽', '👨🏽\u200d🦽\u200d➡', '®', '👅', '🎾', '😉', '🤦🏽\u200d♀️', '🤦🏿\u200d♂️', '👶']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
Processing depth (1, 2, 4, 9):   4%|▍         | 4/100 [02:14<46:59, 29.37s/it]2025-01-22 06:08:09.471 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:08:09.503 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2915, 2920) --> . Daniel went to the
2025-01-22 06:08:09.504 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel moved to the office.
2025-01-22 06:08:09.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4859, 4864) --> . Daniel moved to the
2025-01-22 06:08:09.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel dropped the football.
2025-01-22 06:08:09.590 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9697, 9701) -->  Daniel dropped the football
2025-01-22 06:08:09.591 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra picked up the apple.
2025-01-22 06:08:09.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21429, 21434) --> . Sandra picked up the
2025-01-22 06:08:09.716 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:08:09.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (14811, 14816) --> . Sandra moved to the
2025-01-22 06:08:09.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went back to the garden.
2025-01-22 06:08:09.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5031, 5037) --> . Sandra went back to the
2025-01-22 06:08:09.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary went back to the hallway.
2025-01-22 06:08:09.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7175, 7181) -->  cold. Mary went back to
2025-01-22 06:08:09.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra left the apple.
2025-01-22 06:08:09.914 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9032, 9036) -->  Sandra left the apple
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:08:12.590 | INFO     | test_jbb_embedding:begin_test:693 - Bridge Square<|eot_id|>
2025-01-22 06:08:12.590 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24147])
2025-01-22 06:08:20.950 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [495.5361842105263, 18.967823498964805, 260.5416666666667, 18.381848817917877, 26.337658898305083], 'topk_indices': array([ 9699,  9698, 24129, 24132, 24133, 24137, 19535, 24135, 24143,
       14816,  7182, 24145, 24142, 24134,  9701,  9700,  2920,     0,
       24146, 24147]), 'topk_tokens': [' the', ' dropped', 'Question', ' was', ' the', ' office', ' hall', ' before', '<|start_header_id|>', ' hallway', ' hallway', '<|end_header_id|>', '<|eot_id|>', ' football', '.', ' football', ' hallway', '<|begin_of_text|>', '\n\n', 'hall'], 'evidence_proportions': [453.0, 315.175, 1342.375, 40.9625]}, 'weight': {'score': [22.353207236842106, 23.433488612836438, 23.455357142857142, 23.434320886043135, 28.71239406779661], 'topk_indices': array([18739, 18783, 14603, 14639, 14648, 19436, 14684, 19378, 14567,
       14502, 20250, 20298, 18071, 18102, 23625, 23491, 21912, 21939,
       23573, 23707]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.665625, 19.909375, 27.341796875, 23.49375]}, 'saliency': {'score': [3.1086297286184212, 0.10878206683488613, 1.5678594680059523, 0.10514715472379199, 0.19405119297868115], 'topk_indices': array([ 5032, 24144,  9697, 24035,  9701, 24132, 24131, 24001,  9698,
       24135, 24137, 19535, 24129, 24146, 14816,  7182,  9700, 24134,
        2920, 24147]), 'topk_tokens': [' Sandra', 'assistant', ' Daniel', '.\n\n', '.', ' was', ' Where', ' context', ' dropped', ' before', ' office', ' hall', 'Question', '\n\n', ' hallway', ' hallway', ' football', ' football', ' hallway', 'hall'], 'evidence_proportions': [2.2697265625, 1.79130859375, 9.3984375, 0.2330078125]}}, 'pred_res': 'Bridge Square<|eot_id|>', 'score': 0}
2025-01-22 06:08:20.957 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:08:20.957 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-56_pid-4_1-2-4-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (1, 2, 4, 9):   5%|▌         | 5/100 [02:26<46:27, 29.35s/it]Processing depth (1, 2, 4, 9):   5%|▌         | 5/100 [02:26<46:29, 29.37s/it]
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:82 - Selected idx: 57
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:86 - Needle: [' Mary went back to the hallway.', ' Daniel went to the hallway.', ' Sandra moved to the hallway.', ' Sandra picked up the apple.', ' Daniel took the milk.', ' Sandra left the apple.', ' Daniel moved to the office.', ' Sandra went back to the garden.', ' Daniel dropped the milk.', ' Sandra picked up the apple.']
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel went to the hallway.', ' Daniel took the milk.', ' Daniel moved to the office.', ' Daniel dropped the milk.', ' Sandra picked up the apple.']
2025-01-22 06:08:21.285 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧎🏽\u200d♂', '🧎\u200d➡', '🐙', '👋🏾', '🇪🇦', '#⃣', '\U0001faf0🏻', '📖', '🚣🏼', '🧑🏼\u200d🏫']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.55s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.63s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.90s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.43s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]
Processing depth (0, 1, 5, 7, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:08:39.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:08:39.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 06:08:39.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel took the milk.
2025-01-22 06:08:39.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3045, 3049) -->  Daniel took the milk
2025-01-22 06:08:39.049 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 06:08:39.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11852, 11857) --> . Daniel moved to the
2025-01-22 06:08:39.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 06:08:39.198 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16801, 16805) -->  Daniel dropped the milk
2025-01-22 06:08:39.198 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:08:39.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21497, 21502) --> . Sandra picked up the
2025-01-22 06:08:39.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went back to the hallway.
2025-01-22 06:08:39.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (757, 763) --> . Mary went back to the
2025-01-22 06:08:39.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the hallway.
2025-01-22 06:08:39.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9282, 9287) --> . Sandra moved to the
2025-01-22 06:08:39.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:08:39.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8542, 8546) -->  Sandra left the apple
2025-01-22 06:08:39.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:08:39.528 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (22323, 22329) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:08:42.242 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:08:42.242 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24217])
2025-01-22 06:08:50.626 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [64.72995923913044, 4.243546784682081, 38.57738095238095, 4.1561792299181, 2.9163804328304597], 'topk_indices': array([24209, 24191,     3,     9, 24189,   429,   568, 24211, 24216,
          24,    26,    28,    23, 24062,    14,   430, 24215,     0,
       24212, 24213]), 'topk_tokens': ['?\n', ':', '<|end_header_id|>', ':', '.\n\n', ' em', ' em', ':', '\n\n', '\n\n', '<|start_header_id|>', '<|end_header_id|>', '4', ' context', '\n', 's', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [152.0, 67.6484375, 27.234375, 64.4296875, 12.8609375]}, 'weight': {'score': [23.27921195652174, 23.452139244426093, 22.49813988095238, 23.453132432474355, 29.668283045977013], 'topk_indices': array([18779, 18823, 14686, 14650, 19418, 19476, 14695, 14731, 14549,
       14614, 20306, 20354, 18148, 18117, 23700, 23566, 21956, 21983,
       23782, 23648]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.498046875, 19.909375, 27.013671875, 23.49375]}, 'saliency': {'score': [0.3980513862941576, 0.023628903931571275, 0.21554129464285715, 0.023105993718993655, 0.021320693794338184], 'topk_indices': array([  546,    19, 24216,    24, 24061, 24189, 24214,   451,    20,
       24177, 24210,   763,    23, 24060,   430, 24190,   568,   429,
          34, 24062]), 'topk_tokens': [' em', '26', '\n\n', '\n\n', ' you', '.\n\n', 'assistant', ' em', ' Jul', ' return', 'Answer', ' hallway', '4', ' provided', 's', 'Question', ' em', ' em', ' hallway', ' context'], 'evidence_proportions': [0.887939453125, 0.457305908203125, 0.148956298828125, 0.434722900390625, 0.080517578125]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:08:50.641 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:08:50.644 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-57_pid-0_0-1-5-7-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 1, 5, 7, 9):   1%|          | 1/100 [00:29<48:12, 29.22s/it]is_0k: False
your chose emoji: ['🚶🏻\u200d♀', '👨🏼\u200d❤\u200d👨🏻', '🧑🏾\u200d🌾', '🖕🏽', '🧽', '👩\u200d❤️\u200d👨', '🏂🏾', '🐻\u200d❄️', '👩🏿\u200d❤️\u200d💋\u200d👩🏻', '🧠']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.31s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.09s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.54s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]
Processing depth (2, 4, 6, 7, 9):   1%|          | 1/100 [00:47<48:12, 29.22s/it]2025-01-22 06:09:09.134 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:09:09.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (634, 639) -->  back to the hallway.
2025-01-22 06:09:09.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel took the milk.
2025-01-22 06:09:09.190 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9724, 9728) -->  Daniel took the milk
2025-01-22 06:09:09.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 06:09:09.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14414, 14419) --> . Daniel moved to the
2025-01-22 06:09:09.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 06:09:09.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16795, 16799) -->  Daniel dropped the milk
2025-01-22 06:09:09.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:09:09.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21449, 21454) --> . Sandra picked up the
2025-01-22 06:09:09.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went back to the hallway.
2025-01-22 06:09:09.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (631, 637) --> . Mary went back to the
2025-01-22 06:09:09.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the hallway.
2025-01-22 06:09:09.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9098, 9103) -->  Press. Sandra moved to
2025-01-22 06:09:09.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:09:09.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8431, 8435) -->  Sandra left the apple
2025-01-22 06:09:09.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:09:09.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (22285, 22291) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:09:12.414 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 06:09:12.414 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24175])
2025-01-22 06:09:20.784 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [419.54755434782606, 16.389755149309288, 267.38169642857144, 15.787142231913483, 16.599668560606062], 'topk_indices': array([ 4901,  4896,  9726,  4902,  8431,   637,  9104, 14416,  4898,
       24173,  9724,   638, 24154,  9727, 24155,  9728,  4897,     0,
       24170, 24171]), 'topk_tokens': [' hallway', '.', ' the', '.', ' Sandra', ' hallway', ' hallway', ' moved', ' went', '<|end_header_id|>', ' Daniel', '.', ' prior', ' milk', ' to', '.', ' Daniel', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [536.5, 713.375, 435.1, 381.9375, 82.06875]}, 'weight': {'score': [23.18274456521739, 23.440844983042435, 23.360863095238095, 23.4411605515041, 29.590198863636363], 'topk_indices': array([18727, 18771, 14582, 14618, 19366, 14627, 14663, 19424, 14481,
       14546, 20286, 20238, 18080, 18049, 23512, 23646, 21928, 21955,
       23594, 23728]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.428125, 26.498046875, 19.909375, 27.013671875, 23.49375]}, 'saliency': {'score': [2.5881559952445654, 0.09230279681877637, 1.701386951264881, 0.08852408666682636, 0.12312270655776515], 'topk_indices': array([24155,     0, 24153,  9100, 24064, 24148, 24020, 16798, 24162,
       24160, 14416,  4898,  4901,  8431,  9724,  9727,   637,  9104,
       24154,  4897]), 'topk_tokens': [' to', '<|begin_of_text|>', ' location', ' Sandra', ' location', 'Question', ' context', ' milk', ' discarded', ' milk', ' moved', ' went', ' hallway', ' Sandra', ' Daniel', ' milk', ' hallway', ' hallway', ' prior', ' Daniel'], 'evidence_proportions': [2.91484375, 4.998046875, 2.3833984375, 2.570068359375, 0.552783203125]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 06:09:20.791 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:09:20.791 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-57_pid-1_2-4-6-7-9.pkl | len: 3 |  size: 2.11 KB
Processing depth (2, 4, 6, 7, 9):   2%|▏         | 2/100 [00:59<48:37, 29.77s/it]is_0k: False
your chose emoji: ['🧖🏿\u200d♂️', '⌚', '⛹🏽', '👩🏻\u200d✈️', '💪', '©️', '🤵🏿', '👩🏻\u200d🦳', '👳🏼\u200d♀', '🏄🏿\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.89s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.79s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.22s/it]
Processing depth (1, 2, 3, 5, 6):   2%|▏         | 2/100 [01:18<48:37, 29.77s/it]2025-01-22 06:09:40.282 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:09:40.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (754, 759) -->  back to the hallway.
2025-01-22 06:09:40.286 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel took the milk.
2025-01-22 06:09:40.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5001, 5005) -->  Daniel took the milk
2025-01-22 06:09:40.314 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 06:09:40.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7629, 7634) --> . Daniel moved to the
2025-01-22 06:09:40.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 06:09:40.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11863, 11867) -->  Daniel dropped the milk
2025-01-22 06:09:40.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:09:40.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14359, 14364) --> . Sandra picked up the
2025-01-22 06:09:40.510 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went back to the hallway.
2025-01-22 06:09:40.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (751, 757) --> . Mary went back to the
2025-01-22 06:09:40.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the hallway.
2025-01-22 06:09:40.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9154, 9159) --> . Sandra moved to the
2025-01-22 06:09:40.577 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:09:40.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8488, 8492) -->  Sandra left the apple
2025-01-22 06:09:40.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:09:40.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (22291, 22297) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:09:43.494 | INFO     | test_jbb_embedding:begin_test:693 - St. Paul<|eot_id|>
2025-01-22 06:09:43.494 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24179])
2025-01-22 06:09:53.066 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [536.5353260869565, 16.16517450996609, 271.3392857142857, 15.447336668323805, 14.757697610294118], 'topk_indices': array([ 5003, 24159,  3040,  3044, 24158,    24, 11864,  5000,   757,
        4965,  5002, 24175,  4976,    14,   758, 24174,  5004,  5005,
        5001,     0]), 'topk_tokens': [' the', ' to', ' went', '.', ' prior', '\n\n', ' dropped', '.', ' hallway', '4', ' took', '<|start_header_id|>', '.', '\n', '.', '<|eot_id|>', ' milk', '.', ' Daniel', '<|begin_of_text|>'], 'evidence_proportions': [612.6, 1052.25, 440.2, 587.6875, 103.3125]}, 'weight': {'score': [23.18274456521739, 23.442922421635927, 22.49813988095238, 23.443992291718452, 29.778492647058822], 'topk_indices': array([18767, 18811, 14643, 14679, 19464, 14724, 14688, 19406, 14542,
       14607, 20332, 20284, 18136, 18105, 23522, 23656, 21961, 21934,
       23738, 23604]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.428125, 26.498046875, 19.909375, 27.013671875, 23.49375]}, 'saliency': {'score': [3.305000636888587, 0.09127392420627946, 1.6093401227678572, 0.08689099668281082, 0.11345414554371554], 'topk_indices': array([ 4965,  4985,  4964,  8491, 24068,  4977,  3043,  7631,  4961,
       11866,  4999,  3040, 24164,  3039, 11864,  5002, 24158,   757,
        5004,  5001]), 'topk_tokens': ['4', ' *\n\n', '185', ' apple', ' location', ' *', ' hallway', ' moved', ' *\n\n', ' milk', ' St', ' went', ' milk', ' Daniel', ' dropped', ' took', ' prior', ' hallway', ' milk', ' Daniel'], 'evidence_proportions': [3.20166015625, 7.34228515625, 2.33857421875, 3.95751953125, 0.6229248046875]}}, 'pred_res': 'St. Paul<|eot_id|>', 'score': 0}
2025-01-22 06:09:53.074 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:09:53.074 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-57_pid-2_1-2-3-5-6.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 3, 5, 6):   3%|▎         | 3/100 [01:31<49:58, 30.91s/it]is_0k: False
your chose emoji: ['🤘🏻', '🙇🏽\u200d♂️', '🤏🏽', '\U0001fae5', '👨🏿\u200d❤️\u200d👨🏾', '🚣\u200d♂', '🇹🇴', '🌔', '🚣\u200d♂️', '🤷\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.33s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.53s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]
Processing depth (0, 2, 4, 5, 9):   3%|▎         | 3/100 [01:49<49:58, 30.91s/it]2025-01-22 06:10:11.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:10:11.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 06:10:11.555 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel took the milk.
2025-01-22 06:10:11.580 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4959, 4963) -->  Daniel took the milk
2025-01-22 06:10:11.580 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 06:10:11.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9700, 9705) --> . Daniel moved to the
2025-01-22 06:10:11.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 06:10:11.692 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11911, 11915) -->  Daniel dropped the milk
2025-01-22 06:10:11.693 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:10:11.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21463, 21468) --> . Sandra picked up the
2025-01-22 06:10:11.806 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went back to the hallway.
2025-01-22 06:10:11.811 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (757, 763) --> . Mary went back to the
2025-01-22 06:10:11.811 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the hallway.
2025-01-22 06:10:11.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9070, 9075) --> . Sandra moved to the
2025-01-22 06:10:11.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:10:11.905 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8426, 8430) -->  Sandra left the apple
2025-01-22 06:10:11.906 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:10:12.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (22303, 22309) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:10:14.736 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:10:14.737 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 06:10:23.106 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [173.62907608695653, 8.454790255287508, 120.58928571428571, 8.200120789190532, 7.5727719907407405], 'topk_indices': array([   25,  4959,    39,  4962,   429,   568, 24084, 24179, 24178,
       24184,    28,    24, 24185,    23,   430,    14, 24203,     0,
       24200, 24201]), 'topk_tokens': ['<|eot_id|>', ' Daniel', '\n\n\n', ' milk', ' em', ' em', '.\n\n', ':', 'Question', ' prior', '<|end_header_id|>', '\n\n', ' to', '4', 's', '\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [225.85, 276.03125, 153.775, 167.921875, 63.90625]}, 'weight': {'score': [23.27921195652174, 23.450532881692002, 22.49813988095238, 23.451523637953155, 29.888503086419753], 'topk_indices': array([18747, 18791, 14653, 14617, 19386, 19444, 14698, 14662, 14581,
       14516, 20258, 20306, 18116, 18085, 23534, 23668, 21973, 21946,
       23616, 23750]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.498046875, 19.909375, 27.013671875, 23.49375]}, 'saliency': {'score': [1.060875934103261, 0.047612303073880535, 0.7299470447358631, 0.0460548584004582, 0.05637463228202161], 'topk_indices': array([   19,  8426, 24050,    24,    20, 24190, 24192, 24165, 17508,
         430, 17518,    23,    39,   568,   429,  4962,  4959,  9075,
       24178, 24184]), 'topk_tokens': ['26', ' Sandra', ' context', '\n\n', ' Jul', ' milk', ' discarded', ' return', 'ente', 's', 'ente', '4', '\n\n\n', ' em', ' em', ' milk', ' Daniel', ' hallway', 'Question', ' prior'], 'evidence_proportions': [1.2439453125, 1.944091796875, 0.798681640625, 1.13446044921875, 0.374560546875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:10:23.114 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:10:23.115 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-57_pid-3_0-2-4-5-9.pkl | len: 3 |  size: 2.06 KB
Processing depth (0, 2, 4, 5, 9):   4%|▍         | 4/100 [02:01<48:54, 30.57s/it]is_0k: False
your chose emoji: ['🏃🏾\u200d♂️', '🔳', '👯\u200d♂', '👫', '🧑🏽\u200d🦽\u200d➡', '🍁', '🥒', '🧑🏻\u200d❤️\u200d💋\u200d🧑🏼', '🏌️\u200d♂️', '👨🏼\u200d🦼\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.83s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.99s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.13s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.51s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.07s/it]
Processing depth (0, 2, 5, 8, 9):   4%|▍         | 4/100 [02:20<48:54, 30.57s/it]2025-01-22 06:10:41.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel went to the hallway.
2025-01-22 06:10:41.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  went to the hallway.
2025-01-22 06:10:41.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel took the milk.
2025-01-22 06:10:41.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4975, 4979) -->  Daniel took the milk
2025-01-22 06:10:41.700 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel moved to the office.
2025-01-22 06:10:41.759 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11862, 11867) --> . Daniel moved to the
2025-01-22 06:10:41.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel dropped the milk.
2025-01-22 06:10:41.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19214, 19218) -->  dropped the milk.
2025-01-22 06:10:41.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:10:41.956 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21465, 21470) --> . Sandra picked up the
2025-01-22 06:10:41.956 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary went back to the hallway.
2025-01-22 06:10:41.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (757, 763) --> . Mary went back to the
2025-01-22 06:10:41.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the hallway.
2025-01-22 06:10:42.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9050, 9055) --> . Sandra moved to the
2025-01-22 06:10:42.004 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:10:42.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (8420, 8424) -->  Sandra left the apple
2025-01-22 06:10:42.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:10:42.154 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (22305, 22311) --> . Sandra went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:10:44.846 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:10:44.847 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24191])
2025-01-22 06:10:53.212 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [206.0774456521739, 10.606701299909068, 135.34821428571428, 10.312067805383023, 9.385425464527026], 'topk_indices': array([ 4975,    24,  7233,  4979,   568,   429,    23, 17538,  4978,
       24170, 24171, 17528, 24189,    14,   430, 17539, 17529,     0,
       24186, 24187]), 'topk_tokens': [' Daniel', '\n\n', 'nes', '.', ' em', ' em', '4', 'arp', ' milk', ' prior', ' to', 'arp', '<|end_header_id|>', '\n', 's', 'ente', 'ente', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [262.0, 360.59375, 115.6625, 267.375, 67.91875]}, 'weight': {'score': [22.216372282608695, 23.447665226915763, 22.49813988095238, 23.449663561076605, 30.028716216216218], 'topk_indices': array([18826, 18782, 14628, 14664, 19484, 19426, 14673, 14709, 14592,
       14527, 20298, 20346, 18151, 18120, 23668, 23534, 21948, 21975,
       23616, 23750]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.871875, 26.498046875, 19.909375, 20.90234375, 23.49375]}, 'saliency': {'score': [1.2425749405570652, 0.06003714541990008, 0.8059140159970238, 0.05826233036523033, 0.07109822453679265], 'topk_indices': array([   34,   763,   451, 24164, 10888,    30,    31, 24176,   430,
        7233,   568,   429,  4975,  9055, 17538,  4978, 17528, 24170,
       17539, 17529]), 'topk_tokens': [' hallway', ' hallway', ' em', 'Question', 'walk', 'Daniel', ' went', ' milk', 's', 'nes', ' em', ' em', ' Daniel', ' hallway', 'arp', ' milk', 'arp', ' prior', 'ente', 'ente'], 'evidence_proportions': [1.48642578125, 2.5360107421875, 0.6124267578125, 1.46142578125, 0.41904296875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:10:53.219 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:10:53.220 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-57_pid-4_0-2-5-8-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 2, 5, 8, 9):   5%|▌         | 5/100 [02:31<48:08, 30.40s/it]Processing depth (0, 2, 5, 8, 9):   5%|▌         | 5/100 [02:32<48:09, 30.41s/it]
2025-01-22 06:10:53.489 | INFO     | __main__:<module>:82 - Selected idx: 58
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the apple before the bedroom? 
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:86 - Needle: [' Sandra moved to the hallway.', ' Mary went back to the hallway.', ' Sandra left the apple.', ' Daniel moved to the office.', ' Sandra went back to the garden.', ' Sandra picked up the apple.', ' Sandra picked up the apple.', ' John went back to the hallway.', ' Daniel went back to the bedroom.', ' Daniel left the apple.', ' Sandra dropped the apple.']
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the office.', ' Daniel went back to the bedroom.', ' Daniel left the apple.', ' Sandra dropped the apple.']
2025-01-22 06:10:53.490 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👨🏽\u200d💻', '🏃\u200d♂️\u200d➡️', '🧗🏻\u200d♂️', '🎎', '👨🏽\u200d⚕️', '5️⃣', '🙌', '🙎\u200d♀️', '🚶🏿\u200d➡️', '🧟\u200d♂']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.59s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.20s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.75s/it]
Processing depth (0, 2, 4, 9):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 06:11:10.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:11:10.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:11:10.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the bedroom.
2025-01-22 06:11:10.657 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4963, 4969) --> . Daniel went back to the
2025-01-22 06:11:10.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 06:11:10.704 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9765, 9769) -->  Daniel left the apple
2025-01-22 06:11:10.704 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the apple.
2025-01-22 06:11:10.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21486, 21490) -->  Sandra dropped the apple
2025-01-22 06:11:10.812 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:11:10.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10560, 10565) --> . Sandra moved to the
2025-01-22 06:11:10.870 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:11:10.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8968, 8974) -->  John went back to the hallway
2025-01-22 06:11:10.915 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:11:10.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6687, 6691) -->  Sandra left the apple
2025-01-22 06:11:10.947 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:11:11.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16093, 16099) -->  city. Sandra went back to
2025-01-22 06:11:11.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:11:11.047 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2665, 2670) --> . Sandra picked up the
2025-01-22 06:11:11.047 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:11:11.060 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2665, 2670) --> . Sandra picked up the
2025-01-22 06:11:11.060 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John went back to the hallway.
2025-01-22 06:11:11.104 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8967, 8973) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:11:13.812 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:11:13.813 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24192])
2025-01-22 06:11:22.244 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [279.24342105263156, 15.749687422501447, 240.29054054054055, 15.198092737592178, 17.13363986545139], 'topk_indices': array([   13,     3,     4, 24046, 24080, 24182, 24175, 18950,     9,
       24183, 24180, 18951,     1,    23, 24188,    14,    24, 24190,
       24187,     0]), 'topk_tokens': ['3', '<|end_header_id|>', '\n\n', ' context', '.\n\n', ' bedroom', ':', ' manner', ':', '?', ' before', ' in', '<|start_header_id|>', '4', '<|start_header_id|>', '\n', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [491.9, 254.70833333333334, 148.40625, 181.0625]}, 'weight': {'score': [23.203125, 23.446058423576094, 23.552153716216218, 23.446087017876376, 29.780164930555557], 'topk_indices': array([18807, 18851, 14614, 14650, 14695, 19504, 19446, 14659, 14578,
       14513, 20318, 20366, 18133, 18164, 23678, 23544, 21973, 22000,
       23760, 23626]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 20.927083333333332, 26.533203125, 28.498046875]}, 'saliency': {'score': [1.576403166118421, 0.08903726049717905, 1.5636184279983107, 0.08560617857637594, 0.12695174747043186], 'topk_indices': array([   34,    19,  1214,    39, 24161,    31,     8, 24183,    20,
       18807,  4969,  6687, 24046,    23, 24174,  2666, 24180,    24,
       24182, 18950]), 'topk_tokens': [' office', '26', 'nes', '\n\n\n', ' return', ' moved', ' Date', '?', ' Jul', 'untlet', ' bedroom', ' Sandra', ' context', '4', 'Question', ' Sandra', ' before', '\n\n', ' bedroom', ' manner'], 'evidence_proportions': [2.5283203125, 1.3749186197916667, 1.0009765625, 1.26416015625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:11:22.251 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:11:22.251 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-58_pid-0_0-2-4-9.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 2, 4, 9):   1%|          | 1/100 [00:28<47:12, 28.61s/it]is_0k: False
your chose emoji: ['🇪🇪', '🕵\u200d♀️', '👩🏻\u200d❤️\u200d💋\u200d👨🏻', '🤞🏿', '🚶🏿\u200d➡️', '💖', '🧞\u200d♀', '🚶🏿\u200d♀️', '🌷', '🧍🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]
Processing depth (0, 3, 5, 6):   1%|          | 1/100 [00:46<47:12, 28.61s/it]2025-01-22 06:11:40.251 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:11:40.252 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:11:40.252 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the bedroom.
2025-01-22 06:11:40.293 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7552, 7558) --> . Daniel went back to the
2025-01-22 06:11:40.294 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 06:11:40.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11894, 11898) -->  Daniel left the apple
2025-01-22 06:11:40.361 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the apple.
2025-01-22 06:11:40.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14317, 14321) -->  Sandra dropped the apple
2025-01-22 06:11:40.439 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:11:40.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10515, 10520) --> . Sandra moved to the
2025-01-22 06:11:40.497 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:11:40.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8948, 8954) -->  John went back to the hallway
2025-01-22 06:11:40.550 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:11:40.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6614, 6618) -->  Sandra left the apple
2025-01-22 06:11:40.584 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:11:40.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15993, 15999) --> . Sandra went back to the
2025-01-22 06:11:40.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:11:40.687 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2535, 2540) --> . Sandra picked up the
2025-01-22 06:11:40.687 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:11:40.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2535, 2540) --> . Sandra picked up the
2025-01-22 06:11:40.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John went back to the hallway.
2025-01-22 06:11:40.750 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8947, 8953) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:11:43.446 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:11:43.446 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24198])
2025-01-22 06:11:51.864 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [497.32236842105266, 24.12623708677686, 356.96452702702703, 23.243792453611665, 27.382552083333334], 'topk_indices': array([ 5869, 24182,  6004,  5875,  6008,  7559,  5876,  6012, 24189,
        5868, 24188,  6013, 24186,  5870,  5877,  6005,     0,  6006,
       24194, 24193]), 'topk_tokens': ['the', ' Where', ' on', ' to', ' from', '.', ' the', ' to', '?', '\n', ' bedroom', ' the', ' before', ' way', ' office', '\n', '<|begin_of_text|>', 'the', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [586.2, 673.625, 341.03125, 278.0625]}, 'weight': {'score': [23.203125, 23.444584194214876, 23.09375, 23.445311852841286, 29.28875], 'topk_indices': array([18834, 18790, 14659, 14695, 14704, 19429, 19487, 14740, 14623,
       14558, 20301, 20349, 18128, 18159, 23684, 23550, 21969, 21996,
       23632, 23766]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 20.927083333333332, 26.533203125, 28.498046875]}, 'saliency': {'score': [2.8021432976973686, 0.13471958128873968, 2.299930057010135, 0.12930234146876554, 0.19970011393229167], 'topk_indices': array([ 6004,  5868,  6003,  6005,  6011,  6013,  6008,  6007, 24180,
        5869, 24185,  2536, 24189,  7558, 24182,  5870, 24186,  6006,
        5877, 24188]), 'topk_tokens': [' on', '\n', ' meet', '\n', ' landing', ' the', ' from', ' way', 'Question', 'the', ' apple', ' Sandra', '?', ' bedroom', ' Where', ' way', ' before', 'the', ' office', ' bedroom'], 'evidence_proportions': [3.0689453125, 3.46484375, 2.3045654296875, 1.97216796875]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:11:51.891 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:11:51.892 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-58_pid-1_0-3-5-6.pkl | len: 3 |  size: 1.98 KB
Processing depth (0, 3, 5, 6):   2%|▏         | 2/100 [00:58<47:43, 29.22s/it]is_0k: False
your chose emoji: ['👨🏾\u200d🦲', '👼🏻', '🤸🏿\u200d♀️', '👰🏽\u200d♂️', '👩🏾\u200d🏭', '🧗🏽\u200d♀', '👦', '🥭', '🦹🏾\u200d♂️', '🇰🇬']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.14s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.24s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.85s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]
Processing depth (0, 4, 5, 7):   2%|▏         | 2/100 [01:16<47:43, 29.22s/it]2025-01-22 06:12:10.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:12:10.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:12:10.113 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the bedroom.
2025-01-22 06:12:10.161 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9713, 9719) --> . Daniel went back to the
2025-01-22 06:12:10.162 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 06:12:10.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11833, 11837) -->  left the apple.
2025-01-22 06:12:10.218 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the apple.
2025-01-22 06:12:10.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16736, 16740) -->  Sandra dropped the apple
2025-01-22 06:12:10.303 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:12:10.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10507, 10512) --> . Sandra moved to the
2025-01-22 06:12:10.360 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:12:10.411 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8937, 8943) -->  John went back to the hallway
2025-01-22 06:12:10.411 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:12:10.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6624, 6628) -->  Sandra left the apple
2025-01-22 06:12:10.450 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:12:10.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16008, 16014) --> . Sandra went back to the
2025-01-22 06:12:10.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:12:10.549 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2541, 2546) --> . Sandra picked up the
2025-01-22 06:12:10.549 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:12:10.567 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2541, 2546) --> . Sandra picked up the
2025-01-22 06:12:10.567 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John went back to the hallway.
2025-01-22 06:12:10.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8936, 8942) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:12:13.291 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:12:13.291 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24234])
2025-01-22 06:12:21.690 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [191.56578947368422, 12.139045479864665, 197.2652027027027, 11.714778484284533, 19.0165280577957], 'topk_indices': array([10079, 10050, 24215, 10044, 10085, 24217, 10086,  9720,     3,
       24225, 24224, 10051,     1,    23, 24222,    14,    24, 24232,
       24229,     0]), 'topk_tokens': [' as', 'po', '.\n\n', ' if', '\n', ':', 'po', '.', '<|end_header_id|>', '?', ' bedroom', 'or', '<|start_header_id|>', '4', ' before', '\n', '\n\n', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [249.1, 212.10416666666666, 169.6875, 110.71875]}, 'weight': {'score': [21.916529605263158, 23.456180887935304, 23.09375, 23.457945293114143, 29.668850806451612], 'topk_indices': array([18872, 18828, 14710, 14674, 19525, 19467, 14755, 14719, 14638,
       14573, 20369, 20417, 18197, 18166, 23714, 23580, 22019, 22046,
       23662, 23796]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 20.927083333333332, 20.421875, 28.498046875]}, 'saliency': {'score': [1.0492136101973684, 0.06869257013875278, 1.266380516258446, 0.06608941241470223, 0.1410424017137097], 'topk_indices': array([24221, 12039,    22,    20, 10044, 10130, 24218, 24225, 24231,
        9719, 10050,    23, 10086, 10512, 10051, 24216,    24,  2542,
       24222, 24224]), 'topk_tokens': [' apple', 'ANT', '202', ' Jul', ' if', ' hallway', ' Where', '?', 'assistant', ' bedroom', 'po', '4', 'po', ' hallway', 'or', 'Question', '\n\n', ' Sandra', ' before', ' bedroom'], 'evidence_proportions': [1.32685546875, 1.0799967447916667, 0.93603515625, 0.7691650390625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 0}
2025-01-22 06:12:21.698 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:12:21.699 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-58_pid-2_0-4-5-7.pkl | len: 3 |  size: 1.98 KB
Processing depth (0, 4, 5, 7):   3%|▎         | 3/100 [01:28<47:40, 29.49s/it]is_0k: False
your chose emoji: ['🙆🏻\u200d♀', '🚴🏿\u200d♀️', '🍑', '👩🏾\u200d❤\u200d👨🏻', '👨🏿\u200d🦲', '🚶🏽\u200d♂', '🧎🏿\u200d♂\u200d➡️', '💱', '👩🏼\u200d❤\u200d👨🏼', '🧙🏾\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.38s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.21s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (2, 5, 6, 9):   3%|▎         | 3/100 [01:45<47:40, 29.49s/it]2025-01-22 06:12:39.032 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:12:39.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4971, 4976) --> . Daniel moved to the
2025-01-22 06:12:39.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the bedroom.
2025-01-22 06:12:39.119 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11912, 11918) --> . Daniel went back to the
2025-01-22 06:12:39.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 06:12:39.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14344, 14348) -->  Daniel left the apple
2025-01-22 06:12:39.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the apple.
2025-01-22 06:12:39.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21516, 21520) -->  Sandra dropped the apple
2025-01-22 06:12:39.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:12:39.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10504, 10509) --> . Sandra moved to the
2025-01-22 06:12:39.358 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:12:39.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8937, 8943) -->  John went back to the hallway
2025-01-22 06:12:39.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:12:39.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6624, 6628) -->  Sandra left the apple
2025-01-22 06:12:39.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:12:39.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16020, 16026) --> . Sandra went back to the
2025-01-22 06:12:39.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:12:39.568 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2535, 2540) --> . Sandra picked up the
2025-01-22 06:12:39.569 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:12:39.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2535, 2540) --> . Sandra picked up the
2025-01-22 06:12:39.583 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John went back to the hallway.
2025-01-22 06:12:39.631 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8936, 8942) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:12:42.272 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 06:12:42.272 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24220])
2025-01-22 06:12:50.657 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [179.60197368421052, 9.652741438568244, 171.6081081081081, 9.271156402590416, 16.381517986918606], 'topk_indices': array([24073, 24118, 24208, 24214, 12066, 24219,     9,     3, 12023,
        2536, 12022, 24074, 24216,     1,    14,    23,    24, 24218,
           0, 24215]), 'topk_tokens': [' you', ' location', ' before', ':', 'ANT', '\n\n', ':', '<|end_header_id|>', 'IC', ' Sandra', 'ANT', ' context', '<|start_header_id|>', '<|start_header_id|>', '\n', '4', '\n\n', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [288.0, 182.0, 125.078125, 95.03125]}, 'weight': {'score': [23.433388157894736, 23.454940240277434, 23.09375, 23.45551019510883, 30.009447674418606], 'topk_indices': array([18837, 18881, 14722, 14686, 14767, 19534, 19476, 14731, 14650,
       14585, 20348, 20396, 18206, 18175, 23576, 23710, 22015, 22042,
       23792, 23658]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 20.927083333333332, 26.533203125, 28.498046875]}, 'saliency': {'score': [1.0293611225328947, 0.054590128993980164, 1.0986196157094594, 0.05222524693299729, 0.1220246248467024], 'topk_indices': array([16026, 24189,  4972, 24208,    22, 24213, 12066, 10509, 24118,
       24202,    20, 11918, 12023, 24210, 24217,    24, 12022,    23,
       24074,  2536]), 'topk_tokens': [' garden', ' return', ' Daniel', ' before', '202', 'Answer', 'ANT', ' hallway', ' location', 'Question', ' Jul', ' bedroom', 'IC', ' bedroom', 'assistant', '\n\n', 'ANT', '4', ' context', ' Sandra'], 'evidence_proportions': [1.61015625, 0.9140625, 0.82928466796875, 0.6763916015625]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 06:12:50.664 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:12:50.664 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-58_pid-3_2-5-6-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (2, 5, 6, 9):   4%|▍         | 4/100 [01:57<46:50, 29.28s/it]is_0k: False
your chose emoji: ['🏄🏾\u200d♂️', '👨🏾\u200d🦼', '👩\u200d❤️\u200d👨', '🧎🏼\u200d♂\u200d➡️', '\U0001faf1', '👏🏾', '🧔🏼\u200d♀️', '🏍', '👩🏻\u200d❤\u200d💋\u200d👨🏿', '🖐️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.75s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.23s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.79s/it]
Processing depth (0, 1, 4, 9):   4%|▍         | 4/100 [02:14<46:50, 29.28s/it]2025-01-22 06:13:08.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:13:08.005 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:13:08.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went back to the bedroom.
2025-01-22 06:13:08.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2978, 2984) -->  tragedy. Daniel went back to
2025-01-22 06:13:08.021 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel left the apple.
2025-01-22 06:13:08.067 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9797, 9801) -->  Daniel left the apple
2025-01-22 06:13:08.067 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the apple.
2025-01-22 06:13:08.171 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21492, 21496) -->  Sandra dropped the apple
2025-01-22 06:13:08.171 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:13:08.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (10576, 10581) --> . Sandra moved to the
2025-01-22 06:13:08.222 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary went back to the hallway.
2025-01-22 06:13:08.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (8974, 8980) -->  John went back to the hallway
2025-01-22 06:13:08.271 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra left the apple.
2025-01-22 06:13:08.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (6610, 6614) -->  left the apple.
2025-01-22 06:13:08.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:13:08.381 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15998, 16004) --> . Sandra went back to the
2025-01-22 06:13:08.381 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra picked up the apple.
2025-01-22 06:13:08.402 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2665, 2670) --> . Sandra picked up the
2025-01-22 06:13:08.403 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra picked up the apple.
2025-01-22 06:13:08.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (2665, 2670) --> . Sandra picked up the
2025-01-22 06:13:08.417 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  John went back to the hallway.
2025-01-22 06:13:08.462 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (8973, 8979) --> . John went back to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:13:11.105 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 06:13:11.105 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24194])
2025-01-22 06:13:19.480 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [397.2138157894737, 18.554208340221525, 260.56418918918916, 17.885239747307374, 37.59147581335616], 'topk_indices': array([   31,  7524,    34,  2986, 24082, 24175,  2985,    23,     1,
       24177, 24176,    14, 24184, 24185,    24, 24192, 24182, 24189,
       24190,     0]), 'topk_tokens': [' moved', 'nes', ' office', '.', '.\n\n', '.\n\n', ' bedroom', '4', '<|start_header_id|>', ':', 'Question', '\n', ' bedroom', '?', '\n\n', '<|end_header_id|>', ' before', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [744.7, 389.46875, 186.875, 184.8125]}, 'weight': {'score': [24.731496710526315, 23.44447945941478, 22.280194256756758, 23.44525100973488, 29.431720890410958], 'topk_indices': array([18839, 18795, 14700, 14664, 14745, 19492, 19434, 14709, 14628,
       14563, 20354, 20306, 18164, 18133, 23546, 23680, 22012, 21985,
       23762, 23628]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 25.766927083333332, 26.533203125, 28.498046875]}, 'saliency': {'score': [2.246851870888158, 0.10491087716913436, 1.5707842852618243, 0.10097822617575859, 0.2809140976161173], 'topk_indices': array([ 2672,  2679, 24175,    39, 24178,    23, 24181, 10264,    38,
        1214,  2666,  7524,    24,    31,    34, 24185,  2985, 24182,
       24176, 24184]), 'topk_tokens': ['�', '�', '.\n\n', '\n\n\n', ' Where', '4', ' apple', ' corner', '***', 'nes', ' Sandra', 'nes', '\n\n', ' moved', ' office', '?', ' bedroom', ' before', 'Question', ' bedroom'], 'evidence_proportions': [3.8263671875, 2.2386881510416665, 1.26239013671875, 1.2691650390625]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 06:13:19.488 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:13:19.488 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-58_pid-4_0-1-4-9.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 1, 4, 9):   5%|▌         | 5/100 [02:25<46:06, 29.12s/it]Processing depth (0, 1, 4, 9):   5%|▌         | 5/100 [02:26<46:16, 29.22s/it]
2025-01-22 06:13:19.759 | INFO     | __main__:<module>:82 - Selected idx: 59
2025-01-22 06:13:19.759 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 06:13:19.759 | INFO     | __main__:<module>:84 - Answer: office
2025-01-22 06:13:19.759 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:13:19.759 | INFO     | __main__:<module>:86 - Needle: [' Sandra moved to the hallway.', ' Sandra picked up the apple.', ' Daniel moved to the office.', ' Daniel picked up the milk.', ' John went back to the hallway.', ' Daniel went back to the bedroom.', ' Sandra went back to the garden.', ' Mary went back to the hallway.', ' Sandra left the apple.', ' Sandra picked up the apple.', ' Daniel left the milk.', ' Sandra dropped the apple.']
2025-01-22 06:13:19.760 | INFO     | __main__:<module>:87 - Real Needle: [' Daniel moved to the office.', ' Daniel picked up the milk.', ' Daniel went back to the bedroom.', ' Daniel left the milk.', ' Sandra dropped the apple.']
2025-01-22 06:13:19.760 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🏃', '🦹🏾\u200d♂', '👨🏾\u200d🦽\u200d➡️', '🏃🏽\u200d➡️', '🛷', '🚵🏼\u200d♂', '🚼', '👨\u200d⚖', '🦹🏼\u200d♀️', '🇧🇮']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.49s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.98s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.46s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.97s/it]
Processing depth (1, 2, 3, 6, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:13:37.814 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:13:37.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2909, 2914) --> . Daniel moved to the
2025-01-22 06:13:37.831 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the milk.
2025-01-22 06:13:37.854 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4910, 4915) --> . Daniel picked up the
2025-01-22 06:13:37.855 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:13:37.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7530, 7536) --> . Daniel went back to the
2025-01-22 06:13:37.895 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 06:13:37.967 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14362, 14366) -->  Daniel left the milk
2025-01-22 06:13:37.967 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra dropped the apple.
2025-01-22 06:13:38.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19274, 19278) -->  dropped the apple.
2025-01-22 06:13:38.075 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:13:38.139 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12333, 12338) --> . Sandra moved to the
2025-01-22 06:13:38.146 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 06:13:38.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9685, 9690) --> . Sandra picked up the
2025-01-22 06:13:38.197 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:13:38.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17603, 17609) -->  city. John went back to
2025-01-22 06:13:38.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:13:38.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3845, 3851) --> . Sandra went back to the
2025-01-22 06:13:38.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary went back to the hallway.
2025-01-22 06:13:38.408 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17605, 17611) -->  John went back to the hallway
2025-01-22 06:13:38.408 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra left the apple.
2025-01-22 06:13:38.529 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (21700, 21704) -->  Sandra left the apple
2025-01-22 06:13:38.529 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra picked up the apple.
2025-01-22 06:13:38.579 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9685, 9690) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:13:41.270 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:13:41.270 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24197])
2025-01-22 06:13:49.675 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [412.87890625, 19.902740040910782, 192.77702702702703, 19.247019740657883, 30.765479244402986], 'topk_indices': array([14365, 24182, 24037, 24155,    24,    25, 24061,  4916,  7536,
       24190,    14, 24170, 24191, 24169, 24076, 24171, 24189,     0,
       24192, 24193]), 'topk_tokens': [' milk', ' milk', '\n', '.\n\n', '\n\n', '<|eot_id|>', '.', '.', ' bedroom', 'Answer', '\n', 'Question', ':', '.\n\n', '.\n\n', ':', '?\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [358.9625, 578.55, 441.75, 558.0625, 84.6953125]}, 'weight': {'score': [21.9443359375, 23.441485185338237, 23.552153716216218, 23.44280413663104, 29.52891791044776], 'topk_indices': array([18782, 18826, 14604, 14640, 19426, 19484, 14649, 14685, 14568,
       14503, 20298, 20346, 18114, 18145, 23658, 23524, 21959, 21986,
       23606, 23740]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 20.927083333333332, 26.462890625, 20.97265625]}, 'saliency': {'score': [2.5498530069986978, 0.10319310392474895, 1.1498858477618243, 0.09915600602120489, 0.24079644502098882], 'topk_indices': array([ 3846, 24086,  4915,  2672,  4911, 24172, 24191,    27, 24176,
       24193, 24171, 14365, 24182, 24184, 24169, 24190,  7536, 24076,
       24189, 24170]), 'topk_tokens': [' Sandra', ' location', ' milk', '�', ' Daniel', ' Where', ':', 'user', ' prior', '<|start_header_id|>', ':', ' milk', ' milk', ' discarded', '.\n\n', 'Answer', ' bedroom', '.\n\n', '?\n', 'Question'], 'evidence_proportions': [2.213037109375, 3.5298828125, 2.4967447916666665, 3.9072265625, 0.4681243896484375]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:13:49.679 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:13:49.680 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-59_pid-0_1-2-3-6-8.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 2, 3, 6, 8):   1%|          | 1/100 [00:29<49:08, 29.78s/it]is_0k: False
your chose emoji: ['↔️', '🧑\u200d💻', '🧏🏻', '🧈', '💬', '🤸🏼', '👩🏿\u200d🤝\u200d👨🏽', '🚧', '🧘\u200d♂️', '🏃🏿\u200d♂️\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.24s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.41s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.08s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.47s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.11s/it]
Processing depth (0, 1, 4, 5, 6):   1%|          | 1/100 [00:48<49:08, 29.78s/it]2025-01-22 06:14:08.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:14:08.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  moved to the office.
2025-01-22 06:14:08.551 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the milk.
2025-01-22 06:14:08.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2974, 2979) -->  tragedy. Daniel picked up
2025-01-22 06:14:08.566 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:14:08.616 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9792, 9798) --> . Daniel went back to the
2025-01-22 06:14:08.617 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 06:14:08.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11922, 11926) -->  Daniel left the milk
2025-01-22 06:14:08.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra dropped the apple.
2025-01-22 06:14:08.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14439, 14443) -->  Sandra dropped the apple
2025-01-22 06:14:08.743 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:14:08.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12554, 12559) --> . Sandra moved to the
2025-01-22 06:14:08.805 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 06:14:08.857 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9798, 9803) -->  bedroom. Sandra picked up
2025-01-22 06:14:08.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:14:08.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17663, 17669) --> . John went back to the
2025-01-22 06:14:08.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:14:08.971 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3865, 3871) --> . Sandra went back to the
2025-01-22 06:14:08.971 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary went back to the hallway.
2025-01-22 06:14:09.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17664, 17670) -->  John went back to the hallway
2025-01-22 06:14:09.066 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra left the apple.
2025-01-22 06:14:09.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (21732, 21736) -->  Sandra left the apple
2025-01-22 06:14:09.172 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra picked up the apple.
2025-01-22 06:14:09.227 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9798, 9803) -->  bedroom. Sandra picked up
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:14:11.876 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 06:14:11.876 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24181])
2025-01-22 06:14:20.240 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [613.4479166666666, 23.535743083984617, 280.03969594594594, 22.55537087513473, 24.010129766949152], 'topk_indices': array([18758, 24179, 24060,  2975, 24155,    14,    31, 24173, 18757,
           1, 11925,  2980,  9798,  2981,    24, 18802, 18759,     0,
       24176, 24177]), 'topk_tokens': [' ga', '<|end_header_id|>', '.\n\n', '.', ':', '\n', ' moved', '?\n', ' the', '<|start_header_id|>', ' milk', ' milk', ' bedroom', '.', '\n\n', ' ga', 'untlet', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [837.55, 652.2, 491.0416666666667, 789.0625, 292.875]}, 'weight': {'score': [24.226236979166668, 23.434853512798245, 24.470861486486488, 23.432477030822486, 28.98013771186441], 'topk_indices': array([18803, 18759, 14605, 14641, 19456, 19398, 14650, 14686, 14569,
       14504, 20318, 20270, 18116, 18085, 23526, 23660, 21958, 21931,
       23742, 23608]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.034375, 28.1703125, 20.927083333333332, 26.462890625, 28.498046875]}, 'saliency': {'score': [3.6736653645833335, 0.13112916841117728, 1.8624432538006757, 0.12494893916536254, 0.18080113297801906], 'topk_indices': array([   27, 24160, 24164, 13437,    30, 13463,     0,  2977,    24,
          34, 24168, 24166, 24154, 18758,    31,  2980, 11925,  9798,
       18802, 18759]), 'topk_tokens': ['user', ' prior', ' where', 'nes', 'Daniel', 'nes', '<|begin_of_text|>', ' picked', '\n\n', ' office', ' discarded', ' milk', 'Question', ' ga', ' moved', ' milk', ' milk', ' bedroom', ' ga', 'untlet'], 'evidence_proportions': [4.383984375, 4.072265625, 2.6668294270833335, 5.3515625, 2.119873046875]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:14:20.247 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:14:20.247 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-59_pid-1_0-1-4-5-6.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 4, 5, 6):   2%|▏         | 2/100 [01:00<49:24, 30.25s/it]is_0k: False
your chose emoji: ['🏃🏻', '⛹🏼\u200d♂️', '🟨', '🇺🇸', '🗑️', '🧑🏾\u200d🦯\u200d➡', '0⃣', '😕', '\U0001faf1🏽', '🤵\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.89s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.10s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.82s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.50s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.02s/it]
Processing depth (2, 4, 5, 6, 7):   2%|▏         | 2/100 [01:19<49:24, 30.25s/it]2025-01-22 06:14:39.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:14:39.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4964, 4969) --> . Daniel moved to the
2025-01-22 06:14:39.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the milk.
2025-01-22 06:14:39.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9792, 9797) --> . Daniel picked up the
2025-01-22 06:14:39.919 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:14:39.982 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11930, 11936) --> . Daniel went back to the
2025-01-22 06:14:39.983 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 06:14:40.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14470, 14474) -->  Daniel left the milk
2025-01-22 06:14:40.052 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra dropped the apple.
2025-01-22 06:14:40.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16805, 16809) -->  Sandra dropped the apple
2025-01-22 06:14:40.132 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:14:40.198 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12565, 12570) --> . Sandra moved to the
2025-01-22 06:14:40.198 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 06:14:40.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9795, 9800) -->  up the milk. Sandra
2025-01-22 06:14:40.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:14:40.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17624, 17630) -->  city. John went back to
2025-01-22 06:14:40.336 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:14:40.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3899, 3905) -->  the ground. Sandra went back
2025-01-22 06:14:40.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary went back to the hallway.
2025-01-22 06:14:40.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17626, 17632) -->  John went back to the hallway
2025-01-22 06:14:40.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra left the apple.
2025-01-22 06:14:40.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (21718, 21722) -->  Sandra left the apple
2025-01-22 06:14:40.545 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra picked up the apple.
2025-01-22 06:14:40.593 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9795, 9800) -->  up the milk. Sandra
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:14:43.247 | INFO     | test_jbb_embedding:begin_test:693 - The hallway<|eot_id|>
2025-01-22 06:14:43.247 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24195])
2025-01-22 06:14:51.652 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [609.2239583333334, 31.54101231557631, 442.4336993243243, 30.33669427929234, 38.923828125], 'topk_indices': array([   25, 24170, 24182, 24174,     9,  9798, 24153, 24175, 24188,
       24059, 24074, 24189,    14, 24168, 24167, 24169, 24187,     0,
       24190, 24191]), 'topk_tokens': ['<|eot_id|>', ' Where', ' discarded', ' prior', ':', '.', '.\n\n', ' to', 'Answer', '.', '.\n\n', ':', '\n', 'Question', '.\n\n', ':', '?\n', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [862.2, 810.5, 391.0208333333333, 752.625, 225.3125]}, 'weight': {'score': [23.198567708333332, 23.440478158449395, 23.941089527027028, 23.43995127869158, 29.436079545454547], 'topk_indices': array([18861, 18817, 14672, 14636, 14681, 14717, 19456, 19514, 14535,
       14600, 20328, 20376, 18186, 18155, 23544, 23678, 21977, 22004,
       23626, 23760]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 20.927083333333332, 26.462890625, 28.498046875]}, 'saliency': {'score': [3.6154581705729165, 0.16260939057850662, 2.5267812368032097, 0.1555517700518999, 0.28500504927201703], 'topk_indices': array([24153, 24177,  4965, 24173,    27,  9797, 24178, 14473, 24189,
       24170, 24180, 24191, 24169, 24174, 24074, 24182, 24188, 24167,
       24187, 24168]), 'topk_tokens': ['.\n\n', ' place', ' Daniel', ' location', 'user', ' milk', ' where', ' milk', ':', ' Where', ' milk', '<|start_header_id|>', ':', ' prior', '.\n\n', ' discarded', 'Answer', '.\n\n', '?\n', 'Question'], 'evidence_proportions': [4.79609375, 4.4890625, 2.2392578125, 5.23828125, 1.4891357421875]}}, 'pred_res': 'The hallway<|eot_id|>', 'score': 0}
2025-01-22 06:14:51.657 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:14:51.657 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-59_pid-2_2-4-5-6-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (2, 4, 5, 6, 7):   3%|▎         | 3/100 [01:31<49:45, 30.78s/it]is_0k: False
your chose emoji: ['🏘️', '🗃️', '🤦🏼\u200d♀', '✊🏽', '🧑🏽\u200d🎄', '🏘', '👨🏾\u200d🦽\u200d➡️', '🏊🏾', '🧜🏾\u200d♂', '🥮']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.56s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.98s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.87s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Processing depth (1, 4, 6, 7, 9):   3%|▎         | 3/100 [01:49<49:45, 30.78s/it]2025-01-22 06:15:09.380 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:15:09.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2903, 2908) --> . Daniel moved to the
2025-01-22 06:15:09.395 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the milk.
2025-01-22 06:15:09.444 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9666, 9671) --> . Daniel picked up the
2025-01-22 06:15:09.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:15:09.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14236, 14242) --> . Daniel went back to the
2025-01-22 06:15:09.515 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 06:15:09.595 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16678, 16682) -->  Daniel left the milk
2025-01-22 06:15:09.596 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra dropped the apple.
2025-01-22 06:15:09.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21463, 21467) -->  Sandra dropped the apple
2025-01-22 06:15:09.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:15:09.760 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12212, 12217) --> . Sandra moved to the
2025-01-22 06:15:09.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 06:15:09.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9669, 9674) -->  up the milk. Sandra
2025-01-22 06:15:09.809 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:15:09.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17544, 17550) --> . John went back to the
2025-01-22 06:15:09.902 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:15:09.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3825, 3831) --> . Sandra went back to the
2025-01-22 06:15:09.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary went back to the hallway.
2025-01-22 06:15:10.044 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17545, 17551) -->  John went back to the hallway
2025-01-22 06:15:10.045 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra left the apple.
2025-01-22 06:15:10.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (21667, 21671) -->  left the apple.
2025-01-22 06:15:10.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra picked up the apple.
2025-01-22 06:15:10.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9669, 9674) -->  up the milk. Sandra
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:15:12.854 | INFO     | test_jbb_embedding:begin_test:693 - The bedroom<|eot_id|>
2025-01-22 06:15:12.854 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24213])
2025-01-22 06:15:21.291 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [661.9869791666666, 25.542401404088373, 594.5084459459459, 24.03845118820899, 35.14171875], 'topk_indices': array([16679, 14242,  3831,    23,    14,  3826, 24186, 24185, 24207,
       24092, 24187, 16682,  9671, 24205, 24198, 16681,  9672,     0,
       24208, 24209]), 'topk_tokens': [' left', ' bedroom', ' garden', '4', '\n', ' Sandra', 'Question', '.\n\n', ':', '.\n\n', ':', '.', ' milk', '?\n', ' milk', ' milk', '.', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [526.275, 858.7, 558.0416666666666, 1265.125, 138.515625]}, 'weight': {'score': [23.198567708333332, 23.448412657443733, 22.23120777027027, 23.450525469383955, 29.997291666666666], 'topk_indices': array([18784, 18828, 14641, 14677, 19423, 14686, 19481, 14722, 14605,
       14540, 20343, 20295, 18122, 18153, 23664, 23530, 21996, 21969,
       23612, 23746]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.909375, 22.3625, 20.927083333333332, 26.462890625, 28.498046875]}, 'saliency': {'score': [3.9908243815104165, 0.1371940957793465, 3.376834353885135, 0.12840243326351225, 0.26838887532552086], 'topk_indices': array([ 2904,  9667,  9668, 24196, 16679, 24192, 24185,  9672, 24206,
       24092, 16678,  3831, 24200, 14242, 24205, 24186,  3826,  9671,
       24198, 16681]), 'topk_tokens': [' Daniel', ' Daniel', ' picked', ' where', ' left', ' prior', '.\n\n', '.', 'Answer', '.\n\n', ' Daniel', ' garden', ' discarded', ' bedroom', '?\n', 'Question', ' Sandra', ' milk', ' milk', ' milk'], 'evidence_proportions': [2.8423828125, 4.757421875, 3.1497395833333335, 8.72021484375, 1.0003662109375]}}, 'pred_res': 'The bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:15:21.309 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:15:21.313 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-59_pid-3_1-4-6-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (1, 4, 6, 7, 9):   4%|▍         | 4/100 [02:01<48:32, 30.33s/it]is_0k: False
your chose emoji: ['🙋\u200d♂', '🌩', '🧖🏼', '🕵🏿', '🗄️', '\U0001faa9', '🧑🏼\u200d🦳', '💆🏾\u200d♀️', '🏃\u200d♀️\u200d➡️', '🧑🏽\u200d❤️\u200d💋\u200d🧑🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.13s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.68s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (3, 5, 6, 7, 9):   4%|▍         | 4/100 [02:18<48:32, 30.33s/it]2025-01-22 06:15:38.808 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Daniel moved to the office.
2025-01-22 06:15:38.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7597, 7602) -->  war. Daniel moved to
2025-01-22 06:15:38.846 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel picked up the milk.
2025-01-22 06:15:38.912 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11906, 11911) --> . Daniel picked up the
2025-01-22 06:15:38.913 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Daniel went back to the bedroom.
2025-01-22 06:15:38.992 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14380, 14386) --> . Daniel went back to the
2025-01-22 06:15:38.992 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel left the milk.
2025-01-22 06:15:39.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16812, 16816) -->  Daniel left the milk
2025-01-22 06:15:39.080 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Sandra dropped the apple.
2025-01-22 06:15:39.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21581, 21585) -->  Sandra dropped the apple
2025-01-22 06:15:39.194 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra moved to the hallway.
2025-01-22 06:15:39.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12353, 12358) -->  Sandra moved to the hallway
2025-01-22 06:15:39.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra picked up the apple.
2025-01-22 06:15:39.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9732, 9737) --> . Sandra picked up the
2025-01-22 06:15:39.313 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went back to the hallway.
2025-01-22 06:15:39.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (17631, 17637) -->  city. John went back to
2025-01-22 06:15:39.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra went back to the garden.
2025-01-22 06:15:39.436 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3887, 3893) -->  the ground. Sandra went back
2025-01-22 06:15:39.437 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary went back to the hallway.
2025-01-22 06:15:39.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17633, 17639) -->  John went back to the hallway
2025-01-22 06:15:39.538 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Sandra left the apple.
2025-01-22 06:15:39.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (21806, 21810) -->  Sandra left the apple
2025-01-22 06:15:39.661 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Sandra picked up the apple.
2025-01-22 06:15:39.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (9732, 9737) --> . Sandra picked up the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:15:42.356 | INFO     | test_jbb_embedding:begin_test:693 - hallway<|eot_id|>
2025-01-22 06:15:42.356 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24217])
2025-01-22 06:15:50.733 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [193.8515625, 11.44415670547917, 140.5810810810811, 11.065158280900738, 17.50020292207792], 'topk_indices': array([24189, 11949,     1, 11999, 24096, 24215,     3, 24209, 11950,
       24191,    23, 12000,    24,    14, 12001, 11958, 11957,     0,
       24212, 24213]), 'topk_tokens': ['.\n\n', 'AY', '<|start_header_id|>', 'AT', '.\n\n', '<|end_header_id|>', '<|end_header_id|>', '?\n', 'ING', ':', '4', 'L', '\n\n', '\n', 'ANT', 'IC', 'ANT', '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [263.8125, 240.525, 123.61458333333333, 275.875, 71.390625]}, 'weight': {'score': [23.920247395833332, 23.444725731863414, 24.81376689189189, 23.44215651906201, 29.247564935064936], 'topk_indices': array([18848, 18804, 14625, 14661, 19501, 14706, 19443, 14670, 14589,
       14524, 20315, 20363, 18142, 18173, 23554, 23688, 22008, 21981,
       23770, 23636]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [23.3734375, 22.3625, 20.927083333333332, 26.462890625, 28.498046875]}, 'saliency': {'score': [1.1839192708333333, 0.06288359483063401, 0.9429354281038851, 0.060422017959403974, 0.13026586755529626], 'topk_indices': array([ 7599, 24210,    20, 24209, 24204, 11950, 24096, 11993,     0,
          23, 17638, 11949, 11999,    24, 24190, 12000, 12357, 12001,
       11958, 11957]), 'topk_tokens': [' Daniel', 'Answer', ' Jul', '?\n', ' discarded', 'ING', '.\n\n', 'AY', '<|begin_of_text|>', '4', ' hallway', 'AY', 'AT', '\n\n', 'Question', 'L', ' hallway', 'ANT', 'IC', 'ANT'], 'evidence_proportions': [1.5810546875, 1.36240234375, 0.7104899088541666, 1.841552734375, 0.51690673828125]}}, 'pred_res': 'hallway<|eot_id|>', 'score': 0}
2025-01-22 06:15:50.743 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:15:50.744 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-59_pid-4_3-5-6-7-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (3, 5, 6, 7, 9):   5%|▌         | 5/100 [02:30<47:30, 30.01s/it]Processing depth (3, 5, 6, 7, 9):   5%|▌         | 5/100 [02:31<47:51, 30.23s/it]
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:82 - Selected idx: 60
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the garden? 
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the kitchen.', ' John went back to the office.', ' Mary moved to the bedroom.', ' Sandra grabbed the football there.', ' Daniel went to the office.', ' Sandra journeyed to the hallway.', ' John went to the kitchen.', ' Sandra moved to the garden.', ' John travelled to the bedroom.']
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra grabbed the football there.', ' Sandra journeyed to the hallway.', ' Sandra moved to the garden.', ' John travelled to the bedroom.']
2025-01-22 06:15:51.034 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👨\u200d👩\u200d👧\u200d👦', '🧓🏿', '⛴', '👩\u200d🦲', '🥴', '🧑🏽\u200d❤\u200d💋\u200d🧑🏻', '🗡', '✝️', '🧑🏼\u200d🦽', '🧝🏼\u200d♀️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.01s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.00s/it]
Processing depth (1, 4, 5, 7):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:16:09.144 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra grabbed the football there.
2025-01-22 06:16:09.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2955, 2960) --> . Sandra grabbed the football
2025-01-22 06:16:09.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the hallway.
2025-01-22 06:16:09.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9663, 9669) --> . Sandra journeyed to the
2025-01-22 06:16:09.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:16:09.266 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11831, 11836) --> . Sandra moved to the
2025-01-22 06:16:09.266 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the bedroom.
2025-01-22 06:16:09.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2961, 2966) --> . John travelled to the
2025-01-22 06:16:09.281 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:16:09.295 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2961, 2966) --> . John travelled to the
2025-01-22 06:16:09.296 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:16:09.351 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11216, 11222) --> . John went back to the
2025-01-22 06:16:09.351 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:16:09.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4695, 4700) -->  the senate. Mary moved
2025-01-22 06:16:09.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:16:09.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2965, 2970) -->  the kitchen. Daniel went
2025-01-22 06:16:09.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 06:16:09.412 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2962, 2967) -->  John travelled to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:16:12.447 | INFO     | test_jbb_embedding:begin_test:693 - the football was grabbed in the Sandra grabbed the football there.<|eot_id|>
2025-01-22 06:16:12.447 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24193])
2025-01-22 06:16:20.867 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [588.5654761904761, 15.662120030170277, 220.94831730769232, 14.942901362375254, 24.875250400641026], 'topk_indices': array([24181, 24187,  3007, 24191,  2958, 24176, 24091,    24,    14,
          28,  2960,  3008, 24180,  2956,  3048,  2957,  9669, 24188,
           0,  2959]), 'topk_tokens': [' before', ':', ' participate', '<|end_header_id|>', ' the', ':', ' location', '\n\n', '\n', '<|end_header_id|>', ' there', ' in', ' football', ' Sandra', ' participate', ' grabbed', ' hallway', '<|eot_id|>', '<|begin_of_text|>', ' football'], 'evidence_proportions': [1277.6, 545.75, 218.0, 321.475]}, 'weight': {'score': [22.005208333333332, 23.443025190114067, 22.470853365384617, 23.44532220537082, 29.205528846153847], 'topk_indices': array([18849, 18805, 14669, 14705, 19502, 14714, 14750, 19444, 14568,
       14633, 20364, 20316, 18143, 18174, 23527, 23661, 21993, 21966,
       23609, 23743]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.934375, 21.213541666666668, 21.040625, 19.990625]}, 'saliency': {'score': [3.8016415550595237, 0.08992296755855823, 1.258544921875, 0.08543705670718767, 0.18227288661858973], 'topk_indices': array([ 2963, 24177,  2925,  3008,    24, 24181, 24175, 24183,  9665,
       24186, 24091,  2960,  9664,  3007, 24180,  3048,  2957,  2956,
        9669,  2959]), 'topk_tokens': [' travelled', ' Where', ' tragedy', ' in', '\n\n', ' before', 'Question', ' garden', ' journey', 'Answer', ' location', ' there', ' Sandra', ' participate', ' football', ' participate', ' grabbed', ' Sandra', ' hallway', ' football'], 'evidence_proportions': [9.2802734375, 3.2425130208333335, 1.19990234375, 1.595703125]}}, 'pred_res': 'the football was grabbed in the Sandra grabbed the football there.<|eot_id|>', 'score': 0}
2025-01-22 06:16:20.883 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:16:20.888 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-60_pid-0_1-4-5-7.pkl | len: 3 |  size: 2.13 KB
Processing depth (1, 4, 5, 7):   1%|          | 1/100 [00:29<49:03, 29.73s/it]is_0k: False
your chose emoji: ['📴', '🙅🏾\u200d♀️', '\U0001fabe', '👩🏽\u200d🎤', '👩🏿\u200d🤝\u200d👩🏽', '🪑', '👻', '🖕🏿', '🧝🏿\u200d♀️', '🧑🏾\u200d❤\u200d🧑🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.08s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Processing depth (0, 3, 5, 7):   1%|          | 1/100 [00:47<49:03, 29.73s/it]2025-01-22 06:16:38.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra grabbed the football there.
2025-01-22 06:16:38.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  grabbed the football there.
2025-01-22 06:16:38.753 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the hallway.
2025-01-22 06:16:38.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7658, 7664) --> . Sandra journeyed to the
2025-01-22 06:16:38.794 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:16:38.861 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11950, 11955) --> . Sandra moved to the
2025-01-22 06:16:38.861 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the bedroom.
2025-01-22 06:16:38.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2987, 2992) -->  tragedy. John travelled to
2025-01-22 06:16:38.876 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:16:38.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2987, 2992) -->  tragedy. John travelled to
2025-01-22 06:16:38.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:16:38.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11415, 11421) --> . John went back to the
2025-01-22 06:16:38.953 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:16:38.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4722, 4727) -->  the senate. Mary moved
2025-01-22 06:16:38.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:16:38.997 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2992, 2997) -->  the kitchen. Daniel went
2025-01-22 06:16:38.997 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 06:16:39.013 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2989, 2994) -->  John travelled to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:16:42.037 | INFO     | test_jbb_embedding:begin_test:693 - There is no information about a football in the text.<|eot_id|>
2025-01-22 06:16:42.037 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24212])
2025-01-22 06:16:50.459 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [352.2142857142857, 18.064407392112326, 116.06430288461539, 17.668630135923536, 12.441978717672415], 'topk_indices': array([ 5883,    34,  5878,  6020,  6013,    35,  6016, 24207, 24208,
        5877,  6017,  5879,  5884,  6014,  5885,  6015,  6021,     0,
        6022,  5886]), 'topk_tokens': [' landing', ' football', 'the', ' landing', ' on', ' there', ' way', '<|eot_id|>', '<|start_header_id|>', '\n', ' from', ' way', ' to', '\n', ' the', 'the', ' to', '<|begin_of_text|>', ' the', ' office'], 'evidence_proportions': [861.2, 206.89583333333334, 229.85, 139.975]}, 'weight': {'score': [22.6796875, 23.448810138344, 23.587740384615383, 23.449328980987257, 29.455639367816094], 'topk_indices': array([18814, 18770, 14616, 14652, 19409, 14661, 14697, 19467, 14580,
       14515, 20329, 20281, 18108, 18139, 23556, 23690, 21976, 21949,
       23638, 23772]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.959375, 21.213541666666668, 21.040625, 25.7984375]}, 'saliency': {'score': [2.095738002232143, 0.10099390389479661, 0.7045223529522235, 0.09861135814253837, 0.09314333159348061], 'topk_indices': array([6036, 6019,   40, 6014,   39, 6038, 5878, 5885,   35, 5884, 6021,
         34, 6016, 5883, 6017, 6020, 6022, 5879, 6015, 5886]), 'topk_tokens': ['      ', ' boat', '\n\n\n', '\n', '***', '      ', 'the', ' the', ' there', ' to', ' to', ' football', ' way', ' landing', ' from', ' landing', ' the', ' way', 'the', ' office'], 'evidence_proportions': [5.209765625, 1.224365234375, 1.28720703125, 0.835888671875]}}, 'pred_res': 'There is no information about a football in the text.<|eot_id|>', 'score': 0}
2025-01-22 06:16:50.488 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:16:50.489 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-60_pid-1_0-3-5-7.pkl | len: 3 |  size: 2.03 KB
Processing depth (0, 3, 5, 7):   2%|▏         | 2/100 [00:59<48:26, 29.65s/it]is_0k: False
your chose emoji: ['🚣🏽\u200d♀️', '🇰🇵', '👩🏾\u200d❤️\u200d👨🏾', '🔈', '🏃🏿', '🥮', '👩\u200d🦼\u200d➡️', '👨🏾\u200d🤝\u200d👨🏻', '🙏🏽', '🧝🏾\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]
Processing depth (1, 4, 6, 9):   2%|▏         | 2/100 [01:16<48:26, 29.65s/it]2025-01-22 06:17:08.141 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra grabbed the football there.
2025-01-22 06:17:08.158 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2865, 2870) --> . Sandra grabbed the football
2025-01-22 06:17:08.158 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the hallway.
2025-01-22 06:17:08.208 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9674, 9680) -->  war. Sandra journeyed to
2025-01-22 06:17:08.209 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:17:08.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14329, 14334) --> . Sandra moved to the
2025-01-22 06:17:08.291 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the bedroom.
2025-01-22 06:17:08.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2871, 2876) --> . John travelled to the
2025-01-22 06:17:08.305 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:17:08.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2871, 2876) --> . John travelled to the
2025-01-22 06:17:08.324 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:17:08.386 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11265, 11271) -->  state. John went back to
2025-01-22 06:17:08.387 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:17:08.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4524, 4529) -->  Mary moved to the bedroom
2025-01-22 06:17:08.410 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:17:08.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (2875, 2880) -->  the kitchen. Daniel went
2025-01-22 06:17:08.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 06:17:08.442 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (2872, 2877) -->  John travelled to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:17:11.475 | INFO     | test_jbb_embedding:begin_test:693 - There is no information about a football in the text.<|eot_id|>
2025-01-22 06:17:11.475 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24163])
2025-01-22 06:17:19.859 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [476.23214285714283, 13.462863630720847, 189.4939903846154, 12.870178852771673, 27.106956845238095], 'topk_indices': array([24060, 24017, 24145, 24157, 24051, 24153,    24, 24161, 24146,
       24151,  2866,    14, 24150,  2867,  2870,  9681, 24061, 24158,
        2869,     0]), 'topk_tokens': [' first', ' context', 'Question', ':', '.\n\n', ' garden', '\n\n', '<|end_header_id|>', ':', ' before', ' Sandra', '\n', ' football', ' grabbed', ' there', ' hallway', ' location', '<|eot_id|>', ' football', '<|begin_of_text|>'], 'evidence_proportions': [890.3, 411.9791666666667, 327.85, 287.65]}, 'weight': {'score': [22.82998511904762, 23.436165480427047, 23.19140625, 23.436957118868943, 29.261904761904763], 'topk_indices': array([18759, 18803, 14649, 14613, 19398, 14694, 14658, 19456, 14512,
       14577, 20318, 20270, 18097, 18128, 23509, 23643, 21948, 21975,
       23591, 23725]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [25.934375, 24.100260416666668, 21.040625, 19.990625]}, 'saliency': {'score': [2.9996744791666665, 0.07682523354092527, 1.0928884652944713, 0.0731850545043262, 0.19864303346664186], 'topk_indices': array([24051, 24132, 24057, 24147, 24156, 14334, 24060,  9677, 24017,
        9676, 24151, 24145,  2870, 24153, 24150, 24061,  2867,  2866,
        9681,  2869]), 'topk_tokens': ['.\n\n', ' return', ' item', ' Where', 'Answer', ' garden', ' first', ' journey', ' context', ' Sandra', ' before', 'Question', ' there', ' garden', ' football', ' location', ' grabbed', ' Sandra', ' hallway', ' football'], 'evidence_proportions': [6.4291015625, 2.5775553385416665, 1.6712890625, 1.40517578125]}}, 'pred_res': 'There is no information about a football in the text.<|eot_id|>', 'score': 0}
2025-01-22 06:17:19.875 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:17:19.875 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-60_pid-2_1-4-6-9.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 4, 6, 9):   3%|▎         | 3/100 [01:28<47:44, 29.53s/it]is_0k: False
your chose emoji: ['\U0001fac4', '🙋🏿\u200d♀', '🦶🏻', '🦹🏼\u200d♀', '🇦🇮', '\U0001faf6🏾', '💇\u200d♂️', '🇧🇳', '4️⃣', '🧛🏼\u200d♂']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.82s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.17s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.92s/it]
Processing depth (0, 1, 6, 8):   3%|▎         | 3/100 [01:46<47:44, 29.53s/it]2025-01-22 06:17:38.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra grabbed the football there.
2025-01-22 06:17:38.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  grabbed the football there.
2025-01-22 06:17:38.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the hallway.
2025-01-22 06:17:38.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3038, 3044) --> . Sandra journeyed to the
2025-01-22 06:17:38.334 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:17:38.413 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14364, 14369) --> . Sandra moved to the
2025-01-22 06:17:38.414 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the bedroom.
2025-01-22 06:17:38.430 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3043, 3048) -->  the hallway. John travelled
2025-01-22 06:17:38.431 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:17:38.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3043, 3048) -->  the hallway. John travelled
2025-01-22 06:17:38.449 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:17:38.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11349, 11355) --> . John went back to the
2025-01-22 06:17:38.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:17:38.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4725, 4730) -->  the senate. Mary moved
2025-01-22 06:17:38.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:17:38.553 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3049, 3054) -->  the kitchen. Daniel went
2025-01-22 06:17:38.553 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 06:17:38.570 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3046, 3051) -->  John travelled to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:17:41.921 | INFO     | test_jbb_embedding:begin_test:693 - There is no information about a football in the text before the mention of the garden.<|eot_id|>
2025-01-22 06:17:41.921 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24200])
2025-01-22 06:17:50.363 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [514.547619047619, 17.457456203776392, 180.26893028846155, 16.85007121677844, 14.206114969135802], 'topk_indices': array([   32,   186,   357,    40, 24188, 24088, 24128,    14, 24098,
       10938,    24,   329,  3044, 24187,    34,   540,    35, 24195,
       24196,     0]), 'topk_tokens': [' grabbed', 'ION', ' half', '\n\n\n', ' before', '.\n\n', ' the', '\n', ' location', ' I', '\n\n', ' of', ' hallway', ' football', ' football', ',', ' there', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [1080.0, 360.875, 204.275, 443.775]}, 'weight': {'score': [22.549107142857142, 23.450114655208033, 23.482271634615383, 23.450863333643817, 30.09567901234568], 'topk_indices': array([18834, 18790, 14684, 14648, 19435, 19493, 14693, 14729, 14612,
       14547, 20307, 20355, 18159, 18128, 23682, 23548, 21951, 21978,
       23764, 23630]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.959375, 21.213541666666668, 21.040625, 25.25]}, 'saliency': {'score': [3.17822265625, 0.09841647221225158, 1.1808495154747596, 0.09457398218949872, 0.10803938500675155], 'topk_indices': array([24088,    63, 24182,    64,   356,    24,   186, 10938,    31,
          39, 24188,   328,    40,   357, 24098,    32, 24187,    35,
        3044,    34]), 'topk_tokens': ['.\n\n', 'MIN', 'Question', 'ISC', ' latter', '\n\n', 'ION', ' I', 'andra', '***', ' before', ' half', '\n\n\n', ' half', ' location', ' grabbed', ' football', ' there', ' hallway', ' football'], 'evidence_proportions': [6.6646484375, 2.0157877604166665, 1.0751953125, 3.18974609375]}}, 'pred_res': 'There is no information about a football in the text before the mention of the garden.<|eot_id|>', 'score': 0}
2025-01-22 06:17:50.368 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:17:50.368 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-60_pid-3_0-1-6-8.pkl | len: 3 |  size: 2.08 KB
Processing depth (0, 1, 6, 8):   4%|▍         | 4/100 [01:59<47:51, 29.91s/it]is_0k: False
your chose emoji: ['👩🏿\u200d❤️\u200d👩🏻', '💠', '🏋\u200d♀️', '☺', '🧎🏼\u200d♂️\u200d➡️', '🇧🇩', '👳🏼\u200d♂', '👩🏾\u200d🎨', '💇🏼\u200d♂', '🧛🏼\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.69s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.05s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.46s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.00s/it]
Processing depth (0, 1, 3, 4):   4%|▍         | 4/100 [02:17<47:51, 29.91s/it]2025-01-22 06:18:09.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra grabbed the football there.
2025-01-22 06:18:09.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 37) -->  grabbed the football there.
2025-01-22 06:18:09.140 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra journeyed to the hallway.
2025-01-22 06:18:09.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3038, 3044) --> . Sandra journeyed to the
2025-01-22 06:18:09.159 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:18:09.206 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7581, 7586) --> . Sandra moved to the
2025-01-22 06:18:09.206 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the bedroom.
2025-01-22 06:18:09.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3043, 3048) -->  the hallway. John travelled
2025-01-22 06:18:09.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:18:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3043, 3048) -->  the hallway. John travelled
2025-01-22 06:18:09.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:18:09.297 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11306, 11312) -->  state. John went back to
2025-01-22 06:18:09.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:18:09.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4735, 4740) -->  the senate. Mary moved
2025-01-22 06:18:09.328 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:18:09.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3049, 3054) -->  the kitchen. Daniel went
2025-01-22 06:18:09.347 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went to the kitchen.
2025-01-22 06:18:09.362 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3046, 3051) -->  John travelled to the kitchen
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:18:12.398 | INFO     | test_jbb_embedding:begin_test:693 - There is no information about a football in the text.<|eot_id|>
2025-01-22 06:18:12.398 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24192])
2025-01-22 06:18:20.803 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [458.57440476190476, 16.39533736309155, 154.18426983173077, 15.862445502086095, 18.4592380275974], 'topk_indices': array([  107, 17705, 24080,    40,    39,    14, 24120,  3044, 24090,
       17662, 24179,    24, 17663,    34,   540,    35, 17706, 24187,
       24188,     0]), 'topk_tokens': ['E', 'ar', '.\n\n', '\n\n\n', '***', '\n', ' the', ' hallway', ' location', 'ison', ' football', '\n\n', 'ed', ' football', ',', ' there', 'ison', '<|eot_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [875.9, 329.1666666666667, 263.025, 392.0875]}, 'weight': {'score': [22.549107142857142, 23.444384170283115, 24.068509615384617, 23.44449074457512, 29.540787337662337], 'topk_indices': array([18836, 18792, 14692, 14656, 19451, 19509, 14701, 14737, 14555,
       14620, 20371, 20323, 18161, 18130, 23546, 23680, 21994, 21967,
       23762, 23628]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.959375, 21.213541666666668, 21.040625, 25.25]}, 'saliency': {'score': [2.7888299851190474, 0.09305133098264104, 0.9953413743239182, 0.08973549145705972, 0.1287019407594359], 'topk_indices': array([  186, 24180,     0,    37,    68,   106,  7487,  7443, 17663,
        7488,    32,    40, 24090,    39, 17662,    35, 24179,    34,
        3044, 17706]), 'topk_tokens': ['ION', ' before', '<|begin_of_text|>', ' PA', 'ION', 'ION', ' Min', 'nes', 'ed', 'nes', ' grabbed', '\n\n\n', ' location', '***', 'ison', ' there', ' football', ' football', ' hallway', 'ison'], 'evidence_proportions': [5.2953125, 1.8938802083333333, 1.3912109375, 2.75390625]}}, 'pred_res': 'There is no information about a football in the text.<|eot_id|>', 'score': 0}
2025-01-22 06:18:20.826 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:18:20.826 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-60_pid-4_0-1-3-4.pkl | len: 3 |  size: 2.04 KB
Processing depth (0, 1, 3, 4):   5%|▌         | 5/100 [02:29<47:40, 30.11s/it]Processing depth (0, 1, 3, 4):   5%|▌         | 5/100 [02:29<47:29, 29.99s/it]
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:82 - Selected idx: 61
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the football was discarded, left or dropped?
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:86 - Needle: [' Sandra journeyed to the hallway.', ' Sandra grabbed the football there.', ' John travelled to the kitchen.', ' Daniel went to the office.', ' John went to the kitchen.', ' Sandra moved to the garden.', ' John went back to the office.', ' Mary moved to the bedroom.', ' Sandra dropped the football.', ' John travelled to the bedroom.']
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra journeyed to the hallway.', ' Sandra grabbed the football there.', ' Sandra moved to the garden.', ' Sandra dropped the football.', ' John travelled to the bedroom.']
2025-01-22 06:18:21.126 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['⛹🏽\u200d♂️', '👩🏽\u200d🚀', '👻', '👩🏿\u200d🍼', '⚒', '🙎🏽\u200d♂', '👩🏿\u200d❤️\u200d💋\u200d👩🏾', '🇸🇰', '🚸', '👐🏾']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.09s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.42s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.01s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.05s/it]
Processing depth (1, 2, 4, 5, 6):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:18:39.383 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:18:39.400 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2966, 2972) -->  tragedy. Sandra journeyed to
2025-01-22 06:18:39.401 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the football there.
2025-01-22 06:18:39.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4924, 4929) --> . Sandra grabbed the football
2025-01-22 06:18:39.426 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:18:39.475 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9688, 9693) --> . Sandra moved to the
2025-01-22 06:18:39.476 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the football.
2025-01-22 06:18:39.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11899, 11903) -->  Sandra dropped the football
2025-01-22 06:18:39.534 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bedroom.
2025-01-22 06:18:39.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14361, 14366) --> . John travelled to the
2025-01-22 06:18:39.609 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:18:39.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4713, 4718) -->  John went to the kitchen
2025-01-22 06:18:39.634 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:18:39.639 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1039, 1044) --> . Daniel went to the
2025-01-22 06:18:39.639 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 06:18:39.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4710, 4715) -->  the senate. John went
2025-01-22 06:18:39.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 06:18:39.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21021, 21027) --> . John went back to the
2025-01-22 06:18:39.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:18:39.865 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17857, 17862) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:18:42.944 | INFO     | test_jbb_embedding:begin_test:693 - The top story of the old Press building overlooking Bridge Square.<|eot_id|>
2025-01-22 06:18:42.944 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 06:18:51.367 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [359.64625, 16.543388291060808, 92.40655048076923, 16.106661306764085, 20.944602272727273], 'topk_indices': array([24192, 19577,     1, 24178, 24199, 11902,  4928, 24179, 24177,
       24084, 24203, 24197, 24190,    14,     0, 24204,  2973, 24205,
       24200, 24201]), 'topk_tokens': [' discarded', ' hall', '<|start_header_id|>', 'Question', ':', ' football', ' football', ':', '.\n\n', '.\n\n', '<|end_header_id|>', '?\n', ' football', '\n', '<|begin_of_text|>', '\n\n', ' hallway', 'hall', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [582.1354166666666, 475.25, 134.7375, 553.5625, 46.83125]}, 'weight': {'score': [24.246875, 23.445439007766026, 21.190204326923077, 23.447036896655213, 29.678571428571427], 'topk_indices': array([18781, 18825, 14611, 14647, 19420, 19478, 14692, 14656, 14569,
       14504, 20378, 20330, 18087, 18118, 23554, 23688, 22020, 21993,
       23770, 23636]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.053385416666668, 25.934375, 21.040625, 28.755859375, 19.990625]}, 'saliency': {'score': [2.38827880859375, 0.09244769815569749, 0.4892390324519231, 0.08964468650472042, 0.152396264014306], 'topk_indices': array([24177, 22741, 24197, 24084, 24050, 24202, 24198,  2969, 22740,
       19577, 24184, 24178, 11902,  4928,  2968, 24192, 24190, 24204,
       24205,  2973]), 'topk_tokens': ['.\n\n', ' after', '?\n', '.\n\n', ' context', 'assistant', 'Answer', ' journey', ' hall', ' hall', ' prior', 'Question', ' football', ' football', ' Sandra', ' discarded', ' football', '\n\n', 'hall', ' hallway'], 'evidence_proportions': [3.5289713541666665, 3.404296875, 0.830810546875, 4.02197265625, 0.25394287109375]}}, 'pred_res': 'The top story of the old Press building overlooking Bridge Square.<|eot_id|>', 'score': 0}
2025-01-22 06:18:51.373 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:18:51.373 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-61_pid-0_1-2-4-5-6.pkl | len: 3 |  size: 2.16 KB
Processing depth (1, 2, 4, 5, 6):   1%|          | 1/100 [00:30<49:39, 30.09s/it]is_0k: False
your chose emoji: ['😿', '🇧🇳', '👨🏾\u200d❤️\u200d👨🏾', '⛹🏾\u200d♀', '👩🏾\u200d❤️\u200d💋\u200d👨🏽', '💆🏻\u200d♀', '\U0001faf6🏽', '🤚🏽', '🛢️', '🍓']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.08s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.97s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.73s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (1, 2, 4, 5, 7):   1%|          | 1/100 [00:47<49:39, 30.09s/it]2025-01-22 06:19:09.378 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:19:09.393 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Sandra journeyed to the
2025-01-22 06:19:09.394 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the football there.
2025-01-22 06:19:09.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4976, 4981) --> . Sandra grabbed the football
2025-01-22 06:19:09.422 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:19:09.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9715, 9720) -->  Sandra moved to the garden
2025-01-22 06:19:09.469 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the football.
2025-01-22 06:19:09.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11891, 11895) -->  Sandra dropped the football
2025-01-22 06:19:09.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bedroom.
2025-01-22 06:19:09.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14600, 14605) -->  bonds. John travelled to
2025-01-22 06:19:09.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:19:09.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4803, 4808) -->  John went to the kitchen
2025-01-22 06:19:09.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:19:09.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1039, 1044) --> . Daniel went to the
2025-01-22 06:19:09.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 06:19:09.666 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4802, 4807) --> . John went to the
2025-01-22 06:19:09.667 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 06:19:09.779 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21021, 21027) --> . John went back to the
2025-01-22 06:19:09.780 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:19:09.872 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17859, 17864) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:19:12.736 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Sandra.<|eot_id|>
2025-01-22 06:19:12.736 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 06:19:21.148 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [259.0834375, 15.27413611592767, 64.32211538461539, 14.969205414029208, 18.812360491071427], 'topk_indices': array([    9,    24, 24192, 19611,  7275, 24204, 24217, 24213, 24191,
       24098, 24193, 24211,     1,    14,  3043,     0, 24218, 24219,
       24214, 24215]), 'topk_tokens': [':', '\n\n', 'Question', ' hall', 'ot', ' football', '<|end_header_id|>', ':', '.\n\n', '.\n\n', ':', '?\n', '<|start_header_id|>', '\n', ' hallway', '<|begin_of_text|>', '\n\n', 'hall', '<|eot_id|>', '<|start_header_id|>'], 'evidence_proportions': [382.9583333333333, 290.85, 139.975, 463.75, 34.0421875]}, 'weight': {'score': [24.8696875, 23.448244364627197, 20.28094951923077, 23.450181131314384, 29.563616071428573], 'topk_indices': array([18859, 18815, 14675, 14639, 19512, 14720, 14684, 19454, 14532,
       14597, 20326, 20374, 18184, 18153, 23696, 23562, 22028, 22001,
       23778, 23644]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 25.934375, 25.5546875, 28.755859375, 24.3984375]}, 'saliency': {'score': [1.6956396484375, 0.08486129361350218, 0.3450352595402644, 0.0829154087934825, 0.1416295369466146], 'topk_indices': array([24064, 11891,  7274,  3038,  4980,  7275, 24216, 24191, 24098,
       24211, 11894, 24198, 24212, 19611, 24206, 24192, 24204, 24218,
        3043, 24219]), 'topk_tokens': [' context', ' Sandra', 'nes', ' Sandra', ' football', 'ot', 'assistant', '.\n\n', '.\n\n', '?\n', ' football', ' prior', 'Answer', ' hall', ' discarded', 'Question', ' football', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [2.1136067708333335, 2.05771484375, 0.9619140625, 3.3983154296875, 0.2035888671875]}}, 'pred_res': 'The football was dropped by Sandra.<|eot_id|>', 'score': 0}
2025-01-22 06:19:21.153 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:19:21.153 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-61_pid-1_1-2-4-5-7.pkl | len: 3 |  size: 2.1 KB
Processing depth (1, 2, 4, 5, 7):   2%|▏         | 2/100 [00:59<48:51, 29.91s/it]is_0k: False
your chose emoji: ['🧖🏼\u200d♂️', '🥿', '🦹🏽\u200d♀', '🧑🏽\u200d❤\u200d🧑🏾', '🥵', '👨🏿\u200d🍳', '⛩️', '👨🏾\u200d🍼', '👨🏿\u200d🤝\u200d👨🏻', '🏌🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.44s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.06s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.24s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.57s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.10s/it]
Processing depth (0, 3, 5, 6, 8):   2%|▏         | 2/100 [01:18<48:51, 29.91s/it]2025-01-22 06:19:39.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:19:39.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:19:39.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the football there.
2025-01-22 06:19:39.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7525, 7530) --> . Sandra grabbed the football
2025-01-22 06:19:39.852 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:19:39.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11838, 11843) -->  Sandra moved to the garden
2025-01-22 06:19:39.910 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the football.
2025-01-22 06:19:39.979 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14344, 14348) -->  Sandra dropped the football
2025-01-22 06:19:39.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bedroom.
2025-01-22 06:19:40.059 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14553, 14558) -->  bonds. John travelled to
2025-01-22 06:19:40.060 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:19:40.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4658, 4663) -->  John went to the kitchen
2025-01-22 06:19:40.083 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:19:40.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1047, 1052) --> . Daniel went to the
2025-01-22 06:19:40.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 06:19:40.112 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4657, 4662) --> . John went to the
2025-01-22 06:19:40.112 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 06:19:40.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20990, 20996) --> . John went back to the
2025-01-22 06:19:40.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:19:40.315 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (18010, 18015) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:19:43.277 | INFO     | test_jbb_embedding:begin_test:693 - The football was dropped by Sandra.<|eot_id|>
2025-01-22 06:19:43.278 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24182])
2025-01-22 06:19:52.741 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1119.723125, 27.207602853008062, 184.13671875, 25.906822001636694, 43.04855769230769], 'topk_indices': array([24166,  7526,    32,    14,  7525, 24177,  7528,  7531,  7527,
       14348, 14347,  7530, 24167, 24180,  7529, 24181,     0,    36,
       24178, 24182]), 'topk_tokens': [' the', ' Sandra', ' journey', '\n', '.', '<|eot_id|>', ' the', '.', ' grabbed', '.', ' football', ' there', ' football', '<|end_header_id|>', ' football', '\n\n', '<|begin_of_text|>', ' hallway', '<|start_header_id|>', 'hall'], 'evidence_proportions': [1732.8333333333333, 1953.4, 417.2, 1299.875, 108.715625]}, 'weight': {'score': [24.8846875, 23.43598563158983, 20.28094951923077, 23.43788392413193, 29.125721153846154], 'topk_indices': array([18796, 18752, 14628, 14592, 19397, 14637, 19455, 14673, 14485,
       14550, 20317, 20269, 18091, 18060, 23517, 23651, 21956, 21983,
       23733, 23599]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 25.934375, 25.5546875, 28.755859375, 24.3984375]}, 'saliency': {'score': [7.6576611328125, 0.1548012573166477, 0.9856673020582932, 0.14613406522040676, 0.30252216045673075], 'topk_indices': array([24179, 24164, 24161, 14345, 24165,    31, 14344, 19554, 24155,
          32, 24169,  7530,  7526,  7527, 24181, 14347, 24167,  7529,
          36, 24182]), 'topk_tokens': ['assistant', ' place', ' prior', ' dropped', ' where', 'andra', ' Sandra', ' hall', 'Question', ' journey', ' discarded', ' there', ' Sandra', ' grabbed', '\n\n', ' football', ' football', ' football', ' hallway', 'hall'], 'evidence_proportions': [11.803385416666666, 13.1921875, 2.7462890625, 9.43701171875, 0.6361572265625]}}, 'pred_res': 'The football was dropped by Sandra.<|eot_id|>', 'score': 0}
2025-01-22 06:19:52.755 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:19:52.755 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-61_pid-2_0-3-5-6-8.pkl | len: 3 |  size: 2.13 KB
Processing depth (0, 3, 5, 6, 8):   3%|▎         | 3/100 [01:31<49:36, 30.68s/it]is_0k: False
your chose emoji: ['💁🏼\u200d♂️', '🤽🏼', '↔', '🐛', '🧍🏼', '🧜🏼', '🏃🏼\u200d♂\u200d➡', '\U0001faf5🏻', '✍🏾', '🧑🏼\u200d🚒']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.78s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.89s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.42s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
Processing depth (1, 3, 5, 7, 8):   3%|▎         | 3/100 [01:49<49:36, 30.68s/it]2025-01-22 06:20:10.767 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:20:10.783 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3037, 3043) --> . Sandra journeyed to the
2025-01-22 06:20:10.784 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the football there.
2025-01-22 06:20:10.826 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7626, 7631) --> . Sandra grabbed the football
2025-01-22 06:20:10.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:20:10.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11842, 11847) --> . Sandra moved to the
2025-01-22 06:20:10.891 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the football.
2025-01-22 06:20:10.978 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16689, 16693) -->  Sandra dropped the football
2025-01-22 06:20:10.979 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bedroom.
2025-01-22 06:20:11.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14442, 14447) --> . John travelled to the
2025-01-22 06:20:11.054 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:20:11.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4835, 4840) -->  John went to the kitchen
2025-01-22 06:20:11.078 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:20:11.084 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1039, 1044) --> . Daniel went to the
2025-01-22 06:20:11.084 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 06:20:11.107 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4834, 4839) --> . John went to the
2025-01-22 06:20:11.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 06:20:11.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (20973, 20979) --> . John went back to the
2025-01-22 06:20:11.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:20:11.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17759, 17764) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:20:14.205 | INFO     | test_jbb_embedding:begin_test:693 - the top story of the old Press building<|eot_id|>
2025-01-22 06:20:14.205 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24201])
2025-01-22 06:20:22.642 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1013.4825, 34.91628708064783, 149.16135817307693, 33.78042291589036, 44.5765625], 'topk_indices': array([ 3040, 24174,  3038,  7629,  3039,  7628,  7631,    14, 16693,
       19609, 24196, 16692, 24197,  7630, 24186,     0, 24199, 24200,
        3043, 24201]), 'topk_tokens': ['ed', 'Question', ' Sandra', ' the', ' journey', ' grabbed', ' there', '\n', '.', ' hall', '<|eot_id|>', ' football', '<|start_header_id|>', ' football', ' football', '<|begin_of_text|>', '<|end_header_id|>', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [1286.75, 1744.6, 379.025, 1557.625, 153.5875]}, 'weight': {'score': [23.0853125, 23.443069843827466, 20.28094951923077, 23.446844077340288, 29.462291666666665], 'topk_indices': array([18851, 18807, 14610, 14646, 14691, 19510, 14655, 19452, 14509,
       14574, 20324, 20372, 18176, 18145, 23536, 23670, 21975, 22002,
       23752, 23618]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 25.934375, 21.040625, 28.755859375, 19.990625]}, 'saliency': {'score': [6.66720703125, 0.20174137506971987, 0.7728236271784856, 0.19443442438204817, 0.3335091145833333], 'topk_indices': array([ 6495, 24184, 24180, 24188, 16690, 16689, 24198,  7627, 24174,
        7631,  3039,  3038,  7628, 19609, 16692, 24200,  7630, 24186,
        3043, 24201]), 'topk_tokens': ['�', ' where', ' prior', ' discarded', ' dropped', ' Sandra', 'assistant', ' Sandra', 'Question', ' there', ' journey', ' Sandra', ' grabbed', ' hall', ' football', '\n\n', ' football', ' football', ' hallway', 'hall'], 'evidence_proportions': [7.335611979166667, 12.228125, 2.37421875, 11.4365234375, 0.78173828125]}}, 'pred_res': 'the top story of the old Press building<|eot_id|>', 'score': 0}
2025-01-22 06:20:22.648 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:20:22.648 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-61_pid-3_1-3-5-7-8.pkl | len: 3 |  size: 2.15 KB
Processing depth (1, 3, 5, 7, 8):   4%|▍         | 4/100 [02:01<48:35, 30.37s/it]is_0k: False
your chose emoji: ['🧜🏽\u200d♂️', '🥮', '🚶🏽\u200d♂', '👩🏻\u200d🏫', '👸🏾', '🧳', '🕉️', '👩🏼\u200d❤\u200d💋\u200d👨🏾', '🗂', '👨🏿\u200d🌾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.77s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.22s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  5.00s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.41s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.00s/it]
Processing depth (0, 1, 4, 7, 8):   4%|▍         | 4/100 [02:19<48:35, 30.37s/it]2025-01-22 06:20:40.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:20:40.867 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:20:40.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra grabbed the football there.
2025-01-22 06:20:40.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2975, 2980) --> . Sandra grabbed the football
2025-01-22 06:20:40.883 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:20:40.936 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9739, 9744) --> . Sandra moved to the
2025-01-22 06:20:40.936 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra dropped the football.
2025-01-22 06:20:41.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16710, 16714) -->  Sandra dropped the football
2025-01-22 06:20:41.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John travelled to the bedroom.
2025-01-22 06:20:41.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14619, 14624) --> . John travelled to the
2025-01-22 06:20:41.110 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:20:41.137 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4724, 4729) -->  John went to the kitchen
2025-01-22 06:20:41.137 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:20:41.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (1047, 1052) --> . Daniel went to the
2025-01-22 06:20:41.143 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John went to the kitchen.
2025-01-22 06:20:41.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (4721, 4726) -->  the senate. John went
2025-01-22 06:20:41.167 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John went back to the office.
2025-01-22 06:20:41.279 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21016, 21022) --> . John went back to the
2025-01-22 06:20:41.280 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:20:41.368 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (17804, 17809) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:20:44.067 | INFO     | test_jbb_embedding:begin_test:693 - The hallway.<|eot_id|>
2025-01-22 06:20:44.067 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24226])
2025-01-22 06:20:52.478 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [2833.98, 70.98917619381733, 289.61658653846155, 67.89714694143436, 84.90777658045977], 'topk_indices': array([ 2977,    14,    32, 24209,    33, 24212, 24222, 16711, 24213,
       24221, 24224,  2980,  2979, 16713, 24211, 16714, 24225,    36,
           0, 24226]), 'topk_tokens': [' grabbed', '\n', ' journey', ' where', 'ed', ' was', '<|start_header_id|>', ' dropped', ' discarded', '<|eot_id|>', '<|end_header_id|>', ' there', ' football', ' football', ' football', '.', '\n\n', ' hallway', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [4406.666666666667, 3773.6, 1134.2, 4467.5, 400.1]}, 'weight': {'score': [23.1003125, 23.450405505798837, 21.190204326923077, 23.453198026098107, 29.694863505747126], 'topk_indices': array([18808, 18852, 14579, 14615, 19511, 14666, 19453, 14630, 14478,
       14543, 20325, 20373, 18177, 18146, 23709, 23575, 21976, 22003,
       23657, 23791]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 25.934375, 21.040625, 28.755859375, 19.990625]}, 'saliency': {'score': [19.25869140625, 0.40661194359032565, 1.5142376239483173, 0.385927840097241, 0.6133493006914511], 'topk_indices': array([   31, 24212, 24208,  2976, 19610, 24209, 16714, 24199,    32,
       16711,  2977, 16710, 24225, 24213,  2980,  2979, 16713, 24211,
          36, 24226]), 'topk_tokens': ['andra', ' was', ' place', ' Sandra', ' hall', ' where', '.', 'Question', ' journey', ' dropped', ' grabbed', ' Sandra', '\n\n', ' discarded', ' there', ' football', ' football', ' football', ' hallway', 'hall'], 'evidence_proportions': [29.800130208333332, 25.59140625, 6.475390625, 33.0390625, 2.03525390625]}}, 'pred_res': 'The hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:20:52.497 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:20:52.498 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-61_pid-4_0-1-4-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (0, 1, 4, 7, 8):   5%|▌         | 5/100 [02:31<47:47, 30.18s/it]Processing depth (0, 1, 4, 7, 8):   5%|▌         | 5/100 [02:31<47:58, 30.30s/it]
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:82 - Selected idx: 62
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the garden? 
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:86 - Needle: [' John went back to the office.', ' Daniel went to the office.', ' Mary grabbed the football there.', ' Sandra journeyed to the hallway.', ' John travelled to the kitchen.', ' Mary moved to the bedroom.', ' Sandra moved to the garden.', ' Mary grabbed the football.', ' Mary discarded the football.', ' Sandra put down the football there.', ' Mary left the football.']
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra journeyed to the hallway.', ' Sandra moved to the garden.', ' Sandra put down the football there.', ' Mary left the football.']
2025-01-22 06:20:52.803 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧗🏾\u200d♂', '🚵🏾\u200d♀️', '👩🏾\u200d❤\u200d💋\u200d👨🏽', '🦸🏾', '🧑🏽\u200d❤️\u200d💋\u200d🧑🏾', '🆎', '🧑\u200d🔬', '🚧', '🤹🏿\u200d♂️', '🛴']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (1, 3, 5, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:21:10.236 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:21:10.254 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2971, 2977) -->  tragedy. Sandra journeyed to
2025-01-22 06:21:10.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:21:10.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7596, 7601) -->  war. Sandra moved to
2025-01-22 06:21:10.292 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:21:10.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11905, 11911) --> . Sandra put down the football
2025-01-22 06:21:10.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 06:21:10.454 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19225, 19229) -->  left the football.
2025-01-22 06:21:10.455 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 06:21:10.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12846, 12852) --> . John went back to the
2025-01-22 06:21:10.524 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:21:10.525 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (143, 148) --> . Daniel went to the
2025-01-22 06:21:10.525 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary grabbed the football there.
2025-01-22 06:21:10.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1265, 1270) --> . Mary grabbed the football
2025-01-22 06:21:10.533 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 06:21:10.624 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17789, 17794) --> . John travelled to the
2025-01-22 06:21:10.624 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:21:10.739 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21763, 21768) --> . Mary moved to the
2025-01-22 06:21:10.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary grabbed the football.
2025-01-22 06:21:10.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (1266, 1270) -->  Mary grabbed the football
2025-01-22 06:21:10.747 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:21:10.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (20901, 20905) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:21:13.503 | INFO     | test_jbb_embedding:begin_test:693 - the hallway<|eot_id|>
2025-01-22 06:21:13.504 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 06:21:21.908 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [283.34375, 15.009914077990747, 176.05422794117646, 14.549908396472487, 19.652825850474684], 'topk_indices': array([ 7449, 24186,  1269, 24192,  7450, 24200, 24196,  7458, 24195,
        7494,  7457, 24193,  9275,  7383,  7427,  7428,  2978,  7503,
        7459,     0]), 'topk_tokens': [' tele', '.\n\n', ' football', ' football', 'graph', '<|eot_id|>', '?', ' or', ' garden', ' tele', ' two', ' before', ' a', 'nes', ' Min', 'nes', ' hallway', ' or', ' three', '<|begin_of_text|>'], 'evidence_proportions': [373.4375, 260.44375, 241.02083333333334, 240.3125]}, 'weight': {'score': [24.338541666666668, 23.450163685558493, 22.624080882352942, 23.450554149588044, 30.01206487341772], 'topk_indices': array([18837, 18793, 14665, 14629, 14674, 19437, 19495, 14710, 14593,
       14528, 20357, 20309, 18162, 18131, 23537, 23671, 21964, 21991,
       23753, 23619]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.053385416666668, 24.5046875, 24.924479166666668, 20.6796875]}, 'saliency': {'score': [1.7425478980654763, 0.0865004514907262, 1.131460750804228, 0.08358960618973962, 0.1436049062994462], 'topk_indices': array([24196,  7493,  7458,  9217, 24187,  2974,  1269, 24192,  7449,
        7457, 24193,  7450, 24195,  7494,  7383,  7503,  7427,  7428,
        7459,  2978]), 'topk_tokens': ['?', ' bogus', ' or', ' few', 'Question', ' journey', ' football', ' football', ' tele', ' two', ' before', 'graph', ' garden', ' tele', 'nes', ' or', ' Min', 'nes', ' three', ' hallway'], 'evidence_proportions': [2.301513671875, 1.537548828125, 1.5699055989583333, 1.4193115234375]}}, 'pred_res': 'the hallway<|eot_id|>', 'score': 100}
2025-01-22 06:21:21.926 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:21:21.926 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-62_pid-0_1-3-5-8.pkl | len: 3 |  size: 1.99 KB
Processing depth (1, 3, 5, 8):   1%|          | 1/100 [00:28<47:50, 28.99s/it]is_0k: False
your chose emoji: ['🤹🏻\u200d♂', '🤦🏽\u200d♀️', '🇨🇩', '🧙🏻\u200d♀', '🧗🏼\u200d♀', '🫑', '🧗\u200d♀️', '🤦\u200d♀️', '💁🏽\u200d♀️', '🧑🏿\u200d❤️\u200d🧑🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (0, 1, 5, 6):   1%|          | 1/100 [00:46<47:50, 28.99s/it]2025-01-22 06:21:39.632 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:21:39.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:21:39.633 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:21:39.648 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3050, 3055) --> . Sandra moved to the
2025-01-22 06:21:39.648 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:21:39.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11912, 11918) --> . Sandra put down the football
2025-01-22 06:21:39.715 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 06:21:39.792 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14445, 14449) -->  Mary left the football
2025-01-22 06:21:39.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 06:21:39.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12853, 12859) --> . John went back to the
2025-01-22 06:21:39.862 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:21:39.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (151, 156) --> . Daniel went to the
2025-01-22 06:21:39.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary grabbed the football there.
2025-01-22 06:21:39.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1273, 1278) --> . Mary grabbed the football
2025-01-22 06:21:39.871 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 06:21:39.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (18053, 18058) --> . John travelled to the
2025-01-22 06:21:39.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:21:40.112 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21706, 21711) --> . Mary moved to the
2025-01-22 06:21:40.112 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary grabbed the football.
2025-01-22 06:21:40.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (1274, 1278) -->  Mary grabbed the football
2025-01-22 06:21:40.120 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:21:40.250 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (20844, 20848) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:21:42.961 | INFO     | test_jbb_embedding:begin_test:693 - The office.<|eot_id|>
2025-01-22 06:21:42.961 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24184])
2025-01-22 06:21:51.330 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [483.0595238095238, 19.74198174639269, 227.80698529411765, 19.045649966849, 44.42873965992647], 'topk_indices': array([24169, 24167,    14, 24166, 17526,  3056, 24171, 24175, 17536,
       17317,    36, 17318,  3069, 24182, 17347, 24174, 24172, 17319,
       24179,     0]), 'topk_tokens': [' was', ':', '\n', 'Question', 'ente', '.', ' football', '?', 'ente', ' the', ' hallway', ' service', '�', '<|end_header_id|>', ' service', ' garden', ' before', ' of', '<|eot_id|>', '<|begin_of_text|>'], 'evidence_proportions': [729.0, 555.25, 349.625, 224.0625]}, 'weight': {'score': [23.250372023809526, 23.441133150039278, 22.624080882352942, 23.442450312344604, 29.520220588235293], 'topk_indices': array([18809, 18765, 14611, 14647, 19416, 19474, 14692, 14656, 14510,
       14575, 20348, 20300, 18103, 18134, 23528, 23662, 21994, 21967,
       23610, 23744]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' atrocities', ' sincere', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 21.040625, 24.924479166666668, 26.462890625]}, 'saliency': {'score': [2.973423549107143, 0.11211561907287489, 1.4009363511029411, 0.10780982711946274, 0.32117675332462087], 'topk_indices': array([ 3051,    32, 17525, 17535,  3067, 17319, 24175, 24168,  3070,
        3055, 17526,  3069, 17536, 24166, 17318, 24171, 17347, 24172,
          36, 24174]), 'topk_tokens': [' Sandra', ' journey', 'arp', 'arp', '�', ' of', '?', ' Where', '�', ' garden', 'ente', '�', 'ente', 'Question', ' service', ' football', ' service', ' before', ' hallway', ' garden'], 'evidence_proportions': [4.582356770833333, 3.038671875, 2.3089192708333335, 1.4752197265625]}}, 'pred_res': 'The office.<|eot_id|>', 'score': 0}
2025-01-22 06:21:51.334 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:21:51.334 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-62_pid-1_0-1-5-6.pkl | len: 3 |  size: 2.01 KB
Processing depth (0, 1, 5, 6):   2%|▏         | 2/100 [00:58<47:45, 29.24s/it]is_0k: False
your chose emoji: ['\U0001fa87', '📮', '🧑🏻\u200d❤️\u200d🧑🏼', '🐣', '👬🏽', '🐵', '🏌🏿', '🏋\u200d♀️', '🧎🏿\u200d♀\u200d➡️', '👨🏻\u200d🦽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.21s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.99s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.43s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]
Processing depth (1, 4, 8, 9):   2%|▏         | 2/100 [01:16<47:45, 29.24s/it]2025-01-22 06:22:09.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:22:09.248 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2975, 2981) -->  tragedy. Sandra journeyed to
2025-01-22 06:22:09.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:22:09.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9695, 9700) --> . Sandra moved to the
2025-01-22 06:22:09.300 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:22:09.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19176, 19182) --> . Sandra put down the football
2025-01-22 06:22:09.409 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 06:22:09.530 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21467, 21471) -->  Mary left the football
2025-01-22 06:22:09.531 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 06:22:09.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12624, 12630) -->  John went back to the office
2025-01-22 06:22:09.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:22:09.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (143, 148) --> . Daniel went to the
2025-01-22 06:22:09.599 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary grabbed the football there.
2025-01-22 06:22:09.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1237, 1242) --> . Mary grabbed the football
2025-01-22 06:22:09.606 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 06:22:09.705 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17770, 17775) --> . John travelled to the
2025-01-22 06:22:09.706 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:22:09.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21735, 21740) --> . Mary moved to the
2025-01-22 06:22:09.829 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary grabbed the football.
2025-01-22 06:22:09.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (1238, 1242) -->  Mary grabbed the football
2025-01-22 06:22:09.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:22:09.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (20868, 20872) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:22:12.570 | INFO     | test_jbb_embedding:begin_test:693 - bedroom<|eot_id|>
2025-01-22 06:22:12.570 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24213])
2025-01-22 06:22:20.934 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [237.21279761904762, 8.76440292575157, 164.3547794117647, 8.346891684946815, 12.451466020331326], 'topk_indices': array([18959,  2977,  2981, 24211, 24200,  2979,  2978,  1241,   149,
       18960,  1242,    14, 21740,    24, 24208, 18961, 18962, 24209,
        2982,     0]), 'topk_tokens': ['\n', ' Sandra', ' the', '<|end_header_id|>', ' football', 'ed', ' journey', ' football', '.', 'the', '.', '\n', ' bedroom', '\n\n', '<|eot_id|>', ' manner', ' in', '<|start_header_id|>', ' hallway', '<|begin_of_text|>'], 'evidence_proportions': [312.484375, 270.0, 161.86458333333334, 196.34375]}, 'weight': {'score': [24.61532738095238, 23.450213701684838, 23.21438419117647, 23.449532886159513, 29.70293674698795], 'topk_indices': array([18862, 18818, 14656, 14692, 14737, 19464, 19522, 14701, 14555,
       14620, 20336, 20384, 18156, 18187, 23557, 23691, 22023, 21996,
       23639, 23773]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.053385416666668, 21.040625, 24.924479166666668, 26.462890625]}, 'saliency': {'score': [1.476533435639881, 0.049879711139781034, 1.0355852912454044, 0.04725259645338174, 0.09409957334219692], 'topk_indices': array([19181,  9697, 24206, 24201,  9700, 24195,   148,    24, 18962,
       18960,  1239, 24203,  9696,  2978, 24200,  1241,  2977, 21740,
       18961,  2982]), 'topk_tokens': [' football', ' moved', 'Answer', ' before', ' garden', 'Question', ' office', '\n\n', ' in', 'the', ' grabbed', ' garden', ' Sandra', ' journey', ' football', ' football', ' Sandra', ' bedroom', ' manner', ' hallway'], 'evidence_proportions': [1.94287109375, 1.55654296875, 1.1071573893229167, 1.2310791015625]}}, 'pred_res': 'bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:22:20.941 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:22:20.942 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-62_pid-2_1-4-8-9.pkl | len: 3 |  size: 2.04 KB
Processing depth (1, 4, 8, 9):   3%|▎         | 3/100 [01:28<47:32, 29.41s/it]is_0k: False
your chose emoji: ['🗜', '👮🏿\u200d♀', '👩🏻\u200d❤️\u200d👩🏼', '🏄\u200d♂️', '👨🏼\u200d⚖️', '🧔🏿\u200d♂️', '📈', '🧑\u200d🧒', '🦸🏻\u200d♀', '👰🏽\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.54s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.87s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Processing depth (0, 6, 7, 9):   3%|▎         | 3/100 [01:45<47:32, 29.41s/it]2025-01-22 06:22:38.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:22:38.727 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:22:38.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:22:38.798 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (14441, 14446) --> . Sandra moved to the
2025-01-22 06:22:38.799 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:22:38.883 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16801, 16807) --> . Sandra put down the football
2025-01-22 06:22:38.883 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 06:22:38.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21482, 21486) -->  Mary left the football
2025-01-22 06:22:38.986 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 06:22:39.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12850, 12856) --> . John went back to the
2025-01-22 06:22:39.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:22:39.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (151, 156) --> . Daniel went to the
2025-01-22 06:22:39.051 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary grabbed the football there.
2025-01-22 06:22:39.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1273, 1278) --> . Mary grabbed the football
2025-01-22 06:22:39.057 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 06:22:39.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17816, 17821) --> . John travelled to the
2025-01-22 06:22:39.148 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:22:39.257 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21768, 21773) --> . Mary moved to the
2025-01-22 06:22:39.257 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary grabbed the football.
2025-01-22 06:22:39.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (1274, 1278) -->  Mary grabbed the football
2025-01-22 06:22:39.264 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:22:39.368 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (20901, 20905) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:22:41.995 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:22:41.995 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24202])
2025-01-22 06:22:50.389 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [454.01190476190476, 13.015426822970461, 146.1847426470588, 12.444466873706004, 20.330382508116884], 'topk_indices': array([   33,    32, 24190, 14443,     9, 17680, 24198,     1,    24,
          14, 24197, 24200, 17319, 17318, 17348, 24201, 17320, 24202,
          36,     0]), 'topk_tokens': ['ed', ' journey', ' before', ' moved', ':', 'ison', '<|start_header_id|>', '<|start_header_id|>', '\n\n', '\n', '<|eot_id|>', '<|end_header_id|>', ' service', ' the', ' service', '\n\n', ' of', 'hall', ' hallway', '<|begin_of_text|>'], 'evidence_proportions': [769.0833333333334, 494.8, 333.7291666666667, 110.84375]}, 'weight': {'score': [23.250372023809526, 23.445522619293534, 22.624080882352942, 23.44684879658385, 29.499594155844157], 'topk_indices': array([18842, 18798, 14609, 14645, 14654, 19437, 19495, 14690, 14573,
       14508, 20309, 20357, 18141, 18110, 23550, 23684, 21996, 21969,
       23766, 23632]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 21.040625, 24.924479166666668, 26.462890625]}, 'saliency': {'score': [2.8897763206845237, 0.07447987607480376, 0.9280323701746324, 0.07083010339835663, 0.15449078052074877], 'topk_indices': array([ 1277, 24056,    24, 14446,    31, 17318, 16806, 24192, 24190,
       17320, 14442, 24189,    32, 14443, 24201, 17680, 17319, 17348,
       24202,    36]), 'topk_tokens': [' football', ' context', '\n\n', ' garden', 'andra', ' the', ' football', ' garden', ' before', ' of', ' Sandra', ' football', ' journey', ' moved', '\n\n', 'ison', ' service', ' service', 'hall', ' hallway'], 'evidence_proportions': [5.17724609375, 2.7390625, 2.1552734375, 0.74871826171875]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:22:50.396 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:22:50.397 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-62_pid-3_0-6-7-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 6, 7, 9):   4%|▍         | 4/100 [01:57<47:04, 29.43s/it]is_0k: False
your chose emoji: ['🇵🇰', '🙆🏾\u200d♂️', '⛹🏿', '🔬', '👩🏻\u200d🦱', '👨\u200d⚕', '☣️', '👩🏻\u200d⚖️', '🚶🏻\u200d♂️\u200d➡', '🏊🏾\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.70s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.25s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.04s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.46s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]
Processing depth (0, 1, 3, 5):   4%|▍         | 4/100 [02:15<47:04, 29.43s/it]2025-01-22 06:23:08.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:23:08.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:23:08.841 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:23:08.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2971, 2976) -->  tragedy. Sandra moved to
2025-01-22 06:23:08.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:23:08.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7556, 7562) --> . Sandra put down the football
2025-01-22 06:23:08.898 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary left the football.
2025-01-22 06:23:08.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11760, 11764) -->  Mary left the football
2025-01-22 06:23:08.960 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John went back to the office.
2025-01-22 06:23:09.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (12624, 12630) --> . John went back to the
2025-01-22 06:23:09.030 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Daniel went to the office.
2025-01-22 06:23:09.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (151, 156) --> . Daniel went to the
2025-01-22 06:23:09.031 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary grabbed the football there.
2025-01-22 06:23:09.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1233, 1238) --> . Mary grabbed the football
2025-01-22 06:23:09.038 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John travelled to the kitchen.
2025-01-22 06:23:09.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (17789, 17794) --> . John travelled to the
2025-01-22 06:23:09.133 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary moved to the bedroom.
2025-01-22 06:23:09.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21714, 21719) --> . Mary moved to the
2025-01-22 06:23:09.249 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Mary grabbed the football.
2025-01-22 06:23:09.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (1234, 1238) -->  Mary grabbed the football
2025-01-22 06:23:09.255 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:23:09.361 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (20852, 20856) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:23:12.243 | INFO     | test_jbb_embedding:begin_test:693 - Sandra journeyed to the hallway.<|eot_id|>
2025-01-22 06:23:12.244 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24192])
2025-01-22 06:23:20.621 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [491.86309523809524, 19.296069694151683, 172.57766544117646, 18.669082047431647, 22.970079210069443], 'topk_indices': array([ 2977, 24176, 24181, 24173,    32,  2978,    24,   173, 24177,
       24175, 24179, 24174,   193, 24183,    36, 24187, 24182, 24190,
       24180,     0]), 'topk_tokens': [' garden', ' Where', ' the', '.\n\n', ' journey', '.', '\n\n', 'ION', ' was', ':', ' football', 'Question', 'ION', '?', ' hallway', '<|eot_id|>', ' garden', '<|end_header_id|>', ' before', '<|begin_of_text|>'], 'evidence_proportions': [939.9166666666666, 415.5875, 306.2708333333333, 193.515625]}, 'weight': {'score': [24.633184523809526, 23.44194564992767, 22.624080882352942, 23.442061283140017, 29.31879340277778], 'topk_indices': array([18793, 18837, 14663, 14699, 19506, 14708, 19448, 14744, 14562,
       14627, 20320, 20368, 18162, 18131, 23536, 23670, 21975, 22002,
       23618, 23752]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 26.8484375, 24.924479166666668, 26.462890625]}, 'saliency': {'score': [3.0751255580357144, 0.109235185472205, 1.0743892894071692, 0.10529571002242777, 0.16853841145833334], 'topk_indices': array([17346,   174,    24, 24173, 17375,  1166, 24177,    31, 10939,
         173, 24176,  2977, 24183,   193,    32, 24179, 24174, 24180,
       24182,    36]), 'topk_tokens': [' service', 'E', '\n\n', '.\n\n', ' service', 'nes', ' was', 'andra', 'walk', 'ION', ' Where', ' garden', '?', 'ION', ' journey', ' football', 'Question', ' before', ' garden', ' hallway'], 'evidence_proportions': [5.802083333333333, 2.491015625, 1.985107421875, 1.349853515625]}}, 'pred_res': 'Sandra journeyed to the hallway.<|eot_id|>', 'score': 100}
2025-01-22 06:23:20.632 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:23:20.632 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-62_pid-4_0-1-3-5.pkl | len: 3 |  size: 2.02 KB
Processing depth (0, 1, 3, 5):   5%|▌         | 5/100 [02:27<47:03, 29.72s/it]Processing depth (0, 1, 3, 5):   5%|▌         | 5/100 [02:28<46:52, 29.60s/it]
2025-01-22 06:23:20.939 | INFO     | __main__:<module>:82 - Selected idx: 63
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the kitchen.', ' Mary grabbed the football.', ' Sandra journeyed to the hallway.', ' Sandra took the milk.', ' Mary moved to the bedroom.', ' Mary grabbed the football there.', ' Sandra moved to the garden.', ' John went back to the office.', ' Daniel went to the office.', ' Mary discarded the football.', ' Sandra put down the milk there.', ' Mary left the football.']
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra journeyed to the hallway.', ' Sandra took the milk.', ' Sandra moved to the garden.', ' Sandra put down the milk there.', ' Mary left the football.']
2025-01-22 06:23:20.940 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👈🏼', '👩🏼\u200d❤\u200d👨🏿', '▫', '⛹🏼', '🙆🏽\u200d♀', '🏋️', '🤵🏻\u200d♂', '🤞🏼', '🇮🇩', '🧗🏻\u200d♂️']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.32s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.01s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
Processing depth (2, 3, 4, 6, 7):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:23:38.945 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:23:38.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4855, 4861) --> . Sandra journeyed to the
2025-01-22 06:23:38.973 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the milk.
2025-01-22 06:23:39.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7451, 7455) -->  Sandra took the milk
2025-01-22 06:23:39.010 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:23:39.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9693, 9698) --> . Sandra moved to the
2025-01-22 06:23:39.058 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the milk there.
2025-01-22 06:23:39.129 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14363, 14369) --> . Sandra put down the milk
2025-01-22 06:23:39.129 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary left the football.
2025-01-22 06:23:39.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16702, 16706) -->  Mary left the football
2025-01-22 06:23:39.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:23:39.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9625, 9630) --> . John travelled to the
2025-01-22 06:23:39.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the football.
2025-01-22 06:23:39.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (22752, 22756) -->  Mary grabbed the football
2025-01-22 06:23:39.373 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:23:39.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10587, 10592) --> . Mary moved to the
2025-01-22 06:23:39.427 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football there.
2025-01-22 06:23:39.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3172, 3177) -->  Mary grabbed the football there
2025-01-22 06:23:39.443 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 06:23:39.553 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21348, 21354) --> . John went back to the
2025-01-22 06:23:39.554 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel went to the office.
2025-01-22 06:23:39.618 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13066, 13071) -->  winter. Daniel went to
2025-01-22 06:23:39.618 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:23:39.689 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (14577, 14581) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:23:42.356 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:23:42.356 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24169])
2025-01-22 06:23:50.733 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [704.2275, 29.197759804732748, 246.2233455882353, 28.191886897109445, 44.46766493055556], 'topk_indices': array([14365, 24048, 24149,    14,  7455, 24154, 14366,  7454, 18934,
       14368, 24156, 19586, 14369, 24164, 24167,     0, 24168,  4861,
       24165, 24169]), 'topk_tokens': [' put', '.\n\n', ' to', '\n', '.', ' milk', ' down', ' milk', ' in', ' milk', ' discarded', ' hall', ' there', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', ' hallway', '<|start_header_id|>', 'hall'], 'evidence_proportions': [944.3333333333334, 839.375, 293.675, 1065.5, 180.203125]}, 'weight': {'score': [23.92875, 23.432646760714878, 23.710018382352942, 23.431741306556628, 29.21209490740741], 'topk_indices': array([18790, 18834, 14649, 14613, 19429, 14658, 19487, 14694, 14572,
       14507, 20349, 20301, 18128, 18159, 23518, 23652, 21979, 21952,
       23734, 23600]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 27.912109375, 21.040625, 24.705729166666668, 26.462890625]}, 'saliency': {'score': [4.454873046875, 0.16682204051304195, 1.6914816463694853, 0.16022644055624818, 0.32641884132667826], 'topk_indices': array([ 3175, 14364, 14365, 14366,  7451, 24142, 18933, 24148,  4857,
        4856, 24154,  7454, 14578, 14368, 19586, 14369, 24156, 24168,
        4861, 24169]), 'topk_tokens': [' football', ' Sandra', ' put', ' down', ' Sandra', 'Question', ' manner', ' prior', ' journey', ' Sandra', ' milk', ' milk', ' discarded', ' milk', ' hall', ' there', ' discarded', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [5.180501302083333, 6.250732421875, 1.85791015625, 6.845377604166667, 1.23101806640625]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:23:50.739 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:23:50.739 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-63_pid-0_2-3-4-6-7.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 3, 4, 6, 7):   1%|          | 1/100 [00:29<48:52, 29.62s/it]is_0k: False
your chose emoji: ['🏝', '💃🏿', '🌉', '🧔🏼\u200d♀', '🧑🏾\u200d🔬', '㊗', '↖️', '👩🏾\u200d🌾', '👉🏽', '🍚']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.02s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.71s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.34s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
Processing depth (0, 3, 4, 7, 9):   1%|          | 1/100 [00:46<48:52, 29.62s/it]2025-01-22 06:24:08.220 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:24:08.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:24:08.221 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the milk.
2025-01-22 06:24:08.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7622, 7626) -->  Sandra took the milk
2025-01-22 06:24:08.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:24:08.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9854, 9859) --> . Sandra moved to the
2025-01-22 06:24:08.318 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the milk there.
2025-01-22 06:24:08.408 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16843, 16849) --> . Sandra put down the milk
2025-01-22 06:24:08.408 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary left the football.
2025-01-22 06:24:08.526 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21497, 21501) -->  left the football.
2025-01-22 06:24:08.526 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:24:08.575 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9702, 9707) --> . John travelled to the
2025-01-22 06:24:08.576 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the football.
2025-01-22 06:24:08.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3180, 3184) -->  Mary grabbed the football
2025-01-22 06:24:08.601 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:24:08.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10764, 10769) --> . Mary moved to the
2025-01-22 06:24:08.670 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football there.
2025-01-22 06:24:08.686 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3179, 3184) --> . Mary grabbed the football
2025-01-22 06:24:08.686 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 06:24:08.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21425, 21431) -->  St. John went back to
2025-01-22 06:24:08.793 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel went to the office.
2025-01-22 06:24:08.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13182, 13187) --> . Daniel went to the
2025-01-22 06:24:08.858 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:24:08.928 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (14695, 14699) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:24:11.563 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:24:11.563 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24248])
2025-01-22 06:24:19.963 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [462.925, 16.943680157519278, 145.35500919117646, 16.302331026269016, 22.728872647849464], 'topk_indices': array([ 7626, 16846,    14, 16845, 24233,    36, 16771, 16848, 19636,
           1,    24,  7625, 24235, 24243, 16849, 24246, 24247,     0,
       24244, 24248]), 'topk_tokens': ['.', ' down', '\n', ' put', ' milk', ' hallway', 'ed', ' milk', ' hall', '<|start_header_id|>', '\n\n', ' milk', ' discarded', '<|eot_id|>', ' there', '<|end_header_id|>', '\n\n', '<|begin_of_text|>', '<|start_header_id|>', 'hall'], 'evidence_proportions': [434.7083333333333, 719.5, 275.75, 729.5833333333334, 82.65625]}, 'weight': {'score': [23.0184375, 23.45446579522494, 23.15280330882353, 23.45534035011574, 29.672043010752688], 'topk_indices': array([18832, 18876, 14654, 14690, 19479, 14740, 19537, 14704, 14618,
       14553, 20399, 20351, 18170, 18201, 23719, 23585, 22046, 22019,
       23801, 23667]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 27.912109375, 21.040625, 24.705729166666668, 20.6796875]}, 'saliency': {'score': [3.0542431640625, 0.0963184749469094, 0.9550359389361214, 0.0920548956230204, 0.16430335916498656], 'topk_indices': array([16771,    24, 24227, 24221, 14696, 16844, 16846, 16845, 16809,
       16770, 24233, 19636, 16848,  7622, 16849,  7625,    36, 24247,
       24235, 24248]), 'topk_tokens': ['ed', '\n\n', ' prior', 'Question', ' discarded', ' Sandra', ' down', ' put', 'present', 'present', ' milk', ' hall', ' milk', ' Sandra', ' there', ' milk', ' hallway', '\n\n', ' discarded', 'hall'], 'evidence_proportions': [2.888671875, 5.41650390625, 1.51298828125, 4.666178385416667, 0.449005126953125]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:24:19.970 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:24:19.970 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-63_pid-1_0-3-4-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (0, 3, 4, 7, 9):   2%|▏         | 2/100 [00:58<48:00, 29.39s/it]is_0k: False
your chose emoji: ['🤛🏿', '👩🏼\u200d❤️\u200d👩🏿', '🦥', '🧑🏾\u200d❤️\u200d🧑🏼', '🗃️', '🌊', '👩🏻\u200d❤\u200d💋\u200d👩🏼', '🧑🏿\u200d🎄', '🚴🏻\u200d♀', '🚶🏽\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.71s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.16s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  5.00s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.50s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.05s/it]
Processing depth (0, 3, 6, 7, 8):   2%|▏         | 2/100 [01:17<48:00, 29.39s/it]2025-01-22 06:24:38.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:24:38.492 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:24:38.493 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the milk.
2025-01-22 06:24:38.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7594, 7598) -->  Sandra took the milk
2025-01-22 06:24:38.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:24:38.611 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14553, 14558) -->  bonds. Sandra moved to
2025-01-22 06:24:38.612 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the milk there.
2025-01-22 06:24:38.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16911, 16917) --> . Sandra put down the milk
2025-01-22 06:24:38.703 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary left the football.
2025-01-22 06:24:38.795 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19232, 19236) -->  left the football.
2025-01-22 06:24:38.795 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:24:38.843 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9664, 9669) --> . John travelled to the
2025-01-22 06:24:38.843 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the football.
2025-01-22 06:24:38.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3224, 3228) -->  Mary grabbed the football
2025-01-22 06:24:38.859 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:24:38.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10620, 10625) --> . Mary moved to the
2025-01-22 06:24:38.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football there.
2025-01-22 06:24:38.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3223, 3228) --> . Mary grabbed the football
2025-01-22 06:24:38.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 06:24:39.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21390, 21396) -->  St. John went back to
2025-01-22 06:24:39.050 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel went to the office.
2025-01-22 06:24:39.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13146, 13151) --> . Daniel went to the
2025-01-22 06:24:39.117 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:24:39.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (14705, 14709) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:24:41.858 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:24:41.859 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24206])
2025-01-22 06:24:50.255 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [464.53875, 17.197393014994425, 146.92865349264707, 16.551662299430642, 27.15478515625], 'topk_indices': array([24179,    32, 24180,    28,    35, 16917,    33,    34,  7597,
          14,    24, 19601,     1, 24201,    36, 24202, 24204,     0,
       24205, 24206]), 'topk_tokens': ['Question', ' journey', ':', '<|end_header_id|>', ' the', ' there', 'ed', ' to', ' milk', '\n', '\n\n', ' hall', '<|start_header_id|>', '<|eot_id|>', ' hallway', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', 'hall'], 'evidence_proportions': [901.5833333333334, 472.1875, 188.81875, 458.0, 155.78125]}, 'weight': {'score': [23.9, 23.44316937915651, 23.15280330882353, 23.443105266563148, 29.58615451388889], 'topk_indices': array([18834, 18790, 14592, 14628, 19444, 14637, 14673, 19502, 14485,
       14550, 20364, 20316, 18128, 18159, 23549, 23683, 21967, 21994,
       23765, 23631]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 27.912109375, 25.4484375, 24.705729166666668, 20.6796875]}, 'saliency': {'score': [3.104658203125, 0.09776675819813913, 0.9685583675608915, 0.09342807493348053, 0.1944891611735026], 'topk_indices': array([   31, 16912, 24185, 24199, 16917,    24, 19327,  3227, 24191,
        7594, 16916, 24193,    32,  7597, 24203, 24179, 19601, 24205,
          36, 24206]), 'topk_tokens': ['andra', ' Sandra', ' prior', 'Answer', ' there', '\n\n', ' hall', ' football', ' milk', ' Sandra', ' milk', ' discarded', ' journey', ' milk', 'assistant', 'Question', ' hall', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [6.0439453125, 3.528076171875, 1.23046875, 2.95166015625, 0.84454345703125]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:24:50.260 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:24:50.260 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-63_pid-2_0-3-6-7-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (0, 3, 6, 7, 8):   3%|▎         | 3/100 [01:29<48:10, 29.80s/it]is_0k: False
your chose emoji: ['🤵🏼\u200d♀', '🇬🇭', '⁉', '🏌🏽\u200d♂️', '👦🏾', '🙍🏻', '💂🏼\u200d♀', '🏫', '👩🏾\u200d❤\u200d💋\u200d👨🏿', '👮🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.93s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.14s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.43s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]
Processing depth (2, 3, 5, 6, 8):   3%|▎         | 3/100 [01:47<48:10, 29.80s/it]2025-01-22 06:25:08.903 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:25:08.931 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4949, 4955) --> . Sandra journeyed to the
2025-01-22 06:25:08.931 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the milk.
2025-01-22 06:25:08.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7523, 7527) -->  Sandra took the milk
2025-01-22 06:25:08.972 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:25:09.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11853, 11858) --> . Sandra moved to the
2025-01-22 06:25:09.033 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the milk there.
2025-01-22 06:25:09.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14365, 14371) --> . Sandra put down the milk
2025-01-22 06:25:09.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary left the football.
2025-01-22 06:25:09.210 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (19229, 19233) -->  left the football.
2025-01-22 06:25:09.211 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:25:09.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9613, 9618) --> . John travelled to the
2025-01-22 06:25:09.262 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the football.
2025-01-22 06:25:09.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3216, 3220) -->  Mary grabbed the football
2025-01-22 06:25:09.277 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:25:09.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10569, 10574) --> . Mary moved to the
2025-01-22 06:25:09.333 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football there.
2025-01-22 06:25:09.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3215, 3220) --> . Mary grabbed the football
2025-01-22 06:25:09.349 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 06:25:09.467 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21387, 21393) -->  St. John went back to
2025-01-22 06:25:09.467 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel went to the office.
2025-01-22 06:25:09.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (13009, 13014) --> . Daniel went to the
2025-01-22 06:25:09.532 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:25:09.605 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (14579, 14583) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:25:12.278 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:25:12.278 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 06:25:20.691 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [522.77, 22.420330691107257, 230.39659926470588, 21.610003543641103, 29.69649920886076], 'topk_indices': array([24098,    24, 24199,    14,     1,  4954, 24204, 24206, 14371,
       14370,  7526,  7527, 19598, 24214, 24217,     0, 24215, 24218,
        4955, 24219]), 'topk_tokens': ['.\n\n', '\n\n', ' to', '\n', '<|start_header_id|>', ' the', ' milk', ' discarded', ' there', ' milk', ' milk', '.', ' hall', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [688.625, 683.0625, 206.575, 694.2916666666666, 251.65625]}, 'weight': {'score': [23.0034375, 23.451090434315912, 23.15280330882353, 23.451973316641144, 30.21064082278481], 'topk_indices': array([18841, 18797, 14651, 14615, 14696, 19499, 14660, 19441, 14509,
       14574, 20313, 20361, 18130, 18099, 23688, 23554, 21991, 21964,
       23770, 23636]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 27.912109375, 21.040625, 24.705729166666668, 20.6796875]}, 'saliency': {'score': [3.1869140625, 0.12859187686426596, 1.5668047736672794, 0.12340389138513688, 0.22851871538765822], 'topk_indices': array([19324, 14368,  7523,  3219, 24192, 14366,  4951, 14371, 24198,
       24216,  4950, 24204, 14580, 14370,  7526, 19598, 24206, 24218,
        4955, 24219]), 'topk_tokens': [' hall', ' down', ' Sandra', ' football', 'Question', ' Sandra', ' journey', ' there', ' prior', 'assistant', ' Sandra', ' milk', ' discarded', ' milk', ' milk', ' hall', ' discarded', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [3.6837565104166665, 4.91943359375, 1.254150390625, 4.415852864583333, 1.28167724609375]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:25:20.701 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:25:20.701 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-63_pid-3_2-3-5-6-8.pkl | len: 3 |  size: 2.1 KB
Processing depth (2, 3, 5, 6, 8):   4%|▍         | 4/100 [01:59<48:05, 30.05s/it]is_0k: False
your chose emoji: ['♓', '🌪', '🙇🏼\u200d♀️', '\U0001fad7', '💤', '🧑🏾\u200d❤\u200d💋\u200d🧑🏽', '🤦🏻\u200d♂️', '🏊🏾\u200d♂️', '🚶🏿\u200d♀\u200d➡️', '🧏🏼\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.00s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.80s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.86s/it]
Processing depth (2, 4, 6, 7, 9):   4%|▍         | 4/100 [02:17<48:05, 30.05s/it]2025-01-22 06:25:38.687 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:25:38.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4951, 4957) --> . Sandra journeyed to the
2025-01-22 06:25:38.713 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the milk.
2025-01-22 06:25:38.762 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9715, 9719) -->  Sandra took the milk
2025-01-22 06:25:38.763 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:25:38.839 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14353, 14358) --> . Sandra moved to the
2025-01-22 06:25:38.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the milk there.
2025-01-22 06:25:38.929 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16780, 16786) --> . Sandra put down the milk
2025-01-22 06:25:38.930 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary left the football.
2025-01-22 06:25:39.036 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21485, 21489) -->  Mary left the football
2025-01-22 06:25:39.036 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:25:39.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (9627, 9632) -->  war. John travelled to
2025-01-22 06:25:39.088 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Mary grabbed the football.
2025-01-22 06:25:39.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3172, 3176) -->  Mary grabbed the football
2025-01-22 06:25:39.105 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:25:39.162 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (10593, 10598) --> . Mary moved to the
2025-01-22 06:25:39.162 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football there.
2025-01-22 06:25:39.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (3171, 3176) --> . Mary grabbed the football
2025-01-22 06:25:39.180 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  John went back to the office.
2025-01-22 06:25:39.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21384, 21390) -->  St. John went back to
2025-01-22 06:25:39.302 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 5 -->  Daniel went to the office.
2025-01-22 06:25:39.369 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 5 at --> (12997, 13002) --> . Daniel went to the
2025-01-22 06:25:39.370 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 6 -->  Mary discarded the football.
2025-01-22 06:25:39.452 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 6 at --> (14566, 14570) -->  Mary discarded the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:25:42.119 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:25:42.120 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24199])
2025-01-22 06:25:50.535 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [553.77875, 17.74740078299314, 178.76539522058823, 16.965584292030815, 19.964504076086957], 'topk_indices': array([16725,  4957,    24,  9717,     1, 16782, 16784, 24186, 24184,
       16783,  9718, 24197, 16785,  9719, 24195, 24198, 16786, 24194,
           0, 24199]), 'topk_tokens': ['on', ' hallway', '\n\n', ' the', '<|start_header_id|>', ' put', ' the', ' discarded', ' milk', ' down', ' milk', '<|end_header_id|>', ' milk', '.', '<|start_header_id|>', '\n\n', ' there', '<|eot_id|>', '<|begin_of_text|>', 'hall'], 'evidence_proportions': [370.4166666666667, 971.875, 212.675, 1076.5833333333333, 52.8984375]}, 'weight': {'score': [23.92875, 23.441456284604577, 23.662224264705884, 23.44064079132668, 29.50090579710145], 'topk_indices': array([18823, 18779, 14602, 14638, 19496, 14683, 19438, 14647, 14496,
       14561, 20310, 20358, 18091, 18122, 23684, 23550, 21966, 21993,
       23632, 23766]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 27.912109375, 21.040625, 24.705729166666668, 26.462890625]}, 'saliency': {'score': [3.5602197265625, 0.10060286452953268, 1.1754617130055147, 0.09550674464389264, 0.14707217009171195], 'topk_indices': array([16724,  4952, 24182, 24178,  9719,  9716, 19595, 14567, 16781,
        9715, 16782, 24198, 16783,  4957, 24184, 24186,  9718, 16785,
       16786, 24199]), 'topk_tokens': [' bay', ' Sandra', ' where', ' prior', '.', ' took', ' hall', ' discarded', ' Sandra', ' Sandra', ' put', '\n\n', ' down', ' hallway', ' milk', ' discarded', ' milk', ' milk', ' there', 'hall'], 'evidence_proportions': [2.2293294270833335, 6.8203125, 1.270556640625, 6.75634765625, 0.364349365234375]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:25:50.540 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:25:50.540 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-63_pid-4_2-4-6-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 4, 6, 7, 9):   5%|▌         | 5/100 [02:29<47:27, 29.98s/it]Processing depth (2, 4, 6, 7, 9):   5%|▌         | 5/100 [02:29<47:24, 29.94s/it]
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:82 - Selected idx: 64
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the football before the garden? 
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:86 - Needle: [' John travelled to the kitchen.', ' Sandra journeyed to the hallway.', ' John went back to the office.', ' Sandra moved to the garden.', ' Mary moved to the bedroom.', ' Mary grabbed the football.', ' Sandra put down the football there.', ' Daniel went to the office.']
2025-01-22 06:25:50.826 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra journeyed to the hallway.', ' Sandra moved to the garden.', ' Sandra put down the football there.', ' Daniel went to the office.']
2025-01-22 06:25:50.827 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['👫', '🧑🏽\u200d🎄', '⛹🏽\u200d♀️', '🏃🏾\u200d♀️\u200d➡️', '🖐', '🧑🏾\u200d🍳', '📮', '👩🏻\u200d⚕', '\U0001faf4🏼', '😃']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.72s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
Processing depth (1, 2, 5, 8):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:26:08.491 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:26:08.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Sandra journeyed to the
2025-01-22 06:26:08.509 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:26:08.535 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4964, 4969) --> . Sandra moved to the
2025-01-22 06:26:08.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:26:08.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11854, 11860) -->  Sandra put down the football there
2025-01-22 06:26:08.600 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:26:08.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9949, 9954) -->  back to the office.
2025-01-22 06:26:08.653 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:26:08.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4371, 4376) -->  execution. John travelled to
2025-01-22 06:26:08.677 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:26:08.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9946, 9952) --> . John went back to the
2025-01-22 06:26:08.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:26:08.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21059, 21064) --> . Mary moved to the
2025-01-22 06:26:08.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:26:08.925 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15892, 15896) -->  grabbed the football.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:26:11.564 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 06:26:11.564 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24159])
2025-01-22 06:26:19.943 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [403.36789772727275, 16.429351874844798, 164.22109375, 15.95387580327529, 38.2738037109375], 'topk_indices': array([24142, 19553,    24, 24156, 24141, 11858, 24150,    14,    28,
       24149, 24147, 24146,     1, 24157, 24155,     0, 24154,  3037,
       24158, 24159]), 'topk_tokens': [':', ' hall', '\n\n', 'assistant', 'Question', ' football', '?', '\n', '<|end_header_id|>', ' garden', ' before', ' football', '<|start_header_id|>', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>', '<|eot_id|>', ' hallway', '\n\n', 'hall'], 'evidence_proportions': [528.25, 302.475, 666.75, 38.34375]}, 'weight': {'score': [22.226917613636363, 23.43587037496896, 21.541015625, 23.43854425787728, 28.96142578125], 'topk_indices': array([18795, 18751, 14610, 14646, 19454, 14655, 19396, 14691, 14509,
       14574, 20268, 20316, 18108, 18077, 23645, 23511, 21957, 21930,
       23727, 23593]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 21.040625, 27.462239583333332, 18.346875]}, 'saliency': {'score': [2.477847012606534, 0.09378122235457537, 0.9661895751953125, 0.09088331130804311, 0.2711653709411621], 'topk_indices': array([22678,  3033, 11859, 24150, 19279, 24013, 19932, 22677,  3032,
       19553, 15894, 24147, 24156, 24141, 11858, 24149, 24146, 24158,
        3037, 24159]), 'topk_tokens': [' after', ' journey', ' there', '?', ' hall', ' context', ' hall', ' hall', ' Sandra', ' hall', ' football', ' before', 'assistant', 'Question', ' football', ' garden', ' football', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [3.0187174479166665, 1.66572265625, 4.527669270833333, 0.18114013671875]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 06:26:19.951 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:26:19.951 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-64_pid-0_1-2-5-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 2, 5, 8):   1%|          | 1/100 [00:28<47:50, 28.99s/it]is_0k: False
your chose emoji: ['🧑🏾\u200d🚀', '🤚🏾', '💇🏻', '🏃\u200d♀️\u200d➡️', '🤙🏻', '🙆🏿', '🦍', '🦵🏿', '⛹🏿', '🧕🏼']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.94s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.80s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.69s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.76s/it]
Processing depth (3, 4, 7, 8):   1%|          | 1/100 [00:46<47:50, 28.99s/it]2025-01-22 06:26:37.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:26:37.316 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7655, 7661) --> . Sandra journeyed to the
2025-01-22 06:26:37.317 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:26:37.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9898, 9903) --> . Sandra moved to the
2025-01-22 06:26:37.376 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:26:37.477 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16773, 16779) -->  Sandra put down the football there
2025-01-22 06:26:37.478 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:26:37.540 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (10047, 10052) -->  back to the office.
2025-01-22 06:26:37.540 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:26:37.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4399, 4404) --> . John travelled to the
2025-01-22 06:26:37.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:26:37.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (10044, 10050) --> . John went back to the
2025-01-22 06:26:37.636 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:26:37.749 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21289, 21294) --> . Mary moved to the
2025-01-22 06:26:37.749 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:26:37.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15928, 15932) -->  Mary grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:26:40.520 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:26:40.520 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24167])
2025-01-22 06:26:48.918 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1178.1491477272727, 42.349436284650395, 691.1234375, 40.776033032161806, 94.40073529411765], 'topk_indices': array([24021,  4401,    14,  7659, 24155, 16777, 15932, 19527,  7656,
        7657,  7658, 24162, 24165, 15931, 24154,     0, 24166, 24163,
        7661, 24167]), 'topk_tokens': [' context', ' travelled', '\n', ' to', ' before', ' football', '.', ' hall', ' Sandra', ' journey', 'ed', '<|eot_id|>', '<|end_header_id|>', ' football', ' football', '<|begin_of_text|>', '\n\n', '<|start_header_id|>', ' hallway', 'hall'], 'evidence_proportions': [2218.1666666666665, 612.6, 1499.5, 110.05625]}, 'weight': {'score': [22.226917613636363, 23.44174079437319, 21.583203125, 23.44438903970491, 29.679917279411764], 'topk_indices': array([18725, 18769, 14559, 14595, 14604, 14640, 19364, 19422, 14523,
       14458, 20242, 20290, 18063, 18094, 23653, 23519, 21969, 21942,
       23601, 23735]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' sincere', ' atrocities', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 21.040625, 27.462239583333332, 18.346875]}, 'saliency': {'score': [7.1993408203125, 0.24426260440370293, 4.4731201171875, 0.23441558554570935, 0.7085364846622243], 'topk_indices': array([16778, 16747, 15929, 16773, 24021, 24157,  7658, 24149, 24164,
       24155,  4401, 16777, 19527,  7657,  7656, 15931, 24166, 24154,
        7661, 24167]), 'topk_tokens': [' there', ' Capt', ' grabbed', ' Sandra', ' context', ' garden', 'ed', 'Question', 'assistant', ' before', ' travelled', ' football', ' hall', ' journey', ' Sandra', ' football', '\n\n', ' football', ' hallway', 'hall'], 'evidence_proportions': [12.791015625, 3.341796875, 10.416341145833334, 0.486474609375]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:26:48.934 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:26:48.934 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-64_pid-1_3-4-7-8.pkl | len: 3 |  size: 2.07 KB
Processing depth (3, 4, 7, 8):   2%|▏         | 2/100 [00:57<47:20, 28.99s/it]is_0k: False
your chose emoji: ['🇭🇰', '🕖', '🙋🏿\u200d♀️', '🏋️\u200d♂️', '🤸🏻\u200d♂', '💑🏿', '🧑\u200d🦱', '🏃🏿\u200d♂\u200d➡', '👳', '👨🏻']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.96s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.15s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.95s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
Processing depth (0, 2, 3, 7):   2%|▏         | 2/100 [01:16<47:20, 28.99s/it]2025-01-22 06:27:07.204 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:27:07.204 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:27:07.204 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:27:07.232 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4925, 4930) --> . Sandra moved to the
2025-01-22 06:27:07.233 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:27:07.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7543, 7549) --> . Sandra put down the football
2025-01-22 06:27:07.278 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:27:07.331 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9959, 9964) -->  back to the office.
2025-01-22 06:27:07.332 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:27:07.354 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4321, 4326) --> . John travelled to the
2025-01-22 06:27:07.355 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:27:07.406 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9956, 9962) --> . John went back to the
2025-01-22 06:27:07.407 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:27:07.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21034, 21039) --> . Mary moved to the
2025-01-22 06:27:07.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:27:07.602 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15820, 15824) -->  Mary grabbed the football
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:27:10.269 | INFO     | test_jbb_embedding:begin_test:693 - the office<|eot_id|>
2025-01-22 06:27:10.269 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24166])
2025-01-22 06:27:18.640 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [815.8998579545455, 18.545108144730854, 230.55234375, 17.642304264931404, 21.600440181902986], 'topk_indices': array([24156,    14,     1,    24,    37,  7548,  7549,   187, 24153,
          32,    35,    34, 24164,    33, 24162, 24161, 24165,     0,
       24166,    36]), 'topk_tokens': [' garden', '\n', '<|start_header_id|>', '\n\n', '.', ' football', ' there', 'ION', ' football', ' journey', ' the', ' to', '<|end_header_id|>', 'ed', '<|start_header_id|>', '<|eot_id|>', '\n\n', '<|begin_of_text|>', 'hall', ' hallway'], 'evidence_proportions': [1986.1666666666667, 397.3, 640.8333333333334, 40.259375]}, 'weight': {'score': [21.55184659090909, 23.43801201952915, 21.583203125, 23.44126944139346, 29.15391791044776], 'topk_indices': array([18846, 18802, 14637, 14601, 19441, 14682, 19499, 14646, 14500,
       14565, 20361, 20313, 18171, 18140, 23524, 23658, 21963, 21990,
       23740, 23606]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 21.040625, 24.924479166666668, 18.346875]}, 'saliency': {'score': [5.358872847123579, 0.10641773569060574, 1.424053955078125, 0.10053607875691005, 0.15101179436071596], 'topk_indices': array([22711, 24154,    34, 22710, 24163,  4323, 19598, 15823, 24156,
          31,  7549,    33,   187, 24148,  7548, 24153,    32, 24165,
       24166,    36]), 'topk_tokens': [' after', ' before', ' to', ' hall', 'assistant', ' travelled', ' hall', ' football', ' garden', 'andra', ' there', 'ed', 'ION', 'Question', ' football', ' football', ' journey', '\n\n', 'hall', ' hallway'], 'evidence_proportions': [13.403645833333334, 2.294140625, 4.167317708333333, 0.19974365234375]}}, 'pred_res': 'the office<|eot_id|>', 'score': 0}
2025-01-22 06:27:18.647 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:27:18.648 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-64_pid-2_0-2-3-7.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 2, 3, 7):   3%|▎         | 3/100 [01:27<47:23, 29.32s/it]is_0k: False
your chose emoji: ['🍵', '🦩', '🤌🏾', '🧑🏻\u200d🔧', '💁\u200d♀', '🚵🏽', '🧎🏿\u200d➡', '👮🏿\u200d♂️', '👨🏽\u200d🦯\u200d➡', '🐠']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.68s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.99s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.91s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (1, 5, 6, 7):   3%|▎         | 3/100 [01:45<47:23, 29.32s/it]2025-01-22 06:27:36.627 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:27:36.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . Sandra journeyed to the
2025-01-22 06:27:36.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:27:36.701 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11921, 11926) --> . Sandra moved to the
2025-01-22 06:27:36.702 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:27:36.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14381, 14387) --> . Sandra put down the football
2025-01-22 06:27:36.773 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:27:36.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9957, 9962) -->  back to the office.
2025-01-22 06:27:36.821 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:27:36.844 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4406, 4411) --> . John travelled to the
2025-01-22 06:27:36.844 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:27:36.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9954, 9960) --> . John went back to the
2025-01-22 06:27:36.893 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:27:37.002 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21059, 21064) --> . Mary moved to the
2025-01-22 06:27:37.002 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:27:37.082 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15920, 15924) -->  grabbed the football.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:27:39.739 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:27:39.739 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24189])
2025-01-22 06:27:48.122 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [293.27663352272725, 12.902913411458334, 146.044140625, 12.53723861283644, 16.836555577531644], 'topk_indices': array([24181,    28, 24186, 19623, 24177, 24179, 24171,     1, 15922,
          14, 14386, 14387, 24176,  3037, 24187, 24184,     0, 24185,
       24188, 24189]), 'topk_tokens': [' \n', '<|end_header_id|>', 'assistant', ' hall', ' before', ' garden', 'Question', '<|start_header_id|>', ' football', '\n', ' football', ' there', ' football', ' hallway', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>', '\n\n', 'hall'], 'evidence_proportions': [345.875, 258.975, 495.125, 22.2421875]}, 'weight': {'score': [21.534801136363637, 23.449146412037038, 20.4265625, 23.453393504140788, 29.945213607594937], 'topk_indices': array([18803, 18847, 14626, 14662, 14707, 19524, 14671, 19466, 14590,
       14525, 20386, 20338, 18172, 18141, 23549, 23683, 22015, 21988,
       23631, 23765]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 21.040625, 24.924479166666668, 18.346875]}, 'saliency': {'score': [1.7819602272727273, 0.07369481949579149, 0.87923583984375, 0.07147152097082039, 0.12434155427956883], 'topk_indices': array([24180, 21064, 19349, 24173, 24182,  3032, 24181, 24043, 24177,
       19623, 24179, 24186, 14387, 24171, 15922, 14386, 24176, 24188,
        3037, 24189]), 'topk_tokens': ['?', ' bedroom', ' hall', ' Where', 'Answer', ' Sandra', ' \n', ' context', ' before', ' hall', ' garden', 'assistant', ' there', 'Question', ' football', ' football', ' football', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [1.9742838541666667, 1.45517578125, 3.264892578125, 0.0984375]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:27:48.129 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:27:48.129 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-64_pid-3_1-5-6-7.pkl | len: 3 |  size: 2.08 KB
Processing depth (1, 5, 6, 7):   4%|▍         | 4/100 [01:57<47:00, 29.38s/it]is_0k: False
your chose emoji: ['🧎\u200d♂️\u200d➡️', '🧎🏿\u200d♀️\u200d➡', '🇻🇺', '🤷\u200d♂️', '🇰🇲', '😨', '🧑🏼\u200d🍳', '👩🏼\u200d🏫', '🚶🏼\u200d♂️\u200d➡', '▪']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.37s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.69s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.67s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.22s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.73s/it]
Processing depth (3, 4, 6, 7):   4%|▍         | 4/100 [02:14<47:00, 29.38s/it]2025-01-22 06:28:05.494 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:28:05.536 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (7471, 7477) --> . Sandra journeyed to the
2025-01-22 06:28:05.537 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra moved to the garden.
2025-01-22 06:28:05.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9710, 9715) --> . Sandra moved to the
2025-01-22 06:28:05.585 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra put down the football there.
2025-01-22 06:28:05.668 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14353, 14359) --> . Sandra put down the football
2025-01-22 06:28:05.669 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Daniel went to the office.
2025-01-22 06:28:05.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (9921, 9926) -->  back to the office.
2025-01-22 06:28:05.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John travelled to the kitchen.
2025-01-22 06:28:05.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4287, 4292) --> . John travelled to the
2025-01-22 06:28:05.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:28:05.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9918, 9924) --> . John went back to the
2025-01-22 06:28:05.827 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:28:05.940 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (21113, 21118) --> . Mary moved to the
2025-01-22 06:28:05.941 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:28:06.022 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (15920, 15924) -->  grabbed the football.
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:28:08.677 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:28:08.677 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24217])
2025-01-22 06:28:17.045 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [558.3849431818181, 20.876959898843932, 320.2859375, 20.140201060881793, 29.544879872311828], 'topk_indices': array([ 9712,    14,  4289, 24207,  9711,  9710,  7472,  7473,  7475,
       14358, 14359,  7474, 24204, 24213, 15922, 24216,     0, 24212,
       24217,  7477]), 'topk_tokens': [' moved', '\n', ' travelled', ' garden', ' Sandra', '.', ' Sandra', ' journey', ' to', ' football', ' there', 'ed', ' football', '<|start_header_id|>', ' football', '\n\n', '<|begin_of_text|>', '<|eot_id|>', 'hall', ' hallway'], 'evidence_proportions': [878.3333333333334, 649.95, 595.8333333333334, 37.94375]}, 'weight': {'score': [21.534801136363637, 23.451496696944673, 20.4265625, 23.455742953304657, 29.273185483870968], 'topk_indices': array([18867, 18823, 14638, 14674, 14719, 19462, 14683, 19520, 14537,
       14602, 20382, 20334, 18141, 18172, 23697, 23563, 21984, 22011,
       23779, 23645]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 21.040625, 24.924479166666668, 18.346875]}, 'saliency': {'score': [3.229705810546875, 0.12056316246580821, 1.8721466064453125, 0.11628519046078821, 0.2152933100218414], 'topk_indices': array([21118,  4292,  9715,  7474, 24205, 24071,  9712, 24207, 24199,
       14359, 24216,  4289,  9711,  7472,  7473, 14358, 24204, 15922,
       24217,  7477]), 'topk_tokens': [' bedroom', ' kitchen', ' garden', 'ed', ' before', ' context', ' moved', ' garden', 'Question', ' there', '\n\n', ' travelled', ' Sandra', ' Sandra', ' journey', ' football', ' football', ' football', 'hall', ' hallway'], 'evidence_proportions': [4.798990885416667, 3.7080078125, 3.8211263020833335, 0.15855712890625]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:28:17.058 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:28:17.062 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-64_pid-4_3-4-6-7.pkl | len: 3 |  size: 2.06 KB
Processing depth (3, 4, 6, 7):   5%|▌         | 5/100 [02:26<46:15, 29.22s/it]Processing depth (3, 4, 6, 7):   5%|▌         | 5/100 [02:26<46:22, 29.29s/it]
2025-01-22 06:28:17.399 | INFO     | __main__:<module>:82 - Selected idx: 65
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the apple was discarded, left or dropped?
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:84 - Answer: hallway
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:86 - Needle: [' Mary grabbed the football.', ' Sandra journeyed to the hallway.', ' Sandra took the apple.', ' John went back to the office.', ' Sandra moved to the garden.', ' John travelled to the kitchen.', ' Mary moved to the bedroom.', ' Sandra put down the apple there.', ' Daniel went to the office.']
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:87 - Real Needle: [' Sandra journeyed to the hallway.', ' Sandra took the apple.', ' Sandra moved to the garden.', ' Sandra put down the apple there.', ' Daniel went to the office.']
2025-01-22 06:28:17.400 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧑🏽\u200d❤\u200d🧑🏿', '🙅🏿\u200d♂️', '🧑🏾\u200d🎓', '🧚🏽', '👱🏾\u200d♀', '💇🏿', '👨🏻\u200d🤝\u200d👨🏾', '👮🏻\u200d♀️', '6⃣', '🚶🏼\u200d➡']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.39s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.63s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.80s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (2, 3, 4, 7, 9):   0%|          | 0/100 [00:17<?, ?it/s]2025-01-22 06:28:34.761 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:28:34.788 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4952, 4958) --> . Sandra journeyed to the
2025-01-22 06:28:34.789 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the apple.
2025-01-22 06:28:34.828 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7608, 7612) -->  Sandra took the apple
2025-01-22 06:28:34.828 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:28:34.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9752, 9757) --> . Sandra moved to the
2025-01-22 06:28:34.882 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the apple there.
2025-01-22 06:28:34.969 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16768, 16774) --> . Sandra put down the apple
2025-01-22 06:28:34.969 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the office.
2025-01-22 06:28:34.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3968, 3973) -->  back to the office.
2025-01-22 06:28:34.989 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary grabbed the football.
2025-01-22 06:28:35.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22498, 22502) -->  Mary grabbed the football
2025-01-22 06:28:35.108 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:28:35.130 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3965, 3971) --> . John went back to the
2025-01-22 06:28:35.131 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 06:28:35.228 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18833, 18838) --> . John travelled to the
2025-01-22 06:28:35.228 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bedroom.
2025-01-22 06:28:35.261 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6675, 6680) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:28:38.038 | INFO     | test_jbb_embedding:begin_test:693 - the top of the float<|eot_id|>
2025-01-22 06:28:38.038 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24165])
2025-01-22 06:28:46.433 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [385.27524038461536, 16.116949582092023, 112.2125, 15.63937614003814, 21.146516927083333], 'topk_indices': array([24137, 19584, 16771, 24150, 24044, 16702, 19583, 16773,    14,
           1,  7611,    24,  4958, 16774, 24163,     0, 24164, 24160,
       24165, 24161]), 'topk_tokens': ['.\n\n', '\n', ' down', ' apple', '.\n\n', 'on', ' hall', ' apple', '\n', '<|start_header_id|>', ' apple', '\n\n', ' hallway', ' there', '<|end_header_id|>', '<|begin_of_text|>', '\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>'], 'evidence_proportions': [393.0, 582.6875, 226.1, 631.0, 82.38125]}, 'weight': {'score': [22.487079326923077, 23.435335464250247, 21.583203125, 23.437893183608324, 29.288020833333334], 'topk_indices': array([18775, 18819, 14612, 14648, 14657, 19484, 19426, 14693, 14511,
       14576, 20298, 20346, 18113, 18144, 23514, 23648, 21948, 21975,
       23730, 23596]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 27.982421875, 21.040625, 24.752604166666668, 18.346875]}, 'saliency': {'score': [2.3733896108774037, 0.089813232421875, 0.6439453125, 0.08689242869741574, 0.15524698893229166], 'topk_indices': array([24054,  4953, 24158, 16740, 16701, 24138, 24162, 16771,    24,
       16770, 24144, 24152, 24150, 19583, 16773,  7611, 16774, 24164,
        4958, 24165]), 'topk_tokens': [' location', ' Sandra', 'Answer', ' bay', ' bay', 'Question', 'assistant', ' down', '\n\n', ' put', ' prior', ' discarded', ' apple', ' hall', ' apple', ' apple', ' there', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [2.2018229166666665, 4.15625, 1.25654296875, 3.9764811197916665, 0.3461181640625]}}, 'pred_res': 'the top of the float<|eot_id|>', 'score': 0}
2025-01-22 06:28:46.439 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:28:46.440 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-65_pid-0_2-3-4-7-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (2, 3, 4, 7, 9):   1%|          | 1/100 [00:28<47:39, 28.89s/it]is_0k: False
your chose emoji: ['🧑🏻\u200d🦼\u200d➡', '🙋🏻\u200d♀️', '🏼', '👨🏾\u200d🦼', '🙉', '🌖', '🕵🏻\u200d♂️', '🥈', '🟩', '🌙']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.47s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.96s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.53s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]
Processing depth (1, 3, 5, 6, 7):   1%|          | 1/100 [00:47<47:39, 28.89s/it]2025-01-22 06:29:04.967 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:29:04.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2909, 2915) --> . Sandra journeyed to the
2025-01-22 06:29:04.984 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the apple.
2025-01-22 06:29:05.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7368, 7372) -->  Sandra took the apple
2025-01-22 06:29:05.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:29:05.100 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11798, 11803) --> . Sandra moved to the
2025-01-22 06:29:05.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the apple there.
2025-01-22 06:29:05.178 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14304, 14310) --> . Sandra put down the apple
2025-01-22 06:29:05.179 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the office.
2025-01-22 06:29:05.201 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3911, 3916) -->  back to the office.
2025-01-22 06:29:05.201 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary grabbed the football.
2025-01-22 06:29:05.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22546, 22550) -->  Mary grabbed the football
2025-01-22 06:29:05.322 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:29:05.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3906, 3912) -->  the ground. John went back
2025-01-22 06:29:05.345 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 06:29:05.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19092, 19097) -->  order. John travelled to
2025-01-22 06:29:05.446 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bedroom.
2025-01-22 06:29:05.483 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6537, 6542) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:29:08.256 | INFO     | test_jbb_embedding:begin_test:693 - Sandra's hand<|eot_id|>
2025-01-22 06:29:08.256 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24185])
2025-01-22 06:29:16.682 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [382.1598557692308, 17.149441355217462, 123.51875, 16.668219544776736, 19.732700892857142], 'topk_indices': array([   23,  2916, 24064, 24179,    24, 24157,  7371, 14310, 24159,
       19581,     1, 24177,    14, 24183,  2915,     0, 24184, 24180,
       24185, 24181]), 'topk_tokens': ['4', '.', '.\n\n', ':', '\n\n', '.\n\n', ' apple', ' there', ':', ' hall', '<|start_header_id|>', '?\n', '\n', '<|end_header_id|>', ' hallway', '<|begin_of_text|>', '\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>'], 'evidence_proportions': [524.2916666666666, 589.5, 195.2125, 489.9583333333333, 103.31875]}, 'weight': {'score': [22.487079326923077, 23.444794422854308, 23.18359375, 23.446042231898765, 30.082589285714285], 'topk_indices': array([18779, 18823, 14649, 14685, 19424, 14694, 14730, 19482, 14613,
       14548, 20344, 20296, 18148, 18117, 23662, 23528, 21975, 21948,
       23744, 23610]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.213541666666668, 27.982421875, 21.040625, 24.752604166666668, 18.346875]}, 'saliency': {'score': [2.4228891225961537, 0.09493353900306971, 0.67818603515625, 0.09194323600015016, 0.14792415073939733], 'topk_indices': array([   24, 24064,  2911, 24170, 24157, 24182, 24164, 14309, 24178,
       24177,  7368, 24172, 14310, 24158,  2910,  7371, 19581, 24184,
        2915, 24185]), 'topk_tokens': ['\n\n', '.\n\n', ' journey', ' apple', '.\n\n', 'assistant', ' prior', ' apple', 'Answer', '?\n', ' Sandra', ' discarded', ' there', 'Question', ' Sandra', ' apple', ' hall', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [2.9689127604166665, 4.417724609375, 1.244775390625, 3.16748046875, 0.456396484375]}}, 'pred_res': "Sandra's hand<|eot_id|>", 'score': 0}
2025-01-22 06:29:16.689 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:29:16.689 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-65_pid-1_1-3-5-6-7.pkl | len: 3 |  size: 2.07 KB
Processing depth (1, 3, 5, 6, 7):   2%|▏         | 2/100 [00:59<48:29, 29.69s/it]is_0k: False
your chose emoji: ['🏋️\u200d♀️', '👩🏿\u200d❤️\u200d💋\u200d👩🏻', '🧔', '🚇', '👨🏿\u200d💻', '🧗🏽\u200d♂️', '🚬', '🗨️', '🏌️\u200d♀', '🚶🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.88s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.30s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.07s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.49s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]
Processing depth (1, 4, 7, 8, 9):   2%|▏         | 2/100 [01:17<48:29, 29.69s/it]2025-01-22 06:29:35.565 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:29:35.580 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2962, 2968) -->  tragedy. Sandra journeyed to
2025-01-22 06:29:35.580 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the apple.
2025-01-22 06:29:35.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9686, 9690) -->  Sandra took the apple
2025-01-22 06:29:35.630 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:29:35.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (16778, 16783) --> . Sandra moved to the
2025-01-22 06:29:35.720 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the apple there.
2025-01-22 06:29:35.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19267, 19273) -->  Sandra put down the apple there
2025-01-22 06:29:35.816 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the office.
2025-01-22 06:29:35.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3911, 3916) -->  back to the office.
2025-01-22 06:29:35.836 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary grabbed the football.
2025-01-22 06:29:35.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22554, 22558) -->  Mary grabbed the football
2025-01-22 06:29:35.948 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:29:35.969 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3906, 3912) -->  the ground. John went back
2025-01-22 06:29:35.969 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 06:29:36.064 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19083, 19088) -->  order. John travelled to
2025-01-22 06:29:36.065 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bedroom.
2025-01-22 06:29:36.101 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6565, 6570) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:29:39.132 | INFO     | test_jbb_embedding:begin_test:693 - the kitchen<|eot_id|>
2025-01-22 06:29:39.132 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24185])
2025-01-22 06:29:47.817 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [263.69350961538464, 14.16988537704647, 115.5796875, 13.817146984508327, 20.6234375], 'topk_indices': array([18914, 24164, 19272, 24159, 24064, 24177,     3, 24172, 24157,
          23,    14,  2969,    24,     1,     0, 24184, 24183, 24180,
       24185, 24181]), 'topk_tokens': [' in', ' prior', ' there', ':', '.\n\n', '?\n', '<|end_header_id|>', ' discarded', '.\n\n', '4', '\n', ' hallway', '\n\n', '<|start_header_id|>', '<|begin_of_text|>', '\n\n', '<|end_header_id|>', '<|eot_id|>', 'hall', '<|start_header_id|>'], 'evidence_proportions': [278.3333333333333, 305.46875, 140.275, 482.9583333333333, 73.00625]}, 'weight': {'score': [24.189603365384617, 23.43754909459236, 23.18359375, 23.43694954591583, 28.834375], 'topk_indices': array([18814, 18770, 14608, 14644, 19480, 14689, 14653, 19422, 14507,
       14572, 20354, 20306, 18108, 18139, 23662, 23528, 21956, 21983,
       23610, 23744]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.053385416666668, 27.982421875, 21.040625, 27.290364583333332, 18.346875]}, 'saliency': {'score': [1.7205294095552885, 0.07884682374136968, 0.6673492431640625, 0.07659126100341858, 0.14625113351004465], 'topk_indices': array([ 2964,  9689, 19272, 19591, 24030, 18913,    23, 24158, 24178,
       24157,    20, 24170, 19271, 24164, 24182,    24, 24172, 24184,
        2969, 24185]), 'topk_tokens': [' Sandra', ' apple', ' there', ' hall', ' context', ' manner', '4', 'Question', 'Answer', '.\n\n', ' Jul', ' apple', ' apple', ' prior', 'assistant', '\n\n', ' discarded', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [1.7115071614583333, 2.271240234375, 0.88369140625, 3.21484375, 0.3344482421875]}}, 'pred_res': 'the kitchen<|eot_id|>', 'score': 0}
2025-01-22 06:29:47.825 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:29:47.826 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-65_pid-2_1-4-7-8-9.pkl | len: 3 |  size: 2.09 KB
Processing depth (1, 4, 7, 8, 9):   3%|▎         | 3/100 [01:30<49:03, 30.35s/it]is_0k: False
your chose emoji: ['🥖', '🇨🇭', '🌞', '👩🏿\u200d🎤', '👩🏾\u200d🦲', '🚣\u200d♂️', '🤰🏽', '❕', '👨🏼\u200d🤝\u200d👨🏾', '👩🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.95s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.86s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.39s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.95s/it]
Processing depth (0, 1, 3, 8, 9):   3%|▎         | 3/100 [01:48<49:03, 30.35s/it]2025-01-22 06:30:05.921 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:30:05.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (32, 38) -->  journeyed to the hallway.
2025-01-22 06:30:05.922 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the apple.
2025-01-22 06:30:05.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (2968, 2972) -->  Sandra took the apple
2025-01-22 06:30:05.937 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:30:05.978 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7637, 7642) --> . Sandra moved to the
2025-01-22 06:30:05.979 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the apple there.
2025-01-22 06:30:06.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19282, 19288) -->  Sandra put down the apple there
2025-01-22 06:30:06.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the office.
2025-01-22 06:30:06.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3985, 3990) -->  back to the office.
2025-01-22 06:30:06.097 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary grabbed the football.
2025-01-22 06:30:06.212 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22557, 22561) -->  Mary grabbed the football
2025-01-22 06:30:06.213 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:30:06.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3982, 3988) --> . John went back to the
2025-01-22 06:30:06.238 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 06:30:06.340 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19098, 19103) -->  order. John travelled to
2025-01-22 06:30:06.340 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bedroom.
2025-01-22 06:30:06.374 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6685, 6690) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:30:09.016 | INFO     | test_jbb_embedding:begin_test:693 - the garden<|eot_id|>
2025-01-22 06:30:09.016 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24240])
2025-01-22 06:30:17.445 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [1524.3149038461538, 47.500659984325374, 282.7, 45.71939961565484, 54.06970844072165], 'topk_indices': array([   14, 12504,    34, 19287,    33, 19286, 19242,    32, 12550,
        2971,  2972, 24238, 24239,    37,  2093,    36, 24235, 24236,
       24240,     0]), 'topk_tokens': ['\n', 'ien', ' to', ' there', 'ed', ' apple', 'n', ' journey', 'u', ' apple', '.', '<|end_header_id|>', '\n\n', '.', ' the', ' hallway', '<|eot_id|>', '<|start_header_id|>', 'hall', '<|begin_of_text|>'], 'evidence_proportions': [2833.6666666666665, 1856.5, 621.9, 1863.6666666666667, 182.5375]}, 'weight': {'score': [23.087139423076923, 23.45780740832405, 22.3734375, 23.459101980617433, 29.90061211340206], 'topk_indices': array([18785, 18829, 14615, 14651, 14660, 19495, 19437, 14696, 14579,
       14514, 20375, 20327, 18105, 18136, 23707, 23573, 22007, 22034,
       23655, 23789]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.276041666666668, 27.982421875, 21.040625, 27.290364583333332, 18.346875]}, 'saliency': {'score': [9.7406005859375, 0.27114630624690633, 1.60391845703125, 0.2598696498726598, 0.4088172519329897], 'topk_indices': array([24213,  2046, 24239, 19282, 12542, 24225,  2083, 12550, 12541,
       24219, 19287, 12504,    31,  2968, 24227, 19286,    32,  2971,
          36, 24240]), 'topk_tokens': ['Question', ' river', '\n\n', ' Sandra', ' Ch', ' apple', ' du', 'u', ' du', ' prior', ' there', 'ien', 'andra', ' Sandra', ' discarded', ' apple', ' journey', ' apple', ' hallway', 'hall'], 'evidence_proportions': [16.802734375, 13.7021484375, 3.553515625, 12.657552083333334, 0.783544921875]}}, 'pred_res': 'the garden<|eot_id|>', 'score': 0}
2025-01-22 06:30:17.453 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:30:17.453 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-65_pid-3_0-1-3-8-9.pkl | len: 3 |  size: 2.05 KB
Processing depth (0, 1, 3, 8, 9):   4%|▍         | 4/100 [01:59<48:06, 30.06s/it]is_0k: False
your chose emoji: ['🏮', '🤷🏼', '🙅🏽\u200d♀️', '🏃🏿\u200d♂\u200d➡️', '👸🏾', '🏋🏿\u200d♀️', '👩🏻\u200d❤\u200d👩🏽', '👨\u200d👩\u200d👦\u200d👦', '🧑🏾\u200d⚖️', '🧎🏼\u200d♀\u200d➡']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.15s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.75s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Processing depth (1, 4, 5, 6, 8):   4%|▍         | 4/100 [02:17<48:06, 30.06s/it]2025-01-22 06:30:35.548 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra journeyed to the hallway.
2025-01-22 06:30:35.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2954, 2960) -->  tragedy. Sandra journeyed to
2025-01-22 06:30:35.563 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra took the apple.
2025-01-22 06:30:35.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9758, 9762) -->  Sandra took the apple
2025-01-22 06:30:35.615 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra moved to the garden.
2025-01-22 06:30:35.682 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11884, 11889) --> . Sandra moved to the
2025-01-22 06:30:35.682 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Sandra put down the apple there.
2025-01-22 06:30:35.765 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14356, 14362) --> . Sandra put down the apple
2025-01-22 06:30:35.766 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Daniel went to the office.
2025-01-22 06:30:35.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (3975, 3980) -->  back to the office.
2025-01-22 06:30:35.790 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary grabbed the football.
2025-01-22 06:30:35.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (22542, 22546) -->  Mary grabbed the football
2025-01-22 06:30:35.911 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John went back to the office.
2025-01-22 06:30:35.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3972, 3978) --> . John went back to the
2025-01-22 06:30:35.935 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the kitchen.
2025-01-22 06:30:36.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (18863, 18868) --> . John travelled to the
2025-01-22 06:30:36.034 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary moved to the bedroom.
2025-01-22 06:30:36.076 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (6623, 6628) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:30:38.959 | INFO     | test_jbb_embedding:begin_test:693 - Sandra put down the apple there.<|eot_id|>
2025-01-22 06:30:38.960 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24241])
2025-01-22 06:30:47.411 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [522.5853365384615, 18.694196244431613, 150.4046875, 18.043920210347963, 18.217912946428573], 'topk_indices': array([24120,  9758, 24235,    23, 24215, 14359, 14362, 14361, 24233,
        9762,    14,     1,  9761, 24239,  2961,     0, 24240, 24236,
       24241, 24237]), 'topk_tokens': ['.\n\n', ' Sandra', ':', '4', ':', ' down', ' there', ' apple', '?\n', '.', '\n', '<|start_header_id|>', ' apple', '<|end_header_id|>', ' hallway', '<|begin_of_text|>', '\n\n', '<|eot_id|>', 'hall', '<|start_header_id|>'], 'evidence_proportions': [548.40625, 914.875, 335.05, 730.4166666666666, 115.90625]}, 'weight': {'score': [23.603966346153847, 23.455955597261177, 21.583203125, 23.4573444215431, 29.569196428571427], 'topk_indices': array([18805, 18849, 14677, 14641, 19456, 14722, 19514, 14686, 14605,
       14540, 20328, 20376, 18143, 18174, 23692, 23558, 21999, 21972,
       23774, 23640]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [26.053385416666668, 27.982421875, 21.040625, 24.752604166666668, 18.346875]}, 'saliency': {'score': [3.3800283578725963, 0.10484366581357758, 0.85101318359375, 0.10070786151780364, 0.133745154555963], 'topk_indices': array([24233, 11889, 24234, 24226, 14358, 24238, 24214, 14362, 19613,
        2957, 24228, 14359, 14357,  2956, 14361,  9758,  9761, 24240,
        2961, 24241]), 'topk_tokens': ['?\n', ' garden', 'Answer', ' apple', ' put', 'assistant', 'Question', ' there', ' hall', ' journey', ' discarded', ' down', ' Sandra', ' Sandra', ' apple', ' Sandra', ' apple', '\n\n', ' hallway', 'hall'], 'evidence_proportions': [3.4002278645833335, 6.67724609375, 1.96904296875, 4.74169921875, 0.4949951171875]}}, 'pred_res': 'Sandra put down the apple there.<|eot_id|>', 'score': 0}
2025-01-22 06:30:47.419 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:30:47.419 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-65_pid-4_1-4-5-6-8.pkl | len: 3 |  size: 2.11 KB
Processing depth (1, 4, 5, 6, 8):   5%|▌         | 5/100 [02:29<47:32, 30.03s/it]Processing depth (1, 4, 5, 6, 8):   5%|▌         | 5/100 [02:30<47:32, 30.02s/it]
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:82 - Selected idx: 66
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the milk before the garden? 
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:85 - Tag: 3-hop
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:86 - Needle: [' John picked up the milk.', ' Sandra went to the garden.', ' Sandra went to the bathroom.', ' John journeyed to the bathroom.', ' Mary moved to the bedroom.', ' John travelled to the garden.', ' Mary grabbed the football.']
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:87 - Real Needle: [' John picked up the milk.', ' John journeyed to the bathroom.', ' John travelled to the garden.', ' Mary grabbed the football.']
2025-01-22 06:30:47.675 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🧑🏾\u200d🦱', '🧜🏻\u200d♂️', '👩🏿\u200d❤️\u200d💋\u200d👨🏼', '🇲🇦', '👩🏾\u200d🚒', '👩🏽\u200d🚀', '🧚🏼\u200d♀', '🈵', '▶️', '👩🏼\u200d❤️\u200d💋\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.15s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.66s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.69s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.75s/it]
Processing depth (2, 3, 4, 6):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 06:31:04.892 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John picked up the milk.
2025-01-22 06:31:04.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4979, 4984) --> . John picked up the
2025-01-22 06:31:04.916 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bathroom.
2025-01-22 06:31:04.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7561, 7567) --> . John journeyed to the
2025-01-22 06:31:04.958 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:31:05.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9728, 9733) --> . John travelled to the
2025-01-22 06:31:05.006 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:31:05.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14391, 14395) -->  Mary grabbed the football
2025-01-22 06:31:05.081 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 06:31:05.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20714, 20719) -->  Sandra went to the garden
2025-01-22 06:31:05.191 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 06:31:05.298 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20715, 20720) -->  went to the garden.
2025-01-22 06:31:05.299 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:31:05.307 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1501, 1506) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:31:08.192 | INFO     | test_jbb_embedding:begin_test:693 - John picked up the milk.<|eot_id|>
2025-01-22 06:31:08.192 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24219])
2025-01-22 06:31:16.620 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [141.41875, 7.0454932499380725, 124.05416666666666, 6.861816264935709, 13.00555046237245], 'topk_indices': array([24200, 24107, 24073,  4984, 24117,     9, 24210,  9734, 24206,
          24, 24218,    14, 24209,  4988, 24207,  4989, 24215, 24217,
           0, 24214]), 'topk_tokens': ['.\n\n', '.\n\n', ' context', ' milk', ' location', ':', '?', '.', ' milk', '\n\n', '\n\n', '\n', ' garden', ' *', ' before', '      ', '<|start_header_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [215.8, 140.625, 137.51875, 54.5078125]}, 'weight': {'score': [21.808203125, 23.453962306993642, 21.416145833333335, 23.45658695786993, 29.300542091836736], 'topk_indices': array([18797, 18841, 14709, 14673, 14754, 19494, 19436, 14718, 14572,
       14637, 20368, 20320, 18135, 18166, 23567, 23701, 22033, 22006,
       23783, 23649]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.334375, 19.4140625, 19.990625, 28.263671875]}, 'saliency': {'score': [0.781378173828125, 0.03953424617051389, 0.6822265625, 0.038522245372643364, 0.09720292383310747], 'topk_indices': array([ 4981,  9733, 24210, 24188, 24218,    24,  4987,     0,  1506,
        7567, 24216, 24117, 24073,  4984, 24201,  4988, 24206, 24207,
        4989, 24209]), 'topk_tokens': [' picked', ' garden', '?', ' return', '\n\n', '\n\n', '      ', '<|begin_of_text|>', ' bedroom', ' bathroom', 'assistant', ' location', ' context', ' milk', 'Question', ' *', ' milk', ' before', '      ', ' garden'], 'evidence_proportions': [1.21142578125, 0.7497965494791666, 0.6830078125, 0.414154052734375]}}, 'pred_res': 'John picked up the milk.<|eot_id|>', 'score': 0}
2025-01-22 06:31:16.634 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:31:16.636 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-66_pid-0_2-3-4-6.pkl | len: 3 |  size: 2.04 KB
Processing depth (2, 3, 4, 6):   1%|          | 1/100 [00:28<47:32, 28.82s/it]is_0k: False
your chose emoji: ['💁🏿\u200d♀️', '🧑🏿\u200d🤝\u200d🧑🏾', '🇲🇵', '🧍🏼\u200d♂', '🧑🏾\u200d🤝\u200d🧑🏾', '🧜🏼', '🧑🏿\u200d✈️', '👨🏻\u200d💼', '🏄🏻\u200d♀', '🧚🏻\u200d♀']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.31s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.79s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.37s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
Processing depth (1, 3, 5, 7):   1%|          | 1/100 [00:46<47:32, 28.82s/it]2025-01-22 06:31:34.465 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John picked up the milk.
2025-01-22 06:31:34.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2921, 2926) --> . John picked up the
2025-01-22 06:31:34.480 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bathroom.
2025-01-22 06:31:34.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7379, 7385) --> . John journeyed to the
2025-01-22 06:31:34.519 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:31:34.597 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (11804, 11809) --> . John travelled to the
2025-01-22 06:31:34.598 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:31:34.688 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16681, 16685) -->  Mary grabbed the football
2025-01-22 06:31:34.691 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 06:31:34.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20642, 20647) -->  Sandra went to the garden
2025-01-22 06:31:34.815 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 06:31:34.925 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20643, 20648) -->  went to the garden.
2025-01-22 06:31:34.926 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:31:34.934 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1389, 1394) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:31:37.611 | INFO     | test_jbb_embedding:begin_test:693 - the bedroom<|eot_id|>
2025-01-22 06:31:37.611 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24147])
2025-01-22 06:31:45.951 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [154.9078125, 7.78205615942029, 149.63333333333333, 7.571801783122538, 12.506268901209678], 'topk_indices': array([ 7385,  2926, 24132, 24001, 24128, 24136, 24141, 24045, 24035,
       24129, 24134, 24130,    14, 24137, 24138, 24145, 24135,     0,
       24143, 24142]), 'topk_tokens': [' bathroom', ' milk', ' was', ' context', '.\n\n', ' the', ':', ' location', '.\n\n', 'Question', ' milk', ':', '\n', ' garden', '?', '<|end_header_id|>', ' before', '<|begin_of_text|>', '<|start_header_id|>', '<|eot_id|>'], 'evidence_proportions': [255.975, 177.1875, 94.8875, 70.1796875]}, 'weight': {'score': [21.808203125, 23.436614906832297, 21.416145833333335, 23.43922221646278, 29.316280241935484], 'topk_indices': array([18781, 18737, 14608, 14644, 19434, 14689, 19376, 14653, 14572,
       14507, 20296, 20248, 18106, 18075, 23631, 23497, 21902, 21929,
       23579, 23713]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.334375, 19.4140625, 19.990625, 28.263671875]}, 'saliency': {'score': [0.866046142578125, 0.04389372957912785, 0.7845377604166667, 0.042751174790716356, 0.09554407673497353], 'topk_indices': array([ 2260, 24128,     8, 24132, 23999, 24116, 24131, 24035,  2923,
       24001, 24045,  2926, 24140, 24138,  1394,  7385, 24129, 24134,
       24137, 24135]), 'topk_tokens': [' Bench', '.\n\n', ' Date', ' was', ' provided', ' return', ' Where', '.\n\n', ' picked', ' context', ' location', ' milk', 'Answer', '?', ' bedroom', ' bathroom', 'Question', ' milk', ' garden', ' before'], 'evidence_proportions': [1.4404296875, 0.9345703125, 0.4791015625, 0.528961181640625]}}, 'pred_res': 'the bedroom<|eot_id|>', 'score': 0}
2025-01-22 06:31:45.959 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:31:45.959 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-66_pid-1_1-3-5-7.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 3, 5, 7):   2%|▏         | 2/100 [00:58<47:33, 29.11s/it]is_0k: False
your chose emoji: ['🙂', '🔑', '👴🏾', '👨🏼\u200d🦯\u200d➡️', '💈', '🇨🇽', '👎🏽', '🙌🏿', '👨🏻\u200d🎨', '👱\u200d♂️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.64s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.16s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.38s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
Processing depth (2, 5, 8, 9):   2%|▏         | 2/100 [01:16<47:33, 29.11s/it]2025-01-22 06:32:04.321 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John picked up the milk.
2025-01-22 06:32:04.346 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (4993, 4998) --> . John picked up the
2025-01-22 06:32:04.346 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bathroom.
2025-01-22 06:32:04.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (11919, 11925) --> . John journeyed to the
2025-01-22 06:32:04.405 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:32:04.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (19261, 19266) -->  John travelled to the garden
2025-01-22 06:32:04.514 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:32:04.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (21466, 21470) -->  grabbed the football.
2025-01-22 06:32:04.621 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 06:32:04.728 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20681, 20686) -->  Sandra went to the garden
2025-01-22 06:32:04.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 06:32:04.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20682, 20687) -->  went to the garden.
2025-01-22 06:32:04.840 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:32:04.850 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1501, 1506) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:32:07.528 | INFO     | test_jbb_embedding:begin_test:693 - St. John<|eot_id|>
2025-01-22 06:32:07.528 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24173])
2025-01-22 06:32:15.908 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [306.828125, 8.691824407470218, 148.22083333333333, 8.358132300857463, 12.229908854166666], 'topk_indices': array([ 4970,  4958, 24061, 24156,  4993,    14,    23, 11921,  4995,
          24, 24160,  4969,  4998, 24161,  4999,  4994, 24171, 24168,
           0, 24169]), 'topk_tokens': [' *', '4', '.\n\n', ':', '.', '\n', '4', ' journey', ' picked', '\n\n', ' milk', '.', ' milk', ' before', '.', ' John', '<|end_header_id|>', '<|eot_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [605.95, 345.9583333333333, 174.5125, 39.625]}, 'weight': {'score': [21.780078125, 23.443973362011914, 21.416145833333335, 23.446611832567, 29.483125], 'topk_indices': array([18770, 18814, 14642, 14606, 19415, 14651, 19473, 14687, 14505,
       14570, 20287, 20335, 18117, 18086, 23525, 23659, 21946, 21973,
       23607, 23741]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [21.334375, 19.4140625, 24.5046875, 22.48046875]}, 'saliency': {'score': [1.7665496826171876, 0.048702742389146264, 0.78505859375, 0.04682203005018045, 0.08985036214192708], 'topk_indices': array([24061,    23,  4996, 24165,  1506, 24157,    24,  4957, 24027,
        4970,  4992, 24155, 24163, 11925, 11921,  4995, 24160, 24161,
        4998,  4994]), 'topk_tokens': ['.\n\n', '4', ' up', ' \n', ' bedroom', ' Where', '\n\n', '185', ' context', ' *', ' St', 'Question', ' garden', ' bathroom', ' journey', ' picked', ' milk', ' before', ' milk', ' John'], 'evidence_proportions': [3.4970703125, 1.890380859375, 1.10703125, 0.2420501708984375]}}, 'pred_res': 'St. John<|eot_id|>', 'score': 0}
2025-01-22 06:32:15.916 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:32:15.917 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-66_pid-2_2-5-8-9.pkl | len: 3 |  size: 1.98 KB
Processing depth (2, 5, 8, 9):   3%|▎         | 3/100 [01:28<47:41, 29.50s/it]is_0k: False
your chose emoji: ['🤴🏼', '👨🏿\u200d❤\u200d👨🏾', '🏃🏼\u200d♀\u200d➡', '🏹', '🤸🏻\u200d♂️', '🏋\u200d♀️', '👵🏾', '▫', '👩🏽\u200d🚀', '🤘🏽']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.71s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.24s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Processing depth (1, 4, 6, 8):   3%|▎         | 3/100 [01:45<47:41, 29.50s/it]2025-01-22 06:32:33.628 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John picked up the milk.
2025-01-22 06:32:33.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (2960, 2965) -->  tragedy. John picked up
2025-01-22 06:32:33.643 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bathroom.
2025-01-22 06:32:33.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (9705, 9711) --> . John journeyed to the
2025-01-22 06:32:33.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:32:33.771 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (14282, 14287) --> . John travelled to the
2025-01-22 06:32:33.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:32:33.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (19242, 19246) -->  grabbed the football.
2025-01-22 06:32:33.868 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 06:32:33.970 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20625, 20630) --> . Sandra went to the
2025-01-22 06:32:33.970 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 06:32:34.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20625, 20630) --> . Sandra went to the
2025-01-22 06:32:34.077 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:32:34.085 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1501, 1506) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:32:36.775 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 06:32:36.775 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24157])
2025-01-22 06:32:45.113 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [259.0765625, 11.150851096854305, 157.09583333333333, 10.854573834196891, 17.490380130597014], 'topk_indices': array([ 9708,  2961,  2963, 24151, 24045, 24147,  8721,  3405,  9707,
       24144, 24156,    24, 24145,    14,  2967,  2966,  9711, 24155,
           0, 24152]), 'topk_tokens': ['ed', '.', ' picked', ':', '.\n\n', ' garden', ' Father', ' could', ' journey', ' milk', '\n\n', '\n\n', ' before', '\n', '.', ' milk', ' bathroom', '<|end_header_id|>', '<|begin_of_text|>', '<|eot_id|>'], 'evidence_proportions': [364.2, 374.9583333333333, 175.8, 57.9453125]}, 'weight': {'score': [22.103515625, 23.439750620860927, 20.413541666666667, 23.442739961139896, 29.443330223880597], 'topk_indices': array([18750, 18794, 14626, 14662, 14671, 19394, 14707, 19452, 14590,
       14525, 20272, 20320, 18088, 18119, 23495, 23629, 21949, 21922,
       23577, 23711]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [27.1421875, 19.4140625, 19.990625, 22.48046875]}, 'saliency': {'score': [1.46527099609375, 0.06302796925929997, 0.82099609375, 0.06139421247571244, 0.1326999948985541], 'topk_indices': array([ 1503,     0,  9683, 24154, 24156,    24, 24045,  2374, 24139,
       24150,  1506,  3405,  2963, 24145, 24147, 24144,  8721,  9707,
        2966,  9711]), 'topk_tokens': [' moved', '<|begin_of_text|>', ' company', 'assistant', '\n\n', '\n\n', '.\n\n', '�', 'Question', 'Answer', ' bedroom', ' could', ' picked', ' before', ' garden', ' milk', ' Father', ' journey', ' milk', ' bathroom'], 'evidence_proportions': [2.2072265625, 2.0215657552083335, 0.9669921875, 0.32623291015625]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 06:32:45.119 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:32:45.121 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-66_pid-3_1-4-6-8.pkl | len: 3 |  size: 2.02 KB
Processing depth (1, 4, 6, 8):   4%|▍         | 4/100 [01:57<47:00, 29.38s/it]is_0k: False
your chose emoji: ['🤦\u200d♀️', '🔻', '👩🏿\u200d🦯\u200d➡️', '📆', '🛂', '🪀', '🎧', '🧑🏽\u200d🦰', '🙍🏽\u200d♂', '🚶🏿\u200d♂\u200d➡️']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.43s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.54s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.65s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.33s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.78s/it]
Processing depth (0, 2, 4, 6):   4%|▍         | 4/100 [02:14<47:00, 29.38s/it]2025-01-22 06:33:02.672 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John picked up the milk.
2025-01-22 06:33:02.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 36) -->  picked up the milk.
2025-01-22 06:33:02.673 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John journeyed to the bathroom.
2025-01-22 06:33:02.699 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4883, 4889) --> . John journeyed to the
2025-01-22 06:33:02.699 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:33:02.769 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9708, 9713) --> . John travelled to the
2025-01-22 06:33:02.770 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  Mary grabbed the football.
2025-01-22 06:33:02.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (14381, 14385) -->  Mary grabbed the football
2025-01-22 06:33:02.863 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Sandra went to the garden.
2025-01-22 06:33:02.980 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (20680, 20685) -->  Sandra went to the garden
2025-01-22 06:33:02.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the bathroom.
2025-01-22 06:33:03.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (20681, 20686) -->  went to the garden.
2025-01-22 06:33:03.087 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Mary moved to the bedroom.
2025-01-22 06:33:03.094 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (1399, 1404) --> . Mary moved to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:33:05.834 | INFO     | test_jbb_embedding:begin_test:693 - John's house.<|eot_id|>
2025-01-22 06:33:05.834 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24205])
2025-01-22 06:33:14.201 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [135.540234375, 7.962693117977528, 61.55625, 7.8238831159765025, 6.967467591002747], 'topk_indices': array([24193,    63,   105,    35,   181,    14,   445,   184,    66,
           1,   584,    68,    67,    24,   446, 24200,   185, 24203,
       24201,     0]), 'topk_tokens': [' before', 'ISC', 'ION', '.', 'G', '\n', ' em', ' P', ' P', '<|start_header_id|>', ' em', 'E', 'ION', '\n\n', 's', '<|eot_id|>', 'ION', '<|end_header_id|>', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [380.65, 66.609375, 66.525, 18.818359375]}, 'weight': {'score': [21.980859375, 23.455350503965633, 21.416145833333335, 23.45783583440202, 29.922218406593405], 'topk_indices': array([18775, 18819, 14659, 14623, 19472, 14668, 14704, 19414, 14522,
       14587, 20286, 20334, 18113, 18144, 23683, 23549, 21964, 21991,
       23765, 23631]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [22.025, 19.4140625, 19.990625, 28.263671875]}, 'saliency': {'score': [0.6932876586914063, 0.04465533531980647, 0.342236328125, 0.04393401974625292, 0.052614149156507556], 'topk_indices': array([  165,   181,   562,   184,    38,   105,   467, 24193,    62,
          66,    64,  7262,    24,    68,   446,    63,   445,    67,
         584,   185]), 'topk_tokens': ['ION', 'G', ' em', ' P', '***', 'ION', ' em', ' before', 'MIN', ' P', 'ENCES', 'nes', '\n\n', 'E', 's', 'ISC', ' em', 'ION', ' em', 'ION'], 'evidence_proportions': [1.9044921875, 0.3419189453125, 0.345361328125, 0.14124298095703125]}}, 'pred_res': "John's house.<|eot_id|>", 'score': 0}
2025-01-22 06:33:14.212 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:33:14.213 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/3-hop_sid-66_pid-4_0-2-4-6.pkl | len: 3 |  size: 1.95 KB
Processing depth (0, 2, 4, 6):   5%|▌         | 5/100 [02:26<46:21, 29.28s/it]Processing depth (0, 2, 4, 6):   5%|▌         | 5/100 [02:26<46:25, 29.32s/it]
2025-01-22 06:33:14.443 | INFO     | __main__:<module>:82 - Selected idx: 67
2025-01-22 06:33:14.443 | INFO     | __main__:<module>:83 - Question: I have provided you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.

If a person obtained an item in the first location and traveled to the second location, the item is also in the second location.
If a person left an item in the first location and moved to the second location, the item remains in the first location.
If a person got an item and traveled to another location, the item will be in the new location unless they dropped it during the journey.

Just return the location directly. Do not write anything else after that.

Question: Where was the location prior to the place where the milk was discarded, left or dropped?
2025-01-22 06:33:14.443 | INFO     | __main__:<module>:84 - Answer: bathroom
2025-01-22 06:33:14.443 | INFO     | __main__:<module>:85 - Tag: 4-hop
2025-01-22 06:33:14.443 | INFO     | __main__:<module>:86 - Needle: [' Mary moved to the bedroom.', ' John journeyed to the bathroom.', ' Sandra went to the garden.', ' John picked up the milk.', ' Sandra went to the bathroom.', ' John travelled to the garden.', ' John dropped the milk.', ' Mary grabbed the football.']
2025-01-22 06:33:14.444 | INFO     | __main__:<module>:87 - Real Needle: [' John journeyed to the bathroom.', ' John picked up the milk.', ' John travelled to the garden.', ' John dropped the milk.', ' Mary grabbed the football.']
2025-01-22 06:33:14.444 | INFO     | __main__:<module>:88 - =============================================
is_0k: False
your chose emoji: ['🙆🏽\u200d♀️', '👩🏻\u200d🦯\u200d➡️', '🧏🏿\u200d♀', '🙆🏿\u200d♀️', '🏃\u200d♂️', '🙍\u200d♂', '🤸🏻\u200d♀', '🧑🏼\u200d🦲', '🚣🏽\u200d♂️', '👨🏻\u200d🦳']
is_0k: False
is_0k: False
is_0k: False
  0%|          | 0/100 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.50s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.71s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.26s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.79s/it]
Processing depth (1, 2, 3, 5, 7):   0%|          | 0/100 [00:16<?, ?it/s]2025-01-22 06:33:31.635 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the bathroom.
2025-01-22 06:33:31.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . John journeyed to the
2025-01-22 06:33:31.651 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John picked up the milk.
2025-01-22 06:33:31.681 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (5000, 5005) --> . John picked up the
2025-01-22 06:33:31.682 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:33:31.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7615, 7620) -->  war. John travelled to
2025-01-22 06:33:31.729 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the milk.
2025-01-22 06:33:31.791 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (11943, 11947) -->  John dropped the milk
2025-01-22 06:33:31.792 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary grabbed the football.
2025-01-22 06:33:31.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (16928, 16932) -->  Mary grabbed the football
2025-01-22 06:33:31.881 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bedroom.
2025-01-22 06:33:31.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16679, 16684) --> . Mary moved to the
2025-01-22 06:33:31.963 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the garden.
2025-01-22 06:33:31.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3222, 3227) --> . Sandra went to the
2025-01-22 06:33:31.981 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 06:33:31.999 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3222, 3227) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:33:34.685 | INFO     | test_jbb_embedding:begin_test:693 - The kitchen.<|eot_id|>
2025-01-22 06:33:34.685 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24211])
2025-01-22 06:33:43.110 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [301.7578125, 11.102482035186256, 144.67708333333334, 10.731050930713547, 51.376863326149426], 'topk_indices': array([24090,    23, 24116,  4999,  5005, 24129, 24206,    14,  5002,
          24, 24130,  5000,  4965,  4976,  5006, 24207,  4977, 24209,
        5001,     0]), 'topk_tokens': ['.\n\n', '4', '.\n', ' St', ' milk', ' to', '<|eot_id|>', '\n', ' picked', '\n\n', ' the', '.', '4', '.', '.', '<|start_header_id|>', ' *', '<|end_header_id|>', ' John', '<|begin_of_text|>'], 'evidence_proportions': [221.5, 615.85, 169.7875, 369.875, 126.375]}, 'weight': {'score': [23.183268229166668, 23.448604113322872, 20.413541666666667, 23.45075071096174, 29.380567528735632], 'topk_indices': array([18792, 18748, 14607, 14643, 14688, 19445, 19387, 14652, 14506,
       14571, 20259, 20307, 18117, 18086, 23536, 23670, 21953, 21980,
       23618, 23752]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.4140625, 21.334375, 23.4546875, 25.728515625, 28.263671875]}, 'saliency': {'score': [1.823089599609375, 0.0633618676259731, 0.8532063802083333, 0.06112479903212254, 0.37564367535470544], 'topk_indices': array([ 5034, 24128,  4961,  3037, 24198, 24190,    24, 24056,  4965,
        4964, 11946, 24100,  5007,  3227,  4999, 24196,  5005,  4977,
        5002,  5001]), 'topk_tokens': ['�', ' moved', ' *\n\n', ' bathroom', ' discarded', ' prior', '\n\n', ' context', '4', '185', ' milk', ' location', '�', ' bathroom', ' St', ' milk', ' milk', ' *', ' picked', ' John'], 'evidence_proportions': [1.1953938802083333, 3.5607421875, 1.0445556640625, 2.4559326171875, 0.932891845703125]}}, 'pred_res': 'The kitchen.<|eot_id|>', 'score': 0}
2025-01-22 06:33:43.117 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:33:43.117 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-67_pid-0_1-2-3-5-7.pkl | len: 3 |  size: 2.03 KB
Processing depth (1, 2, 3, 5, 7):   1%|          | 1/100 [00:28<47:06, 28.55s/it]is_0k: False
your chose emoji: ['👨🏻\u200d❤\u200d👨🏼', '🤛🏻', '🏍', '👨🏽\u200d❤\u200d👨🏿', '🇬🇾', '👨\u200d🦽\u200d➡️', '🛥', '🤷🏼', '🙋\u200d♂️', '👩🏼\u200d❤\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.39s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.81s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.96s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.48s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
Processing depth (1, 2, 3, 5, 6):   1%|          | 1/100 [00:46<47:06, 28.55s/it]2025-01-22 06:34:01.423 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the bathroom.
2025-01-22 06:34:01.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (3031, 3037) --> . John journeyed to the
2025-01-22 06:34:01.445 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John picked up the milk.
2025-01-22 06:34:01.474 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (4946, 4951) --> . John picked up the
2025-01-22 06:34:01.474 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:34:01.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (7627, 7632) -->  war. John travelled to
2025-01-22 06:34:01.518 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the milk.
2025-01-22 06:34:01.581 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (12013, 12017) -->  John dropped the milk
2025-01-22 06:34:01.582 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary grabbed the football.
2025-01-22 06:34:01.658 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (14564, 14568) -->  Mary grabbed the football
2025-01-22 06:34:01.659 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bedroom.
2025-01-22 06:34:01.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16609, 16614) -->  Mary moved to the bedroom
2025-01-22 06:34:01.740 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the garden.
2025-01-22 06:34:01.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3222, 3227) --> . Sandra went to the
2025-01-22 06:34:01.756 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 06:34:01.772 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3222, 3227) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:34:04.500 | INFO     | test_jbb_embedding:begin_test:693 - Dan Rice's circus<|eot_id|>
2025-01-22 06:34:04.501 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24225])
2025-01-22 06:34:12.923 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [152.74479166666666, 9.287224492322933, 172.925, 9.0434133283724, 9.921875], 'topk_indices': array([ 3037,  3223,  8482,     3,     4,  8484,     1, 16776, 24198,
       24104,  8506, 24199, 24197,    14,  3227,    24, 24220, 24223,
           0, 24221]), 'topk_tokens': [' bathroom', ' Sandra', ' Rice', '<|end_header_id|>', '\n\n', ' circus', '<|start_header_id|>', 'present', 'Question', '.\n\n', "'s", ':', '.\n\n', '\n', ' bathroom', '\n\n', '<|eot_id|>', '<|end_header_id|>', '<|begin_of_text|>', '<|start_header_id|>'], 'evidence_proportions': [189.39583333333334, 180.2, 96.3625, 209.84375, 76.828125]}, 'weight': {'score': [23.183268229166668, 23.453359749050684, 22.222395833333334, 23.45439107135475, 29.551695478723403], 'topk_indices': array([18800, 18756, 14636, 14600, 19407, 14681, 19465, 14645, 14559,
       14494, 20337, 20289, 18125, 18094, 23680, 23546, 21955, 21982,
       23628, 23762]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' atrocities', ' sincere', ' atrocities', ' sincere', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [19.4140625, 21.334375, 23.4546875, 25.728515625, 28.263671875]}, 'saliency': {'score': [0.88653564453125, 0.05256441471225184, 1.1037190755208333, 0.051085121255400184, 0.07277387253781582], 'topk_indices': array([24210, 24218, 24185, 16737, 24104,  8506, 24114, 24212, 24070,
       24197,  8505,  8482, 24204, 16776,  3037,    24,  3223, 24198,
        8484,  3227]), 'topk_tokens': [' milk', 'Answer', ' return', 'present', '.\n\n', "'s", ' location', ' discarded', ' context', '.\n\n', ' Rice', ' Rice', ' prior', 'present', ' bathroom', '\n\n', ' Sandra', 'Question', ' circus', ' bathroom'], 'evidence_proportions': [0.9687906901041666, 0.9748046875, 0.57529296875, 1.35107421875, 0.57733154296875]}}, 'pred_res': "Dan Rice's circus<|eot_id|>", 'score': 0}
2025-01-22 06:34:12.947 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:34:12.947 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-67_pid-1_1-2-3-5-6.pkl | len: 3 |  size: 2.12 KB
Processing depth (1, 2, 3, 5, 6):   2%|▏         | 2/100 [00:58<47:51, 29.30s/it]is_0k: False
your chose emoji: ['🙍🏿\u200d♀️', '👨🏻\u200d❤\u200d👨🏼', '🃏', '👩🏾\u200d🌾', '🧑🏼\u200d❤\u200d💋\u200d🧑🏿', '🆔', '💇🏻\u200d♂', '🤷🏼', '✊🏻', '👩🏻\u200d🤝\u200d👨🏾']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.46s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.93s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.40s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.02s/it]
Processing depth (0, 3, 4, 7, 9):   2%|▏         | 2/100 [01:16<47:51, 29.30s/it]2025-01-22 06:34:31.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  John journeyed to the bathroom.
2025-01-22 06:34:31.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (31, 37) -->  journeyed to the bathroom.
2025-01-22 06:34:31.603 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  John picked up the milk.
2025-01-22 06:34:31.640 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (7646, 7651) --> . John picked up the
2025-01-22 06:34:31.641 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  John travelled to the garden.
2025-01-22 06:34:31.694 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (9874, 9879) --> . John travelled to the
2025-01-22 06:34:31.695 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 3 -->  John dropped the milk.
2025-01-22 06:34:31.792 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 3 at --> (16791, 16795) -->  John dropped the milk
2025-01-22 06:34:31.792 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 4 -->  Mary grabbed the football.
2025-01-22 06:34:31.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 4 at --> (21442, 21446) -->  Mary grabbed the football
2025-01-22 06:34:31.908 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 0 -->  Mary moved to the bedroom.
2025-01-22 06:34:31.997 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 0 at --> (16594, 16599) --> . Mary moved to the
2025-01-22 06:34:31.998 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 1 -->  Sandra went to the garden.
2025-01-22 06:34:32.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 1 at --> (3222, 3227) --> . Sandra went to the
2025-01-22 06:34:32.018 | INFO     | test_jbb_embedding:find_multi_needle_idx:530 - evidence 2 -->  Sandra went to the bathroom.
2025-01-22 06:34:32.039 | INFO     | test_jbb_embedding:find_multi_needle_idx:538 - find evidence 2 at --> (3222, 3227) --> . Sandra went to the
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
2025-01-22 06:34:34.675 | INFO     | test_jbb_embedding:begin_test:693 - The bathroom<|eot_id|>
2025-01-22 06:34:34.675 | INFO     | test_jbb_embedding:begin_test:695 - torch.Size([1, 24177])
2025-01-22 06:34:44.174 | INFO     | test_jbb_embedding:begin_test:746 - {'embedding': {'grad': {'score': [206.32421875, 11.346290839536808, 185.68333333333334, 11.044127469864545, 8.392159598214286], 'topk_indices': array([24171, 24056, 24066, 24149,    35, 24150, 24151, 17545,    14,
        3227,    23, 24022, 17535, 24175,    24, 17546, 24172, 17536,
       24173,     0]), 'topk_tokens': [':', '.\n\n', ' location', '.\n\n', ' bathroom', 'Question', ':', 'arp', '\n', ' bathroom', '4', ' context', 'arp', '<|end_header_id|>', '\n\n', 'ente', '<|eot_id|>', 'ente', '<|start_header_id|>', '<|begin_of_text|>'], 'evidence_proportions': [334.1458333333333, 239.425, 142.075, 194.234375, 65.6171875]}, 'weight': {'score': [22.842447916666668, 23.438580438378825, 20.413541666666667, 23.441052695621558, 29.07388392857143], 'topk_indices': array([18837, 18793, 14618, 14654, 14679, 19490, 14715, 19432, 14582,
       14517, 20358, 20310, 18150, 18119, 23660, 23526, 21992, 21965,
       23608, 23742]), 'topk_tokens': ['untlet', 'untlet', ' vastly', ' vastly', ' sincere', ' atrocities', ' sincere', ' atrocities', ' reluctantly', ' reluctantly', ' lively', ' lively', ' fireworks', ' fireworks', ' disastrous', ' disastrous', ' respectable', ' respectable', ' vigilant', ' vigilant'], 'evidence_proportions': [20.9375, 21.334375, 19.990625, 25.728515625, 28.263671875]}, 'saliency': {'score': [1.2211380004882812, 0.0643306407186983, 0.9845865885416667, 0.06260878926880747, 0.06164240155901227], 'topk_indices': array([   20, 24170, 24149, 24164, 24160, 24156,    31, 24162,    23,
        7651,    24, 24066, 24022, 17545, 24150,    35, 17535,  3227,
       17546, 17536]), 'topk_tokens': [' Jul', 'Answer', '.\n\n', ' discarded', ' where', ' prior', ' journey', ' milk', '4', ' milk', '\n\n', ' location', ' context', 'arp', 'Question', ' bathroom', 'arp', ' bathroom', 'ente', 'ente'], 'evidence_proportions': [1.9857584635416667, 1.33515625, 0.75380859375, 1.251708984375, 0.4852752685546875]}}, 'pred_res': 'The bathroom<|eot_id|>', 'score': 100}
2025-01-22 06:34:44.200 | INFO     | modelzipper.tutils:auto_save_data:314 - pkl file saved successfully!
2025-01-22 06:34:44.204 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to /mnt/petrelfs/tangzecheng/repos/Long-form-reasoning/preliminary/babilong_random5x100/results/gws/Meta-Llama-3.1-8B-Instruct/11900/label/4-hop_sid-67_pid-2_0-3-4-7-9.pkl | len: 3 |  size: 2.07 KB
Processing depth (0, 3, 4, 7, 9):   3%|▎         | 3/100 [01:29<48:48, 30.19s/it]is_0k: False
your chose emoji: ['👨🏼\u200d🎨', '🧘🏾\u200d♀', '👩🏽\u200d❤️\u200d👩🏼', '🏂🏼', '📄', '⛴', '🇭🇲', '🧎🏼\u200d♂', '🧖🏾', '📊']
is_0k: False
is_0k: False
is_0k: False

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.17s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.76s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.29s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
Processing depth (1, 2, 3, 6, 7):   3%|▎         | 3/100 [01:47<48:48, 30.19s/it]