_wandb:
    value:
        cli_version: 0.18.6
        m:
            - "1": train/lr
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/global_step
              "6":
                - 3
              "7": []
            - "1": train/gpt_loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/loss_mean
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": eval/global_step
              "6":
                - 3
              "7": []
        python_version: 3.10.4
        t:
            "1":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
            "2":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
            "3":
                - 7
                - 13
                - 16
                - 23
                - 55
                - 66
            "4": 3.10.4
            "5": 0.18.6
            "6": 4.46.2
            "8":
                - 5
            "12": 0.18.6
            "13": linux-x86_64
adam_betas:
    value:
        - 0.9
        - 0.95
adam_offload:
    value: false
apply_chat_template:
    value: true
aux_loss_coef:
    value: 0
bf16:
    value: true
ckpt_path:
    value: /mnt/petrelfs/tangzecheng/llm-data-exp-space-2/zecheng/remote_ckpt/model/llama3-8b-sft
dataset:
    value: /mnt/petrelfs/tangzecheng/llm-data-exp-space-2/zecheng/transfer_data/Qwen_query_answer_gen
dataset_probs:
    value: "1.0"
disable_fast_tokenizer:
    value: true
disable_trace_cache:
    value: false
eval_split:
    value: test
eval_steps:
    value: -1
flash_attn:
    value: true
grad_accum_dtype:
    value: null
gradient_checkpointing:
    value: true
gradient_checkpointing_use_reentrant:
    value: false
input_key:
    value: instruction_str
input_template:
    value: "User: {}\nAssistant: "
l2:
    value: 0
learning_rate:
    value: 5e-06
load_checkpoint:
    value: false
load_in_4bit:
    value: false
local_rank:
    value: 0
logging_steps:
    value: 1
lora_alpha:
    value: 16
lora_dropout:
    value: 0
lora_rank:
    value: 32
lr_scheduler:
    value: cosine_with_min_lr
max_ckpt_mem:
    value: 1e+08
max_ckpt_num:
    value: 3
max_epochs:
    value: 5
max_len:
    value: 64000
max_norm:
    value: 1
max_samples:
    value: 1e+08
micro_train_batch_size:
    value: 1
num_process:
    value: 2
output_key:
    value: pred_str
packing_samples:
    value: false
pretrain:
    value: meta-llama/Meta-Llama-3-8B-Instruct
pretrain_mode:
    value: false
return_eval:
    value: false
ring_attn_size:
    value: 1
ring_head_stride:
    value: 1
save_path:
    value: /mnt/petrelfs/tangzecheng/llm-data-exp-space-2/zecheng/remote_ckpt/model/llama3-8b-sft
save_steps:
    value: 15
seed:
    value: 42
target_modules:
    value: all-linear
tokenizer_chat_template:
    value: null
train_batch_size:
    value: 64
train_split:
    value: train
use_tensorboard:
    value: null
use_wandb:
    value: f81f2a236e712350a0ec153e02f43d1366c856a5
wandb_group:
    value: null
wandb_org:
    value: null
wandb_project:
    value: debug_openrlhf_train_sft
wandb_run_name:
    value: llama3-8b-sft-vanilla
zero_stage:
    value: 3
zpg:
    value: 1
