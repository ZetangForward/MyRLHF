{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dir_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama\"\n",
    "dataset_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct\")\n",
    "# dir_path = \"/data/pub_data/check_inference/check_inference/llama\"\n",
    "# dataset_path = \"/data/pub_data/processed_multi_hop/filter_en\"\n",
    "file_names = auto_read_dir(dir_path)\n",
    "file_names.sort()\n",
    "content_drop_1 = auto_read_data(os.path.join(dir_path, file_names[0]))\n",
    "content_drop_2 = auto_read_data(os.path.join(dir_path, file_names[1]))\n",
    "content_drop_3 = auto_read_data(os.path.join(dir_path, file_names[2]))\n",
    "\n",
    "all_file_names = auto_read_dir(dataset_path)\n",
    "content = []\n",
    "for file_name in all_file_names:\n",
    "    content.extend(auto_read_data(os.path.join(dataset_path, file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是补充不完整的prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_item(args):\n",
    "    item, tokenizer, drop_num = args\n",
    "    prompt, full_evi_prompt, answer, selected_ids = process_item(item, drop_num)\n",
    "\n",
    "    input_data = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True, tokenize=False,\n",
    "    )\n",
    "\n",
    "    meta_data = copy.deepcopy(item)\n",
    "    meta_data.pop('concat_content')\n",
    "    meta_data.pop('instruction_format')\n",
    "    meta_data.pop('answer')\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"ori_prompt\": full_evi_prompt,\n",
    "        \"message\": input_data,\n",
    "        \"answer\": answer,\n",
    "        \"meta_data\": meta_data, \n",
    "        \"selected_ids\": selected_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_data(content, tokenizer, drop_num=1, num_workers=24):\n",
    "    if num_workers is None:\n",
    "        num_workers = max(1, cpu_count() - 1)  # Default: use all CPUs except one\n",
    "\n",
    "    # Prepare arguments for process_data_item\n",
    "    args = [(item, tokenizer, drop_num) for item in content]\n",
    "\n",
    "    # Use multiprocessing to parallelize processing\n",
    "    with Pool(num_workers) as pool:\n",
    "        all_inference_content = list(tqdm(pool.imap(process_data_item, args), total=len(content)))\n",
    "\n",
    "    return all_inference_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的数据集仅用来进行测试开发使用，一共32条训练，32条测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先从每个数据集中取相同数目的数据出来\n",
    "all_training_data = []\n",
    "\n",
    "all_content_data = content_drop_1 + content_drop_2 + content_drop_3\n",
    "all_content_data = all_content_data[:512]\n",
    "for item in all_content_data:\n",
    "    all_training_data.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": item[\"pred\"][0]}\n",
    "        ],\n",
    "    })\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "\n",
    "# 从中随机抽取 32 条作为 validation 数据\n",
    "validation_size = 32\n",
    "\n",
    "# 打乱数据集索引并取前 32 条作为 validation\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_data/merge_v1_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构造训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_drop_1[1]['answer'] == content_drop_2[1]['answer'] == content_drop_3[1]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先从每个数据集中取相同数目的数据出来\n",
    "post_proc_drop_1, post_proc_drop_2, post_proc_drop_3 = [], [], []\n",
    "\n",
    "# 从每个子集里面sample 3200条数据再合并到一起\n",
    "random.shuffle(content_drop_1)\n",
    "random.shuffle(content_drop_2)\n",
    "random.shuffle(content_drop_3)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for item in tqdm(content_drop_1): ## 首先检查输出是否符合预期格式\n",
    "    model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "    if len(model_pred) < 100:\n",
    "        continue\n",
    "    post_proc_drop_1.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": model_pred}\n",
    "        ],\n",
    "    })\n",
    "    cnt += 1\n",
    "    if cnt >= 3200:\n",
    "        break\n",
    "print(f\"Post_proc_drop_1 size: {len(post_proc_drop_1)}\")\n",
    "\n",
    "cnt = 0\n",
    "for item in tqdm(content_drop_2): ## 首先检查输出是否符合预期格式\n",
    "    model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "    if len(model_pred) < 100:\n",
    "        continue\n",
    "    post_proc_drop_2.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": model_pred}\n",
    "        ],\n",
    "    })\n",
    "    cnt += 1\n",
    "    if cnt >= 3200:\n",
    "        break\n",
    "print(f\"Post_proc_drop_2 size: {len(post_proc_drop_2)}\")\n",
    "\n",
    "cnt = 0\n",
    "for item in tqdm(content_drop_3): ## 首先检查输出是否符合预期格式\n",
    "    model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "    if len(model_pred) < 100:\n",
    "        continue\n",
    "    post_proc_drop_3.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "            {\"role\": \"assistant\", \"content\": model_pred}\n",
    "        ],\n",
    "    })\n",
    "    cnt += 1\n",
    "    if cnt >= 3200:\n",
    "        break\n",
    "print(f\"post_proc_drop_3 size: {len(post_proc_drop_3)}\")\n",
    "\n",
    "\n",
    "all_training_data = post_proc_drop_1 + post_proc_drop_2 + post_proc_drop_3\n",
    "random.shuffle(all_training_data)\n",
    "\n",
    "\n",
    "# 确保所有字段格式一致，处理潜在的问题\n",
    "for record in all_training_data:\n",
    "    if not isinstance(record[\"chosen\"], list):\n",
    "        record[\"chosen\"] = [record[\"chosen\"]]\n",
    "    if not isinstance(record[\"rejected\"], list):\n",
    "        record[\"rejected\"] = [record[\"rejected\"]]\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "\n",
    "# 从中随机抽取 600 条作为 validation 数据\n",
    "validation_size = 600\n",
    "\n",
    "# 打乱数据集索引并取前 400 条作为 validation\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_qwen_data/merge_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
