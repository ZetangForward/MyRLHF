{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mModelZipper is ready for launchğŸš€ | Current VersionğŸ¦„ >>> 0.2.7 <<< | AOE TimeğŸ•’ 2025-01-05 21:22:31\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-05 17:22:33.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodelzipper.tutils\u001b[0m:\u001b[36mauto_read_dir\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mnumber of files with prefix '' and suffix '': 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_1.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n",
      "\u001b[31mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_2.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_3.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-05 17:22:46.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodelzipper.tutils\u001b[0m:\u001b[36mauto_read_dir\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mnumber of files with prefix '' and suffix '': 7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_7.pkl | file size: 160.75 MB | file type: pkl\u001b[0m\n",
      "\u001b[31mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_4.pkl | file size: 157.27 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_5.pkl | file size: 139.24 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_1.pkl | file size: 158.65 MB | file type: pkl\u001b[0m\n",
      "\u001b[34mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_3.pkl | file size: 159.17 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_6.pkl | file size: 156.75 MB | file type: pkl\u001b[0m\n",
      "\u001b[97mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_2.pkl | file size: 162.81 MB | file type: pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dir_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama\"\n",
    "dataset_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct\")\n",
    "# dir_path = \"/data/pub_data/check_inference/check_inference/llama\"\n",
    "# dataset_path = \"/data/pub_data/processed_multi_hop/filter_en\"\n",
    "file_names = auto_read_dir(dir_path)\n",
    "file_names.sort()\n",
    "content_drop_1 = auto_read_data(os.path.join(dir_path, file_names[0]))\n",
    "content_drop_2 = auto_read_data(os.path.join(dir_path, file_names[1]))\n",
    "content_drop_3 = auto_read_data(os.path.join(dir_path, file_names[2]))\n",
    "\n",
    "all_file_names = auto_read_dir(dataset_path)\n",
    "content = []\n",
    "for file_name in all_file_names:\n",
    "    content.extend(auto_read_data(os.path.join(dataset_path, file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sample(content, num_sample, tokenizer=None, add_meta_info=False, return_list=[]):\n",
    "    cnt = 0\n",
    "    for item in tqdm(content): \n",
    "        model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "        if tokenizer:\n",
    "            pred_ids = tokenizer(model_pred, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "            if pred_ids.size(-1) < 50 or pred_ids.size(-1) > 400:\n",
    "                continue\n",
    "        elif len(model_pred) < 100: ## é¦–å…ˆæ£€æŸ¥è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼\n",
    "            continue\n",
    "        \n",
    "        if add_meta_info:\n",
    "            all_clues = [i['content'] for i in item['meta_data']['clue_docs']]\n",
    "        else:\n",
    "            all_clues = []\n",
    "\n",
    "        return_list.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"chosen\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": model_pred}\n",
    "            ],\n",
    "            \"meta_info\": all_clues,\n",
    "        })\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt >= num_sample:\n",
    "            break\n",
    "    print(f\"number of samples: {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‹é¢çš„æ•°æ®é›†ä»…ç”¨æ¥è¿›è¡Œæµ‹è¯•å¼€å‘ä½¿ç”¨ï¼Œä¸€å…±128æ¡è®­ç»ƒï¼Œ32æ¡æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:08<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 206\n",
      "Train dataset size: 174\n",
      "Validation dataset size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [00:00<00:00, 563.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 375.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# é¦–å…ˆä»æ¯ä¸ªæ•°æ®é›†ä¸­å–ç›¸åŒæ•°ç›®çš„æ•°æ®å‡ºæ¥\n",
    "all_training_data = []\n",
    "\n",
    "all_content_data = content_drop_1[:256]\n",
    "construct_sample(all_content_data, 3200, tokenizer, True, all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "\n",
    "# ä»ä¸­éšæœºæŠ½å– 32 æ¡ä½œä¸º validation æ•°æ®\n",
    "validation_size = 32\n",
    "\n",
    "# æ‰“ä¹±æ•°æ®é›†ç´¢å¼•å¹¶å–å‰ 32 æ¡ä½œä¸º validation\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# ä½¿ç”¨ Hugging Face çš„ select æ–¹æ³•åˆ›å»ºæ–°çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# å°† train å’Œ validation æ•°æ®é›†ä¿å­˜åˆ°ä¸€ä¸ª dict ä¸­\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†é•¿åº¦éªŒè¯\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# ä¿å­˜åˆ°æœ¬åœ°\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ„é€ çœŸå®è®­ç»ƒæ•°æ®é›†\n",
    "1. æ·»åŠ evidenceï¼Œè¾…åŠ©å®šä½éšå¼æ¨ç†è¿‡ç¨‹ä¸­çš„evidenceä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 4580/7000 [03:36<01:54, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4514/7000 [03:36<01:48, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4520/7000 [03:36<01:58, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 4606/7000 [03:39<01:54, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200\n",
      "Train dataset size: 9000\n",
      "Validation dataset size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (11/11 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9000/9000 [00:16<00:00, 530.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:01<00:00, 540.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ä»æ¯ä¸ªå­é›†é‡Œé¢sample 3200æ¡æ•°æ®å†åˆå¹¶åˆ°ä¸€èµ·\n",
    "random.shuffle(content_drop_1)\n",
    "random.shuffle(content_drop_2)\n",
    "random.shuffle(content_drop_3)\n",
    "\n",
    "manager = Manager()\n",
    "return_list = manager.list()\n",
    "\n",
    "# åˆ›å»ºè¿›ç¨‹\n",
    "process1 = multiprocessing.Process(target=construct_sample, args=(content_drop_1, 3200, tokenizer, True, return_list))\n",
    "process2 = multiprocessing.Process(target=construct_sample, args=(content_drop_2, 3200, tokenizer, True, return_list))\n",
    "process3 = multiprocessing.Process(target=construct_sample, args=(content_drop_3, 3200, tokenizer, True, return_list))\n",
    "\n",
    "# å¯åŠ¨è¿›ç¨‹\n",
    "process1.start()\n",
    "process2.start()\n",
    "process3.start()\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ\n",
    "process1.join()\n",
    "process2.join()\n",
    "process3.join()\n",
    "\n",
    "all_training_data = list(return_list)\n",
    "random.shuffle(all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "validation_size = 600\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# ä½¿ç”¨ Hugging Face çš„ select æ–¹æ³•åˆ›å»ºæ–°çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# å°† train å’Œ validation æ•°æ®é›†ä¿å­˜åˆ°ä¸€ä¸ª dict ä¸­\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†é•¿åº¦éªŒè¯\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# ä¿å­˜åˆ°æœ¬åœ°\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
