{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dir_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama\"\n",
    "dataset_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct\")\n",
    "# dir_path = \"/data/pub_data/check_inference/check_inference/llama\"\n",
    "# dataset_path = \"/data/pub_data/processed_multi_hop/filter_en\"\n",
    "file_names = auto_read_dir(dir_path)\n",
    "file_names.sort()\n",
    "content_drop_1 = auto_read_data(os.path.join(dir_path, file_names[0]))\n",
    "content_drop_2 = auto_read_data(os.path.join(dir_path, file_names[1]))\n",
    "content_drop_3 = auto_read_data(os.path.join(dir_path, file_names[2]))\n",
    "\n",
    "all_file_names = auto_read_dir(dataset_path)\n",
    "content = []\n",
    "for file_name in all_file_names:\n",
    "    content.extend(auto_read_data(os.path.join(dataset_path, file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sample(content, num_sample, tokenizer=None, add_meta_info=False, return_list=[]):\n",
    "    cnt = 0\n",
    "    for item in tqdm(content): \n",
    "        model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "        if tokenizer:\n",
    "            pred_ids = tokenizer(model_pred, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "            if pred_ids.size(-1) < 50 or pred_ids.size(-1) > 400:\n",
    "                continue\n",
    "        elif len(model_pred) < 100: ## 首先检查输出是否符合预期格式\n",
    "            continue\n",
    "        \n",
    "        if add_meta_info:\n",
    "            all_clues = [i['content'] for i in item['meta_data']['clue_docs']]\n",
    "        else:\n",
    "            all_clues = []\n",
    "\n",
    "        return_list.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"chosen\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": model_pred}\n",
    "            ],\n",
    "            \"meta_info\": all_clues,\n",
    "        })\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt >= num_sample:\n",
    "            break\n",
    "    print(f\"number of samples: {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的数据集仅用来进行测试开发使用，一共32条训练，32条测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先从每个数据集中取相同数目的数据出来\n",
    "all_training_data = []\n",
    "\n",
    "all_content_data = content_drop_1[:64]\n",
    "construct_sample(all_content_data, 3200, tokenizer, True, all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "\n",
    "# 从中随机抽取 32 条作为 validation 数据\n",
    "validation_size = 32\n",
    "\n",
    "# 打乱数据集索引并取前 32 条作为 validation\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造真实训练数据集\n",
    "1. 添加evidence，辅助定位隐式推理过程中的evidence位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从每个子集里面sample 3200条数据再合并到一起\n",
    "random.shuffle(content_drop_1)\n",
    "random.shuffle(content_drop_2)\n",
    "random.shuffle(content_drop_3)\n",
    "\n",
    "manager = Manager()\n",
    "return_list = manager.list()\n",
    "\n",
    "# 创建进程\n",
    "process1 = multiprocessing.Process(target=construct_sample, args=(content_drop_1, 3200, tokenizer, True, return_list))\n",
    "process2 = multiprocessing.Process(target=construct_sample, args=(content_drop_2, 3200, tokenizer, True, return_list))\n",
    "process3 = multiprocessing.Process(target=construct_sample, args=(content_drop_3, 3200, tokenizer, True, return_list))\n",
    "\n",
    "# 启动进程\n",
    "process1.start()\n",
    "process2.start()\n",
    "process3.start()\n",
    "\n",
    "# 等待所有进程完成\n",
    "process1.join()\n",
    "process2.join()\n",
    "process3.join()\n",
    "\n",
    "all_training_data = list(return_list)\n",
    "random.shuffle(all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "validation_size = 600\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
