{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/tangzecheng/anaconda3/envs/zecheng/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-01-05 21:22:31\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-05 17:22:33.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodelzipper.tutils\u001b[0m:\u001b[36mauto_read_dir\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mnumber of files with prefix '' and suffix '': 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_1.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n",
      "\u001b[31mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_2.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama/inference_drop_3.pkl | file size: 1.28 GB | file type: pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-05 17:22:46.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodelzipper.tutils\u001b[0m:\u001b[36mauto_read_dir\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mnumber of files with prefix '' and suffix '': 7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_7.pkl | file size: 160.75 MB | file type: pkl\u001b[0m\n",
      "\u001b[31mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_4.pkl | file size: 157.27 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_5.pkl | file size: 139.24 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_1.pkl | file size: 158.65 MB | file type: pkl\u001b[0m\n",
      "\u001b[34mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_3.pkl | file size: 159.17 MB | file type: pkl\u001b[0m\n",
      "\u001b[32mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_6.pkl | file size: 156.75 MB | file type: pkl\u001b[0m\n",
      "\u001b[97mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en/train_processed_en_snap_2.pkl | file size: 162.81 MB | file type: pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dir_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/llama\"\n",
    "dataset_path = \"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/filter_en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct\")\n",
    "# dir_path = \"/data/pub_data/check_inference/check_inference/llama\"\n",
    "# dataset_path = \"/data/pub_data/processed_multi_hop/filter_en\"\n",
    "file_names = auto_read_dir(dir_path)\n",
    "file_names.sort()\n",
    "content_drop_1 = auto_read_data(os.path.join(dir_path, file_names[0]))\n",
    "content_drop_2 = auto_read_data(os.path.join(dir_path, file_names[1]))\n",
    "content_drop_3 = auto_read_data(os.path.join(dir_path, file_names[2]))\n",
    "\n",
    "all_file_names = auto_read_dir(dataset_path)\n",
    "content = []\n",
    "for file_name in all_file_names:\n",
    "    content.extend(auto_read_data(os.path.join(dataset_path, file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sample(content, num_sample, tokenizer=None, add_meta_info=False, return_list=[]):\n",
    "    cnt = 0\n",
    "    for item in tqdm(content): \n",
    "        model_pred = item[\"pred\"][0].split('\\n\\n')[0]\n",
    "        if tokenizer:\n",
    "            pred_ids = tokenizer(model_pred, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "            if pred_ids.size(-1) < 50 or pred_ids.size(-1) > 400:\n",
    "                continue\n",
    "        elif len(model_pred) < 100: ## 首先检查输出是否符合预期格式\n",
    "            continue\n",
    "        \n",
    "        if add_meta_info:\n",
    "            all_clues = [i['content'] for i in item['meta_data']['clue_docs']]\n",
    "        else:\n",
    "            all_clues = []\n",
    "\n",
    "        return_list.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"chosen\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": item[\"answer\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]}, \n",
    "                {\"role\": \"assistant\", \"content\": model_pred}\n",
    "            ],\n",
    "            \"meta_info\": all_clues,\n",
    "        })\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt >= num_sample:\n",
    "            break\n",
    "    print(f\"number of samples: {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的数据集仅用来进行测试开发使用，一共128条训练，32条测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:08<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 206\n",
      "Train dataset size: 174\n",
      "Validation dataset size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 174/174 [00:00<00:00, 563.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 32/32 [00:00<00:00, 375.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 首先从每个数据集中取相同数目的数据出来\n",
    "all_training_data = []\n",
    "\n",
    "all_content_data = content_drop_1[:256]\n",
    "construct_sample(all_content_data, 3200, tokenizer, True, all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "\n",
    "# 从中随机抽取 32 条作为 validation 数据\n",
    "validation_size = 32\n",
    "\n",
    "# 打乱数据集索引并取前 32 条作为 validation\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造真实训练数据集\n",
    "1. 添加evidence，辅助定位隐式推理过程中的evidence位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 4580/7000 [03:36<01:54, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 4514/7000 [03:36<01:48, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 4520/7000 [03:36<01:58, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 4606/7000 [03:39<01:54, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3200\n",
      "Train dataset size: 9000\n",
      "Validation dataset size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (11/11 shards): 100%|██████████| 9000/9000 [00:16<00:00, 530.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 600/600 [00:01<00:00, 540.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 从每个子集里面sample 3200条数据再合并到一起\n",
    "random.shuffle(content_drop_1)\n",
    "random.shuffle(content_drop_2)\n",
    "random.shuffle(content_drop_3)\n",
    "\n",
    "manager = Manager()\n",
    "return_list = manager.list()\n",
    "\n",
    "# 创建进程\n",
    "process1 = multiprocessing.Process(target=construct_sample, args=(content_drop_1, 3200, tokenizer, True, return_list))\n",
    "process2 = multiprocessing.Process(target=construct_sample, args=(content_drop_2, 3200, tokenizer, True, return_list))\n",
    "process3 = multiprocessing.Process(target=construct_sample, args=(content_drop_3, 3200, tokenizer, True, return_list))\n",
    "\n",
    "# 启动进程\n",
    "process1.start()\n",
    "process2.start()\n",
    "process3.start()\n",
    "\n",
    "# 等待所有进程完成\n",
    "process1.join()\n",
    "process2.join()\n",
    "process3.join()\n",
    "\n",
    "all_training_data = list(return_list)\n",
    "random.shuffle(all_training_data)\n",
    "\n",
    "dataset = Dataset.from_list(all_training_data)\n",
    "validation_size = 600\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "validation_indices = indices[:validation_size]\n",
    "train_indices = indices[validation_size:]\n",
    "\n",
    "# 使用 Hugging Face 的 select 方法创建新的训练集和验证集\n",
    "train_dataset = dataset.select(train_indices)\n",
    "validation_dataset = dataset.select(validation_indices)\n",
    "\n",
    "# 将 train 和 validation 数据集保存到一个 dict 中\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# 打印数据集长度验证\n",
    "print(f\"Train dataset size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset_dict['validation'])}\")\n",
    "\n",
    "# 保存到本地\n",
    "dataset_dict.save_to_disk(\"/mnt/petrelfs/tangzecheng/local_data/processed_multi_hop/random_drop/train_llama_data/merge_v1_w_clues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
