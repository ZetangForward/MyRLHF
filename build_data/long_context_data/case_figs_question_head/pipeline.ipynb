{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modelzipper.tutils import *\n",
    "import datasets\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "import sys\n",
    "sys.path.append(\"/data/zecheng/acl2025/MyRLHF/inference\")\n",
    "from utils.babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "sys.path.append(\"/data/zecheng/acl2025/MyRLHF/build_data/long_context_data\")\n",
    "from pipeline_sg import preprocess_item\n",
    "sys.path.append(\"/data/zecheng/acl2025/MyRLHF/evaluation/babilong\")\n",
    "from eval import compare_answers, TASK_LABELS\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "model_name = '/data/zecheng/hf_models/Meta-Llama-3.1-8B-Instruct'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"balanced_low_0\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "logger.info('begin to load datasets')\n",
    "# with open(\"test_sample.pkl\", \"rb\") as f:\n",
    "#     test_case = pickle.load(f)\n",
    "# test_case = content[0][0]['content']  # DEBUG\n",
    "data = auto_read_data(\"/data/zecheng/Long-form-reasoning-data/data/generated_tasks/qa3/4k.json\")\n",
    "test_case = data[0]\n",
    "test_case['task'] = 'qa3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(data):\n",
    "    item['task'] = 'qa3'\n",
    "    golden_input_text, model_pred_text, golden_input_ids, pred_input_ids, natural_input_ids, golden_question_pos, pred_question_pos, golden_answer_pos, pred_answer_pos, golden_offset_mapping, pred_offset_mapping, reference_pos, natural_text_reference_pos, res = preprocess_item(item, model, tokenizer, model.device)\n",
    "    logger.info(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_input_text, model_pred_text, golden_input_ids, pred_input_ids, natural_input_ids, golden_question_pos, pred_question_pos, golden_answer_pos, pred_answer_pos, golden_offset_mapping, pred_offset_mapping, reference_pos, natural_text_reference_pos, res = preprocess_item(test_case, model, tokenizer, model.device)\n",
    "print(golden_input_ids.shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(loss, chunk_pos, anno_pos=None):\n",
    "    token_seq = []\n",
    "    id_dict = {}\n",
    "    cnt = 0\n",
    "    for start, end in chunk_pos:\n",
    "        token_seq.extend(range(start, end))\n",
    "        for value in range(start, end):\n",
    "            id_dict[value] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # 绘制整个 loss 序列\n",
    "    plt.plot(range(len(loss)), loss.numpy(), label='Loss per Token', color='blue')\n",
    "    \n",
    "    # 绘制每个区间的 loss，区间为 token_seq 中的索引\n",
    "    if anno_pos:\n",
    "        anno_re_ids = []\n",
    "        for start, end in anno_pos:\n",
    "            for i in range(start, end):\n",
    "                anno_re_ids.append(id_dict[i])\n",
    "        plt.plot(anno_re_ids, loss[anno_re_ids], marker='o', linestyle='--', color='red', label='Loss of Reference Chunks')\n",
    "\n",
    "    # 添加图形标签\n",
    "    plt.xlabel('Token Index')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Loss per Token with Continuous Chunk Highlighted')\n",
    "    plt.legend()\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without question intervention\n",
    "trunc_len=4096\n",
    "sliding_window=1024\n",
    "theta = 2.0\n",
    "expand_size = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss_f = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    output_full = model(golden_input_ids)\n",
    "    loss_overall = loss_f(output_full.logits[0, :-1, :], golden_input_ids[0, 1:]).to(torch.float).cpu().numpy()\n",
    "    ppl_full = np.exp(loss_overall.mean())\n",
    "\n",
    "    _, max_len = golden_input_ids.shape\n",
    "    key_tokens = []\n",
    "    chunk_score = dict()\n",
    "\n",
    "    chunk_num = int(np.ceil((max_len - trunc_len)) / sliding_window)\n",
    "    question_ipt_ids = golden_input_ids[:, golden_question_pos[0]: golden_question_pos[1]]\n",
    "    question_length = question_ipt_ids.size(1)\n",
    "\n",
    "    # testing inference with reference chunks\n",
    "    all_sub_chunks, referece_loss, chunk_pos, key_ref_pos = [], [], [], []\n",
    "    for ref_pos in reference_pos:\n",
    "        all_sub_chunks.append(golden_input_ids[:, ref_pos[0]: ref_pos[1]])\n",
    "        referece_loss.append(loss_overall[ref_pos[0]-expand_size: ref_pos[1]+expand_size])\n",
    "        chunk_pos.append((ref_pos[0]-expand_size, ref_pos[1]+expand_size))\n",
    "        key_ref_pos.append((ref_pos[0], ref_pos[1]))\n",
    "    \n",
    "    reference_input_ids = torch.cat(all_sub_chunks, dim=1)\n",
    "    loss_full = torch.tensor(np.concatenate(referece_loss, axis=0))[1:]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # combined_ref_ipt_ids = torch.cat([question_ipt_ids, reference_input_ids], dim=1)\n",
    "    output_ref = model(reference_input_ids)\n",
    "    loss_ref = loss_f(output_ref.logits[0, :-1, :], reference_input_ids[0, 1:]).to(torch.float).cpu()\n",
    "    # loss_ref = loss_f(output_ref.logits[0, question_length:-1, :], reference_input_ids[0, 1:]).to(torch.float).cpu()\n",
    "    viz(loss_ref, [(0, loss_ref.size(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with question intervention\n",
    "trunc_len=4096\n",
    "sliding_window=1024\n",
    "theta = 2.0\n",
    "expand_size = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss_f = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    output_full = model(golden_input_ids)\n",
    "    loss_overall = loss_f(output_full.logits[0, :-1, :], golden_input_ids[0, 1:]).to(torch.float).cpu().numpy()\n",
    "    ppl_full = np.exp(loss_overall.mean())\n",
    "\n",
    "    _, max_len = golden_input_ids.shape\n",
    "    key_tokens = []\n",
    "    chunk_score = dict()\n",
    "\n",
    "    chunk_num = int(np.ceil((max_len - trunc_len)) / sliding_window)\n",
    "    question_ipt_ids = golden_input_ids[:, golden_question_pos[0]: golden_question_pos[1]]\n",
    "    question_length = question_ipt_ids.size(1)\n",
    "\n",
    "    # testing inference with reference chunks\n",
    "    all_sub_chunks, referece_loss, chunk_pos, key_ref_pos = [], [], [], []\n",
    "    for ref_pos in reference_pos:\n",
    "        all_sub_chunks.append(golden_input_ids[:, ref_pos[0]: ref_pos[1]])\n",
    "        referece_loss.append(loss_overall[ref_pos[0]-expand_size: ref_pos[1]+expand_size])\n",
    "        chunk_pos.append((ref_pos[0]-expand_size, ref_pos[1]+expand_size))\n",
    "        key_ref_pos.append((ref_pos[0], ref_pos[1]))\n",
    "    \n",
    "    reference_input_ids = torch.cat(all_sub_chunks, dim=1)\n",
    "    loss_full = torch.tensor(np.concatenate(referece_loss, axis=0))[1:]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    combined_ref_ipt_ids = torch.cat([question_ipt_ids, reference_input_ids], dim=1)\n",
    "    output_ref = model(combined_ref_ipt_ids)\n",
    "\n",
    "    loss_ref = loss_f(output_ref.logits[0, question_length:-1, :], reference_input_ids[0, 1:]).to(torch.float).cpu()\n",
    "    viz(loss_full, chunk_pos, key_ref_pos)\n",
    "    # loss_discrepancy = (torch.logical_and(torch.abs(loss_ref - loss_full) > theta, loss_full < theta)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ref_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
