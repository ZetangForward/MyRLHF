[2024-10-24 17:33:31,368] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 17:33:37,015] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-10-24 17:33:37,019] [INFO] [runner.py:585:main] cmd = /public/home/zecheng/anaconda3/envs/zecheng/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None openrlhf/cli/train_sft_dev.py --max_len 64000 --dataset /data/zecheng/lcm_stack/dataset/training_data/Qwen_query_answer_gen --input_key instruction_str --output_key pred_str --train_batch_size 64 --micro_train_batch_size 2 --lora_rank 32 --apply_chat_template --pretrain /public/home/zecheng/workspace/hf_models/Meta-Llama-3.1-8B-Instruct --save_path /public/home/zecheng/workspace/zecheng/ckpt/acl2025/checkpoint/model/llama3.1-8b-tool-sft --ckpt_path /public/home/zecheng/workspace/zecheng/ckpt/acl2025//checkpoint/opt/llama3.1-8b-tool-sft --save_steps 50 --num_process 32 --logging_steps 1 --eval_steps -1 --zero_stage 3 --max_epochs 3 --packing_samples --bf16 --flash_attn --learning_rate 5e-6 --gradient_checkpointing --use_tensorboard /public/home/zecheng/workspace/zecheng/ckpt/acl2025/checkpoint/tensorboard/llama3.1-8b-tool-sft/tensorboard --disable_fast_tokenizer --ring_attn_size 2
[2024-10-24 17:33:39,431] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
