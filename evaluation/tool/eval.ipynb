{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from eval_loc import API_Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-22 12:33:29.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodelzipper.tutils\u001b[0m:\u001b[36mauto_read_data\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mbegin to read data from /mnt/petrelfs/tangzecheng/local_data/inference_results/llama-3_1-8B-Instruct/retrieval_then_gen/rapid_single_api/tool_calling/preds_rapid_single_api.jsonl | file size: 6.17 MB | file type: jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = auto_read_data('/mnt/petrelfs/tangzecheng/local_data/inference_results/llama-3_1-8B-Instruct/retrieval_then_gen/rapid_single_api/tool_calling/preds_rapid_single_api.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucket_id': '0',\n",
       " 'query': \"<QUERY> I'm looking for customer reviews of a specific product with ID 11577822456427762145. Can you provide me with all reviews from the US, written in English, with a rating of 4 or 5 out of 5? I'd like to get the first 20 reviews, but please include a way to scroll through more if there are additional reviews beyond that initial batch. </QUERY>\\n\",\n",
       " 'testing_setting': 'num_100_pool',\n",
       " 'dataset_name': 'rapid_single_api',\n",
       " 'call_parameters': [{'api_name': 'product_reviews',\n",
       "   'call_parameter': {'product_id': '11577822456427762145'}}],\n",
       " 'source_docs': '<TOOL_DOC>\\n<API_1>\\n>>> api_name: product_reviews\\n>>> api_description: Get all product reviews. Infinite pagination/scrolling is supported using the *limit* and *offset* parameters.\\n>>> param_properties:\\nproperty_name: product_id | description: Product id of the product for which to fetch reviews. | type: STRING | default_value: None\\nproperty_name: country | description: Country code of the region/country.\\nValid values: see https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2\\nDefault: `us`. | type: STRING | default_value: None\\nproperty_name: language | description: The language of the results.\\nValid values: see https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\\nDefault: `en`. | type: STRING | default_value: None\\nproperty_name: offset | description: Number of reviews to skip.\\nValid values: integers from 0-30000\\nDefault: `0`. | type: STRING | default_value: None\\nproperty_name: rating | description: Only return reviews with user rating greater than the specified value.\\nValid values: 1-5. | type: STRING | default_value: None\\nproperty_name: limit | description: Maximum number of product reviews to return.\\nValid values: integers from 0-100.\\nDefault: `10`. | type: STRING | default_value: None\\n>>> required_properties: product_id\\n</API_1>\\n</TOOL_DOC>',\n",
       " 'retrieval_res': '<API_1>\\n>>> api_name: product_reviews\\n>>> api_description: Get all product reviews. Infinite pagination/scrolling is supported using the *limit* and *offset* parameters.\\n>>> param_properties:\\nproperty_name: product_id | description: Product id of the product for which to fetch reviews. | type: STRING | default_value: None\\nproperty_name: country | description: Country code of the region/country. | type: STRING | default_value: None\\nproperty_name: language | description: The language of the results. | type: STRING | default_value: None\\nproperty_name: offset | description: Number of reviews to skip. | type: STRING | default_value: None\\nproperty_name: rating | description: Only return reviews with user rating greater than the specified value. | type: STRING | default_value: None\\nproperty_name: limit | description: Maximum number of product reviews to return. | type: STRING | default_value: None\\n>>> required_properties: product_id\\n</API_1>',\n",
       " 'second_query': 'You have already retrieved the relevant information source\\nBased on your retrieval source, please proceed with the tool calling while adhering to the following requirements:\\nFollowing your planning, construct your answer and enclose it within <ANSWER></ANSWER>.\\nIn implementing each API call, adhere to the following guidelines:\\n1. Correctly use the API ID, formatted as <API_{{id}}></API_{{id}}>.\\n2. Enclose each parameter and its value separately using <PARAM>{{param}}</PARAM> and <VALUE>{{value}}</VALUE>.\\n3. If the same API is called multiple times with different parameters, repeat the use of <API_{{id}}></API_{{id}}> for each instance, followed by the specific parameters for that call.\\n',\n",
       " 'pred': ['<ANSWER>\\n\\nTo get the customer reviews of the product with ID 11577822456427762145, we will use the <API_1></API_1> endpoint with the following parameters:\\n\\n<API_1>\\n<PARAM>product_id</PARAM><VALUE>11577822456427762145</VALUE>\\n<PARAM>country</PARAM><VALUE>us</VALUE>\\n<PARAM>language</PARAM><VALUE>en</VALUE>\\n<PARAM>rating</PARAM><VALUE>4</VALUE>\\n<PARAM>limit</PARAM><VALUE>20</VALUE>\\n</API_1>\\n\\nThis will return the first 20 reviews from the US, written in English, with a rating of 4 or 5 out of 5.\\n\\nTo scroll through more reviews, we can use the <API_1></API_1> endpoint with the offset parameter, which allows us to skip a certain number of reviews and retrieve the next batch.\\n\\nFor example, to get the next 20 reviews, we can call the endpoint with the offset parameter set to 20:\\n\\n<API_1>\\n<PARAM>product_id</PARAM><VALUE>11577822456427762145</VALUE>\\n<PARAM>country</PARAM><VALUE>us</VALUE>\\n<PARAM>language</PARAM><VALUE>en</VALUE>\\n<PARAM>rating</PARAM><VALUE>4</VALUE>\\n<PARAM>limit</PARAM><VALUE>20</VALUE>\\n<PARAM>offset</PARAM><VALUE>20</VALUE>\\n</API_1>\\n\\nWe can repeat this process, incrementing the offset parameter by 20 each time, to retrieve the next batch of reviews.\\n\\nNote that the maximum offset value is 30000, so we can retrieve up to 30000 reviews in total using this method.\\n\\nAlternatively, we can use the <API_1></API_1> endpoint with the limit parameter set to a higher value, such as 50 or 100, to retrieve more reviews in a single call.\\n\\nFor example:\\n\\n<API_1>\\n<PARAM>product_id</PARAM><VALUE>11577822456427762145</VALUE>\\n<PARAM>country</PARAM><VALUE>us</VALUE>\\n<PARAM>language</PARAM><VALUE>en</VALUE>\\n<PARAM>rating</PARAM><VALUE>4</VALUE>\\n<PARAM>limit</PARAM><VALUE>50</VALUE>\\n</API_1>\\n\\nThis will return the first 50 reviews from the US, written in English, with']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评测 long-context tool 的生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tool_subdirs = auto_read_dir(\"/mnt/petrelfs/tangzecheng/local_data/inference_results/llama-3_1-8B-Instruct\", file_suffix=\"api\")\n",
    "# all_tool_subdirs = auto_read_dir(\"/mnt/petrelfs/tangzecheng/local_data/inference_results/Qwen-2-5-7b-instruct\", file_suffix=\"api\")\n",
    "tool_location_subdirs = [auto_read_dir(subdir, file_suffix=\"location\")[0] for subdir in all_tool_subdirs]\n",
    "tool_location_files = [auto_read_dir(subdir, file_suffix=\"jsonl\")[0] for subdir in tool_location_subdirs]\n",
    "\n",
    "logger.info(tool_location_files)\n",
    "\n",
    "all_content = dict([(os.path.basename(file).split('.')[0], auto_read_data(file)) for file in tool_location_files])\n",
    "all_res = {}\n",
    "retrieval_res = {}\n",
    "\n",
    "for task, content in all_content.items():\n",
    "    logger.info(f\"task: {task} | length content: {len(content)}\")\n",
    "    api_processor = API_Evaluator(content, task)\n",
    "    eval_res = api_processor.eval_api_res()\n",
    "    retrieval_res[task] = api_processor.get_retrieval_res()[\"predictions\"]\n",
    "    all_res[task] = eval_res\n",
    "    \n",
    "print(all_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tool_subdirs = auto_read_dir(\"/mnt/petrelfs/tangzecheng/local_data/inference_results/llama-3_1-8B-Instruct\", file_suffix=\"api\")\n",
    "# all_tool_subdirs = auto_read_dir(\"/mnt/petrelfs/tangzecheng/local_data/inference_results/Qwen-2-5-7b-instruct\", file_suffix=\"api\")\n",
    "tool_location_subdirs = [auto_read_dir(subdir, file_suffix=\"location\")[0] for subdir in all_tool_subdirs]\n",
    "tool_location_files = [auto_read_dir(subdir, file_suffix=\"jsonl\")[0] for subdir in tool_location_subdirs]\n",
    "\n",
    "logger.info(tool_location_files)\n",
    "\n",
    "all_content = dict([(os.path.basename(file).split('.')[0], auto_read_data(file)) for file in tool_location_files])\n",
    "all_res = {}\n",
    "\n",
    "for task, content in all_content.items():\n",
    "    logger.info(f\"task: {task} | length content: {len(content)}\")\n",
    "    api_processor = API_Evaluator(content, task)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tool_subdirs = auto_read_dir(\"/mnt/petrelfs/tangzecheng/local_data/inference_results/llama-3_1-8B-Instruct\", file_suffix=\"api\")\n",
    "tool_calling_subdirs = [auto_read_dir(subdir, file_suffix=\"location\")[0] for subdir in all_tool_subdirs]\n",
    "tool_calling_files = [auto_read_dir(subdir, file_suffix=\"jsonl\")[0] for subdir in tool_calling_subdirs]\n",
    "\n",
    "logger.info(tool_calling_files)\n",
    "all_content = dict([(os.path.basename(file).split('.')[0], auto_read_data(file)) for file in tool_calling_files])\n",
    "\n",
    "# for task, content in all_content.items():\n",
    "#     api_evalator = API_Evaluator(content, task)\n",
    "#     eval_res = api_evalator.eval_api_res()\n",
    "#     logger.info(f\"task: {task}\\n{eval_res}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(true, pred):\n",
    "    true_set, pred_set = set(true), set(pred)\n",
    "    tp = len(true_set & pred_set)\n",
    "    fp = len(pred_set - true_set)\n",
    "    fn = len(true_set - pred_set)\n",
    "    \n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    if tp + fn > 0:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "        \n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_model_output(model_results, golden_results):\n",
    "    api_name_recall = 0\n",
    "    api_id_correct = 0\n",
    "    param_name_precision = 0\n",
    "    param_name_recall = 0\n",
    "    param_name_f1 = 0\n",
    "    value_precision = 0\n",
    "    value_recall = 0\n",
    "    value_f1 = 0\n",
    "    num_samples = len(golden_results)\n",
    "    \n",
    "    for model, golden in zip(model_results, golden_results):\n",
    "        # Check API Name Recall\n",
    "        if model['api_name'] in [g['api_name'] for g in golden_results]:\n",
    "            api_name_recall += 1\n",
    "\n",
    "        # Check API ID Accuracy\n",
    "        api_id_correct += int(int(model['api_id']) == golden['api_id'])\n",
    "\n",
    "        # Check call_parameter names\n",
    "        model_param_names = [list(param.keys())[0] for param in model['call_parameter']]\n",
    "        golden_param_names = list(golden['call_parameter'].keys())\n",
    "        \n",
    "        p_precision, p_recall, p_f1 = calculate_f1(golden_param_names, model_param_names)\n",
    "        param_name_precision += p_precision\n",
    "        param_name_recall += p_recall\n",
    "        param_name_f1 += p_f1\n",
    "\n",
    "        # Check call_parameter values\n",
    "        model_param_values = [list(param.values())[0] for param in model['call_parameter']]\n",
    "        golden_param_values = list(golden['call_parameter'].values())\n",
    "        \n",
    "        v_precision, v_recall, v_f1 = calculate_f1(golden_param_values, model_param_values)\n",
    "        value_precision += v_precision\n",
    "        value_recall += v_recall\n",
    "        value_f1 += v_f1\n",
    "\n",
    "    # Calculate averages\n",
    "    recall_average = api_name_recall / num_samples\n",
    "    api_id_accuracy = api_id_correct / num_samples\n",
    "    param_name_precision_average = param_name_precision / num_samples\n",
    "    param_name_recall_average = param_name_recall / num_samples\n",
    "    param_name_f1_average = param_name_f1 / num_samples\n",
    "    value_precision_average = value_precision / num_samples\n",
    "    value_recall_average = value_recall / num_samples\n",
    "    value_f1_average = value_f1 / num_samples\n",
    "\n",
    "    return {\n",
    "        \"api_name_recall\": recall_average,\n",
    "        \"api_id_accuracy\": api_id_accuracy,\n",
    "        \"param_name_precision\": param_name_precision_average,\n",
    "        \"param_name_recall\": param_name_recall_average,\n",
    "        \"param_name_f1\": param_name_f1_average,\n",
    "        \"value_precision\": value_precision_average,\n",
    "        \"value_recall\": value_recall_average,\n",
    "        \"value_f1\": value_f1_average\n",
    "    }\n",
    "\n",
    "def evaluate_all_samples(all_model_results, all_golden_results):\n",
    "    total_api_name_recall = 0\n",
    "    total_api_id_accuracy = 0\n",
    "    total_param_name_precision = 0\n",
    "    total_param_name_recall = 0\n",
    "    total_param_name_f1 = 0\n",
    "    total_value_precision = 0\n",
    "    total_value_recall = 0\n",
    "    total_value_f1 = 0\n",
    "    num_samples = len(all_golden_results)\n",
    "\n",
    "    for model_results, golden_results in zip(all_model_results, all_golden_results):\n",
    "        # Evaluate each sample\n",
    "        pred_results = model_results['pred_param']\n",
    "        evaluation_result = evaluate_model_output(pred_results, golden_results)\n",
    "\n",
    "        # Accumulate results\n",
    "        total_api_name_recall += evaluation_result[\"api_name_recall\"]\n",
    "        total_api_id_accuracy += evaluation_result[\"api_id_accuracy\"]\n",
    "        total_param_name_precision += evaluation_result[\"param_name_precision\"]\n",
    "        total_param_name_recall += evaluation_result[\"param_name_recall\"]\n",
    "        total_param_name_f1 += evaluation_result[\"param_name_f1\"]\n",
    "        total_value_precision += evaluation_result[\"value_precision\"]\n",
    "        total_value_recall += evaluation_result[\"value_recall\"]\n",
    "        total_value_f1 += evaluation_result[\"value_f1\"]\n",
    "\n",
    "    # Calculate averages\n",
    "    overall_results = {\n",
    "        \"api_name_recall\": total_api_name_recall / num_samples,\n",
    "        \"api_id_accuracy\": total_api_id_accuracy / num_samples,\n",
    "        \"param_name_precision\": total_param_name_precision / num_samples,\n",
    "        \"param_name_recall\": total_param_name_recall / num_samples,\n",
    "        \"param_name_f1\": total_param_name_f1 / num_samples,\n",
    "        \"value_precision\": total_value_precision / num_samples,\n",
    "        \"value_recall\": total_value_recall / num_samples,\n",
    "        \"value_f1\": total_value_f1 / num_samples\n",
    "    }\n",
    "\n",
    "    return overall_results\n",
    "\n",
    "# Example usage\n",
    "print(\"test simgle sample generated result\")\n",
    "\n",
    "model_results = {\n",
    "    'plan': None,\n",
    "    'pred_param': [\n",
    "        {'api_id': '0', 'api_name': 'patreon', 'call_parameter': [{'username': 'username'}]},\n",
    "        {'api_id': '1', 'api_name': 'minecraft', 'call_parameter': [{'username': 'username'}]}\n",
    "    ]\n",
    "}\n",
    "    \n",
    "golden_results = [\n",
    "    {'api_name': 'patreon', 'call_parameter': {'username': 'username'}, 'api_id': 0},\n",
    "    {'api_name': 'minecraft', 'call_parameter': {'username': 'username'}, 'api_id': 1}\n",
    "]\n",
    "\n",
    "evaluation_result = evaluate_model_output(model_results['pred_param'], golden_results)\n",
    "print(evaluation_result)\n",
    "\n",
    "print(\"test all sample generated results\")\n",
    "\n",
    "# Example usage with multiple samples\n",
    "all_model_results = [\n",
    "    model_results, \n",
    "    model_results\n",
    "]\n",
    "\n",
    "all_golden_results = [\n",
    "    [{'api_name': 'patreon', 'call_parameter': {'username': 'username'}, 'api_id': 0},\n",
    "     {'api_name': 'minecraft', 'call_parameter': {'username': 'username'}, 'api_id': 1}],\n",
    "    # Add corresponding golden samples here\n",
    "]\n",
    "\n",
    "overall_evaluation_result = evaluate_all_samples(all_model_results, all_golden_results)\n",
    "print(overall_evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(s):\n",
    "    try:\n",
    "        # Extracting ANSWER section\n",
    "        plan_match = re.search(r\"<PLAN>(.*?)</PLAN>\", s, re.DOTALL)\n",
    "        answer_match = re.search(r\"<ANSWER>(.*?)</ANSWER>\", s, re.DOTALL)\n",
    "\n",
    "        if plan_match is None or answer_match is None:\n",
    "            return None\n",
    "\n",
    "        plan = plan_match.group(1).strip().split('\\n')\n",
    "        answer_content = answer_match.group(1)\n",
    "\n",
    "        # Extract API blocks using a pattern that includes parameters\n",
    "        api_blocks_pattern = r\"(<API_\\d+>.*?</API_\\d+>.*?)(?=<API_\\d+>|$)\"\n",
    "        api_blocks = re.findall(api_blocks_pattern, answer_content, re.DOTALL)\n",
    "\n",
    "        # Define patterns for extracting API name and parameters\n",
    "        api_id_name_pattern = r\"<API_(\\d+)>\\s*(.*?)\\s*</API_\\d+>\"\n",
    "        param_value_pattern = r\"<PARAM>\\s*(.*?)\\s*</PARAM>\\s*<VALUE>\\s*(.*?)\\s*</VALUE>\"\n",
    "\n",
    "        result = []\n",
    "        for api_block in api_blocks:\n",
    "            # Extract API id and name\n",
    "            api_id_name = re.search(api_id_name_pattern, api_block)\n",
    "            if api_id_name:\n",
    "                api_id, api_name = api_id_name.groups()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Extract parameters and values associated with this API block\n",
    "            params_values = re.findall(param_value_pattern, api_block)\n",
    "            call_parameters = [{param: value} for param, value in params_values]\n",
    "\n",
    "            result.append({\n",
    "                \"api_id\": api_id,\n",
    "                \"api_name\": api_name,\n",
    "                \"call_parameter\": call_parameters\n",
    "            })\n",
    "\n",
    "        # Creating the final JSON structure\n",
    "        return {\n",
    "            \"plan\": plan,\n",
    "            \"pred_param\": result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple_api\n",
    "rapid_multiple_api = \"/data/zecheng/lcm_stack/dataset/processed_dataset/rapid_multiple_api/rapid_multiple_api.json\"\n",
    "\n",
    "with open(rapid_multiple_api, \"r\") as f:\n",
    "    rapid_multiple_api = json.load(f)\n",
    "\n",
    "preds_rapid_multiple_api = auto_read_data(\"/data/zecheng/lcm_stack/dataset/inference_results/llama-3_1-8B-Instruct/sft_stage_1/preds_rapid_multiple_api.jsonl\")\n",
    "num_buckets = len(set([item['bucket_id'] for item in preds_rapid_multiple_api]))\n",
    "bucket_pred_rapid_multiple_api = dict([(f'bucket_{i}', []) for i in range(num_buckets)])\n",
    "\n",
    "for item in preds_rapid_multiple_api:\n",
    "    bucket_pred_rapid_multiple_api[item[\"bucket_id\"]].append(item)\n",
    "\n",
    "total_f1_scores = {'api_name': [], 'param_name': [], 'param_value': []}\n",
    "total_avg_f1_scores = {'api_name': 0, 'param_name': 0, 'param_value': 0}\n",
    "total_cases, num_calculate_cases = 0, 0\n",
    "overall_success = True  # Track overall success across all cases\n",
    "\n",
    "success_query, failed_query = [], []\n",
    "all_model_preds, all_golden_results = [], []\n",
    "next_round_inputs_collector = []\n",
    "for bucket_id in bucket_pred_rapid_multiple_api:\n",
    "    for cnt, output in enumerate(bucket_pred_rapid_multiple_api[bucket_id]):\n",
    "        golden_query = rapid_multiple_api[bucket_id]['query'][cnt]\n",
    "        golden_parameter = rapid_multiple_api[bucket_id]['call_parameters'][cnt]\n",
    "        api_index_dict = rapid_multiple_api[bucket_id]['api_index_dict']\n",
    "        for tmp in golden_parameter:\n",
    "            golden_api_id = api_index_dict[tmp['api_name']]\n",
    "            tmp['api_id'] = golden_api_id\n",
    "        \n",
    "        pred, query = output['pred'], output['query']\n",
    "        total_cases += 1\n",
    "        if golden_query == query:\n",
    "            num_calculate_cases += 1\n",
    "            model_preds = extract_parameters(pred)\n",
    "            if model_preds is None:\n",
    "                model_preds = {'plan': [], 'pred_param': []}  # Failure to extract parameters\n",
    "            \n",
    "            all_model_preds.append(model_preds)\n",
    "            all_golden_results.append(golden_parameter)\n",
    "\n",
    "all_res = evaluate_all_samples(all_model_preds, all_golden_results)\n",
    "pprint(all_res)\n",
    "print(f\"Total cases: {total_cases}, Num calculated cases: {num_calculate_cases}\")\n",
    "print(f\"Num collected next round input: {len(next_round_inputs_collector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "res = auto_read_data(\"/mnt/petrelfs/tangzecheng/local_data/first_retrieval_res/rapid_multiple_api/tool_calling/num_200_pool.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(res['1'].keys())\n",
    "# print(res['1']['retrieval_res'][0])\n",
    "pprint(res['1']['system_prompt'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
